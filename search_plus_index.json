{"./":{"url":"./","title":"简介","keywords":"","body":"Bai-Yingjie.github.io 个人笔记汇总 https://bai-yingjie.github.io/大部分笔记只是流水账, 毫无章法.我的习惯是越新的内容越在上面, 需要看的话建议倒叙阅读.有部分笔记跨越了好几年, 所以有时候底下的内容看起来略简单. "},"notes/as_title_kaiyuan.html":{"url":"notes/as_title_kaiyuan.html","title":"开源","keywords":"","body":"如题 "},"notes/my_opensource.html":{"url":"notes/my_opensource.html","title":"我的开源项目","keywords":"","body":" adaptiveservice hello例子 client端 server端 gshell topid adaptiveservice godevsig/adaptiveservice是用golang实现的一个简单的服务发现框架. 服务端发布服务, 客户端发现服务. 服务使用{\"publisher\", \"service\"}来标识. publisher类似github repo中的组织名或用户名, 标识这个服务的代码拥有者. service就是服务的名字, 类似github repo的repo名. 在一个服务网络中, 可以有同名服务的多个实例, 对客户端来说, 无论哪个实例, 都提供一样的服务. 客户端可以根据策略, 比如随机策略, 或者延迟最小策略, 选择其中一个实例来通信. 支持root registry和局域网广播发现. 消息的交互只有三个API. Send(msg)msg一般定义于服务端, 是golang的struct, 表示服务端可以处理哪些struct. client import后就可以直接使用. Recv(msgPtr)msgPtr是要接收的msg的指针. 在golang中, 由于gc, 并不需要提前申请内存. SendRecv(msg, msgPtr)这个形式的API可以当作RPC来使用. server端的消息处理使用弹性伸缩的worker pool, 自动支持并发. client端支持多路复用, 提高并发. 支持在同一个进程内使用, 支持在同一个系统内使用, API都一样. 内置反向代理, 从而NAT内的服务也能被外部发现并连接. 参考: README 使用说明 hello例子 client端 client要先发现并连接到服务, 然后就可以收发消息. package main import ( \"fmt\" as \"github.com/godevsig/adaptiveservice\" msg \"github.com/godevsig/adaptiveservice/examples/hello/message\" ) func main() { c := as.NewClient() conn := server端 server端需要定义可以处理的struct, 以及其handler. 在这个struct到达server端的时候, 框架在worker pool里分配worker, handler被框架在worker中调用. message.go中定义消息struct如下: package message import ( \"strings\" as \"github.com/godevsig/adaptiveservice\" ) // HelloRequest is the request from clients type HelloRequest struct { Who string Question string } // HelloReply is the reply of HelloRequest to clients type HelloReply struct { Answer string } // Handle handles msg. func (msg HelloRequest) Handle(stream as.ContextStream) (reply interface{}) { answer := \"I don't know\" question := strings.ToLower(msg.Question) switch { case strings.Contains(question, \"who are you\"): answer = \"I am hello server\" case strings.Contains(question, \"how are you\"): answer = \"I am good\" } return HelloReply{answer + \", \" + msg.Who} } func init() { as.RegisterType(HelloRequest{}) as.RegisterType(HelloReply{}) } helloserver.go中启动service: package main import ( \"fmt\" as \"github.com/godevsig/adaptiveservice\" msg \"github.com/godevsig/adaptiveservice/examples/hello/message\" ) func main() { s := as.NewServer().SetPublisher(\"example\") knownMsgs := []as.KnownMessage{msg.HelloRequest{}} if err := s.Publish(\"hello\", knownMsgs); err != nil { fmt.Println(err) return } if err := s.Serve(); err != nil { // ctrl+c to exit fmt.Println(err) } } gshell godevsig/gshellos是基于adaptive service的一个go服务编排的框架. gshellos使用了go解释器yaegi, 使gshell框架得以在任意gshell节点运行位于中央仓库grepo下的go代码. 文档: README topid topid是基于gshell的一个app, 代码位于grepo, 可以采集linux的进程信息, 并配合topid chart服务, 可以在web上实时显示CPU和MEM的占用. 得益于gshell, topid很方便在各个CPU arch下面运行, 没有运行时依赖. topid usage topid chart usage "},"notes/my_upstream.html":{"url":"notes/my_upstream.html","title":"我的upstream commit","keywords":"","body":" kernel buildroot kernel https://lore.kernel.org/patchwork/patch/1173547https://lore.kernel.org/patchwork/patch/1173548https://lore.kernel.org/patchwork/patch/1158715 buildroot https://git.busybox.net/buildroot/commit/?id=bacf2157193c1565f37c260685a03426a1be5656 "},"notes/as_title_system.html":{"url":"notes/as_title_system.html","title":"系统分析和性能","keywords":"","body":"如题 "},"notes/as_title_perf_misc.html":{"url":"notes/as_title_perf_misc.html","title":"调试和分析记录系列","keywords":"","body":"如题 "},"notes/profiling_调试和分析记录5.html":{"url":"notes/profiling_调试和分析记录5.html","title":"调试和分析记录5","keywords":"","body":" ftrace调查ENOSYS问题 背景 问题现象 调查 narrow down 真的没有这个系统调用吗? clock_gettime调用的到底是什么? ftrace登场 谁调用了目标函数? 目标函数又调用了什么其他函数? __ia32_sys_clock_gettime32 返回ENOSYS? 还有哪些函数可能有问题 解决 补充知识 vdso 系统调用流程 ftrace调查ENOSYS问题 背景 x86_64平台, cloud hypervisor启动VM, guest kernel版本是5.15, 64bit模式, 打开了CONFIG_IA32_EMULATION=y rootfs是其他系统编译出来的, 32bit模式, 基于busybox. 问题现象 kernel启动后进入rootfs的/init->/sbin/init, 启动失败, 进入sh循环. date命令显示系统时间不对, strace date发现如下错误: clock_gettime(CLOCK_REALTIME, 0xffaf59ec) = -1 ENOSYS (Function not implemented) 调查 narrow down 在有些系统中, strace date没有clock_gettime系统调用, 也能正确获取时间, 是什么原理?答: kernel给每个进程都自动插入了vdso, 用来加速clock_gettime getimeofday等系统调用. 在使用vdso的情况下, 不会触发系统调用, 但也会调用相应的内核函数.这里先假设由于某种原因, vdso机制没有生效, 因为我们这里的情况是, strace抓到了clock_gettime系统调用.vdso的知识详见下面的补充知识小节. sh能运行, 说明32bit的程序可以执行. 但为什么date不行?答: 经过实验, 发现似乎和时间相关的系统调用都不对, 比如mkdir dddd是成功的, 但touch asdf文件会失败, 错误也是ENOSYSutimensat(AT_FDCWD, \"asdf\", [UTIME_NOW, UTIME_NOW], 0) = -1 ENOSYS (Function not implemented) 真的没有这个系统调用吗? clock_gettime调用的到底是什么? 到这里就要用到ftrace了, 再复习一下, ftrace能回答两类关键的调用路径问题: 某个内核函数的执行过程中, 调用了哪些函数? 某个内核函数被什么路径调用的? 这里的第一个关键点在于, 我们到底是在调查哪个内核函数? 我们知道clock_gettime是C库角度看到的系统调用, 那内核对应的是哪个函数? 这里可以有两个办法: 使用原始的vmlinux, 用nm或者objdump等工具解析符号 用/proc/kallsyms 我们这里使用后者, 通常的kernel都会打开kallsyms选项: # cat /proc/kallsyms | grep clock_gettime ffffffff811823f0 W __x64_sys_clock_gettime32 ffffffff81182410 W __ia32_sys_clock_gettime32 ffffffff811e60c0 T __x64_sys_clock_gettime ffffffff811e6180 T __ia32_sys_clock_gettime ffffffff811e8720 t pc_clock_gettime ffffffff81883370 t ptp_clock_gettime ffffffff81885ca0 t ptp_vclock_gettime 再看看utimensat # cat /proc/kallsyms | grep utimensat ffffffff81182630 W __x64_sys_utimensat_time32 ffffffff81182650 W __ia32_sys_utimensat_time32 ffffffff813b5a40 T __x64_sys_utimensat ffffffff813b5b00 T __ia32_sys_utimensat 注: 符号表的W表示后面的符号是个weak函数. 详见man nm ftrace登场 现在我们知道了要观察的函数, 下面就用ftrace回答这两个问题: 谁调用了目标函数? 具体用法这里就不详细解释了, 直接给出命令: # 因为我是在init非常初始阶段, 需要手动mount proc和sysfs /bin/busybox mount -t proc none /proc /bin/busybox mount -t sysfs none /sys cd /sys/kernel/debug/tracing # 把目标函数填入ftrace_filter, ftrace自己就会解析通配符 echo *clock_gettime* > set_ftrace_filter # 打开function tracer echo function > current_tracer # 打开stack trace功能 echo 1 > options/func_stack_trace # 执行目标命令触发目标函数调用 echo 1 > tracing_on /bin/busybox date /bin/busybox.alpine date echo 0 > tracing_on 为了比较, 我执行了64位的date程序(/bin/busybox.alpine date时间是正确的), 结果如下: busybox-250 [000] ..... 21201.311547: __ia32_sys_clock_gettime32 => __ia32_sys_clock_gettime32 => __do_fast_syscall_32 => do_fast_syscall_32 => do_SYSENTER_32 => entry_SYSENTER_compat_after_hwframe busybox.alpine-254 [000] ..... 21671.440194: __x64_sys_clock_gettime => __x64_sys_clock_gettime => do_syscall_64 => entry_SYSCALL_64_after_hwframe 目标函数又调用了什么其他函数? cd /sys/kernel/debug/tracing echo 0 > tracing_on echo *clock_gettime* > set_graph_function echo function_graph > current_tracer echo 1 > tracing_on /bin/busybox date /bin/busybox.alpine date echo 0 > tracing_on cat trace 结果: /sys/kernel/debug/tracing # cat trace # tracer: function_graph # # CPU DURATION FUNCTION CALLS # | | | | | | | 0) + 27.124 us | __ia32_sys_clock_gettime32(); 0) 0.305 us | __ia32_sys_clock_gettime32(); ------------------------------------------ 0) busybox-207 => busybox-210 ------------------------------------------ 0) | __x64_sys_clock_gettime() { 0) | posix_get_realtime_timespec() { 0) 0.556 us | ktime_get_real_ts64(); 0) 1.819 us | } 0) 0.934 us | put_timespec64(); 0) + 11.616 us | } __ia32_sys_clock_gettime32 返回ENOSYS? 现在看下来, __ia32_sys_clock_gettime32这个函数很可能返回ENOSYS. 对vmlinux做objdump, 能看到__ia32_sys_clock_gettime32以及前后几个类似的函数, 汇编都很短, 似乎不是正常的实现: 从include/linux/syscalls.h看到 追到kernel/time/posix-timers.c注意到这里有个编译开关: CONFIG_COMPAT_32BIT_TIME 还有哪些函数可能有问题 以__ia32开头, 以32结尾的Weak函数: /sys/kernel/debug/tracing # cat /proc/kallsyms | grep __ia32 | grep \"32$\" ffffffff8117ee90 W __ia32_sys_io_getevents_time32 ffffffff8117ef10 W __ia32_sys_io_pgetevents_time32 ffffffff8117ef70 W __ia32_compat_sys_io_pgetevents_time32 ffffffff8117f610 W __ia32_sys_timerfd_settime32 ffffffff8117f690 W __ia32_sys_timerfd_gettime32 ffffffff8117f810 W __ia32_sys_futex_time32 ffffffff8117fe90 W __ia32_sys_mq_timedsend_time32 ffffffff8117ff10 W __ia32_sys_mq_timedreceive_time32 ffffffff81180410 W __ia32_sys_semtimedop_time32 ffffffff81181310 W __ia32_sys_recvmmsg_time32 ffffffff81181330 W __ia32_compat_sys_recvmmsg_time32 ffffffff81182150 W __ia32_sys_time32 ffffffff81182190 W __ia32_sys_stime32 ffffffff811821d0 W __ia32_sys_utime32 ffffffff81182210 W __ia32_sys_adjtimex_time32 ffffffff81182250 W __ia32_sys_sched_rr_get_interval_time32 ffffffff81182290 W __ia32_sys_nanosleep_time32 ffffffff811822d0 W __ia32_sys_rt_sigtimedwait_time32 ffffffff811822f0 W __ia32_compat_sys_rt_sigtimedwait_time32 ffffffff81182350 W __ia32_sys_timer_settime32 ffffffff81182390 W __ia32_sys_timer_gettime32 ffffffff811823d0 W __ia32_sys_clock_settime32 ffffffff81182410 W __ia32_sys_clock_gettime32 ffffffff81182450 W __ia32_sys_clock_getres_time32 ffffffff81182490 W __ia32_sys_clock_nanosleep_time32 ffffffff811824d0 W __ia32_sys_utimes_time32 ffffffff81182510 W __ia32_sys_futimesat_time32 ffffffff81182550 W __ia32_sys_pselect6_time32 ffffffff81182570 W __ia32_compat_sys_pselect6_time32 ffffffff811825d0 W __ia32_sys_ppoll_time32 ffffffff811825f0 W __ia32_compat_sys_ppoll_time32 ffffffff81182650 W __ia32_sys_utimensat_time32 ffffffff81182690 W __ia32_sys_clock_adjtime32 解决 在menuconfig里打开CONFIG_COMPAT_32BIT_TIME, 问题解决 补充知识 vdso vdso是virtual ELF dynamic shared object, 一般只有libc等非常基础的库才会和vdso打交道. man vdso里面摘要如下: 使用时#include void *vdso = (uintptr_t) getauxval(AT_SYSINFO_EHDR); vdso是内核提供的一个小的共享库, 被内核自动map到每个进程的地址空间中. vdso的出现是为了解决频繁使用系统调用的性能问题, 典型的场景是log里面为了打印时间戳而频繁调用clock_gettime. 这种场景下, 使用vdso能避免用户态和内核态的上下文切换的开销. 比如32bit x86, 软件通过int $0x80软中断来触发系统调用, 但这个指令很expensive, 它会走完成的中断处理流程(部分发生在CPU的microcode, 部分发生在kernel) 新的处理器有更快更专门的系统调用指令, vdso就提供了这样的封装, 使得libc不用自己去搞清楚该用哪个指令, vdso会自动处理好. 比如在vdso的作用下 Now a call to gettimeofday(2) changes from a system call to a normal function call and a few memory accesses. getauxval这个API可以获取vdso的基地址, 这个地址上是个完整的ELF格式的文件. C库可以在第一次打开的时候查询里面的符号表, 找到可用的功能并缓存, 后面就不用再查询了. 函数符号的命名遵循惯例, 比如gettimeofday系统调用就对应vdso的__vdso_gettimeofday, 或者带__kernel前缀的. 用ldd可以看到vdso, 比如i386的linux-gate.so.1, 或者x86_64的linux-vdso.so.1 strace抓不到vdso的\"系统调用\" 系统调用流程 从调用过程来看, 32bit app的syscall路径是: /* * 32-bit SYSENTER entry. * * 32-bit system calls through the vDSO's __kernel_vsyscall enter here * on 64-bit kernels running on Intel CPUs. * * The SYSENTER instruction, in principle, should *only* occur in the * vDSO. In practice, a small number of Android devices were shipped * with a copy of Bionic that inlined a SYSENTER instruction. This * never happened in any of Google's Bionic versions -- it only happened * in a narrow range of Intel-provided versions. * * SYSENTER loads SS, RSP, CS, and RIP from previously programmed MSRs. * IF and VM in RFLAGS are cleared (IOW: interrupts are off). * SYSENTER does not save anything on the stack, * and does not save old RIP (!!!), RSP, or RFLAGS. * * Arguments: * eax system call number * ebx arg1 * ecx arg2 * edx arg3 * esi arg4 * edi arg5 * ebp user stack * 0(%ebp) arg6 */ //32bit的vdso调用从这里入口. vdso使用SYSENTER指令. entry_SYSENTER_compat_after_hwframe @ arch/x86/entry/entry_64_compat.S do_SYSENTER_32 @ arch/x86/entry/common.c do_fast_syscall_32 __do_fast_syscall_32 //nr就是syscall的number int nr = syscall_32_enter(regs); unsigned int unr = nr; unr = array_index_nospec(unr, IA32_NR_syscalls); //查表syscall table, 得到具体处理函数并调用 regs->ax = ia32_sys_call_table[unr](regs); __ia32_sys_clock_gettime32 上面用到了syscall table, 在: @ arch/x86/entry/syscall_32.c #ifdef CONFIG_IA32_EMULATION #define __SYSCALL_WITH_COMPAT(nr, native, compat) __SYSCALL(nr, compat) #else #define __SYSCALL_WITH_COMPAT(nr, native, compat) __SYSCALL(nr, native) #endif #define __SYSCALL(nr, sym) extern long __ia32_##sym(const struct pt_regs *); #include #undef __SYSCALL #define __SYSCALL(nr, sym) __ia32_##sym, __visible const sys_call_ptr_t ia32_sys_call_table[] = { #include }; 做为对比, 64bit的syscall路径是: entry_SYSCALL_64_after_hwframe do_syscall_64 __x64_sys_clock_gettime "},"notes/profiling_调试和分析记录4.html":{"url":"notes/profiling_调试和分析记录4.html","title":"调试和分析记录4","keywords":"","body":" 分离符号表 使用strace分析eoe_filter占用高的问题 统计 使用其他版本的libc 背景 直接使用是报错的 解决 LD_LIBRARY_PATH LD_PRELOAD 最终解决 没有ldd看共享库 strace查看一个进程的open文件过程 计算CPU load utime和cutime的关系 /proc/stat 上述所有相的和是CPU总的tick数 进程cpud利用率的计算 strace看系统调用时间 I2C访问时间过长 在mips上使用perf和火焰图 用perf record生成原始数据 用perf script解析调用栈 在mint虚拟机上, 生成火焰图 分离符号表 用objcopy --add-gnu-debuglink的连接功能, 可以把strip后的可执行文件, 和只带debug信息的符号文件链接起来 #先做出只带debug符号的文件 objcopy --only-keep-debug \"${tostripfile}\" \"${debugdir}/${debugfile}\" #把原始可执行文件strip strip --strip-debug --strip-unneeded \"${tostripfile}\" #设置debuglink objcopy --add-gnu-debuglink=\"${debugdir}/${debugfile}\" \"${tostripfile}\" 这样, strip后的文件能够单独运行; 在需要符号表的时候, 按照约定的路径${debugdir}/${debugfile}把debug文件放进去, 再运行strip file的时候, 符号就能找到 这里说的strip的符号指.debug相关的小节和.symtab .strtab 用strip --strip-unneeded命令后, 这些小节都会被strip掉. 而--only-keep-debug则只保留这些小节. [27] .debug_aranges PROGBITS 00000000 3ed510 004f99 00 C 0 0 8 [28] .debug_info PROGBITS 00000000 3f24a9 03b36a 00 C 0 0 1 [29] .debug_abbrev PROGBITS 00000000 42d813 00324a 00 C 0 0 1 [30] .debug_line PROGBITS 00000000 430a5d 054ab1 00 C 0 0 1 [31] .debug_str PROGBITS 00000000 48550e 013d3f 01 MSC 0 0 1 [32] .debug_loc PROGBITS 00000000 49924d 0097b3 00 C 0 0 1 [33] .debug_ranges PROGBITS 00000000 4a2a00 0085c7 00 C 0 0 8 [34] .gnu.attributes GNU_ATTRIBUTES 00000000 2e8a6f 000012 00 0 0 1 [35] .symtab SYMTAB 00000000 2e8a84 092910 10 36 27035 4 [36] .strtab STRTAB 00000000 37b394 07217a 00 0 0 1 使用strace分析eoe_filter占用高的问题 统计 / # timeout 10 strace -c -p `pidof eoe_filter` strace: Process 27238 attached strace: Process 27238 detached % time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- 52.00 0.130000 15 8225 read 48.00 0.120000 14 8225 write ------ ----------- ----------- --------- --------- ---------------- 注strace的选项: -c: Count time, calls, and errors for each system call and report a summary on program exit. timeout也很实用, 带超时的执行一个命令. 使用其他版本的libc 背景 我想在板子上运行netstat, 但板子上的版本是busybox版本, 功能不全. 但是我几个月之前编译过完整版本的netstat, 想拷到板子上直接运行 直接使用是报错的 wget http://172.24.213.190:8088/bin/netstat ~ # ./netstat ./netstat: relocation error: ./netstat: symbol h_errno version GLIBC_PRIVATE not defined in file libc.so.6 with link time reference 说明libc版本不匹配 解决 LD_LIBRARY_PATH 首先想到拷贝原libc.so版本, 全在/root目录下执行 报一样的错误 wget http://172.24.213.190:8088/lib/libc.so.6 ~ # LD_LIBRARY_PATH=`pwd` ./netstat ./netstat: relocation error: ./netstat: symbol h_errno version GLIBC_PRIVATE not defined in file libc.so.6 with link time reference LD_PRELOAD 使用LD_PRELOAD报错不一样, 但还是不行 ~ # LD_PRELOAD='/root/libc.so.6' ./netstat ERROR: ld.so: object '/root/libc.so.6' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored. ./netstat: relocation error: ./netstat: symbol h_errno version GLIBC_PRIVATE not defined in file libc.so.6 with link time reference 最终解决 使用ld.so来执行. 背景知识是: 所有可执行的文件实际上都是ld.so来执行的 wget http://172.24.213.190:8088/lib/ld-2.16.so //注意使用--library-path执行so路径, 本文这里就包括了老版本的libc.so ./ld-2.16.so --library-path `pwd` ./netstat -apn 没有ldd看共享库 ~ # LD_TRACE_LOADED_OBJECTS=1 ./htop linux-vdso.so.1 (0x773c9000) libncurses.so.6 => /usr/lib/libncurses.so.6 (0x77338000) libm.so.6 => /lib/libm.so.6 (0x77247000) libc.so.6 => /lib/libc.so.6 (0x77091000) /lib32-fp/ld.so.1 (0x7738b000) ~ # LD_TRACE_LOADED_OBJECTS=1 ./perf linux-vdso.so.1 (0x77141000) libunwind.so.8 => /usr/lib/libunwind.so.8 (0x770b6000) libunwind-mips.so.8 => /usr/lib/libunwind-mips.so.8 (0x77066000) libpthread.so.0 => /lib/libpthread.so.0 (0x77033000) librt.so.1 => /lib/librt.so.1 (0x77012000) libm.so.6 => /lib/libm.so.6 (0x76f21000) libdl.so.2 => /lib/libdl.so.2 (0x76f00000) libelf.so.1 => /usr/lib/libelf.so.1 (0x76ecf000) libdw.so.1 => /usr/lib/libdw.so.1 (0x76e6e000) libcrypto.so.1.0.0 => /usr/lib/libcrypto.so.1.0.0 (0x76ca4000) libz.so.1 => /usr/lib/libz.so.1 (0x76c7f000) libc.so.6 => /lib/libc.so.6 (0x76ac9000) /lib32-fp/ld.so.1 (0x77103000) libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x76a9e000) 用man ld.so来看详细的ld程序信息. 我们知道linux启动一个elf程序, 实际上是先启动ld程序, 然后ld来启动elf. ld.so也可以直接启动: /lib/ld-linux.so.* [OPTIONS] [PROGRAM [ARGUMENTS]] The programs ld.so and ld-linux.so* find and load the shared objects (shared libraries) needed by a program, prepare the program to run, and then run it. ld会看一些环境变量: LD_ASSUME_KERNEL LD_BIND_NOW: ld在加载app的时候就解决所有的符号引用, 而不是当符号被引用的时候才解析. LD_LIBRARY_PATH: 动态库搜索路径. 常用 LD_PRELOAD: 预加载的so列表, 作用在其他so之前 LD_TRACE_LOADED_OBJECTS: 后面跟任何值, 效果和ldd一样 LD_DEBUG=all: 输出ld的debug信息到stderr LD_DEBUG_OUTPUT: LD_DEBUG的输出到指定位置 strace查看一个进程的open文件过程 strace支持filter. 比如我想trace htop 16122的所有open文件的过程, 来看看它到底怎么得到进程树的. strace -p 16122 -e trace=openat -o s.log #其他举例 #open和openat都是打开文件, 但它们是不一样的系统调用 -e trace=open,stat #支持正则 -e trace=/regex #其他常用 //跟踪文件相关的: 类似 -e trace=open,stat,chmod,unlink,... -e trace=%file //跟踪进程管理相关的, 比如 fork, wait, and exec -e trace=%process //跟踪网络相关的 -e trace=%network //跟踪信号相关的 -e trace=%signal //IPC相关的 -e trace=%ipc //文件描述符相关的 -e trace=%desc //mmap相关的 -e trace=%memory 计算CPU load 参考: linux/Documentation/filesystems/proc.txt 参考: man proc搜索stat (14) utime %lu Amount of time that this process has been scheduled in user mode, measured in clock ticks (divide by sysconf(_SC_CLK_TCK)). This includes guest time, guest_time (time spent running a virtual CPU, see below), so that applications that are not aware of the guest time field do not lose that time from their calculations. (15) stime %lu Amount of time that this process has been scheduled in kernel mode, measured in clock ticks (divide by sysconf(_SC_CLK_TCK)). (16) cutime %ld : 在wait返回后更新 Amount of time that this process's waited-for children have been scheduled in user mode, measured in clock ticks (divide by sysconf(_SC_CLK_TCK)). (See also times(2).) This includes guest time, cguest_time (time spent running a virtual CPU, see below). (17) cstime %ld Amount of time that this process's waited-for children have been scheduled in kernel mode, measured in clock ticks (divide by sysconf(_SC_CLK_TCK)). (24) rss %ld Resident Set Size: number of pages the process has in real memory. This is just the pages which count toward text, data, or stack space. This does not include pages which have not been demand-loaded in, or which are swapped out. utime和cutime的关系 比如一个bash进程, 只起了一个外部程序, 比如 ./tooManyTimer: 对bash进程来说 utime是自己用户态执行的时间 cutime是./tooManyTimer的执行时间, 平时不更新, 只有子进程退出的时候, 父进程waitpid返回才更新 对子进程./tooManyTimer来说: utime是自己的执行时间, 包括所有线程 这个程序没有子进程, cutime一直是0 /proc/stat 同样的man proc的/proc/stat小节有讲 /proc/stat kernel/system statistics. Varies with architecture. Common entries include: cpu 10132153 290696 3084719 46828483 16683 0 25195 0 175628 0 cpu0 1393280 32966 572056 13343292 6130 0 17875 0 23933 0 The amount of time, measured in units of USER_HZ (1/100ths of a second on most architectures, use sysconf(_SC_CLK_TCK) to obtain the right value), that the sys‐ tem (\"cpu\" line) or the specific CPU (\"cpuN\" line) spent in various states: user (1) Time spent in user mode. nice (2) Time spent in user mode with low priority (nice). system (3) Time spent in system mode. idle (4) Time spent in the idle task. This value should be USER_HZ times the second entry in the /proc/uptime pseudo-file. iowait (since Linux 2.5.41) (5) Time waiting for I/O to complete. This value is not reliable, for the following reasons: 1. The CPU will not wait for I/O to complete; iowait is the time that a task is waiting for I/O to complete. When a CPU goes into idle state for out‐ standing task I/O, another task will be scheduled on this CPU. 2. On a multi-core CPU, the task waiting for I/O to complete is not running on any CPU, so the iowait of each CPU is difficult to calculate. 3. The value in this field may decrease in certain conditions. irq (since Linux 2.6.0-test4) (6) Time servicing interrupts. softirq (since Linux 2.6.0-test4) (7) Time servicing softirqs. steal (since Linux 2.6.11) (8) Stolen time, which is the time spent in other operating systems when running in a virtualized environment guest (since Linux 2.6.24) (9) Time spent running a virtual CPU for guest operating systems under the control of the Linux kernel. guest_nice (since Linux 2.6.33) (10) Time spent running a niced guest (virtual CPU for guest operating systems under the control of the Linux kernel). 上述所有相的和是CPU总的tick数 The amount of time, measured in units of USER_HZ (1/100ths of a second on most architectures, use sysconf(_SC_CLK_TCK) to obtain the right value) 一般这个USER_HZ是100, 就是说每个CPU, 每秒有100个tick. while true; do t0=`cat /proc/stat | grep cpu0 | awk '{print $2+$3+$4+$5+$6+$7+$8+$9+$10}'`;sleep 1;t1=`cat /proc/stat | grep cpu0 | awk '{print $2+$3+$4+$5+$6+$7+$8+$9+$10}'`;echo $(($t1 - $t0));done 101 104 103 101 一个系统上的所有核(每行)的和是差不多的. $ cat /proc/stat | grep cpu cpu 9669309 24418 17471207 10263522262 224627 0 828332 14242740 0 0 cpu0 533764 573 734456 427550594 11586 0 98187 615793 0 0 cpu1 383788 2184 626860 428073382 12937 0 39272 510523 0 0 cpu2 321855 1849 602457 428122456 12505 0 15910 490530 0 0 cpu3 302345 599 617397 427862041 12518 0 20603 638439 0 0 #在一个24核的qemu/kvm虚拟机上, 分别对每个CPU所有field相加, 结果是差不多的 $ cat /proc/stat | grep cpu | awk '{print $2+$3+$4+$5+$6+$7+$8+$9+$10}' 1.03062e+10 429553194 429657165 429575794 429462193 ... 进程cpud利用率的计算 先算pid的tick和上次的差值 再算总的tick和上次的差值 两个差值相除 strace看系统调用时间 strace -ttT选项可以显示每个系统调用花费的时间: man strace -tt If given twice, the time printed will include the microseconds. -T Show the time spent in system calls. This records the time difference between the beginning and the end of each system call. Linux Mint 19.1 Tessa $ strace -ttT ls 17:51:12.683070 execve(\"/bin/ls\", [\"ls\"], 0x7ffd92408b48 /* 35 vars */) = 0 17:51:12.683597 brk(NULL) = 0x558b9cd69000 17:51:12.683844 access(\"/etc/ld.so.nohwcap\", F_OK) = -1 ENOENT (No such file or directory) 17:51:12.684012 access(\"/etc/ld.so.preload\", R_OK) = -1 ENOENT (No such file or directory) 17:51:12.684179 openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3 17:51:12.684336 fstat(3, {st_mode=S_IFREG|0644, st_size=93197, ...}) = 0 17:51:12.684529 mmap(NULL, 93197, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f6953d86000 17:51:12.684713 close(3) = 0 17:51:12.684924 access(\"/etc/ld.so.nohwcap\", F_OK) = -1 ENOENT (No such file or directory) 17:51:12.685325 openat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libselinux.so.1\", O_RDONLY|O_CLOEXEC) = 3 17:51:12.685523 read(3, \"\\177ELF\\2\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0>\\0\\1\\0\\0\\0\\20b\\0\\0\\0\\0\\0\\0\"..., 832) = 832 17:51:12.685682 fstat(3, {st_mode=S_IFREG|0644, st_size=154832, ...}) = 0 17:51:12.685842 mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f6953d84000 17:51:12.686006 mmap(NULL, 2259152, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f695394e000 可以看到, 普通的系统调用在0.1ms以下. I2C访问时间过长 09:14:41.979375 ioctl(38, _IOC(0, 0x7, 0x7, 0), 0x7ffbe168) = -1 EIO (Input/output error) 这里的0.115278秒(115ms)实在是太长了. 09:14:41.857370 readv(37, [{iov_base=\"\\0\\0\\1\\0\\0\\0\\0\\0\\0\\0\\0000\\0\\0\\0\\1\\0\\0\\0\\4\\0\\3\\251\\\"\\0\\0\\0\\2\\0\\0\\0\\4\"..., iov_len=72}], 1) = 72 09:14:41.857861 stat(\"/etc/localtime\", {st_mode=S_IFREG|0644, st_size=127, ...}) = 0 09:14:41.858430 clock_gettime(CLOCK_MONOTONIC, {tv_sec=240055, tv_nsec=913917513}) = 0 09:14:41.858764 openat(AT_FDCWD, \"/dev/i2c-14\", O_RDWR|O_LARGEFILE) = 38 09:14:41.859242 ioctl(38, _IOC(0, 0x7, 0x7, 0), 0x7ffbe198) = -1 EIO (Input/output error) 09:14:41.975311 close(38) = 0 09:14:41.976146 openat(AT_FDCWD, \"/isam/slot_default/temp/temp0/temp\", O_RDONLY|O_LARGEFILE) = 38 09:14:41.976959 lseek(38, 0, SEEK_SET) = 0 09:14:41.977252 read(38, \" 34.500\\0\", 31) = 9 09:14:41.978402 read(38, \"\", 22) = 0 09:14:41.978903 close(38) = 0 09:14:41.979192 openat(AT_FDCWD, \"/dev/i2c-14\", O_RDWR|O_LARGEFILE) = 38 09:14:41.979375 ioctl(38, _IOC(0, 0x7, 0x7, 0), 0x7ffbe168) = -1 EIO (Input/output error) 09:14:42.095006 close(38) = 0 09:14:42.095775 openat(AT_FDCWD, \"/dev/i2c-14\", O_RDWR|O_LARGEFILE) = 38 09:14:42.096574 ioctl(38, _IOC(0, 0x7, 0x7, 0), 0x7ffbe168) = -1 EIO (Input/output error) 09:14:42.215039 close(38) = 0 09:14:42.215294 epoll_wait(3, [], 32, 0) = 0 09:14:42.215670 openat(AT_FDCWD, \"/dev/i2c-15\", O_RDWR|O_LARGEFILE) = 38 09:14:42.215964 ioctl(38, _IOC(0, 0x7, 0x7, 0), 0x7ffbe198) = -1 EIO (Input/output error) 09:14:42.335088 close(38) = 0 09:14:42.335935 openat(AT_FDCWD, \"/dev/i2c-4\", O_RDWR|O_LARGEFILE) = 38 09:14:42.336191 ioctl(38, _IOC(0, 0x7, 0x7, 0), 0x7ffbe148) = 2 09:14:42.337503 close(38) = 0 09:14:42.338201 openat(AT_FDCWD, \"/dev/i2c-4\", O_RDWR|O_LARGEFILE) = 38 在mips上使用perf和火焰图 用perf record生成原始数据 注意, 在mips上, 必须用-g --call-graph dwarf才能生成火焰图 #对整个系统做profiling perf record -F 500 -e cycles -g --call-graph dwarf -a -- sleep 30 #对某个进程做profiling perf record -F 500 -e cycles -g --call-graph dwarf -p `pidof switch_hwa_app` -- sleep 30 perf record -F 500 -e cycles -g --call-graph dwarf -p `pidof xpon_hwa_app` -- sleep 30 perf record -F 500 -e cycles -g --call-graph dwarf -p `pidof xponhwadp` -- sleep 30 #也可以只对用户态做profiling, 用-e cycles:u perf record -F 1000 -e cycles:u -g --call-graph dwarf -p 18852 -- sleep 60 用perf script解析调用栈 #先准备好目录, 用sshfs存放script后的文件 sshfs bsfs yingjieb@172.24.213.190:/repo/yingjieb/ms/gopoc/poclog-ver20200101/profiling #准备符号表目录, 一般板子上的二进制是strip过的, 要解析调用栈, 必须有符号表 #这里的sysroot就相当于板子的根目录 sshfs wsfs chwang@135.251.206.190:/repo/chwang/ms/buildroot_poc/output/host/mips64-buildroot-linux-gnu/sysroot #即使sysroot目录, typeB里面的二进制也是没有的 #先用下面的命令找出哪些符号不能解析, 手动拷到sysroot对应目录下 perf script | grep unknown | sort -u #比如下面这几个 /usr/lib/libevent_core-2.1.so.6.0.2 /mnt/isam/NW89AA62.801/switch_hwa_fglt-b_app /mnt/isam/NW89AA62.801/libyAPI.so.0.0.2 /lib/libc-2.16.so /usr/lib/libhxps.so #拷完用这个确认一下 perf script --symfs wsfs --kallsyms /proc/kallsyms | grep unknown | sort -u 准备工作完成后, 开始解析perf data #用symfs指定root目录, 用kallsyms指定kernel符号表 perf script --symfs wsfs --kallsyms /proc/kallsyms > bsfs/sysidle-switch_hwa_app/perf.script 在mint虚拟机上, 生成火焰图 yingjieb@yingjieb-VirtualBox ~/work/share/gopoc/perflogpoc-ver20191227/profiling/sysidle-switch_hwa_app Linux Mint 19.1 Tessa $ cat perf.script | ~/repo/FlameGraph/stackcollapse-perf.pl | ~/repo/FlameGraph/flamegraph.pl > ~/sf_work/tmp/poc.svg "},"notes/profiling_调试和分析记录3.html":{"url":"notes/profiling_调试和分析记录3.html","title":"调试和分析记录3","keywords":"","body":" 关于线程的group leader 相关panic 可能的错误点1 可能的错误点2 这部分在schedule中的位置 补充: 僵尸进程 binutils包括 gcc 查看include搜索路径 linux执行文件的过程 执行ELF文件 load elf文件 执行脚本文件 任意类型文件 kernel online doc 查看编译器的文件路径 用size命令查看elf文件各个section的大小 linux内存统计, man top 查看一个进程运行时的状态 printk 更新 中断代码里用printk? printk会阻塞进程吗? 动态debug打印, pr_debug要打开CONFIG_DYNAMIC_DEBUG cmd line传kernel module的参数 关于模块的符号 关于kallsyms 关于线程的group leader 有人问怎么区分一个进程下面的是子进程还是线程, https://unix.stackexchange.com/questions/434092/how-does-linux-tell-threads-apart-from-child-processes 回答: 在task_struct里, group_leader也是task_struct的指针 进程的线程都有相同的group_leader 而进程的子进程的group_leader是自己 所以group_leader说的是线程组的leader, 也就是tgid? kernel/fork.c中 如果是新进程, 那group_leader就是自己 如果是新线程, 那group_leader是创建自己的进程 copy_process() ... p->pid = pid_nr(pid); if (clone_flags & CLONE_THREAD) { p->exit_signal = -1; p->group_leader = current->group_leader; p->tgid = current->tgid; } else { if (clone_flags & CLONE_PARENT) p->exit_signal = current->group_leader->exit_signal; else p->exit_signal = (clone_flags & CSIGNAL); p->group_leader = p; p->tgid = p->pid; } 相关panic 在板子重启过程中, 出了kernel panic ... CPU 1 Unable to handle kernel paging request at virtual address 0000000000000008 [ 4172.962012] (c01 22036 app_finis) CPU: 1 PID: 22036 Comm: app_finish Tainted: G O 4.9.79-Cavium-Octeon #5 [ 4172.972714] (c01 22036 app_finis) task: 800000008d505700 task.stack: 80000000882d4000 [ 4172.980547] $ 0 : 0000000000000000 ffffffff80e0a760 0000001800005103 0000000000000000 [ 4172.988617] $ 4 : 80000000881a9d00 80000000043972f8 80000000881a9f00 0000000000000000 [ 4172.996686] $ 8 : 000000000000003f 000000000000001f 0000000000000001 6db6db6db6db6db7 [ 4173.004754] $12 : 0000000000000000 ffffffff84080018 ffffffff809e9a00 00000000100e0000 [ 4173.012823] $16 : 800000008d505700 8000000004396b00 80000000881a9d00 0000000000000001 [ 4173.020894] $20 : 0000000000000000 ffffffff8102fea0 ffffffff80e2a130 ffffffff80e2a130 [ 4173.028962] $24 : 00000000100dc03c ffffffff808dc258 [ 4173.037032] $28 : 80000000882d4000 80000000882d7c00 ffffffffffffffff ffffffff80e0a760 [ 4173.045103] (c01 22036 app_finis) Hi : 0000000000000001 [ 4173.050593] (c01 22036 app_finis) Lo : 0000000000000000 [ 4173.056090] (c01 22036 app_finis) epc : ffffffff8087c9e8 octeon_prepare_arch_switch+0x18/0x38 [ 4173.064800] (c01 22036 app_finis) ra : ffffffff80e0a760 __schedule+0x168/0xa58 [ 4173.072284] Status: 14009ce2 KX SX UX KERNEL EXL [ 4173.077067] (c01 22036 app_finis) Cause : 00800008 (ExcCode 02) [ 4173.082990] (c01 22036 app_finis) BadVA : 0000000000000008 [ 4173.088480] (c01 22036 app_finis) PrId : 000d9602 (Cavium Octeon III) [ 4173.095009] [ 4173.215593] (c01 22036 app_finis) Call Trace: [ 4173.219956] (c01 22036 app_finis) [] octeon_prepare_arch_switch+0x18/0x38 [ 4173.228317] (c01 22036 app_finis) [] __schedule+0x168/0xa58 [ 4173.235459] (c01 22036 app_finis) [] schedule+0x40/0xb8 [ 4173.242255] (c01 22036 app_finis) [] pipe_wait+0x78/0xa0 [ 4173.249136] (c01 22036 app_finis) [] pipe_read+0x1a0/0x2f8 [ 4173.256191] (c01 22036 app_finis) [] __vfs_read+0xdc/0x150 [ 4173.263246] (c01 22036 app_finis) [] vfs_read+0xa8/0x150 [ 4173.270127] (c01 22036 app_finis) [] SyS_read+0x60/0xf8 [ 4173.276923] (c01 22036 app_finis) [] syscall_common+0x18/0x44 对虚拟地址0000000000000008的访问造成了这个panic, 因为没有对应的page.这很可能是类似对空指针的结构体访问 access to ptr->member 这里的member的offset是8 ptr为空 根据调用栈, 最后的函数是octeon_prepare_arch_switch /linux-octeon-sdk3.1/arch/mips/kernel/octeon-cpu.c void octeon_prepare_arch_switch(struct task_struct *next) { struct task_struct *group_leader = next->group_leader; union octeon_cvmemctl cvmmemctl; cvmmemctl.u64 = read_c0_cvmmemctl(); #if defined(CONFIG_CAVIUM_OCTEON_USER_MEM_PER_PROCESS) cvmmemctl.s.xkmemenau = test_tsk_thread_flag(group_leader, TIF_XKPHYS_MEM_EN) ? 1 : 0; #endif #if defined(CONFIG_CAVIUM_OCTEON_USER_IO_PER_PROCESS) cvmmemctl.s.xkioenau = test_tsk_thread_flag(group_leader, TIF_XKPHYS_IO_EN) ? 1 : 0; #endif write_c0_cvmmemctl(cvmmemctl.u64); } 第一个疑点:next是否为空, 导致next->group_leader异常. 那就要看group_leader在struct task_struct的offset是否为8 -- 不是, 很远. 可能的错误点1 往下看, test_tsk_thread_flag(group_leader, TIF_XKPHYS_MEM_EN) 它调用的都是宏函数. static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag) { return test_ti_thread_flag(task_thread_info(tsk), flag); } #define task_thread_info(task) ((struct thread_info *)(task)->stack) static inline int test_ti_thread_flag(struct thread_info *ti, int flag) { return test_bit(flag, (unsigned long *)&ti->flags); } 注意task_thread_info, 它找struct task_struct里面的void *stack, 当作thread_info. 而它的offset正好是8 include/linux/sched.h struct task_struct { volatile long state; /* -1 unrunnable, 0 runnable, >0 stopped */ void *stack; ... } 可能错误点1: next->group_leader为空. 这里next是下一个准备运行的任务. 如果: 它是个进程, 那它的group_leader是自身, 不可能为空. 它是个线程, 这个线程准备被运行, 说明它所在的\"线程组\"还在. 即使组长先退出了, 这个组员的p->group_leader也没有谁有必要改成NULL. 不太可能在这. 可能的错误点2 后面的指针访问有ti->flags, 是在thread_info找flags thread_info在arch下面, 每个CPU架构都有. flags的offset也刚好是8 arch/mips/include/asm/thread_info.h struct thread_info { struct task_struct *task; /* main task structure */ unsigned long flags; /* low level flags */ ... } 可能的错误点2: ti = next->group_leader->stack为空, 导致ti->flags对0地址访问. 这个地方可能出错. 即: p存在, p->group_leader也存在, 但p->group_leader->stack已经被释放: p是个线程, 否则p->group_leader就是自己了. 自己的stack被释放了还要被调度执行吗? p->group_leader是创建p的进程, 但应该已经是僵尸进程了. -- 但为什么p没有退出呢? 比如p->group_leader调用了pthread_exit因此其他线程不退出? 首先要搞清楚这个stack从哪里来? 在kernel/fork.c中, stack在复制task_struct, 被重新alloc. _do_fork struct task_struct *p p = copy_process(...) p = dup_task_struct(current, node); stack = alloc_thread_stack_node(tsk, node); tsk->stack = stack; 在free_task里面, stack被释放并设为NULL free_task(struct task_struct *tsk) release_task_stack(tsk); free_thread_stack(tsk); tsk->stack = NULL 在include/linux/sched.h中, 如果对task_struct的引用减小到0, 就调用__put_task_struct, 后者会调用free_task static inline void put_task_struct(struct task_struct *t) { if (atomic_dec_and_test(&t->usage)) __put_task_struct(t); } 这部分在schedule中的位置 __schedule(void) context_switch prepare_task_switch(rq, prev, next); prepare_arch_switch(next); octeon_prepare_arch_switch(struct task_struct *next) struct task_struct *group_leader = next->group_leader; cvmmemctl.s.xkmemenau = test_tsk_thread_flag(group_leader, TIF_XKPHYS_MEM_EN) ? 1 : 0; test_ti_thread_flag(task_thread_info(tsk), flag); int test_ti_thread_flag(struct thread_info *ti, int flag) test_bit(flag, (unsigned long *)&ti->flags); /linux-octeon-sdk3.1/kernel/sched/core.c static inline void prepare_task_switch(struct rq *rq, struct task_struct *prev, struct task_struct *next) { trace_sched_switch(prev, next); msa_switch(prev, next); sched_info_switch(prev, next); perf_event_task_sched_out(prev, next); fire_sched_out_preempt_notifiers(prev, next); prepare_lock_switch(rq, next); prepare_arch_switch(next); } 补充: 僵尸进程 僵尸进程：先于父进程终止，但是父进程没有对其进行善后处理（wait方法: 获取终止子进程有关信息，释放它仍占有的资源）。消灭僵尸进程的唯一方法是终止其父进程。 任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。这是每个 子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。 binutils包括 The GNU Binutils are a collection of binary tools. The main ones are: ld - the GNU linker. as - the GNU assembler. But they also include: addr2line - Converts addresses into filenames and line numbers. ar - A utility for creating, modifying and extracting from archives. c++filt - Filter to demangle encoded C++ symbols. dlltool - Creates files for building and using DLLs. gold - A new, faster, ELF only linker, still in beta test. gprof - Displays profiling information. nlmconv - Converts object code into an NLM. nm - Lists symbols from object files. objcopy - Copies and translates object files. objdump - Displays information from object files. ranlib - Generates an index to the contents of an archive. readelf - Displays information from any ELF format object file. size - Lists the section sizes of an object or archive file. strings - Lists printable strings from files. strip - Discards symbols. windmc - A Windows compatible message compiler. windres - A compiler for Windows resource files. gcc 查看include搜索路径 gcc -xc -E -v - ./mips64-octeon-linux-gnu-gcc gcc -xc -E -v - #include \"...\" search starts here: #include search starts here: /repo/yingjieb/ms/buildroot73/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/include /repo/yingjieb/ms/buildroot73/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/include-fixed /repo/yingjieb/ms/buildroot73/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/../../../../mips64-octeon-linux-gnu/include /repo/yingjieb/ms/buildroot73/output/host/mips64-buildroot-linux-gnu/sysroot/usr/include End of search list. linux执行文件的过程 参考: https://ownyourbits.com/2018/05/23/the-real-power-of-linux-executables/ 执行ELF文件 一般在shell下执行一个文件, 先调用fork(), 再调用execve() 重点是内核态的execve() 在fs/exec.c SYSCALL_DEFINE3(execve, const char __user *, filename, const char __user *const __user *, argv, const char __user *const __user *, envp) { return do_execve(getname(filename), argv, envp); } 总的来说, 这个系统调用会找到可执行的文件, load到内存; 然后执行 先要准备环境, 包括准备虚拟内存, 拷贝从父进程继承的环境变量, 准备动态库 load elf文件 内核在fs/binfmt_elf.c解析要被执行的elf文件有个INTERP, 指的是动态链接器, 其路径是写死的:比如/lib64/ld-linux-x86-64.so.2 $ readelf -l /usr/bin/sleep Elf file type is DYN (Shared object file) Entry point 0x1710 There are 9 program headers, starting at offset 64 Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flags Align PHDR 0x0000000000000040 0x0000000000000040 0x0000000000000040 0x00000000000001f8 0x00000000000001f8 R E 0x8 INTERP 0x0000000000000238 0x0000000000000238 0x0000000000000238 0x000000000000001c 0x000000000000001c R 0x1 [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2] LOAD 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000006a38 0x0000000000006a38 R E 0x200000 LOAD 0x0000000000006bb0 0x0000000000206bb0 0x0000000000206bb0 0x00000000000004d0 0x0000000000000690 RW 0x200000 DYNAMIC 0x0000000000006c78 0x0000000000206c78 0x0000000000206c78 0x00000000000001b0 0x00000000000001b0 RW 0x8 NOTE 0x0000000000000254 0x0000000000000254 0x0000000000000254 0x0000000000000044 0x0000000000000044 R 0x4 GNU_EH_FRAME 0x0000000000005bf0 0x0000000000005bf0 0x0000000000005bf0 0x000000000000025c 0x000000000000025c R 0x4 GNU_STACK 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000000000 RW 0x10 GNU_RELRO 0x0000000000006bb0 0x0000000000206bb0 0x0000000000206bb0 0x0000000000000450 0x0000000000000450 R 0x1 这个runtime linker, 也就是ld.so, 会被内核执行. 它在文件系统里load其他共享库, 比如libc.so内核加载ld.so的代码 static int load_elf_binary(struct linux_binprm *bprm) { ... if (elf_interpreter) { unsigned long interp_map_addr = 0; elf_entry = load_elf_interp(&loc->interp_elf_ex, interpreter, &interp_map_addr, load_bias, interp_elf_phdata); ... } 用ldd能看出来. $ ldd /usr/bin/sleep linux-vdso.so.1 (0x00007ffc9dbfe000) libc.so.6 => /usr/lib/libc.so.6 (0x00007f71d401d000) /lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x00007f71d45e1000) 这里的vdso.so是个虚拟的so, 用来加速只读的系统调用, 不用加载; ld.so内核已经加载了, 所以上面只有libc.so.6需要ld来加载.ld.so安装被执行的elf, 把动态库都加载好了后, 最后跳转到程序的AT_ENTRY, 开始执行. 所以: Linux执行一个ELF文件的过程, 其实是执行ld.so, 再由后者去真正完成加载和执行. 执行脚本文件 脚本文件不是ELF, 只是可执行的文本文件, 那linux怎么执行它们呢?实际上, 内核可以执行任何类型的文件. 内核首先读这个文件的头128字节, 来确定用哪个handler来执行这个文件.对不同类型的文件, 可以注册不同的handler. /* * cycle the list of binary formats handler, until one recognizes the image */ int search_binary_handler(struct linux_binprm *bprm) { ... list_for_each_entry(fmt, &formats, lh) { ... retval = fmt->load_binary(bprm); // OYB: the first 128B are in bprm->buf[128] ... } ... } 脚本文件一般都在文件第一行, 比如有#!/bin/python2的, 是用python2来执行. 任意类型文件 上面说到, 内核可以执行任意文件, 只要有对应的handler 内核提供binfmt_misc接口来注册handler. #比如注册.jpg用feh打开 echo ':fehjpg:E::jpg::/usr/bin/feh:' > /proc/sys/fs/binfmt_misc/register kernel online doc https://www.kernel.org/doc/html/v5.2/ 可以搜索, 比如kprobe https://www.kernel.org/doc/html/v5.2/trace/kprobetrace.html 查看编译器的文件路径 用--print-file-name=libc.so /repo/yingjieb/ms/buildrootmlt/output/host/opt/ext-toolchain/bin/mips64-octeon-linux-gnu-gcc -mabi=n32 --print-file-name=libc.so.6 /repo/yingjieb/ms/buildrootmlt/output/host/opt/ext-toolchain/bin/../mips64-octeon-linux-gnu/sys-root/lib/../lib32/libc.so.6 /repo/yingjieb/ms/buildroot73/output/host/opt/ext-toolchain/bin/mips64-octeon-linux-gnu-gcc-mabi=n32 --print-file-name=libc.so.6 /repo/yingjieb/ms/buildroot73/output/host/opt/ext-toolchain/bin/../mips64-octeon-linux-gnu/sys-root/lib64/../lib32/libc.so.6 用size命令查看elf文件各个section的大小 /repo/yingjieb/ms/buildrootmlt/output/build/linux-custom/fs/ubifs # -A更详细点 size -A ubifs.o ubifs.o.73 size -B ubifs.o ubifs.o.73 linux内存统计, man top 查看一个进程运行时的状态 相关文档: linux/Documentation/filesystems/proc.txt #很详细, 包括各种id, 线程数, vm状态, cat /proc/14338/status printk 更新 中断代码里用printk? 偶然翻到几年前的一篇笔记, 记的是在内存初始化时, 注册中断处理函数, 如果内存初始化有ECC错误, 会触发中断处理. 在中断handler里面, 用了printk打印debug信息. 现在想想, 在中断上下文调用printk真的合适吗? 有人在stackoverflow上也提出了相同的问题: 在中断里调用printk真的那么不对吗? https://stackoverflow.com/questions/8738951/printk-inside-an-interrupt-handler-is-it-really-that-bad 这里面说printk应该是有buffer机制的, 在buffer满了以后才真正IO(比如串口)开始输出吗? 也有人说printk的设计初衷是debug, 时可以在中断里使用的 -- 如果中断里不能用, 如果还有buffer机制不能及时输出, printk就没那么有用了. -- 有道理 高票答案的解释是: printk并不是\"延迟\"输出, 如果log level高的话, 会立即输出. 如果觉得printk太慢, 可以用trace_printk打印到ftrace的buffer里, 后者可以用debugfs看到. printk会阻塞进程吗? printk will just try to lock the console to print the message; if the lock is already taken then the output is queued to a buffer but the function will never block. 不会. printk尝试输出到串口, 如果获取不到锁, 就打印到buffer里. 这个buffer也有锁, 但是spin lock, 所以中断也可以打printk printk就是设计来在任何地方打印的 前面说到的buffer的spin lock, 有人认为效率不高 https://lwn.net/Articles/779550/ 因为同一时刻真正往串口打印的只能有一个进程, 这个进程在调用printk的时候, printk会把其他进程或中断上下文的log也一起打出来. 这会带来时间上的不确定性. 动态debug打印, pr_debug要打开CONFIG_DYNAMIC_DEBUG #不打开这个选项, pr_debug根本就不会编译 CONFIG_DYNAMIC_DEBUG=y #打开以后, 用下面的语法 echo 8 > /proc/sys/kernel/printk echo 'file kernel/module.c +p' > /sys/kernel/debug/dynamic_debug/control insmod mymodule.ko #文档 Documentation/dynamic-debug-howto.txt #例子, +p是说打开pr_debug, +f是打印函数名, +l行号, +m是模块名, +t是线程id, flag可以连用 echo -n 'file svcsock.c line 1603 +p' > /dynamic_debug/control echo -n 'module nfsd +p' > echo -n 'func svc_process +p' > #打开所有debug消息 echo -n '+p' > /dynamic_debug/control #打印模块名和函数名 echo -n '+mf' > /dynamic_debug/control #启动的时候, 用下面的形式: QUERY符合上述语法 dyndbg=\"QUERY\", module.dyndbg=\"QUERY\" dyndbg=\"file ec.c +p\" #比如在cmdline加 dyndbg=\"func cvmx_nand_page_write +p; func ubi_io_write +p\" dyndbg=\"func cvmx_nand_page_write line 1877-1878 +p; func ubi_io_write +pf\" include/linux/printk.h bootoctlinux $(loadaddr) 'coremask=0x0f ctxt=OSW isamversion=ZAM8AA62.990, prozone=0x4000000,0x80000000 logbuffer=uboot,0x20000,0x7be00000 bpsflag=0x4,0x7be20000 bpcommit=0,0x140000,0x160000 mtdparts=octeon_nand0:0x8000000@0x0(recovery),-@0x8000000(nand);octeon_nand1:0x40000000@0x0(nand1);bootflash:0x20000@0x140000(statusA),0x20000@0x160000(statusB),0x140000@0x180000(bootA),0x140000@0x2c0000(bootB),0x1900000@0x400000(linuxA),0x1900000@0x1d00000(linuxB),0x20000@0x120000(preferred_oswp),0x80000@0x3600000(management_a),0x80000@0x3680000(management_b),0x120000@0x0(bps) linux_fit_image=0x1e00000,0x7a000000 unpreferred_oswp=0 console=ttyS1,115200 config_overlay=reboot=0 loglevel=8 debug ignore_loglevel dyndbg=\"func cvmx_nand_page_write +p; func ubi_io_write +p\" ' cmd line传kernel module的参数 #insmod insmod my_module param=value #cmdline vmlinux ... my_module.param=value ... #module_name.para=xxx #octeon-nand.debug=1 setenv kernel_extra_args config_overlay=reboot=0 loglevel=8 debug ignore_loglevel octeon-nand.debug=1 关于模块的符号 模块可以用kernel或者其他模块export出来的符号在引用其他模块的符号时, 要先加载被依赖的ko比如mymodule2.ko用了mymodule1.ko的print_hello()函数, 如果单独加载mymodule2.ko时 $ sudo insmod mymodule2.ko insmod: ERROR: could not insert module mymodule2.ko: Unknown symbol in module rishi@rishi-VirtualBox:~/mydev/publications/lkw/doc/code/04_exporting_symbols/exporting_symbols$ dmesg [15588.009164] mymodule2: Unknown symbol add_two_numbers (err 0) [15588.009171] mymodule2: Unknown symbol GLOBAL_VARIABLE (err 0) [15588.009176] mymodule2: Unknown symbol print_hello (err 0) 先加载mymodule1.ko, 再加载mymodule2.ko, 就没问题. 总结: 根据网上文章的说法, 加载mymodule1.ko, 会把print_hello加到全局符号表. insmod会在加载mymodule2.ko的时候, 在kernel全局符号表里找到print_hello的地址 关于kallsyms /proc/kallsyms包括内核所有符号, 包括没有被EXPORT_SYMBOL()的符号 #没有sudo的话, kallsyms也能看到符号名, 但地址都是0 less /proc/kallsyms #sudo后, 能看到实际地址 sudo less /proc/kallsyms #比如 ffffffff9d0206b0 t xen_write_cr0 ffffffff9d020810 t xen_set_ldt ffffffff9d0209e0 T xen_running_on_version_or_later ffffffff9d020a00 T xen_copy_trap_info # 其中的类型标记, t T的含义为 #T表示全局的代码, 即被EXPORT_SYMBOL()过的 #t表示局部的代码 #详细标记见man nm \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the BSS data section. This section typically contains zero-initialized or uninitialized data, although the exact behavior is system dependent. \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"I\" The symbol is an indirect reference to another symbol. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized or zero-initialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. "},"notes/profiling_调试和分析记录2.html":{"url":"notes/profiling_调试和分析记录2.html","title":"调试和分析记录2","keywords":"","body":" 查看PCI的MSI中断 一次中断执行记录和KVM虚拟机相关的trace 中断发生在core 42上 core 42的正常流程 perf sched虚拟机 lmbench的lat_mem_rd怎么算cache访问延迟 pstack看qemu的进程 lsof详细输出 关于共享cluster的2core 关于latency和throughput 关于timer cache预取 快速找到一个进程的log 编译调试手段 加帧指针 强制不优化, C语言 __attribute__ 关于no hz 只加isolcpus 在isolcpus基础上, 加nohz_full和rcu_nocbs IPI中断和arch_timer 结论 动态调频和中断 调试testpmd 现象 打开coredump并用gdb调试 是数组越界吗? -- 第一次问 看汇编 扩大栈 检查内存分布 还是回到数组越界 记录一下gdb用到的命令 调试testpmd续 macswap段错误 修改栈大小 objdump汇编和C混合显示 coredump和GDB 查看优化level的详细开关 查看PCI的MSI中断 /proc/interrupts里能显示中断的信息, 但不能很方便的对应到是哪个设备的中断. 比如在我的VM里, 有: $ cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 2: 1020348 924296 763104 823621 GICv3 27 Level arch_timer 4: 116 0 0 0 GICv3 33 Level uart-pl011 42: 0 0 0 0 GICv3 23 Level arm-pmu 43: 0 0 0 0 pl061 3 Edge ACPI:Event 44: 1 0 0 0 ITS-MSI 16384 Edge aerdrv, PCIe PME, pciehp 45: 1 0 0 0 ITS-MSI 18432 Edge aerdrv, PCIe PME, pciehp 46: 1 0 0 0 ITS-MSI 20480 Edge aerdrv, PCIe PME, pciehp 47: 1 0 0 0 ITS-MSI 22528 Edge aerdrv, PCIe PME, pciehp 48: 1 0 0 0 ITS-MSI 24576 Edge aerdrv, PCIe PME, pciehp 49: 0 0 0 0 ITS-MSI 1572864 Edge virtio1-config 50: 0 0 0 0 ITS-MSI 1572865 Edge virtio1-control 51: 0 0 0 0 ITS-MSI 1572866 Edge virtio1-event 52: 95112 0 0 0 ITS-MSI 1572867 Edge virtio1-request 53: 0 0 0 0 ITS-MSI 524288 Edge virtio0-config 54: 0 29863 59165 0 ITS-MSI 524289 Edge virtio0-input.0 55: 0 0 0 1 ITS-MSI 524290 Edge virtio0-output.0 56: 2 0 0 0 ITS-MSI 2097152 Edge virtio2-config 57: 186776 0 0 0 ITS-MSI 2097153 Edge virtio2-input.0 58: 0 1 0 0 ITS-MSI 2097154 Edge virtio2-output.0 59: 0 0 0 0 ITS-MSI 1048576 Edge xhci_hcd 60: 0 0 0 0 ITS-MSI 1048577 Edge xhci_hcd 61: 0 0 0 0 ITS-MSI 1048578 Edge xhci_hcd 62: 0 0 0 0 ITS-MSI 1048579 Edge xhci_hcd 63: 0 0 0 0 ITS-MSI 1048580 Edge xhci_hcd IPI0: 35739 68329 56984 48197 Rescheduling interrupts IPI1: 5 4 3 3 Function call interrupts IPI2: 0 0 0 0 CPU stop interrupts IPI3: 0 0 0 0 CPU stop (for crash dump) interrupts IPI4: 0 0 0 0 Timer broadcast interrupts IPI5: 2 0 0 0 IRQ work interrupts IPI6: 0 0 0 0 CPU wake-up interrupts 有几个中断想知道对应的设备是什么 查pci, 我这里想找eth1$ lspci 01:00.0 Ethernet controller: Red Hat, Inc Virtio network device (rev 01) 03:00.0 SCSI storage controller: Red Hat, Inc Virtio SCSI (rev 01) 04:00.0 Ethernet controller: Red Hat, Inc Virtio network device (rev 01) 看看eth1的pci$ ethtool -i eth1 bus-info: 0000:04:00.0 到/sys/bus下找pci号bai@localhost /sys/bus/pci/devices/0000:04:00.0 找到对应的中断号$ ls msi_irqs/ 56 57 58 可以修改一下中断route到哪个CPU, 比如下面把处理中断的CPU从0改为1bai@localhost /proc/irq/57 $ cat effective_affinity 1 bai@localhost /proc/irq/57 $ cat smp_affinity_list 0 [root@localhost 57]# echo 1 > smp_affinity_list [root@localhost 57]# cat smp_affinity_list 1 [root@localhost 57]# cat smp_affinity 2 一次中断执行记录和KVM虚拟机相关的trace 用ftrace的function_graph抓到的一次中断过程. #系统通过isolcpus=2-23,26-47 只保留4个核给系统(centos 7.5)使用. HOST: 0 1 24 25 #OVS的pmd进程跑在四个核上 OVS: 12 13 14 15 #两个VM分别pin了4个CPU VM1: 26 27 28 29 VM2: 40 41 42 43 中断发生在core 42上 gic_handle_irq() __handle_domain_irq() // 44 us irq_enter() rcu_irq_enter() tick_irq_enter() _local_bh_enable() irq_find_mapping() generic_handle_irq() handle_percpu_devid_irq() arch_timer_handler_phys() hrtimer_interrupt() gic_eoimode1_eoi_irq() irq_exit() idle_cpu() tick_nohz_irq_exit() rcu_irq_exit() __handle_domain_irq() // 120 us irq_enter() irq_find_mapping() generic_handle_irq() handle_percpu_devid_irq() arch_timer_handler_phys() hrtimer_interrupt() __hrtimer_run_queues() __remove_hrtimer() kvm_timer_expire() kvm_timer_earliest_exp() queue_work_on() __queue_work() get_work_pool() insert_work() wake_up_worker() wake_up_process() try_to_wake_up() update_rq_clock() ttwu_do_activate() activate_task() enqueue_task_fair() enqueue_entity() update_curr() update_cfs_shares() account_entity_enqueue() __enqueue_entity() wq_worker_waking_up() ttwu_do_wakeup() __hrtimer_get_next_event() tick_program_event() clockevents_program_event() gic_eoimode1_eoi_irq() irq_exit() idle_cpu() tick_nohz_irq_exit() rcu_irq_exit() //中断处理结束 core 42的正常流程 cpu_pm_exit() cpu_pm_notify() rcu_irq_enter_irqson() __atomic_notifier_call_chain() notifier_call_chain() gic_cpu_pm_notifier() arch_timer_cpu_pm_notify() fpsimd_cpu_pm_notifier() cpu_pm_pmu_notify() hyp_init_cpu_pm_notifier() cpu_hyp_reinit() kvm_get_idmap_vector() kvm_mmu_get_httbr() kvm_arm_init_debug() kvm_vgic_init_cpu_hardware() rcu_irq_exit_irqson() rcu_irq_exit() sched_idle_set_state() cpuidle_reflect() rcu_idle_exit() arch_cpu_idle_exit() tick_nohz_idle_exit() ktime_get() arch_counter_read() tick_nohz_restart_sched_tick() account_idle_ticks() sched_ttwu_pending() schedule_idle() //切换到kworker-332前的准备 rcu_note_context_switch() update_rq_clock() pick_next_task_fair() put_prev_task_idle() pick_next_entity() set_next_entity() fpsimd_thread_switch() hw_breakpoint_thread_switch() uao_thread_switch() qqos_sched_in() -0 => kworker-332 //idle进程已经切换到kworker finish_task_switch() _raw_spin_lock_irq() process_one_work() //kworker代码 -- 84 us find_worker_executing_work() set_work_pool_and_clear_pending() kvm_timer_inject_irq_work() kvm_vcpu_wake_up() swake_up() swake_up_locked() wake_up_process() try_to_wake_up() _cond_resched() rcu_all_qs() pwq_dec_nr_in_flight() worker_enter_idle() schedule() rcu_note_context_switch() update_rq_clock() deactivate_task() dequeue_task_fair() dequeue_entity() hrtick_update() wq_worker_sleeping() pick_next_task_fair() check_cfs_rq_runtime() pick_next_entity() set_next_entity() fpsimd_thread_switch() hw_breakpoint_thread_switch() uao_thread_switch() qqos_sched_in() kworker-332 => CPU 2/K-6915 //kworker已经切换到CPU 2/K, 后者是qemu的VCPU进程 finish_task_switch() kvm_sched_in() kvm_arch_vcpu_load() kvm_vgic_load() prepare_to_swait() kvm_vcpu_check_block() kvm_arch_vcpu_runnable() kvm_cpu_has_pending_timer() finish_swait() ktime_get() kvm_arch_vcpu_unblocking() //-- 11 us kvm_timer_unschedule() //从这里开始的几个顶级函数, 是在kvm_arch_vcpu_ioctl_run()里调用的, 后者是用户态通过VCPU_RUN ioctl调用的, 它在一个循环里执行VM的代码直到时间片结束. _cond_resched() kvm_pmu_flush_hwstate() kvm_pmu_update_state() kvm_timer_flush_hwstate() //20 us kvm_timer_update_state() kvm_timer_should_fire.part.9() kvm_timer_update_irq.isra.5() kvm_vgic_inject_irq() vgic_lazy_init() vgic_get_irq() vgic_queue_irq_unlock() vgic_target_oracle() kvm_vcpu_kick() //奇怪的是函数里的if (kvm_vcpu_wake_up(vcpu))为什么没出现? irq_set_irqchip_state() kvm_vgic_flush_hwstate() kvm_timer_should_notify_user() kvm_pmu_should_notify_user() kvm_pmu_sync_hwstate() kvm_timer_sync_hwstate() kvm_vgic_sync_hwstate() kvm_arm_setup_debug() //trace_kvm_entry(*vcpu_pc(vcpu)); 我自己看代码加的, 进入VM //guest_enter_irqoff() //对rcu来说, 执行guest代码和切换到user代码差不多 rcu_note_context_switch() kvm_arm_clear_debug() handle_exit() kvm_handle_wfx() kvm_vcpu_block() ktime_get() kvm_arch_vcpu_blocking() prepare_to_swait() kvm_vcpu_check_block() schedule() //调度到idle进程 CPU 2/K-6915 => -0 //切换回idle finish_task_switch() //接着执行第40行idle进程未完成的schedule_idle(), 时间过去了467 us, idle被换出了467 us do_idle() quiet_vmstat() tick_nohz_idle_enter() set_cpu_sd_state_idle() __tick_nohz_idle_enter() arch_cpu_idle_enter() tick_check_broadcast_expired() cpuidle_get_cpu_driver() rcu_idle_enter() cpuidle_not_available() cpuidle_select() call_cpuidle() cpuidle_enter() cpuidle_enter_state() sched_idle_set_state() acpi_idle_lpi_enter() cpu_pm_enter() psci_cpu_suspend_enter() psci_cpu_suspend() __invoke_psci_fn_smc() //陷入到EL3 //@ 585311.6 us idle了585ms //从此开始新的一轮 cpu_pm_exit() perf sched虚拟机 在出现ping延迟高的情况时, 比如VM1 ping VM2延迟高, 在VM1上用tcpdump看时间戳是看不出来的, 都是正常的. 但在VM2上执行tcpdump, 会马上破坏掉问题的复现, ping延迟会马上变低. 在VM执行ping的过程中, 在VM里面收集调度的信息, 看看是不是VM里面的调度问题, 2个VM都执行:sudo perf sched record -a用这个方法基本上是看不出什么来的. lmbench的lat_mem_rd怎么算cache访问延迟 在固定时间内, 默认100ms, 除以iteration的次数; 一次iteration是N次内存操作的循环, N是设计好的, 现在是100. 这样的话, 时间已知, 操作数已知, 那么每次操作的延迟就知道了. 最后一项是iteration数, lmbench有个benchmp_interval()函数, 用来在迭代中\"提前\"算出iteration数. # fprintf(stderr, \"%.5f %.3f %.2f %ld %.2f\\n\", range / (1024. * 1024.), result, (double)gettime(), count, (double)get_n()); $ taskset -c 9 ./lat_mem_rd 1 8192 \"stride=8192 0.00781 1.154 1076726.00 100 9328365.00 0.01172 1.154 1099508.00 100 9525582.00 0.01562 1.154 1101837.00 100 9545763.00 0.02344 1.154 1099507.00 100 9525582.00 0.03125 1.154 1099505.00 100 9525582.00 0.04688 1.154 1101850.00 100 9545763.00 0.06250 1.154 1099507.00 100 9525582.00 0.09375 5.771 1101823.00 100 1909153.00 0.12500 5.771 1098107.00 100 1902703.00 0.18750 5.771 1098104.00 100 1902703.00 0.25000 5.771 1101823.00 100 1909153.00 0.37500 5.771 1098120.00 100 1902703.00 0.50000 5.772 1098255.00 100 1902703.00 0.75000 45.280 1099220.00 100 242759.00 1.00000 45.281 1099246.00 200 121380.00 pstack看qemu的进程 在调试两个VM互相ping高延迟的问题. 两个VM通过OVS的vhost-user连接 #pstack如果没记错应该是调用gdb然后bt出调用栈的, 所以会停住进程里的所有线程一小会 $ sudo pstack 42084 Thread 9 (Thread 0xffff8afeede0 (LWP 42085)): #0 0x0000ffff8b46ac40 in syscall () from /lib64/libc.so.6 #1 0x0000000000a91054 in qemu_futex_wait (f=0x1699390 , val=4294967295) at /home/bai/share/repo/hxt/qemu/include/qemu/futex.h:29 #2 0x0000000000a912b8 in qemu_event_wait (ev=0x1699390 ) at util/qemu-thread-posix.c:445 #3 0x0000000000aab7b0 in call_rcu_thread (opaque=0x0) at util/rcu.c:261 #4 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #5 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 Thread 8 (Thread 0xffff8a7dede0 (LWP 42086)): #0 0x0000ffff8b5307e8 in sigwait () from /lib64/libpthread.so.0 #1 0x0000000000a8e1d4 in sigwait_compat (opaque=0x2794f970) at util/compatfd.c:36 #2 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #3 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 Thread 7 (Thread 0xffff88e7ede0 (LWP 42096)): #0 0x0000ffff8b465460 in ppoll () from /lib64/libc.so.6 #1 0x0000000000a8a9d8 in qemu_poll_ns (fds=0xfffd780008c0, nfds=1, timeout=-1) at util/qemu-timer.c:322 #2 0x0000000000a8ddbc in aio_poll (ctx=0x279953b0, blocking=true) at util/aio-posix.c:629 #3 0x0000000000694388 in iothread_run (opaque=0x279950e0) at iothread.c:64 #4 0x0000000000a91468 in qemu_thread_start (args=0x27995780) at util/qemu-thread-posix.c:504 #5 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #6 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 Thread 6 (Thread 0xffff8968ede0 (LWP 42098)): #0 0x0000ffff8b4665bc in ioctl () from /lib64/libc.so.6 #1 0x0000000000498f30 in kvm_vcpu_ioctl (cpu=0xffff876f0010, type=44672) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:2093 #2 0x0000000000498828 in kvm_cpu_exec (cpu=0xffff876f0010) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:1930 #3 0x0000000000460b74 in qemu_kvm_cpu_thread_fn (arg=0xffff876f0010) at /home/bai/share/repo/hxt/qemu/cpus.c:1215 #4 0x0000000000a91468 in qemu_thread_start (args=0x279f03e0) at util/qemu-thread-posix.c:504 #5 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #6 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 Thread 5 (Thread 0xffff89e9ede0 (LWP 42100)): #0 0x0000ffff8b4665bc in ioctl () from /lib64/libc.so.6 #1 0x0000000000498f30 in kvm_vcpu_ioctl (cpu=0xffff876a0010, type=44672) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:2093 #2 0x0000000000498828 in kvm_cpu_exec (cpu=0xffff876a0010) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:1930 #3 0x0000000000460b74 in qemu_kvm_cpu_thread_fn (arg=0xffff876a0010) at /home/bai/share/repo/hxt/qemu/cpus.c:1215 #4 0x0000000000a91468 in qemu_thread_start (args=0x27a0d0d0) at util/qemu-thread-posix.c:504 #5 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #6 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 Thread 4 (Thread 0xffff8764ede0 (LWP 42101)): #0 0x0000ffff8b4665bc in ioctl () from /lib64/libc.so.6 #1 0x0000000000498f30 in kvm_vcpu_ioctl (cpu=0xffff87650010, type=44672) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:2093 #2 0x0000000000498828 in kvm_cpu_exec (cpu=0xffff87650010) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:1930 #3 0x0000000000460b74 in qemu_kvm_cpu_thread_fn (arg=0xffff87650010) at /home/bai/share/repo/hxt/qemu/cpus.c:1215 #4 0x0000000000a91468 in qemu_thread_start (args=0x27a24e60) at util/qemu-thread-posix.c:504 #5 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #6 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 Thread 3 (Thread 0xffff86deede0 (LWP 42102)): #0 0x0000ffff8b4665bc in ioctl () from /lib64/libc.so.6 #1 0x0000000000498f30 in kvm_vcpu_ioctl (cpu=0xffff86df0010, type=44672) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:2093 #2 0x0000000000498828 in kvm_cpu_exec (cpu=0xffff86df0010) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:1930 #3 0x0000000000460b74 in qemu_kvm_cpu_thread_fn (arg=0xffff86df0010) at /home/bai/share/repo/hxt/qemu/cpus.c:1215 #4 0x0000000000a91468 in qemu_thread_start (args=0x27a3ce50) at util/qemu-thread-posix.c:504 #5 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #6 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 Thread 2 (Thread 0xfffd50e0ede0 (LWP 43789)): #0 0x0000ffff8b52e4bc in do_futex_wait () from /lib64/libpthread.so.0 #1 0x0000ffff8b52e59c in __new_sem_wait_slow () from /lib64/libpthread.so.0 #2 0x0000ffff8b52e690 in sem_timedwait () from /lib64/libpthread.so.0 #3 0x0000000000a90eb0 in qemu_sem_timedwait (sem=0x279a8610, ms=10000) at util/qemu-thread-posix.c:292 #4 0x0000000000a8969c in worker_thread (opaque=0x279a8590) at util/thread-pool.c:92 #5 0x0000000000a91468 in qemu_thread_start (args=0x27cbb660) at util/qemu-thread-posix.c:504 #6 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #7 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 Thread 1 (Thread 0xffff8bf3cf40 (LWP 42084)): #0 0x0000ffff8b465460 in ppoll () from /lib64/libc.so.6 #1 0x0000000000a8a9d8 in qemu_poll_ns (fds=0x27fc1180, nfds=10, timeout=-1) at util/qemu-timer.c:322 #2 0x0000000000a8bd70 in os_host_main_loop_wait (timeout=-1) at util/main-loop.c:258 #3 0x0000000000a8be44 in main_loop_wait (nonblocking=0) at util/main-loop.c:522 #4 0x000000000069ce94 in main_loop () at vl.c:1943 #5 0x00000000006a4f6c in main (argc=79, argv=0xffffc24fcdf8, envp=0xffffc24fd078) at vl.c:4734 lsof详细输出 #把进程5923的所有线程的fd都显示出来, 用-K; -a是指选项上的\"and\", 这里是说-K和-p两个选项连用 #感觉和只看进程的差不多, 多个线程只是重复列了一遍而已 sudo lsof -Ka -p 5923 #注: 在centos上好像不行 关于共享cluster的2core AW是2core共享一个512K的L2 cache, 用lscpu -p | column -ts,可以看到 比如, core 10和core 33是共享一个L2 cache的. 在跑DPDK pmd(pull mode driver)的时候, 尽量不要用同个cluster的2个core 比如这个命令就强制运行pmd在core 10和core33:sudo arm64-armv8a-linuxapp-gcc/app/testpmd -w 0004:01:00.0 --master-lcore 30 -c 0x240000400 -n 6 -- -a --txq=2 --rxq=2 --mbcache=512 --rxd=4096 --txd=4096 --forward-mode=macswap --nb-cores=2 --stat 2RFC2544测试0丢包, 最大throughput只有10G线速的85.037% 而作为对比, 用分属于不通cluster的2个core来跑pmd, 此时pmd运行在core32和core33sudo arm64-armv8a-linuxapp-gcc/app/testpmd -w 0004:01:00.0 -l 31-43 -n 6 -- -a --txq=2 --rxq=2 --mbcache=512 --rxd=4096 --txd=4096 --forward-mode=macswap --nb-cores=2 --stat 2同样跑RFC2544测试0丢包测试, 最大能到在10G线速的97%. 而单核跑pmd的性能, 也能到线速的97%. 以上测试都进行了至少3遍, 现象一致. 结论: 在跑dpdk testpmd的macswap转发时, 2core@2cluster和单核比性能略好. 2core@1cluster比单核差, 10%左右. 对于类似PMD的程序, 两个core跑的东西完全独立, 尽量安排2个core到不同的cluster. 1个cluster里面的2个core, 从容量上共享512K L2 cache, 同时也共享L2到Ring Bus上的带宽. 关于latency和throughput 比如IP forwarding 单核处理速度是3.3M pps 报文这个东西的处理流程可以想象成汽车组装流水线, 3.3M pps说的是这个流水线0.3us就能出一辆整车.0.3us说的是在上百个流水线节点最慢的节点需要的时间, 他是整个系统的瓶颈, 决定了出整车的速度.但从0开始算到第一个整车出来, 整个过程需要的时间是latency.这个latency应该远远大于0.3us(流水线深度越深, 这个差距应该越大) 据反馈一个报文的latency在100us左右. 另外, 从单核的throughput也不能简单算出latency, 比如肯德基排队, 一个店员服务一个人要5分钟, 那么这个店员一个小时的throughput是20人, 但如果排队人数多, 对在队尾的人来说, 从他排队开始算, 到得到服务结束,latency不止5分钟, 比如他前面10个人, 他的latency是10*5+5分钟. 所以单核的latency, 包括了核处理的时间, 和排队的时间, 要减小latency, 要减小队列. 关于timer If I’m not wrong timer counter is a system-wide64-bit count, which counts up from zero, giving a consistent view of time across cores. Each core can then manage its own comparators, typically to generate per-core timer interrupt. The CNTFRQ_EL0 register reports the frequency of the system timer. The CNTPCT_EL0 register reports the current count value. CNTKCTL_EL1 controls whether EL0 can access the system timer. cache预取 在测试OVS性能的时候, 发现热点函数rtevhost_dequeue_burst有两条指令占比比较高:![](img/profiling调试和分析记录2_20221017235829.png) 这个pmd进程, 用rte_vhost_dequeue_burst从vhost的vring里面取报文, 然后发给物理口.这时用perf stat对这个核查一下性能统计sudo perf stat -a -d -d -e cycles -e cpu-clock -e instructions -e stalled-cycles-backend -e stalled-cycles-frontend -e branch-misses -e branch-loads -e l2cache_0/total-requests/ -C 32 -r 6 -- sleep 10 发现有43%的backed stall, 说明CPU在等待data cache, IPC是1.66 43,015,925,614 instructions # 1.66 insn per cycle # 0.26 stalled cycles per insn 11,282,848,844 stalled-cycles-backend # 43.42% backend cycles idle 通过gdb跟踪这个进程, 结合代码, 问题出在第1144行附近 这时注意到可能是desc这个地址没有cache, 而这个地址是descs数组根据偏移量而来desc = &descs[desc_idx]; 结合gdb, 有: #dev结构体 (gdb) p *dev $2 = {mem = 0xbcd670480, features = 5637144576, protocol_features = 55, vid = 0, flags = 3, vhost_hlen = 12, broadcast_rarp = {cnt = 0}, nr_vring = 2, dequeue_zero_copy = 0, virtqueue = { 0xbcd6cca80, 0xbcd6c4080, 0x0 }, ifname = \"/tmp/dpdkvhostuser0\", '\\000' , log_size = 0, log_base = 0, log_addr = 0, mac = {addr_bytes = \"\\000\\000\\000\\000\\000\"}, mtu = 0, notify_ops = 0x94afb8 , nr_guest_pages = 0, max_guest_pages = 8, guest_pages = 0xfffdf8000900, slave_req_fd = 112} #调用copy_desc_to_mbuf传参如下, 注意此时desc就是descs数组的地址 (gdb) p desc $4 = (struct vring_desc *) 0x40007ff98000 (gdb) p sz $6 = 300 (gdb) p idx $7 = 290 #程序走到copy_desc_to_mbuf里面, 根据1137行算出来的desc地址 (gdb) p desc $14 = (struct vring_desc *) 0x40007ff99220 #vring_desc结构体有16个字节, descs数组第290个元素的地址就是0x40007ff99220 (gdb) p/x 0x40007ff98000+16*290 $7 = 0x40007ff99220 所以调用copy_desc_to_mbuf的代码如下, 我加了cache预取指令, 1659行 快速找到一个进程的log sudo lsof -p `pgrep ovs-vswitchd` | grep log 编译调试手段 加帧指针 -fno-omit-frame-pointer 强制不优化, C语言 __attribute__ __attribute__((optimize(\"O0\"))) //比如: void __attribute__((optimize(\"O0\"))) foo(unsigned char data) { // unmodifiable compiler code } 实测有效 有关attribute请参考: https://gcc.gnu.org/onlinedocs/gcc/Attribute-Syntax.html 关于no hz https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt CONFIG_HZ_PERIODIC=y或CONFIG_NO_HZ=n(老内核), 传统模式, 周期tick触发调度 CONFIG_NO_HZ_IDLE=y或CONFIG_NO_HZ=y(老内核), idle的CPU忽略调度, 也称dyntick-idle模式 CONFIG_NO_HZ_FULL=y, 如果一个CPU只run一个进程, 就不需要调度, 进入这种模式的CPU也称adaptive-ticks CPUs默认no CPU是这种模式, 但在kernel cmdline里面可以加nohz_full=4,5,30,31来指定adaptive-ticks CPU, 同时还应加入rcu_nocbs=4,5,30,31启动参数 NO_HZ=y和NO_HZ_FULL=y可以共存 HXT的kernel config $ cat /boot/config-4.14.15-6.hxt.aarch64 | grep -i hz CONFIG_NO_HZ_COMMON=y # CONFIG_HZ_PERIODIC is not set # CONFIG_NO_HZ_IDLE is not set CONFIG_NO_HZ_FULL=y # CONFIG_NO_HZ_FULL_ALL is not set CONFIG_NO_HZ=y CONFIG_HZ_100=y # CONFIG_HZ_250 is not set # CONFIG_HZ_300 is not set # CONFIG_HZ_1000 is not set CONFIG_HZ=100 kernel启动参数加了isolcpus=4,5,30,31 nohz_full=4,5,30,31 rcu_nocbs=4,5,30,31确实看到/proc/interrupts里面, arch_timer和IPI5不再增加 但testpmd跑起来还是在增加 testpmd跑在core 5上, 用perf看统计: 只加isolcpus $ sudo perf stat -a -d -d -d -C 5 -r 6 -- sleep 10 Performance counter stats for 'system wide' (6 runs): 10001.613808 cpu-clock (msec) # 1.000 CPUs utilized ( +- 0.00% ) 1,854 context-switches # 0.185 K/sec ( +- 0.31% ) 0 cpu-migrations # 0.000 K/sec 0 page-faults # 0.000 K/sec 24,980,302,415 cycles # 2.498 GHz ( +- 0.00% ) (81.80%) 39,758,964,098 instructions # 1.59 insn per cycle ( +- 0.00% ) (81.80%) branches 23,938,460 branch-misses ( +- 0.02% ) (81.80%) 18,878,052,718 L1-dcache-loads # 1887.501 M/sec ( +- 0.00% ) (81.80%) 55,552,098 L1-dcache-load-misses # 0.29% of all L1-dcache hits ( +- 0.01% ) (81.80%) LLC-loads LLC-load-misses 14,512,068,781 L1-icache-loads # 1450.973 M/sec ( +- 0.01% ) (81.80%) 178,881 L1-icache-load-misses ( +- 0.71% ) (81.80%) 18,890,206,505 dTLB-loads # 1888.716 M/sec ( +- 0.00% ) (81.80%) 1,433,974 dTLB-load-misses # 0.01% of all dTLB cache hits ( +- 0.42% ) (81.84%) 179,646 iTLB-loads # 0.018 M/sec ( +- 0.48% ) (72.79%) 0 iTLB-load-misses # 0.00% of all iTLB cache hits (72.76%) L1-dcache-prefetches L1-dcache-prefetch-misses 10.000882365 seconds time elapsed ( +- 0.00% ) 在isolcpus基础上, 加nohz_full和rcu_nocbs 对比前面的场景, context-switches和iTLB-loads都显著减少了, L1-icache-load-misses也下降了一个数量级, 其他没有明显变化 $ sudo perf stat -a -d -d -d -C 5 -r 6 -- sleep 10 Performance counter stats for 'system wide' (6 runs): 10004.600842 cpu-clock (msec) # 1.000 CPUs utilized ( +- 0.02% ) 58 context-switches # 0.006 K/sec ( +- 3.28% ) 0 cpu-migrations # 0.000 K/sec 0 page-faults # 0.000 K/sec 24,988,134,060 cycles # 2.498 GHz ( +- 0.02% ) (81.81%) 39,771,226,748 instructions # 1.59 insn per cycle ( +- 0.02% ) (81.81%) branches 24,361,683 branch-misses ( +- 0.02% ) (81.81%) 18,889,305,775 L1-dcache-loads # 1888.062 M/sec ( +- 0.02% ) (81.81%) 55,348,908 L1-dcache-load-misses # 0.29% of all L1-dcache hits ( +- 0.02% ) (81.81%) LLC-loads LLC-load-misses 14,366,129,596 L1-icache-loads # 1435.952 M/sec ( +- 0.01% ) (81.81%) 19,547 L1-icache-load-misses ( +- 1.33% ) (81.81%) 18,901,566,631 dTLB-loads # 1889.287 M/sec ( +- 0.02% ) (81.81%) 1,960,269 dTLB-load-misses # 0.01% of all dTLB cache hits ( +- 0.26% ) (81.83%) 19,819 iTLB-loads # 0.002 M/sec ( +- 0.91% ) (72.77%) 0 iTLB-load-misses # 0.00% of all iTLB cache hits (72.75%) L1-dcache-prefetches L1-dcache-prefetch-misses 10.003905246 seconds time elapsed ( +- 0.02% ) IPI中断和arch_timer 在跑DPDK的时候, 我在cmdline里面隔离了core isolcpus=4,5,30,31 然后在core4,5上跑DPDK单流单q的io fwd, 实际是core4跑命令行, core5跑pmd. 但发现core4和5都还有arch_timer和IPI5(IRQ work interrupts)中断, 并且core5上的IPI5中断快速增加, 停了DPDK后增长变慢. 通过linux代码得知irq_work_queue发IPI中断给自己, 主要的作用是提供一个在中断上下文运行work的手段 那么是谁调用了它, 又是干什么事情呢?用perf probe抓一下调用栈 sudo perf probe --add irq_work_queue sudo perf record -e probe:irq_work_queue -a -C5 -g -- sleep 10 sudo perf report -n 得到 + 68.32% pkt_burst_io_forward - 21.45% el0_irq gic_handle_irq __handle_domain_irq generic_handle_irq handle_percpu_devid_irq arch_timer_handler_phys hrtimer_interrupt __hrtimer_run_queues tick_sched_timer tick_sched_handle.isra.14 update_process_times scheduler_tick task_tick_fair irq_work_queue 这里解释一下, 因为是中断触发的, 第一行pkt_burst_io_forward和这个中断并没有绝对关系, 它是被打断的函数, 不知为何, 被perf当做调用函数了; 补充一下, IPI中断类型有 enum ipi_msg_type { IPI_RESCHEDULE, IPI_CALL_FUNC, IPI_CPU_STOP, IPI_CPU_CRASH_STOP, IPI_TIMER, IPI_IRQ_WORK, IPI_WAKEUP }; 结论 arch_timer和IPI5这两个中断导致的context-switches, kernel启动参数增加nohz_full和rcu_nocbs后, context-switches由原来的185次/秒减小到6次/秒 动态调频和中断 在kernel启动cmdline加isolcpus=31-37 nohz_full=31-37 rcu_nocbs=31-37会导致CPU无法正确动态调频, 比如跑pmd的core并没有满频率跑, 故可推断以上参数会把调频的driver的中断禁止掉, 导致频率不对. 此时需要强制最高频率: #查实际频率 sudo cpupower -c all frequency-info | grep \"current CPU frequency\" #强制最高频率, 重启会失效 sudo cpupower frequency-set -g performance 调试testpmd 现象 42上跑pktgen, 43上跑testpmd43上开了SR-IOV, 配了2个VF$ sudo bash -c \"echo 2 > /sys/class/net/enP5p1s0/device/sriov_numvfs\"然后43上跑testpmd, fwd为macswap模式 sudo ./build/app/testpmd -l 37-45 -n 6 -w 0005:01:00.0 txq_inline=256,rxq_cqe_comp_en=1,txqs_min_inline=8,txq_mpw_en=1 -- -i testpmd> set fwd macswap Set macswap packet forwarding mode testpmd> start 跑一会出现segment fault 打开coredump并用gdb调试 ulimit -c unlimited #把core文件写到/tmp下面 sudo bash -c 'echo \"/tmp/core-%e-%p-%t\" > /proc/sys/kernel/core_pattern' #gdb打开core文件 sudo gdb ./build/app/testpmd /tmp/core-lcore-slave-38-31988-1539675229 是数组越界吗? -- 第一次问 free[]是个数组, 程序在free[blk_n++] = m;出现段错误 但是gdb却告诉我们: 数组大小65535, 在访问1915个元素时出现问题, 似乎没有问题 考虑到65535个元素的数组还是比较大, 是不是栈不够了呢? (gdb) p elts_n $23 = 65535 (gdb) p blk_n $24 = 1915 看汇编 汇编 sp向下增长 free[]数组是在栈上的数组, 大小是运行时确定的. 新的C标准支持这种操作, 实现上大概是这样: 运行时算好偏移量, 比如上面代码中, x2就是偏移量, sp减去这个偏移量, 即sp向下增长出该数组大小的空间, 并把基地址保存在其他寄存器中, 比如上面代码中的x5. x5可以做为free[]数组的基地址. 在给数组赋值的过程中, 地址向上增长. 扩大栈 栈大小是在运行时决定的, readelf -a或objdump都看不到栈大小 修改栈大小可以调函数在代码里改: setrlimit(), 也可以用ulimit -s 16384改, 默认的栈大小是8192(即8M) 改大了还是出一样的问题 检查内存分布 用gdb的命令maintenance info sections可以看到程序的内存分布中间间杂的64K只读页(隔离页)就是保护栈越界的, 这里就可以看到它的作用. (gdb) p &free[1914] $5 = (struct rte_mbuf **) 0xffffb3420000 根据gdb的输出, 算一算每个section的大小看到出问题的地址0xffffb3420000是一个64K的只读页上, 这个不是栈的页, 它上面的16384K的页(们)才是栈说明栈溢出 cat | *\" '{printf \"%s %s %dK\\n\",$2,$3,(strtonum($3)-strtonum($2))/1024}' 0xffffb1410000->0xffffb2410000 at 0x07650000: load103 ALLOC LOAD HAS_CONTENTS 0xffffb2410000->0xffffb2420000 at 0x08650000: load104 ALLOC LOAD READONLY HAS_CONTENTS 0xffffb2420000->0xffffb3420000 at 0x08660000: load105 ALLOC LOAD HAS_CONTENTS 0xffffb3420000->0xffffb3430000 at 0x09660000: load106 ALLOC LOAD READONLY HAS_CONTENTS EOF 0xffffb1410000 0xffffb2410000 16384K 0xffffb2410000 0xffffb2420000 64K 0xffffb2420000 0xffffb3420000 16384K 0xffffb3420000 0xffffb3430000 64K 还是回到数组越界 回过头来再看gdb, 有两个疑点: 数组元素个数elts_n为65535, 但它是从const uint16_t elts_n = 1 elts_n;来, 不可能是奇数 sp地址和栈顶地址相差小于65535*8(free数组大小按65535算)# 栈顶地址是上面查到的栈地址空间的最大值 (gdb) p 0xffffb3420000-0xffffb341c430 $14 = 15312 到这里就很明显了, elts_n被踩, 实际数组并没有那么大. 实际被踩踏的栈很大, 直到踩到\"隔离\"页. 增大栈大小不管用, 也印证了这一点. 结合代码, 栈上的数据(elts_n就在栈上)被502行, 数组越界赋值所踩踏. free[blk_n++] = m; 记录一下gdb用到的命令 gdb ./build/app/testpmd (gdb) b mlx5_tx_complete (gdb) b mlx5_rxtx.h:498 #带条件的断点, GDB每次停下来判断一下, 不满足条件继续往下执行 (gdb) b mlx5_rxtx.h:498 if elts_n>512 #暂时禁止/打开全部断点 (gdb) disable/enable #暂时禁止/打开某个断点 (gdb) disable/enable 序号, 比如 1 2 3 # run命令接受被调程序的参数 (gdb) r -l 37-45 -n 6 -w 0005:01:00.0 txq_inline=256,rxq_cqe_comp_en=1,txqs_min_inline=8,txq_mpw_en=1 -- -i #寄存器名前面加$可以直接用 (gdb) p $x29 调试testpmd续 macswap段错误 在前面已经分析过, macswap段错误的原因是数组越界 这段程序负责回收mbuf, 根据txq->wqe_pi生产者的index, 找到txq->wqes里面对应的wqe, 其对应的硬件定义如下: struct mlx5_wqe_ctrl { uint32_t ctrl0; uint32_t ctrl1; uint32_t ctrl2; uint32_t ctrl3; } __rte_aligned(MLX5_WQE_DWORD_SIZE); elts_tail根据wqe的ctrl3得出 (gdb) p *(volatile struct mlx5_wqe_ctrl *)0xfff7a02b3e00 $52 = {ctrl0 = 251167745, ctrl1 = 81465856, ctrl2 = 134217728, ctrl3 = 63836} elts_tail = ctrl->ctrl3 //从硬件读出来的, 所谓的硬件是wqe elts_tail=63836 elts_free是上一次的txq->elts_tail 591行free数组越界导致段错误 通常情况下free和tail的增长情况 perf抓函数里面参数, 参考笔记profiling_perf命令备忘录 通过加打印:950行, 抓到的2次异常情况 testpmd> PANIC in mlx5_tx_complete_m(): Array index exceeding: elts_free:2685 elts_tail:1576 testpmd> PANIC in mlx5_tx_complete(): Array index exceeding: elts_free:40737 elts_tail:39488 下面想知道, 假定elts_tail发生了\"突变\", 那能抓到这次突变吗? 突变的上一次值是多少? 有什么规律? 运行sudo perf record -e probe_testpmd:mlx5_tx_complete_m -a -C 34 -- sleep 30抓30秒, 时间太长则数据量太大. 看运气, 多抓几次.或者: while test ! -z `pgrep testpmd`;do sudo perf record -e probe_testpmd:mlx5_tx_complete_m -a -C 34 -- sleep 30; done 抓到了, 最后一行 继续, 看代码继续往上找源头:sudo perf probe -x arm64-armv8a-linuxapp-gcc/app/testpmd --add 'mlx5_tx_complete_m+240 txq->wqe_pi:u16 elts_tail:u16' 继续找:cq_ci正常的, wqe_pi却不对了: sudo perf probe -x arm64-armv8a-linuxapp-gcc/app/testpmd --add 'mlx5_tx_complete_m+228 cq_ci:u16 txq->wqe_pi:u16 elts_tail:u16' lcore-slave-34 14766 [034] 251661.247316: probe_testpmd:mlx5_tx_complete_m: (701ed8) cq_ci=24176 cq_ci_u16=24177 wqe_pi=64394 elts_tail_u16=64024 lcore-slave-34 14766 [034] 251661.247321: probe_testpmd:mlx5_tx_complete_m: (701ed8) cq_ci=24177 cq_ci_u16=24178 wqe_pi=64408 elts_tail_u16=64056 lcore-slave-34 14766 [034] 251661.247329: probe_testpmd:mlx5_tx_complete_m: (701ed8) cq_ci=24178 cq_ci_u16=24179 wqe_pi=64159 elts_tail_u16=63456 修改栈大小 # 单位是1024, 这里是8192K # ulimit的单位一般都是1024, 详见help ulimit $ ulimit -s 8192 # 要改大点也可以 $ ulimit -s 16384 # 改完后用pmap可以看到原来8192的变成了16384 sudo pmap -p 13234 -x 0000ffffb5930000 128 128 0 r-x-- /usr/lib64/ld-2.17.so 0000ffffb5950000 64 64 64 r---- /usr/lib64/ld-2.17.so 0000ffffb5960000 64 64 64 rw--- /usr/lib64/ld-2.17.so 0000ffffddfd0000 192 128 128 rw--- [ stack ] # 似乎pmap的栈地址不准, 比如上面说栈是0000ffffddfd0000开始,192K # 但我实际调试时, 栈是下面16384K大小的空间 16384K: 0xffffb2420000->0xffffb3420000 at 0x08660000: load105 ALLOC LOAD HAS_CONTENTS 64K: 0xffffb3420000->0xffffb3430000 at 0x09660000: load106 ALLOC LOAD READONLY HAS_CONTENTS objdump汇编和C混合显示 #用objdump也可以混合显示，-S选项 objdump -dS luajit > luajit.dump coredump和GDB ulimit -c unlimited sudo bash -c 'echo \"/tmp/core-%e-%p-%t\" > /proc/sys/kernel/core_pattern' sudo gdb -tui ./build/app/testpmd /tmp/core-lcore-slave-38-41301-1539583957 查看优化level的详细开关 gcc -Q --help=optimizers -O2 > O2 gcc -Q --help=optimizers -O3 > O3 vimdiff O2 O3 "},"notes/profiling_调试和分析记录1.html":{"url":"notes/profiling_调试和分析记录1.html","title":"调试和分析记录1","keywords":"","body":" 调试usb错误 反向工程Anbox的abs命令 启动gdb uftrace使用 编译安装 使用 和flamegraph配合生成svg chrome tracing模式, \"代码界的示波器\" 实例, 分析redis redis benchmark(客户端)部分 redis server部分 调试程序卡住 pstack proc perf perf需要的内核配置 perf top __kernel_gettimeofday perf可以重复run一个程序, 并给出差异 frontend stall, backend stall perf -e perf top里面的children是什么意思? strace 跟踪多线程 调试usb错误 echo -n 'module xhci_hcd =p' > /sys/kernel/debug/dynamic_debug/control echo xhci-hcd >> /sys/kernel/debug/tracing/set_event 0) Reboot the machine in order to put it in a consistent state; 1) echo \"module xhci_hcd +flpt\" > /sys/kernel/debug/dynamic_debug/control 2) echo nop > /sys/kernel/debug/tracing/current_tracer 3) echo 81920 > /sys/kernel/debug/tracing/buffer_size_kb 4) echo 0 > /sys/kernel/debug/tracing/trace 5) echo 1 > /sys/kernel/debug/tracing/tracing_on 6) echo 1 > /sys/kernel/debug/tracing/events/xhci-hcd/enable After reproduce the issue, you should collect /sys/kernel/debug/tracing/trace. Problem is that the file might be huge, much larger than the kernel log you provided for instance. 反向工程Anbox的abs命令 Canonical 提供的Android container方案里面, 所有的命令都用二进制程序abs进行了封装, 包括setup, start, stop, 运行demo都在里面, 目的是对其内部操作和原理保密. 在此我们尝试对其反向工程, 目的是能够让我们有能力不依赖这个二进制程序, 能够手动运行cantainer的命令, 比如openarena的安装和demo的运行. 初步分析这个abs程序是用go语言写的. 但我自己对go语言也从未接触, 下面的内容有错误在所难免. 需要准备: gdb, ubuntu自带 反向的过程也是调试的过程, 需要安装golang的基础环境 apt install golang 参考网上文章 https://golang.org/doc/gdb http://web.cecs.pdx.edu/~jrb/cs201/lectures/handouts/gdbcomm.txt 启动gdb gdb abs因为GDB对go支持不是太好, 需要加载一个extention才能看go內建的数据结构.下面这个runtime-gdb.py就是.source /usr/share/go-1.6/src/runtime/runtime-gdb.py 另外为了打印好看些, 我一般喜欢 set print pretty on我们需要了解abs的大概符号表的情况, 所以列出所有函数info functions可以找到main打头的函数如下, abs的结构很简单: void main(void); void main.allocBinderNode(struct string); void main.bootstrapBaseImage(void); void main.cleanupAllContainers(void); void main.cleanupAllContainers.func1(struct string, map[string]interface {}); void main.createContainer(struct string); void main.ensureBinderNodes(void); void main.ensureStateDirExists(void); void main.ensureUserIsRoot(void); void main.forEachLxdContainer(func(string, map[string]interface {})); void main.init(void); void main.lxcCmd(struct []string, struct []uint8, error); void main.lxcCmdOrFail(struct []string); void main.main(void); void main.parallelize(struct []func()); void main.parallelize.func1(struct sync.WaitGroup *, func()); void main.removeContainer(struct string, map[string]interface {}); void main.removeDirAndItsContent(struct string, error); void main.restartAnbox(struct string); void main.runCleanup(void); void main.runInParallel(func(string)); void main.runInParallel.func1(struct string, map[string]interface {}); void main.runInParallel.func1.1(void); void main.runRestartAnbox(void); void main.runSetup(void); void main.runSetupDemo1(void); void main.runStart(void); void main.runStartDemo1(void); void main.runStop(void); void main.setupDemo1(struct string); void main.startContainer(struct string); void main.startDemo1(struct string); void main.stopContainer(struct string); void main.waitForAndroidToBoot(struct string); 我们重点关注main.lxcCmd, 用来发真正的命令给container, 用abs的stop命令试一下, 我们的基本判断是这个stop命令底层会向每个container实例发送lxc stop命令, 看看是不是: (gdb) b main.lxcCmd (gdb) r stop (gdb) bt #0 main.lxcCmd (args= []string = {...}, ~r1= []uint8, ~r2=...) at /tmp/tmp.E2wa2oqEu9/abs/abs.go:60 #1 0x00000000000c616c in main.forEachLxdContainer (run={void (struct string, map[string]interface {})} 0x4420071e18) at /tmp/tmp.E2wa2oqEu9/abs/abs.go:105 #2 0x00000000000c836c in main.runInParallel (action={void (struct string)} 0x4420071ee8) at /tmp/tmp.E2wa2oqEu9/abs/abs.go:360 #3 0x00000000000c8804 in main.runStop () at /tmp/tmp.E2wa2oqEu9/abs/abs.go:408 #4 0x00000000000c9838 in main.main () at /tmp/tmp.E2wa2oqEu9/abs/abs.go:504 (gdb) info args args = []string = {\"list\", \"--format=json\"} ~r1 = []uint8 ~r2 = { tab = 0x0, data = 0xbeafb } 可以看到首先会list出所有的container后面不断执行 c bt info args 可以看到完整的指令序列, 机器上只起了一个实例 \"list\", \"--format=json\" \"info\", \"abs-anbox-0\" \"stop\", \"-f\", \"abs-anbox-0\" ./abs -num-containers=1 setup的命令序列: \"list\", \"--format=json\" \"delete\", \"-f\", \"abs-anbox-base\" \"init\", \"anbox\", \"abs-anbox-base\", \"-s\", \"default\", \"-c\", \"security.nesting=true\" \"config\", \"set\", \"abs-anbox-base\", \"raw.lxc\", \"lxc.cap.drop=\" \"config\", \"set\", \"abs-anbox-base\", \"user.anbox.platform\", \"null\" \"config\", \"set\", \"abs-anbox-base\", \"user.anbox.gles_driver\", \"null\" \"config\", \"device\", \"add\", \"abs-anbox-base\", \"ashmem\", \"unix-char\", \"path=/dev/ashmem\", \"mode=0666\" \"config\", \"device\", \"add\", \"abs-anbox-base\", \"eth0\", \"nic\", \"nictype=bridged\", \"parent=lxdbr0\", \"name=eth0\" \"config\", \"device\", \"add\", \"abs-anbox-base\", \"binder\", \"unix-char\", \"path=/dev/binder\", \"mode=0666\" \"info\", \"abs-anbox-base\" \"start\", \"abs-anbox-base\" \"exec\", \"abs-anbox-base\", \"--\", \"pidof\", \"com.android.systemui\" \"exec\", \"abs-anbox-base\", \"--\", \"adb\", \"install\", \"/userdata/openarena.apk\" \"exec\", \"abs-anbox-base\", \"--\", \"adb\", \"shell\", \"pm\", \"grant\", \"ws.openarena.sdl\", \"android.permission.WRITE_EXTERNAL_STORAGE\" \"exec\", \"abs-anbox-base\", \"--\", \"adb\", \"shell\", \"pm\", \"grant\", \"ws.openarena.sdl\", \"android.permission.READ_EXTERNAL_STORAGE\" \"exec\", \"abs-anbox-base\", \"--\", \"adb\", \"shell\", \"pm\", \"grant\", \"ws.openarena.sdl\", \"android.permission.RECORD_AUDIO\" \"exec\", \"abs-anbox-base\", \"--\", \"mkdir\", \"-p\", \"/var/lib/anbox/data/media/0/Android/data/\" \"exec\", \"abs-anbox-base\", \"--\", \"tar\", \"xJf\", \"/userdata/openarena-data.tar.xz\", \"-C\", \"/var/lib/anbox/data/media/0/Android/data/\" \"exec\", \"abs-anbox-base\", \"--\", \"chown\", \"-R\", \"1023:1023\", \"/var/lib/anbox/data/media/0/Android/data/ws.openarena.sdl\" \"exec\", \"abs-anbox-base\", \"--\", \"adb\", \"shell\", \"mkdir\", \"-p\", \"/sdcard/Android/data/ws.openarena.sdl/files/.openarena/baseoa/demos/\" \"exec\", \"abs-anbox-base\", \"--\", \"adb\", \"push\", \"/userdata/demo0002.dm_71\", \"/sdcard/Android/data/ws.openarena.sdl/files/.openarena/baseoa/demos/demo0002.dm_71\" \"exec\", \"abs-anbox-base\", \"--\", \"adb\", \"shell\", \"am\", \"force-stop\", \"ws.openarena.sdl\" \"exec\", \"abs-anbox-base\", \"--\", \"adb\", \"shell\", \"am\", \"start\", \"ws.openarena.sdl/.MainActivity\" \"info\", \"abs-anbox-base\" \"stop\", \"-f\", \"abs-anbox-base\" \"config\", \"device\", \"remove\", \"abs-anbox-base\", \"binder\" \"info\", \"abs-anbox-base\" \"copy\", \"--container-only\", \"abs-anbox-base\", \"abs-anbox-0\" \"config\", \"device\", \"add\", \"abs-anbox-0\", \"binder\", \"unix-char\", \"source=/dev/binder1\", \"path=/dev/binder\", \"mode=0666\" ./abs start的命令序列: \"list\", \"--format=json\" \"start\", \"abs-anbox-0\" \"exec\", \"abs-anbox-0\", \"--\", \"pidof\", \"com.android.systemui\" ./abs start-demo1指令序列: \"list\", \"--format=json\" \"exec\", \"abs-anbox-0\", \"--\", \"adb\", \"shell\", \"am\", \"force-stop\", \"ws.openarena.sdl\" \"exec\", \"abs-anbox-0\", \"--\", \"adb\", \"shell\", \"am\", \"start\", \"ws.openarena.sdl/.MainActivity\" uftrace使用 uftrace能够按时间线记录每个函数执行时间https://github.com/namhyung/uftrace/wiki/Tutorial 编译安装 git clone https://github.com/namhyung/uftrace.git 在64K pagesize的系统上, 需要改下代码, 否则会有mprotect错误 diff --git a/libmcount/plthook.c b/libmcount/plthook.c index eb3c541..f88244c 100644 --- a/libmcount/plthook.c +++ b/libmcount/plthook.c @@ -26,7 +26,7 @@ static bool *plthook_dynsym_resolved; static unsigned long got_addr; static volatile bool segv_handled; -#define PAGE_SIZE 4096 +#define PAGE_SIZE getpagesize() #define PAGE_ADDR(addr) ((void *)((addr) & ~(PAGE_SIZE - 1))) static void segv_handler(int sig, siginfo_t *si, void *ctx) 如果需要看kernel的, 需要root运行, 带-k, 并且要求kernel有CONFIG_FUNCTION_GRAPH_TRACER=y make sudo make install 使用 被调试的程序需要用-pg选项来编译, 更多信息可搜索mcount -pg Generate extra code to write profile information suitable for the analysis program gprof. You must use this option when compiling the source files you want data about, and you must also use it when linking. Makefile加-pg之后, 就可以调试了: sudo /usr/local/bin/uftrace -F main record src/redis-server redis.conf 按时间线replay sudo /usr/local/bin/uftrace replay replay, 显示时间线 sudo /usr/local/bin/uftrace replay -f elapsed,duration,tid sudo /usr/local/bin/uftrace replay -t 5us 按占用时间report sudo /usr/local/bin/uftrace report 显示调用关系 sudo /usr/local/bin/uftrace graph 和perf基于采样的模型相比, 这个是完整时间线的, 所有的带-pg编译的函数都记录在案. 可以说uftrace更宏观. 每个函数都会被记录, 比如一个for循环里面调用memcpy函数, 时间是累加的. 和flamegraph配合生成svg sudo /usr/local/bin/uftrace dump --flame-graph | ~/yingjieb/git/FlameGraph/flamegraph.pl > redis.svg chrome tracing模式, \"代码界的示波器\" sudo /usr/local/bin/uftrace dump --chrome > redis.json 用chrome:tracing模式打开这个json文件 ALT+鼠标滚轮, zoom in and out w s 放大缩小, a d 左右 左上角问号是help 实例, 分析redis 起redis server, 然后用redis benchmark来做客户端压力测试 sudo /usr/local/bin/uftrace -d redis-server.data record src/redis-server redis.conf sudo /usr/local/bin/uftrace -d redis-benchmark.data record src/redis-benchmark -t set -n 10000 -r 100000000 sudo /usr/local/bin/uftrace dump --chrome -d redis-benchmark.data > redis-benchmark.json sudo /usr/local/bin/uftrace dump --chrome -d redis-server.data > redis-server.json 只关注23秒到25秒 sudo /usr/local/bin/uftrace dump --chrome -d redis-server.data -r 23s~25s > redis-server-2s.json redis benchmark(客户端)部分 用chrome打开这两个json文件, 先来看redis benchmark: 整体分为三部分: 先建立50个到server的连接, 并加入epoll epoll_wait来等待事件, random来生成key, value默认用xxx, 先50个write, 等50个read, 刚开始是很整齐的. 这说明这是个同步的过程, 50个client, 每个client都是发送request, 等待server回复, 再发下一个request. 到后期因为server端调度的关系, 这50个client的读写混在一起了, 但对每个client来说, 还是同步的. Clients and Servers are connected via a networking link. Such a link can be very fast (a loopback interface) or very slow (a connection established over the Internet with many hops between the two hosts). Whatever the network latency is, there is a time for the packets to travel from the client to the server, and back from the server to the client to carry the reply. This time is called RTT (Round Trip Time). It is very easy to see how this can affect the performances when a client needs to perform many requests in a row (for instance adding many elements to the same list, or populating a database with many keys). For instance if the RTT time is 250 milliseconds (in the case of a very slow link over the Internet), even if the server is able to process 100k requests per second, we'll be able to process at max four requests per second. 所以这里就要说一下pipeline, redis支持pipeline, 比如-P 16, 是说一次网络来回里面, 执行16个command, 这比默认的一个来回一个command要高效的多, 除了节省网络来回的时间, 还节省了socket的read write开销, 通常这些开销包括用户态和内核态的切换开销. 每个处理都用gettimeofday来打时间戳 都处理完毕, close这些连接, 并按latency排序 redis server部分 从宏观上看, server在24秒的时候开始真正干活 前面是初始化, 然后epoll等待, 直到client发出10000个set请求 初始化内容包括解析命令行, server配置, lua脚本引擎, 共享对象, 起块存储bio线程(三个), 监听服务端口, 从磁盘加载数据等等, 初始化里面有很多zmalloc/zrealloc调用, 其中有一些竟然有pthreadmutex_lock, 而其他没有![](img/profiling调试和分析记录1_20221019112837.png) 在aeProcessEvents里面处理事件, 先accept TCP的连接, 应该是50个(对应client起50个并发)sudo /usr/local/bin/uftrace replay -f elapsed,duration,tid -d redis-server.data -F acceptTcpHandlertcpnodelay keepalive setsockopt aeCreateFileEvent zmalloc listcreate dictcreate readQueryFromClient一共调用了10050次, 应该是处理每个client的set请求的核心函数, 而每个epollwait后, 一般情况下, 都处理50个Query, 和client的并发数一致. 但偶尔有例外. ![](img/profiling调试和分析记录1_20221019112921.png) 每个Query都先从socket read, gettimeofday, malloc内存, 创建string对象, 字符串操作, lookup, hash, dbAdd, 移除过期的, 准备个client的reply到链表 这次的50个set请求全部处理完毕后, reply记录到一个链表里面, 然后马上发送回client 最后把这50个连接close掉 调试程序卡住 pstack 当一个程序卡住不动, 用pstack来看他当前的调用栈 比如: $ pstack 33451 #0 0x0000ffff8a694f34 in __accept_nocancel () from /lib64/libc.so.6 #1 0x0000000000401d94 in tcp_accept () #2 0x0000000000401a1c in server_main () #3 0x0000000000401adc in main () 其实pstack只是个脚本, 它用的是gdb, 就是下面的效果 /usr/bin/gdb --quiet -nx /proc/33451/exe 33451 在gdb里执行 bt gdb prog progID和gdb -p progID是一样的 没有pstack用下面命令也行: gdb -batch -ex bt -p 1234 proc /proc/PID/stack里面有kenel的栈 $ cat /proc/33451/stack [] __switch_to+0x6c/0x78 [] inet_csk_accept+0x24c/0x2b0 [] inet_accept+0x50/0x168 [] SyS_accept4+0xf4/0x1e8 [] SyS_accept+0x34/0x40 [] el0_svc_naked+0x24/0x28 [] 0xffffffffffffffff perf perf默认是per-thread mode, 在这个thread运行时计数, 不运行时不计数. 加-a表示统计整个系统 cycle是CPU运行时的cycle, 不计idle用-p attach 一个进程, 默认统计这个进程的所有线程和子进程用-t attach一个线程perf report -k 指定vmlinux, 用来解析内核perf top -C 指定core, \"Z\"键刷新perf record -c 1000, 是指这个event每1000次记录一次, 因为全记录开销大. perf需要的内核配置 For kernel analysis, I'm using CONFIG_KPROBES=y and CONFIG_KPROBE_EVENTS=y, to enable kernel dynamic tracing, and CONFIG_FRAME_POINTER=y, for frame pointer-based kernel stacks. For user-level analysis, CONFIG_UPROBES=y and CONFIG_UPROBE_EVENTS=y, for user-level dynamic tracing. perf top perf top -g可以带调用栈, 但默认是按children累加模式排序, 用下面的命令可以按self排序我一般习惯是加--no-children sudo perf top -g --no-children sudo perf top -g --no-children -p `pidof redis-server` __kernel_gettimeofday So __kernel_gettimeofday being called by AW is actually the VDSO code for ARM64. If we look at that function, we see 3 potentially ugly ops (2 DMBs and an ISB). The profile hit data suggest that the ISB is hurting us the most (we sampled at the mrs 92% of the time but it’s most likely the ISB that is the gate). I’m not sure if any of this code can be cleaned up:We started to look at this test and before getting to deep into the analysis, we looked at the profiles of the x86 2699v4 system (on the left side of the screen capture below) and Amberwing (on the right)What caught our attention was X86’s use of the __vdso_gettimeofday function. I’m hoping some of the kernel experts on copy can shed light on whether we support VDSO (Virtual Dynamic Shared Object) on AW. My understanding is that this is a far more efficient way of system calling some commonly used kernel system call function like gettimeofday. I found the following link on the web which helps explain VDSO:http://www.linuxjournal.com/content/ creating-vdso-colonels-other-chickenThe link talks about use of glibc but I’m more interested in the hacking of the kernel approach which appears to be the method used by x86. IN short: a program can make a syscall and not have to endure the overhead of the memory-hopping between user and kernel segments that a traditional syscall would require. Do we have such support? If not, is this something we should be providing. perf可以重复run一个程序, 并给出差异 注意下面, -a表示统计整个系统, 不加默认的是统计后面command $ sudo perf stat -r 5 -a sleep 1 Performance counter stats for 'system wide' (5 runs): 48055.429090 cpu-clock (msec) # 47.417 CPUs utilized ( +- 0.01% ) 364 context-switches # 0.008 K/sec ( +- 7.83% ) 7 cpu-migrations # 0.000 K/sec ( +- 16.08% ) 26 page-faults # 0.001 K/sec ( +- 8.77% ) 137,412,037 cycles # 0.003 GHz ( +- 10.30% ) 57,080,600 instructions # 0.42 insn per cycle ( +- 22.67% ) 3 branches # 0.000 K/sec ( +- 44.72% ) 398,150 branch-misses # 13271653.33% of all branches ( +- 13.29% ) 1.013468517 seconds time elapsed ( +- 0.08% ) $ sudo perf stat -r 5 sleep 1 Performance counter stats for 'sleep 1' (5 runs): 0.605980 task-clock (msec) # 0.001 CPUs utilized ( +- 2.47% ) 1 context-switches # 0.002 M/sec 0 cpu-migrations # 0.000 K/sec 54 page-faults # 0.089 M/sec ( +- 1.08% ) 1,502,877 cycles # 2.480 GHz ( +- 2.48% ) 757,929 instructions # 0.50 insn per cycle ( +- 2.06% ) 0 branches # 0.000 K/sec 15,060 branch-misses # 0.00% of all branches ( +- 1.94% ) 1.001679779 seconds time elapsed ( +- 0.04% ) frontend stall, backend stall frontend stall and backend stall performance counters.这个信息在kernel4.6上, 用perf stat能看出来. frontend stall说的是分支预测方面的指标, 提示了icache的问题 backend stall说的是data cache方面的指标, 提示数据miss之类的. perf -e perf stat -e instructions -p xxx -r37见手册21.6 DAB PMU Enumerations perf top里面的children是什么意思? self是本函数执行的时间, 不包括本函数调用的子函数.children 是本函数时间加上所有的子函数时间, 就是说, 由这个函数调用开去的所有时间都算上. Let me show you an example: $ cat abc.c #define barrier() asm volatile(\"\" ::: \"memory\") void a(void) { int i; for (i = 0; i main -> c -> b -> a callchain show up in the output. Finally, it looks like below with both option enabled: strace 跟踪多线程 strace -f strace可以跟踪perl脚本，使用方法是直接strace xxx.perl "},"notes/调试和分析工具概览.html":{"url":"notes/调试和分析工具概览.html","title":"profiling和debugging工具相关","keywords":"","body":"eBPF和uprobe在调试用户态程序的异同 uprobe方法本质上是代码注入, 在被probe进程的虚拟内存上, 把用户态代码替换成breakpoint指令, 以及注册相应的handler来完成调试任务. handler运行的就是被注入的代码. eBPF本质是内核解释器执行脚本, 在解释器的JIT环境下执行. 目前我的理解是: uprobe等probe方法更暴力, 需要将代码\"注入\"到被调试进程空间. 而eBPF通过解释执行的方法, 更安全, 更可控, 但可能效率低一点. 调试手段性能损耗对比: 调试工具图解 性能观测工具 性能评测工具 性能优化工具 "},"notes/debugging_gdb备忘录.html":{"url":"notes/debugging_gdb备忘录.html","title":"gdb备忘录","keywords":"","body":" gdb tui模式 gdb看所有线程的调用栈 远程调试app, 用gdbserver和gdb远程调试板子 远程调试uboot, 用jtag做gdb server 远程调试, over串口 gdb调试nand gdb基本 查看当前要执行的行 临时变量 自动显示变量, 每次c后或n后, 自动显示其值 按结构体方式查看内存地址 显示内存内容 计算数值, 16进制显示 指定显示格式 执行任意函数 查看结构体定义 汇编和C混合显示 info命令族 加入libc符号表 调试模块, 加载符号表 查看地址处的代码 显示内存布局 gdb tui模式 tui模式下更像个ide调试界面, 信息更丰富. #可以先gdb program core来启动, 在gdb里面用layout命令来显示源码 asm和reg窗口 #Ctrl + x，再按a：回到传统模式，即退出layout，回到执行layout之前的调试窗口 #Ctrl + x, o: 切换激活窗口, 切换到命令窗口可以上下翻历史命令 layout split layout src layout reg #刷新窗口 refresh 或Ctrl + L #上一条命令快捷键: Ctrl+p gdb看所有线程的调用栈 (gdb) info threads (gdb) thread apply all bt full 远程调试app, 用gdbserver和gdb远程调试板子 板子上配好ip: 192.168.2.12 #直接带好参数 ~ # gdbserver :9123 /isam/user/eoe_filter -n eth0 -t tap-fwd -E 我在mint上, 用sshfs mount了服务器的buildroot目录; 所以我在mint上就能操作: export PATH=$PATH:~/work/share/buildroot73/output/host/opt/ext-toolchain/bin #cd到被调试的app目录下 yingjieb@yingjieb-VirtualBox ~/work/share/buildroot73/output/build/isam-linux-target-apps-22ef847b216e5d579c016ed9dd9795d393b56001 #这个eoe_filter是带符号表的 #能自动找到sysroot:/home/yingjieb/work/share/buildroot73/output/host/opt/ext-toolchain/mips64-octeon-linux-gnu/sys-root/ Linux Mint 19.1 Tessa $ mips64-octeon-linux-gnu-gdb eoe_filter #板子上的程序会停在__start, __start是lib32/ld.so.1的符号 (gdb) target remote 192.168.2.12:9123 (gdb) handle SIG42 nostop noprint (gdb) set print pretty on #先断点到这个函数: (gdb) b openTapItf #继续执行直到断点 (gdb) c #结合代码, 我要看tap设备是否创建成功: (gdb) b eoe_filter.c:116 #在另外一个窗口ip a能看到tap-fwd (gdb) info locals #再往下跟, 发现是子线程调用了exit(), 导致tap设备被注销了. (gdb) next 远程调试uboot, 用jtag做gdb server cd /repo2/yingjieb/cavium/sdk31_508/usr/local/Cavium_Networks/OCTEON-SDK/tools/bin //gdb调试elf文件 mipsisa64-octeon-elf-gdb /repo2/yingjieb/u-boot-octeon-sdk3.1/u-boot-octeon_fpxtb //启动空的gdb后, 再加载elf文件 file /repo2/yingjieb/u-boot-octeon-sdk3.1/u-boot-octeon_fpxtb //可以设置architecture, 如果直接加载elf文件则不用设置 set architecture mips:octeon3 //启动远程调试 target remote 135.251.9.60:30000 //使用monitor指令 monitor help monitor B::break.set 0xc00a2ff8 /program //查看程序地址? info line eth_send //设置程序参数 gdb --args prog args set args --ddr0spd=fpxtbspd --ddr_clock_hz=667000000 b init_octeon3_ddr3_interface 远程调试, over串口 在板子上: board: stty -F /dev/ttyS0 115200 stty -F /dev/ttyS0 460800 stty -F /dev/ttyS0 -a ( taskset 1 gdbserver /dev/ttyS0 ./isam_app useSTDIO) & 在服务器上: ASBLX28:/repo/yingjieb/fdt063/sw/vobs/esam/build/reborn/buildroot-isam-reborn-cavium-fgltb/output/host/usr/bin $ mips64-octeon-linux-gnu-gdb /repo/yingjieb/fdt063/sw/vobs/esam/build/fglt-b/OS/application/isam_app.nostrip #这个135.251.199.198:2009是串口服务器地址 (gdb) target remote 135.251.199.198:2009 (gdb) set sysroot /repo/yingjieb/fdt063/sw/vobs/esam/build/reborn/buildroot-isam-reborn-cavium-fgltb/output/staging (gdb) set remotetimeout 60 (gdb) handle SIG42 nostop noprint (gdb) set print pretty on (gdb) detach gdb调试nand target: devmem 0x1070000000500 64 0 devmem 0x1070000000508 64 0 devmem 0x1070000000510 64 0 devmem 0x1070000000518 64 0 echo ttyS1 > /sys/module/kgdboc/parameters/kgdboc echo g > /proc/sysrq-trigger 从kgdb切换到kdb, 盲敲$3#33 host: mips64-octeon-linux-gnu-gdb vmlinux target remote 135.251.199.198:2114 set print pretty on set sysroot /repo/yingjieb/fdt063/sw/vobs/esam/build/reborn/buildroot-isam-reborn-cavium-fgltb/output/staging set remotetimeout 20 detach bt c gdb基本 查看当前要执行的行 frame, 简写f 临时变量 set $i=\"hello\" 自动显示变量, 每次c后或n后, 自动显示其值 display 变量名 按结构体方式查看内存地址 p *(struct mtd_info *)0x80000000885dc018 (gdb) p ((struct txq *)0xffff8119f800)->elts_n 显示内存内容 #从priv->data这个地址开始, 显示2048个单位(2048), 每个单位一个字节(b), 按16进制显示(x) x/2048xb priv->data 计算数值, 16进制显示 (gdb) p /x 0xffff90c8c430+1914*8 $7 = 0xffff90c90000 指定显示格式 #有时候, gdb不能准确显示一个uint16_t类型的变量的值, 比如 (gdb) p nb_pkt_per_burst $19 = 65568 #此时需要用x加FMT来看, 8xh是说显示8个, 安装16进制halfword显示 (gdb) x/8xh &nb_pkt_per_burst 0x9f0c36 : 0x0020 0x0001 0x0000 0x0000 0x0000 0x0040 0x0000 0x0000 #如果知道这个变量是uint16_t, 下面的也可以; 相当于x/1dh (gdb) x/dh &nb_pkt_per_burst 0x9f0c36 : 32 执行任意函数 (gdb) p system(\"nandtest -k -o0x0 -l0x20000 /dev/mtd6\") $1 = 1 查看结构体定义 (gdb) ptype trx_sys_t 汇编和C混合显示 (gdb) disassemble /m function (gdb) disassemble /m lj_meta_lookup (gdb) disassemble /m dp_netdev_input__ info命令族 #源码信息 info source #当前局部变量 info locals info files info threads info sharedlibrary info signals info line info tasks info variables info symbol info stack info registers #还有不少 help info 加入libc符号表 #必须使/repo/yingjieb/glibc-2.16.0能够被板子访问到 gdb oflt.nostrip set args useSTDIO set print pretty on set sysroot /repo/yingjieb/fdt063/sw/vobs/esam/build/reborn/buildroot-isam-reborn-cavium-fgltb/output/staging #dir的意思是增加源文件的search path到gdb dir /repo/yingjieb/glibc-2.16.0 dir /repo/yingjieb/glibc-2.16.0/nptl dir /repo/yingjieb/glibc-ports-2.16.0 handle SIG42 nostop noprint b cs_dpll_resetTest b oak_spi_datatransfer b board_commands.c:153 b fork.c:133 b do_system 调试模块, 加载符号表 #加载ko符号表 /sys/module/spi_oak_island/sections # cat .text 0xffffffffc0002000 /sys/module/spi_oak_island/sections # cat .data 0xffffffffc0000000 (gdb) add-symbol-file /repo/yingjieb/fdt063/sw/vobs/esam/build/reborn/buildroot-isam-reborn-cavium-fgltb/output/build/isam-linux-drivers-custom/misc/spi-oak-island.ko 0xffffffffc0002000 -s .data 0xffffffffc000000 查看地址处的代码 # source file and line number for an instruction address info line *0x # source lines around an instruction address list *0x # assembly instructions at an address disas 0x, or x/20i 0x 显示内存布局 #显示所有内存section info files #比上面命令显示更全 maintenance info sections "},"notes/profiling_perf命令备忘录.html":{"url":"notes/profiling_perf命令备忘录.html","title":"perf命令备忘录","keywords":"","body":" perf ftrace 看一个内核函数运行时调用的所有函数 看一个内核函数被调用的情况: 什么时间, 什么进程 和trace-cmd的对比 perf查看OVS转发路径执行时间 perf probe实现机制初探 源代码 entry.S 跟踪do_debug_exception 时钟中断的调用路径 perf trace perf作为kprobe/uprobe前端 用户态probe, 带参数, 带返回值 内核态probe, 指定vmlinux, 带参数带返回值 perf调度时间 perf会用中断 perf的几种模式 perf应用首先查统计 perf记录 perf只看某个共享库的采样 perf使用的内核符号路径 perf probe内核函数, 访问结构体变量成员 perf函数带参数 perf函数内部变量 perf event 说明 perf ftrace perf ftrace是对ftrace的function_graph和function的包装. 运行结束时reset ftrace的配置. perf ftrace -t : 可以是function_graph或function -p : PID, 多pid用逗号隔开 -a : system wide. 意思是后面跟了command, 通常只会perf这个command的事件, 但加-a可以打开整个系统的事件. 没有command一般要加-a -C : cpu号, 多cpu用逗号. 比如1-2,5,7-9 -T : 对哪个函数做trace, 支持通配符. 可以多个-T联用. 传递给set_ftrace_filter -N : 不对哪个函数做trace. 也可以多个联用. 传递给set_ftrace_notrace -G : 对function_graph有效, trace函数调用的子函数. 可以多个联用. 传递给set_graph_function -g : 和-G相反 -D : function graph的深度 看一个内核函数运行时调用的所有函数 # 看do_notify_resume都调用了哪些函数及其子函数, 10秒后输出结果到stdout sudo perf ftrace -t function_graph -a -G do_notify_resume -- sleep 10 # 部分结果 0) | do_notify_resume() { 0) ==========> | 0) | gic_handle_irq() { 0) | handle_IPI() { 0) | irq_enter() { 0) 0.350 us | rcu_irq_enter(); 0) 2.200 us | } 0) | __wake_up() { 0) | __wake_up_common_lock() { 0) 0.200 us | _raw_spin_lock_irqsave(); 0) 0.250 us | __wake_up_common(); 0) 0.150 us | _raw_spin_unlock_irqrestore(); 0) 5.350 us | } 0) 6.950 us | } 0) | irq_exit() { 0) 0.200 us | idle_cpu(); 0) 0.200 us | rcu_irq_exit(); 0) 3.550 us | } 0) + 18.350 us | } 0) + 20.550 us | } 0) 看一个内核函数被调用的情况: 什么时间, 什么进程 # 看哪个进程调用了do_notify_resume sudo perf ftrace -t function -a -T do_notify_resume -- sleep 10 # 部分结果 perf-2129 [025] d... 2009.215155: do_notify_resume -2130 [024] d... 2009.215231: do_notify_resume 和trace-cmd的对比 trace-cmd也能完成上面两项任务, 比如sudo trace-cmd stream -p function -l irqfd_inject sleep 10和sudo trace-cmd stream -p function_graph -g do_notify_resume sleep 1 perf ftrace目前只支持function和function_graph, 而trace-cmd还支持其他event联用, 比如: sudo trace-cmd record -p function -l irqfd_inject -e sched -T sleep 10 sudo trace-cmd record -p function_graph -g do_notify_resume -e sched -T sleep 1 sudo trace-cmd report --cpu 0 perf查看OVS转发路径执行时间 场景是两个VM互相ping, 一秒一次. OVS走dpdk的vhost接口. 那么转发时间, 也就是报文在OVS路径下的延迟, 是从netdev_rxq_recv收包, 到netdev_send结束的时间. 上面两个函数是通过OVS代码转发流程来的, 代码要熟, 详见OVS架构和代码 下面用perf记录这个时间: # 首先看看有没有这两个函数 $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd -F | grep netdev_send netdev_send netdev_send_wait $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd -F | grep netdev_rxq_recv netdev_rxq_recv # 有的, 增加动态probe点 # 收包函数 $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd --add netdev_rxq_recv # 发包函数, 这里的%return表示要probe这个函数返回点 $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd --add netdev_send%return # 记录30秒, -R表示记录所有打开的counter(默认是tracepoint counters) $ sudo perf record -e probe_ovs:netdev_rxq_recv -e probe_ovs:netdev_send -R -t 38726 -- sleep 30 # 看结果 sudo perf script | less perf probe实现机制初探 在上面例子中, 令人惊讶的是, 在perf record期间, ping的延迟增加15%, 不是非常大. 下图是在上面一个perf运行期间, 再起一个perf命令 sudo perf record -C 13 -g -- sleep 60得出的; 是的, 用另一个perf来看本perf的运行情况! sudo perf script | less其中work_pending函数看起来是个入口函数, 根据probe的原理: 替换被probe指令为break, 那work_pending应该是个异常处理函数? 源代码 perf probe -L work_pending提示在arch/arm64/kernel/entry.S:915这个还是比较准的, 如果用cscope工具会找到include/linux/workqueue.h里面的work_pending宏... 为什么说perf probe -L准呢?因为一般同一个源文件的符号相近, 如果看kernel所有符号/proc/kallsyms的话, 会发现它附近的符号都是在entry.S里面的. $ sudo cat /proc/kallsyms | grep -n3 work_pending 58-ffff0000080834b4 t el0_error_naked 59-ffff0000080834d0 t ret_fast_syscall 60-ffff0000080835c8 t ret_fast_syscall_trace 61:ffff0000080835d0 t work_pending 62-ffff0000080835e0 t ret_to_user 63-ffff0000080835f0 t finish_ret_to_user 64-ffff000008083700 t el0_svc entry.S 我们看到, 整个文件只有ret_fast_syscall和ret_to_user调用了work_pending 看代码, 应该走ret_to_user, 重点看el0_dbg, 因为它出现在调用栈里 应该是断点指令触发异常, 到异常入口el0_dbg, el0_dbg先调用do_debug_exception, 然后在ret_to_user里面, 调用的work_pending, 再往下执行, 如上面调用栈所示. 跟踪do_debug_exception 一般的, 跟踪一个函数的执行及其调用子函数的流程, 用sudo trace-cmd stream -p function_graph -g function_name -P PID 但do_debug_exception有点特殊, 会提示错误: $ sudo trace-cmd stream -p function_graph -g do_debug_exception -P 38726 trace-cmd: Invalid argument Failed to write do_debug_exception to set_graph_function. Perhaps this function is not available for tracing. run 'trace-cmd list -f do_debug_exception' to see if it is. 因为代码里, do_debug_exception有NOKPROBE_SYMBOL(do_debug_exception);声明: 同文件里的show_pte函数, 是个普通函数, 就能正常执行sudo trace-cmd stream -p function_graph -g show_pte -P 38726 尝试用systemtap看inf->fn是哪个函数, 思路是先要得到运行时fn的地址, 然后用symname解析符号表得到函数名: # 先查具体函数位置 $ stap -l 'kernel.function(\"do_debug_exception\")' kernel.function(\"do_debug_exception@arch/arm64/mm/fault.c:843\") # 能够看到$info:struct siginfo结构体 $ stap -L 'kernel.statement(\"do_debug_exception@arch/arm64/mm/fault.c:862\")' kernel.statement(\"do_debug_exception@arch/arm64/mm/fault.c:862\") $addr:long unsigned int $esr:unsigned int $regs:struct pt_regs* $inf:struct fault_info const* $info:struct siginfo # 用systemtap的内建函数symname()得到符号名; 但还是有错误! $ sudo stap -e 'probe kernel.statement(\"do_debug_exception@arch/arm64/mm/fault.c:862\") {printf(\"%-16s %6d [%03d] %s %24s \",execname(),tid(),cpu(),usecs_to_string(gettimeofday_us()),probefunc()) printf(\":: %s\\n\", symname($inf->fn))}' WARNING: probe kernel.statement(\"do_debug_exception@arch/arm64/mm/fault.c:862\") (address 0xffff00000808128c) registration error (rc -22) 最后还是有错误: 注册probe时错误. 根据我现在的理解, 原因可能是: 在entry.S中, el0_dbg调用的do_debug_exception, 已经是在breakpoint的异常处理了; 而stap尝试再次注册一个动态probe, 那么需要替换目标地址的指令为break, 如果可以的话, 会造成breakpoint异常处理无限嵌套, 即处理break异常的函数再次触发break异常. 时钟中断的调用路径 从结果里还能找到arch_timer中断的记录 perf trace perf trace和strace功能类似, 据说性能影响更小, 输出格式也更整齐一些, 便于查看; 于strace相比, 不足之处是没有参数解析 perf trace默认带时间戳, 并有每条系统调用的耗时标记, 相比之下, strace -tt也能带时间戳, 但没有耗时标记. sudo perf trace taskset -c 1 echo aaa 对比sudo strace -tt taskset -c 1 echo aaa perf作为kprobe/uprobe前端 比如下面这个程序 #include #include #include static int func_1_cnt; static int func_2_cnt; static void func_1(void) { func_1_cnt++; } static void func_2(void) { func_2_cnt++; } int main(int argc, void **argv) { int number; while(1) { sleep(1); number = rand() % 10; if (number 用户态probe, 带参数, 带返回值 用perf可以这么调试: gcc -g -o test test.c #动态probe点, 底层是uprobe实现 perf probe -x ./test func_2_entry=func_2 #带返回值的函数 perf probe -x ./test func_2_exit=func_2%return perf probe -x ./test test_15=test.c:15 #带局部变量的probe点 perf probe -x ./test test_25=test.c:25 number perf record -e probe_test:func_2_entry -e probe_test:func_2_exit -e probe_test:test_15 -e probe_test:test_25 ./test 内核态probe, 指定vmlinux, 带参数带返回值 #用-k指定vmlinux perf probe -k vmlinux kfree_entry=kfree perf probe -k vmlinux kfree_exit=kfree%return #可以用-s指定kernel源码目录 perf probe -s ./ kfree_mid=mm/slub.c:3408 x perf record -e probe:kfree_entry -e probe:kfree_exit -e probe:kfree_mid sleep 10 perf调度时间 #先record sudo perf sched record -a -C 34 -- sleep 60 #按进程显示sched事件的汇总, 比如ping进程运行了多长时间, 平均调度延迟多少, 最大调度延迟多少. sudo perf sched latency #perf sched底层使用kernel的sched系列静态probe点, 比如sched:sched_switch, sched:sched_wakeup等 #显示详细的sched事件 sudo perf sched script #timehist分析上面的事件, 变成更友好的逐条显示 sudo perf sched timehist perf会用中断 /proc/interrupts显示, perf过程中, 会使用GICv3 23 Level arm-pmu 比如我对core 5做perf: perf -a -C 5, 那core 5就会看到很多arm-pmu的中断, 大约3000个/秒 perf的几种模式 perf可以有几种监视模式 按进程: perf默认监视进程的所有子进程和线程, fork()和pthread_create()都自动继承perf_events用-p就是这种模式, perf直接启动某个program也是这种模式;选项-i关闭监视子进程和线程. 按线程: 线程活动时, 才监视; 线程被调度出去时, 停止监视; 线程在CPU间迁移时, 统计会被保存然后被新CPU重新恢复.用-t来使用这种模式 按CPU: 监视CPU上面跑的所有线程用-a监视所有CPU, 也可以监视某几个CPU, 比如-a -C 0,2-3 perf应用首先查统计 #硬件统计 sudo perf stat -a -d -d -d -e cpu-clock -e cycles -e instructions -e stalled-cycles-backend -e stalled-cycles-frontend -e branch-misses -e branch-loads -e L1-dcache-store-misses -e L1-dcache-stores -e l2cache_0/total-requests/ -C 5 -- sleep 3 #默认详细统计 sudo perf stat -a -d -d -d -C 5 -r 6 -- sleep 10 #反复跑6次 sudo perf stat -a -d -d -e cycles -e cpu-clock -e instructions -e stalled-cycles-backend -e stalled-cycles-frontend -e branch-misses -e branch-loads -e l2cache_0/total-requests/ -C 5 -r 6 -- sleep 10 perf记录 perf只看某个共享库的采样 下面的命令只看线程号是2019(libvirtd线程)的主体进程和libvirt.so共享库的采样, 去掉了libc, libpthread, 和kernel相关的采样. # --dsos是共享对象的列表, --comms是主命令列表 sudo perf top -e cycles:u -t 2019 --dsos libvirtd,libvirt.so.0.4006.0 perf使用的内核符号路径 /usr/lib/debug/lib/modules/4.14.62-5.hxt.aarch64/vmlinux perf probe内核函数, 访问结构体变量成员 perf也可以probe内核函数 # 查看源码, 与stap不同的是, perf probe -L 从函数名开始记行数 $ sudo perf probe -L irqfd_inject # 查看变量 $ sudo perf probe -V irqfd_inject:6 # 添加probe点 sudo perf probe --add 'irqfd_inject+36 work:x64 kvm:x64' # 可以访问结构体成员, 很nice! sudo perf probe --add 'irqfd_inject+36 work:x64 kvm->userspace_pid:x64' # 甚至可以访问2级结构体引用, 牛! 按照C的语法来的, 指针用->, 否则用. sudo perf probe --add 'irqfd_inject+36 work:x64 kvm->arch.max_vcpus:x64' # 根据提示, 开始record sudo perf record -e probe:irqfd_inject -a sleep 10 # 解析记录 sudo perf script | less perf函数带参数 #比如我想看testpmd里面, mlx5_tx_burst_mpw的参数 $ sudo perf probe -x arm64-armv8a-linuxapp-gcc/app/testpmd -V mlx5_tx_burst_mpw Available variables at mlx5_tx_burst_mpw @ (unknown_type dpdk_txq struct rte_mbuf** pkts uint16_t pkts_n #增加动态函数点, 带参数 $ sudo perf probe -x arm64-armv8a-linuxapp-gcc/app/testpmd --add 'mlx5_tx_burst_mpw pkts_n' Added new event: probe_testpmd:mlx5_tx_burst_mpw (on mlx5_tx_burst_mpw in /home/bai/share/repo/hxt/mainstream-dpdk-stable/arm64-armv8a-linuxapp-gcc/app/testpmd with pkts_n) You can now use it in all perf tools, such as: perf record -e probe_testpmd:mlx5_tx_burst_mpw -aR sleep 1 #记录5分钟, 实际验证对性能的影响比gdb小很多, 在64B小包100%线速发包的时候, 转发性能降低50%; 在50%线速发包的时候, 对转发性能几乎没影响; 如果用gdb打断点加if的方式, 性能影响非常大, 慢百倍. sudo perf record -e probe_testpmd:mlx5_tx_burst_mpw -a -C 34 -- sleep 300 #分析, 能显示参数pkts_n #perf script查看每次的记录;这个文件太大, 用less; --ns是用ns显示时间 sudo perf script --ns | less #perf report能根据pkts_n排序, 很强大 sudo perf report perf函数内部变量 #先列出可以probe的变量, -L表示--line; mlx5_tx_complete其实是个inline的函数 sudo perf probe -x arm64-armv8a-linuxapp-gcc/app/testpmd -L mlx5_tx_complete #看第38行的可选变量 sudo perf probe -x arm64-armv8a-linuxapp-gcc/app/testpmd -V mlx5_tx_complete:38 #因为inline的关系, 这个函数在不同的调用函数里都出现了, 这里只列出一个. @ struct mlx5_txq_data* txq struct rte_mbuf** free struct rte_mempool* pool uint16_t cq_ci uint16_t elts_free uint16_t elts_m uint16_t elts_n uint16_t elts_tail unsigned int blk_n #add这个probe点要用以下格式, 详见man perf probe [[GROUP:]EVENT=]FUNC[@SRC][:RLN|+OFFS|%return|;PTN] [ARG ...] 其中: ARG: [NAME=]LOCALVAR|$retval|%REG|@SYMBOL[:TYPE] TYPE可以比如: u8/u16/u32/u64/s8/s16/s32/s64, x8/x16/x32/x64, 或string #增加probe点 $ sudo perf probe -x arm64-armv8a-linuxapp-gcc/app/testpmd --add 'mlx5_tx_complete_m+328 elts_free elts_tail' #更改一下, 用u16表示 $ sudo perf probe -x arm64-armv8a-linuxapp-gcc/app/testpmd --add 'mlx5_tx_complete_m+328 elts_free:u16 elts_tail:u16' #开始record, 观察到dpdk转发性能直接下降为3%, 非常大! #注: 性能下降取决于这个event被触发了多少次. 我这个probe点正好在这个函数里面的while循环里 sudo perf record -e probe_testpmd:mlx5_tx_complete_m -a -C 34 -- sleep 30 #逐条记录看 sudo perf script | less #截取一部分, 可以看到基本上是每个event都捕捉的 elts_free_u16=65408 elts_tail_u16=65440 elts_free_u16=65440 elts_tail_u16=65472 elts_free_u16=65472 elts_tail_u16=65504 elts_free_u16=65504 elts_tail_u16=0 elts_free_u16=0 elts_tail_u16=32 elts_free_u16=32 elts_tail_u16=72 elts_free_u16=72 elts_tail_u16=104 elts_free_u16=104 elts_tail_u16=144 elts_free_u16=144 elts_tail_u16=200 elts_free_u16=200 elts_tail_u16=232 elts_free_u16=232 elts_tail_u16=292 elts_free_u16=292 elts_tail_u16=344 elts_free_u16=344 elts_tail_u16=376 后记 #为了能抓到这两个变量, 我在代码里while之前加了nop指令; 而之前的触发点是在while循环里 asm volatile(\"nop\"); 再进行record时, 由于event触发频率大大减小, dpdk转发性能下降到50%; 对比之前是3% 运行record过程中, htop可以看到有内核态程序开始占用CPU, 我估计是内核态相应PMU中断(该中断由程序\"断点\"硬件触发), 在中断里记录函数变量占用的时间 perf event 说明 man perf_event_open #火焰图 sudo perf record -g ping 5.5.5.1 sudo perf script | ~/repo/FlameGraph/stackcollapse-perf.pl | ~/repo/FlameGraph/flamegraph.pl > ping-localhost.svg #动态probe点, 可以加在你的app里, 也可以加在内核里 sudo perf probe -x `which ovs-vswitchd` --add netdev_linux_tap_batch_send sudo perf probe --add dev_hard_start_xmit #record+动态probe点, -e添加你关注的统计 sudo perf record -e probe_ovs:netdev_linux_send -e probe_ovs:netdev_linux_tap_batch_send -e probe:dev_hard_start_xmit -e net:net_dev_start_xmit -p 3470 -g -o perf-ping22.data -- sleep 30 #perf也能看符号表, 跟readelf差不多 perf probe -x /lib64/libibverbs.so.1 -F perf probe -x `which ovs-vswitchd` -F | grep ibv #列出动态probe点 $ sudo perf probe --list probe_testpmd:mlx5_tx_burst_mpw (on mlx5_tx_burst_mpw+64@drivers/net/mlx5/mlx5_rxtx.c in /home/bai/share/repo/hxt/mainstream-dpdk-stable/arm64-armv8a-linuxapp-gcc/app/testpmd with pkts_n) #删除动态点 sudo perf probe -d probe_testpmd:mlx5_tx_burst_mpw #-n显示采样个数 sudo perf report -n #查看详细cache统计 sudo perf stat -d -d -d -C 38 #记录线程30秒 sudo perf record -g -t 3546 -- sleep 30 #记录核5 10秒, run 6次并显示差值 sudo perf stat -a -d -d -d -C 5 -r 6 -- sleep 10 # probe组, 用大括号括起来; 后面加:S是说组长(第一个成员)做为采样方式, 详见man perf list sudo perf probe --add irq_work_queue sudo perf top -e '{cycles,probe:irq_work_queue}:S' -a -C5 -g --no-children sudo perf record -e probe:irq_work_queue -a -C5 -g -- sleep 10 sudo perf report -n "},"notes/profiling_perf命令备忘录2.html":{"url":"notes/profiling_perf命令备忘录2.html","title":"perf命令备忘录2","keywords":"","body":" 命令记录 统计系统调用次数 添加符号表 背景 MIPS板子解析用户态调用栈要用-g --call-graph dwarf 给perf script添加符号表 拷贝script后的文件到PC机, 生成火焰图 命令记录 #使用perf的filter功能. 详见 调试和分析记录 -- perf使用实例 perf record -e syscalls:sys_enter_write -aR -g --call-graph dwarf --filter 'fd == 2' -- sleep 30 统计系统调用次数 perf stat就是统计 perf stat --log-fd 1 -e 'syscalls:sys_enter_*' -p 5464 -- sleep 10 | egrep -v \"[[:space:]]+0\" 添加符号表 背景 在板子上运行的perf record, 采样并记录程序运行时的PC地址. 这时是不需要符号表的. 在用perf script解析时, 要用符号表. 如果当时运行的app被strip过了, 是没有符号表的.没有符号表强行解析, 结果是不准的. MIPS板子解析用户态调用栈要用-g --call-graph dwarf #对pid为18852 记录60秒, 频率是1000HZ, 也就是1ms一次采样 #cycles:u表示只对用户态采样, 也可以用cycles:k只对内核态采样 perf record -F 1000 -e cycles:u -g --call-graph dwarf -p 18852 -- sleep 60 给perf script添加符号表 用--symfs指定个目录, perf script会当这个目录为/来找符号表 #record命令生成perf.data, 是perf script默认的data文件 #要成功运行下面的命令, 要把没有被strip的app, 按照根目录的相对位置, 放到--symfs下面 #我这里用buildroot的output/staging目录 sshfs wsfs user@ip:/buildroot_poc/output/host/mips64-buildroot-linux-gnu/sysroot #可以先用这个命令找到需要哪些符号表 perf script | grep unknown | sort -u #因为我们用了--symfs, 它就不在默认的路径下找符号表了 #包括kernel的符号表也就找不到了; 所以要用--kallsyms 配合使用 perf script --symfs wsfs --kallsyms /proc/kallsyms > bsfs/sysidle-switch_hwa_app/perf-u.script 拷贝script后的文件到PC机, 生成火焰图 cat perf.script | ~/repo/FlameGraph/stackcollapse-perf.pl | ~/repo/FlameGraph/flamegraph.pl > sample.svg "},"notes/profiling_perf学习笔记之实战篇.html":{"url":"notes/profiling_perf学习笔记之实战篇.html","title":"perf学习笔记之实战篇","keywords":"","body":" 分析实战 预备知识strace 理解pthread_create mutex竞争 用perf分析 分析实战 如何分析我的misc/atomic-test/pthread_mutex.c 这个文件根据core个数起相应的线程, 这些线程都要对一个全局变量操作, 这些操作用锁保护. 预备知识strace 比如在一个窗口sleep 666在另外一个窗口strace这个pid然后在sleep窗口ctrl+c, 此时strace能捕捉到这个signal # strace -p32611 Process 32611 attached restart_syscall( ) = ? ERESTART_RESTARTBLOCK (Interrupted by signal) --- SIGINT {si_signo=SIGINT, si_code=SI_KERNEL} --- +++ killed by SIGINT +++ 当一个系统调用正在执行, 比如select, 当其他进程也调用了系统调用, strace会记录select被打断又恢复的过程 [pid 28772] select(4, [3], NULL, NULL, NULL [pid 28779] clock_gettime(CLOCK_REALTIME, {1130322148, 939977000}) = 0 [pid 28772] ) = 1 (in [3]) 当一个正在执行的syscall被signal打断时, 当signal处理完毕后, 这个被打断的syscall会重新执行. strace也会记录这个过程 read(0, 0x7ffff72cf5cf, 1) = ? ERESTARTSYS (To be restarted) --- SIGALRM (Alarm clock) @ 0 (0) --- rt_sigreturn(0xe) = 0 read(0, \"\"..., 1) = 0 -tt选项可以打印时间, 精确到ms; -T可以打出每个系统调用的时间 -e trace=all是默认的, 可以写trace=open,close,read,write或file, 或process, network, signal, ipc, memory等. 理解pthread_create 这里要用到strace, -o选项是输出到文件, -f是跟踪线程, -T是看看每个系统调用的运行时间# strace -T -f -o strace.a.out ./a.out 下面分析这个strace的结果: pthread_create实际是调用的clone这里的主进程/线程是9229, 这里的clone传的参数很多, 比如CLONE_THREAD, 一共要clone4个线程, 在这里是:9230 9231 9232 9233算上主进程, 这里一共有5个进程 9229 clone(child_stack=0x7f78c9e39ff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7f78c9e3a9d0, tls=0x7f78c9e3a700, child_tidptr=0x7f78c9e3a9d0) = 9230 9229 clone(child_stack=0x7f78c9638ff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7f78c96399d0, tls=0x7f78c9639700, child_tidptr=0x7f78c96399d0) = 9231 获取线程调用的是gettid() 用sched_getaffinity()绑定线程 有很多系统调用被打断, 又恢复运行9231 ) = 0 9230 ) = 9230 9229 ) = 0 9231 gettid( 9230 sched_getaffinity(9229, 128, 9229 clone( 9231 ) = 9231 9230 {f}) = 8 9231 sched_getaffinity(9229, 128, {f}) = 8 9230 sched_setaffinity(9230, 128, {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0} 9232 set_robust_list(0x7f78c8e389e0, 24 9231 sched_setaffinity(9231, 128, {2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0} 9232 ) = 0 9230 ) = 0 9229 child_stack=0x7f78c8e37ff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7f78c8e389d0, tls=0x7f78c8e38700, child_tidptr=0x7f78c8e389d0) = 9232 9232 gettid( mutex竞争 9230 9231 9232 9233这四个线程(对kenel来说就是4个进程), 抢mutex for (i = 0; i 这个东西是调用的pthread_mutex_lock void mutex_enter(mutex_t *m) pthread_mutex_lock(&g_mutex); void mutex_exit(mutex_t *m) pthread_mutex_unlock(&g_mutex); 在linux系统下, 这实际是futex()系统调用(好像只有linux有)这里op一般是 FUTEX_WAIT: 等uaddr地址的值为wal FUTEX_WAKE: 唤醒 值得注意的是: 对独立空间的n个进程来说, 这个uaddr不一定一样, 但如果物理地址是一样的(比如通过共享内存技术共享数据的几个独立进程), futex()在内核态好像认为是同一个锁. 对线程来说, 这个uaddr肯定是一样的. 我们这个例子里是一样的. #include #include int futex(int *uaddr, int op, int val, const struct timespec *timeout, int *uaddr2, int val3); 你会发现很多syscall unfinished, 然后resumed;这是因为strace在跟踪syscall的时候, 一个线程的syscall A还没执行完, 另一个线程的syscall B又开始了, 这时候strace会报告说A还没有完, B就已经在运行了. 注意A并没有停止, 而是一直运行的, 因为我这个是4核的CPU.如果是单核的CPU, 在多线程情况下, 因为进程抢占的关系, kernel在执行一个syscall, 也可能被其他线程抢占;这时候其实strace的打印和多核CPU是差不多的, 不同的是, 被抢占的syscall真的就被暂停了. 114 9270 futex(0x6020c0, FUTEX_WAIT_PRIVATE, 2, NULL 115 9269 futex(0x6020c0, FUTEX_WAKE_PRIVATE, 1 116 9270 ) = -1 EAGAIN (Resource temporarily unavailable) 117 9269 ) = 0 118 9271 futex(0x6020c0, FUTEX_WAKE_PRIVATE, 1 119 9270 futex(0x6020c0, FUTEX_WAIT_PRIVATE, 2, NULL 120 9268 futex(0x6020c0, FUTEX_WAKE_PRIVATE, 1 121 9271 ) = 0 122 9270 ) = -1 EAGAIN (Resource temporarily unavailable) 123 9268 ) = 0 124 9271 futex(0x6020c0, FUTEX_WAIT_PRIVATE, 2, NULL 125 9270 futex(0x6020c0, FUTEX_WAIT_PRIVATE, 2, NULL 126 9268 futex(0x6020c0, FUTEX_WAIT_PRIVATE, 2, NULL 127 9269 futex(0x6020c0, FUTEX_WAKE_PRIVATE, 1 128 9268 ) = -1 EAGAIN (Resource temporarily unavailable) 129 9269 ) = 1 130 9271 ) = 0 131 9268 futex(0x6020c0, FUTEX_WAKE_PRIVATE, 1 132 9271 futex(0x6020c0, FUTEX_WAKE_PRIVATE, 1 133 9270 ) = 0 134 9271 ) = 0 135 9270 futex(0x6020c0, FUTEX_WAKE_PRIVATE, 1 用perf分析 有了上面基本的理解, 下面就开始用核武器perf了. 如果说strace提供了一个整体流程的视图, 那么perf提供了所有片段的细节. 这就好比一幅画, strace是轮廓骨架, perf就是上面的每一处细节. # ./perf top -gp `pidof a.out` 任何用户态的进程, 都要通过system_call来和kernel交互, 这里就截取了system_call的调用情况:perf top说明有56.08%的采样都落在了system_call里面, 也就是说, 这个程序的内核态执行时间为56.08%而system_call函数本身只花了4.67%的时间.其中竟然wake花的时间被wait还多? glibc是通过__lll_lock_wait来实现mutex的, 底层是futex系统调用(只限于linux) 既然是mutex, 那么竞争不到锁就得休眠, 这时候就要看schedule函数的调用情况了:schedule及其子函数共用了1.98%, 其中99.28%是发生在一个叫futex_wait_queue_me里面 一旦这个锁可用, 怎么唤醒线程的呢?这里有个sysret_check, 对应system_call, 好像是从内核态返回用户态会调. 搜索关键词wake可以找到这个东东futex_wake, 似乎是在do_futex里面调用的, 应该是资源可用以后才调它. 这个应该更详细点 我还看到了wake_up_process的身影.- 0.00% 0.00% [kernel] [k] wake_up_process 最后给出这个程序总的执行时间:这期间CPU比较高 下面来看看把mutex改成spinlock的时间, pthread同样提供了spinlock: pthread_spin_lock() 执行期间CPU都打满了400%, 而之前mutex时, CPU是300% 最后要说的是, 像这种一个变量加减的问题, 不需要锁. gcc有内置的原子操作:https://gcc.gnu.org/onlinedocs/gcc-4.9.2/gcc/_005f_005fatomic-Builtins.html#_005f_005fatomic-Builtins 用这个方法, 程序效率得到了显著提升 这个是线程体的代码: 这个是锁的代码: 再补充一下: 如果用下面的while来实现mutex, 结果如何呢?while(os_atomic_test_and_set_ulint(&m->lock, 1)); 竟然让人大跌眼镜:(太差了吗) "},"notes/profiling_perf学习笔记之入门篇.html":{"url":"notes/profiling_perf学习笔记之入门篇.html","title":"perf学习笔记之入门篇","keywords":"","body":" perf初体验 perf简介 列出事件 perf top 事件计数 perf stat 系统剖析 perf record 静态剖析 perf record -e 动态剖析 perf probe 剖析结果 关于符号 关于调用栈 -g 使用举例 事件 举例 CPU剖析 硬件事件举例 静态perf 统计新进程的创建 调查谁调用了connect()系统调用 调查sk buffer的使用 动态跟踪 Kernel: tcp_sendmsg() 带size的Kernel: tcp_sendmsg() 用户态的malloc ping的usage跟踪 带变量的ping 查看系统调用次数 misc perf wiki里的例子 https://perf.wiki.kernel.org/index.php/Tutorial perf初体验 早上专家使用perf来分析mysql的延时问题, 用perf做系统分析简直是神奇的体验, 和以前的土办法相比, 是从盲人摸象到全球GPS. perf简介 perf是基于event的, 可以是硬件的一些perf events, 也可以是软件的一些桩.它用来解决以下问题: 哪些代码占用CPU最多? 代码调用路径是什么? 哪些代码路径产生了L2 cache miss 哪些代码路径正在申请内存, 申请了多大? 什么触发了一次TCP传输? 一个特定的内核函数是否在使用? 使用频率是多少? 一个线程离开这个CPU的原因是什么? perf是内核提供的, 在tools/perf下面, 需要ftrace支持. 相关的内核文档在tools/perf/Documentation 列出事件 perf支持多种event类型, 用perf list可以列出来. 比如列出所有调度相关的事件 perf list 'sched:*' perf top # Show system calls by process, refreshing every 2 seconds: # 实时显示perf top, 显示次数-n, 按comm排序-s perf top -e raw_syscalls:sys_enter -ns comm 事件计数 perf stat # CPU counter statistics for the specified command: 统计一个命令的执行情况, 不带-a, 带-a表示要统计整个系统 perf stat command # Detailed CPU counter statistics (includes extras) for the specified command: 更详细的统计 perf stat -d command # CPU counter statistics for the specified PID, until Ctrl-C: 统计某个进程 perf stat -p PID # CPU counter statistics for the entire system, for 5 seconds: 统计整个系统, 注意这是-a的意义 perf stat -a sleep 5 # Various basic CPU statistics, system wide, for 10 seconds: 统计某些event, 整个系统 perf stat -e cycles,instructions,cache-references,cache-misses,bus-cycles -a sleep 10 # Various CPU level 1 data cache statistics for the specified command: 对某个命令统计L1相关的东东 perf stat -e L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores command # Various CPU data TLB statistics for the specified command: 对某个命令统计TLB相关的东东 perf stat -e dTLB-loads,dTLB-load-misses,dTLB-prefetch-misses command # Various CPU last level cache statistics for the specified command: LLC是系统的最后一级cache perf stat -e LLC-loads,LLC-load-misses,LLC-stores,LLC-prefetches command # Count system calls for the specified PID, until Ctrl-C: 统计某个进程的所有系统调用 perf stat -e 'syscalls:sys_enter_*' -p PID # Count system calls for the entire system, for 5 seconds: 统计整个系统的系统调用 perf stat -e 'syscalls:sys_enter_*' -a sleep 5 # Count scheduler events for the specified PID, until Ctrl-C: 统计某个进程的调度事件 perf stat -e 'sched:*' -p PID # Count scheduler events for the specified PID, for 10 seconds: 统计某个进程的调度事件10秒钟, 这么说perf的对象是-p的进程而不是后面的命令? perf stat -e 'sched:*' -p PID sleep 10 # Count ext4 events for the entire system, for 10 seconds: 统计整个系统的ext4事件, 10秒 perf stat -e 'ext4:*' -a sleep 10 # Count block device I/O events for the entire system, for 10 seconds: 统计整个系统的block事件, 10秒 perf stat -e 'block:*' -a sleep 10 系统剖析 perf record # Sample on-CPU functions for the specified command, at 99 Hertz: 99hz记录一个命令 perf record -F 99 command # Sample on-CPU functions for the specified PID, at 99 Hertz, until Ctrl-C: 99hz记录一个进程 perf record -F 99 -p PID # Sample on-CPU functions for the specified PID, at 99 Hertz, for 10 seconds: 99hz记录一个进程10秒 perf record -F 99 -p PID sleep 10 # Sample CPU stack traces for the specified PID, at 99 Hertz, for 10 seconds: 99hz记录一个进程10秒, 带调用栈 perf record -F 99 -p PID -g -- sleep 10 # Sample CPU stack traces for the PID, using dwarf to unwind stacks, at 99 Hertz, for 10 seconds: 99hz记录一个进程10秒, 带调用栈(dwarf方式) perf record -F 99 -p PID -g dwarf sleep 10 # Sample CPU stack traces for the entire system, at 99 Hertz, for 10 seconds: 99hz记录整个系统10秒, 带调用栈 perf record -F 99 -ag -- sleep 10 # Sample CPU stack traces for the entire system, with dwarf stacks, at 99 Hertz, for 10 seconds: 99hz记录整个系统10秒, 带调用栈(dwarf方式) perf record -F 99 -ag dwarf sleep 10 # Sample CPU stack traces, once every 10,000 Level 1 data cache misses, for 5 seconds: 记录L1 cache miss, 每10000次miss采样一次, 整个系统5秒, 带调用栈 perf record -e L1-dcache-load-misses -c 10000 -ag -- sleep 5 # Sample CPU stack traces, once every 100 last level cache misses, for 5 seconds: 采样LLC miss, 100次sample一次, 5秒 perf record -e LLC-load-misses -c 100 -ag -- sleep 5 # Sample on-CPU kernel instructions, for 5 seconds: 采样整个系统的内核态热点代码, 5秒 perf record -e cycles:k -a -- sleep 5 # Sample on-CPU user instructions, for 5 seconds: 采样整个系统的用户态热点代码, 5秒 perf record -e cycles:u -a -- sleep 5 # Sample on-CPU instructions precisely (using PEBS), for 5 seconds: 不知道这个PEBS是啥东东 perf record -e cycles:p -a -- sleep 5 # Perform branch tracing (needs HW support), for 1 second: 采样分支, 1秒 perf record -b -a sleep 1 静态剖析 perf record -e # Trace new processes, until Ctrl-C: 采样新的进程, 整个系统 perf record -e sched:sched_process_exec -a # Trace all context-switches, until Ctrl-C: 采样所有的上下文切换, 整个系统 perf record -e context-switches -a # Trace all context-switches with stack traces, until Ctrl-C: 采样所有的上下文切换, 整个系统, 带调用栈 perf record -e context-switches -ag # Trace all context-switches with stack traces, for 10 seconds: 采样所有的上下文切换, 整个系统, 带调用栈, 10秒 perf record -e context-switches -ag -- sleep 10 # Trace CPU migrations, for 10 seconds: 采样任务在CPU间的migration, 10秒 perf record -e migrations -a -- sleep 10 # Trace all connect()s with stack traces (outbound connections), until Ctrl-C: 采样整个系统的connect调用, 即outbound连接 perf record -e syscalls:sys_enter_connect -ag # Trace all accepts()s with stack traces (inbound connections), until Ctrl-C: 采样整个系统accept调用, 即inbound连接 perf record -e syscalls:sys_enter_accept* -ag # Trace all block device (disk I/O) requests with stack traces, until Ctrl-C: 采样整个系统的IO请求? perf record -e block:block_rq_insert -ag # Trace all block device issues and completions (has timestamps), until Ctrl-C: 采样整个系统的IO issue? perf record -e block:block_rq_issue -e block:block_rq_complete -a # Trace all block completions, of size at least 100 Kbytes, until Ctrl-C: 利用filter功能, 保留100K字节的, 但是谁知道这里的filter里面应该写什么???? perf record -e block:block_rq_complete --filter 'nr_sector > 200' # Trace all block completions, synchronous writes only, until Ctrl-C: 利用filter, 只看同步写 perf record -e block:block_rq_complete --filter 'rwbs == \"WS\"' # Trace all block completions, all types of writes, until Ctrl-C: 利用filter, 只看写 perf record -e block:block_rq_complete --filter 'rwbs ~ \"*W*\"' # Trace all minor faults (RSS growth) with stack traces, until Ctrl-C: RSS growth是啥? perf record -e minor-faults -ag # Trace all page faults with stack traces, until Ctrl-C: 页缺失? perf record -e page-faults -ag # Trace all ext4 calls, and write to a non-ext4 location, until Ctrl-C: 把输出文件改到非ext4的地方 perf record -e 'ext4:*' -o /tmp/perf.data -a # Trace kswapd wakeup events, until Ctrl-C: kswapd是啥? perf record -e vmscan:mm_vmscan_wakeup_kswapd -ag 动态剖析 perf probe # Add a tracepoint for the kernel tcp_sendmsg() function entry (\"--add\" is optional): 添加内核tcp_sendmsg()函数到跟踪点 perf probe --add tcp_sendmsg # Remove the tcp_sendmsg() tracepoint (or use \"--del\"): 删除刚才加的tcp_sendmsg() perf probe -d tcp_sendmsg # Add a tracepoint for the kernel tcp_sendmsg() function return: 还是tcp_sendmsg(), 在跟踪点加到return perf probe 'tcp_sendmsg%return' # Show available variables for the kernel tcp_sendmsg() function (needs debuginfo): 显示tcp_sendmsg()的参数 perf probe -V tcp_sendmsg # Show available variables for the kernel tcp_sendmsg() function, plus external vars (needs debuginfo): 显示tcp_sendmsg()的参数, 还有外部变量, 会显示很多全局变量 perf probe -V tcp_sendmsg --externs # Show available line probes for tcp_sendmsg() (needs debuginfo): 显示tcp_sendmsg()里可用的行号, 难道还不是所有行都能用? perf probe -L tcp_sendmsg # Show available variables for tcp_sendmsg() at line number 81 (needs debuginfo): 显示tcp_sendmsg()的81行处的变量 perf probe -V tcp_sendmsg:81 # Add a tracepoint for tcp_sendmsg(), with three entry argument registers (platform specific): 再加三个参数寄存器? 比如X86的 ax dx cx perf probe 'tcp_sendmsg %ax %dx %cx' # Add a tracepoint for tcp_sendmsg(), with an alias (\"bytes\") for the %cx register (platform specific): bytes就用cx的东西? perf probe 'tcp_sendmsg bytes=%cx' # Trace previously created probe when the bytes (alias) variable is greater than 100: 难道filter里面是入参? perf record -e probe:tcp_sendmsg --filter 'bytes > 100' # Add a tracepoint for tcp_sendmsg() return, and capture the return value: 这个实用, 跟踪tcp_sendmsg()的返回, 并捕捉返回值 perf probe 'tcp_sendmsg%return $retval' # Add a tracepoint for tcp_sendmsg(), and \"size\" entry argument (reliable, but needs debuginfo): 加上入参size perf probe 'tcp_sendmsg size' # Add a tracepoint for tcp_sendmsg(), with size and socket state (needs debuginfo): 同时捕捉sk->__sk_common.skc_state, 这个sk也是个入参 perf probe 'tcp_sendmsg size sk->__sk_common.skc_state' # Tell me how on Earth you would do this, but don't actually do it (needs debuginfo): 这个也有用, 详细显示这个命令到底搞了些什么玩意儿, 但不真正做. perf probe -nv 'tcp_sendmsg size sk->__sk_common.skc_state' # Trace previous probe when size is non-zero, and state is not TCP_ESTABLISHED(1) (needs debuginfo): 还是利用filter, 过滤入参的某些条件 perf record -e probe:tcp_sendmsg --filter 'size > 0 && skc_state != 1' -a # Add a tracepoint for tcp_sendmsg() line 81 with local variable seglen (needs debuginfo): 加某个函数的某行, 并加个变量 perf probe 'tcp_sendmsg:81 seglen' # Add a tracepoint for do_sys_open() with the filename as a string (needs debuginfo): 带文件名, 字符串格式. 结果好像也没多什么 perf probe 'do_sys_open filename:string' # Add a tracepoint for myfunc() return, and include the retval as a string: 返回值为字符串 perf probe 'myfunc%return +0($retval):string' # Add a tracepoint for the user-level malloc() function from libc: 加libc里面的malloc perf probe -x /lib64/libc.so.6 malloc # List currently available dynamic probes: 列出新加的probe点 perf probe -l 剖析结果 # Show perf.data in an ncurses browser (TUI) if possible: perf report # Show perf.data with a column for sample count: perf report -n # Show perf.data as a text report, with data coalesced and percentages: perf report --stdio # List all raw events from perf.data: 配合perf record -e选项使用 perf script # List all raw events from perf.data, with customized fields: 指定某些域 perf script -f time,event,trace # Dump raw contents from perf.data as hex (for debugging): perf script -D # Disassemble and annotate instructions with percentages (needs some debuginfo): perf annotate --stdio 关于符号 perf需要符号表来把地址转成符号,所以需要为一些软件安装符号包, 比如openssh-server-dbgsym libc6-dbgsym coreutils-dbgsym 编内核的时候需要CONFIG_KALLSYMS 如果是自己编译的软件, 要保证最后的elf不能被strip 对于JIT虚拟机, perf也是支持的, 但需要JIT自己维护一个map表 关于调用栈 -g 编译的时候要带上frame pointers选项, 这个是gcc的选项-fno-omit-frame-pointer, 这个默认是关闭的(-O2) 内核也需要打开帧选项CONFIG_FRAME_POINTER=y, 这时可以看到整个的调用路径 补充: CONFIG_DEBUG_FS CONFIG_KPROBE_EVENT CONFIG_KPROBES 这个也要打开, 才能使用probe功能 Tracers里面的东东也打开一些 关于用户态的调用栈, 如果没有编帧指针, 可以加-g dwarf, 这个东东利用了libunwind来解析用户态调用栈. 使用举例 Performance counter summaries, including IPC, for the gzip command: 统计摘要 # perf stat gzip largefile Count all scheduler process events for 5 seconds, and count by tracepoint: 统计5秒内的线程调度 # perf stat -e 'sched:sched_process_*' -a sleep 5 Trace all scheduler process events for 5 seconds, and count by both tracepoint and process name: report方式统计5秒内的线程调度 # perf record -e 'sched:sched_process_*' -a sleep 5 # perf report Trace all scheduler process events for 5 seconds, and dump per-event details: 同上, 显示的更详细? # perf record -e 'sched:sched_process_*' -a sleep 5 # perf script Trace read() syscalls, when requested bytes is less than 10: 统计字节小于10的read()系统调用 # perf record -e 'syscalls:sys_enter_read' --filter 'count 事件 事件分三种: 硬件事件: 比如CPU cycle, 内存停顿sycle, L2 miss 软件事件: 各种kernel计数器, 比如任务迁移计数 跟踪点事件: 基于ftrace框架, 功能强大, 比如统计系统调用, TCP事件, 文件系统IO, 磁盘IO, 和几乎每个内核函数的callback. 这里的事件按组命名, \"sock:\" 套接字事件, \"sched:\" 调度事件.对这些事件的perf, 能够显示很多细节的东西, 比如时间戳, 调用路径等等. 另外, 还有两种事件: profiling事件: 用perf record -FHz来做CPU利用率的剖析. 动态跟踪事件: 可以制定任意的函数来跟踪, 在内核态基于kprobes, 用户态基于uprobes. 举例 用gzip压缩一个110M的vmlinux GG bin # cp /usr/src/linux/vmlinux . GG bin # ./perf stat -d gzip vmlinux Performance counter stats for 'gzip vmlinux': 9662.335548 task-clock (msec) # 0.999 CPUs utilized 32 context-switches # 0.003 K/sec 0 cpu-migrations # 0.000 K/sec 128 page-faults # 0.013 K/sec 0 cycles # 0.000 GHz stalled-cycles-frontend stalled-cycles-backend 37,714,770,423 instructions 8,249,209,924 branches # 853.749 M/sec 302,481,164 branch-misses # 3.67% of all branches 7,449,830,657 L1-dcache-loads # 771.018 M/sec 1,350,272,478 L1-dcache-load-misses # 18.12% of all L1-dcache hits LLC-loads LLC-load-misses 9.676657443 seconds time elapsed CPU剖析 # perf record -F 99 -a -g -- sleep 30 # perf report --stdio 硬件事件举例 # perf record -e L1-dcache-load-misses -c 10000 -ag -- sleep 5 静态perf 统计gzip的系统调用, 注意这里的重定向2>&1, 否则awk抓不到东西.默认perf的输出应该是stderr, 而awk的输入是stdout GG bin # ./perf stat -e 'syscalls:sys_enter_*' gzip vmlinux -fk 2>&1 | awk '$1 != 0' Performance counter stats for 'gzip vmlinux -fk': 1 syscalls:sys_enter_utimensat 1 syscalls:sys_enter_unlink 3 syscalls:sys_enter_newfstat 1 syscalls:sys_enter_lseek 3,800 syscalls:sys_enter_read 3,692 syscalls:sys_enter_write 1 syscalls:sys_enter_access 1 syscalls:sys_enter_fchmod 1 syscalls:sys_enter_fchown 5 syscalls:sys_enter_open 7 syscalls:sys_enter_close 4 syscalls:sys_enter_mprotect 1 syscalls:sys_enter_brk 1 syscalls:sys_enter_munmap 4 syscalls:sys_enter_rt_sigprocmask 12 syscalls:sys_enter_rt_sigaction 1 syscalls:sys_enter_exit_group 8 syscalls:sys_enter_mmap 9.798567700 seconds time elapsed 注: 用strace -c也可以统计系统调用, 但是用perf的overhead要低很多, 因为perf的buffer data都在kernel里. strace是用ptrace attach到被跟踪的线程上的, 类似于debugger, 代价很高. 下面是性能的对比:可以看到, perf是原来的2.5X, 而strace是62X. # dd if=/dev/zero of=/dev/null bs=512 count=10000k 5242880000 bytes (5.2 GB) copied, 3.53031 s, 1.5 GB/s # perf stat -e 'syscalls:sys_enter_*' dd if=/dev/zero of=/dev/null bs=512 count=10000k 5242880000 bytes (5.2 GB) copied, 9.14225 s, 573 MB/s # strace -c dd if=/dev/zero of=/dev/null bs=512 count=10000k 5242880000 bytes (5.2 GB) copied, 218.915 s, 23.9 MB/s 统计新进程的创建 GG bin # ./perf record -e sched:sched_process_exec -a Lowering default frequency rate to 3200. Please consider tweaking /proc/sys/kernel/perf_event_max_sample_rate. 注: 这个命令会在前台独占, 所以要测试比如man ls, 需要在另外的窗口执行.执行完, 把这个前台进程ctrl+c掉 ^C[ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.061 MB perf.data (~2657 samples) ] 然后, 显示报告, -n是把sample计数也打出来.可以看到, 一共调用了15次, 而bzip2调了5次 GG bin # ./perf report -n --sort comm --stdio # Samples: 15 of event 'sched:sched_process_exec' # Event count (approx.): 15 # # Overhead Samples Command # ........ ............ ....... # 33.33% 5 bzip2 6.67% 1 groff 6.67% 1 grotty 6.67% 1 less 6.67% 1 locale 6.67% 1 man 6.67% 1 manconv 6.67% 1 nroff 6.67% 1 preconv 6.67% 1 tbl 6.67% 1 troff 调查谁调用了connect()系统调用 比如一个服务, 我们想知道它发起了哪些连接, 以及为什么要发起这些连接. 我在一个窗口执行# ./perf record -e syscalls:sys_enter_connect -ag 在另外一个窗口执行# ping www.baidu.com 然后停止perf, 运行# ./perf report -n, 这里-n是显示sample次数 怎么读这个图呢?首先第一行, 有4次syscalls:sys_enter_connect事件.然后看第二列, 这一列是说直接调用syscalls:sys_enter_connect的地方, 就是__GI___libc_connect, 只有它在第二列是100%.第一列是间接调用到syscalls:sys_enter_connect的地方, 因为在perf期间, 只有ping在运行, 所以ping的_start肯定占了100%的connect调用(间接) 这里有趣的是, main只占了其中的75%, 那另外25%就是在main之前调的. 下面只看main里面直接调用connect的路径, 有两条, 我们看以下33.33%的那条, 可以看到整个从_start到main再到connect的整个调用路径. 这里就回答了谁调用了这个函数?为什么调用? 调查sk buffer的使用 通过调查sk buffer的consumption, 可以知道有哪些东东导致了network IO. 和上面一样, 两个窗口# ./perf record -e 'skb:consume_skb' -ag 还是ping一下百度# ping www.baidu.com 然后看结果一共138次, sshd用了79.71%, 一个叫swapper的东东占了12.32%, ping占了7.97%我这里省略了其他的间接调用 Samples: 138 of event 'skb:consume_skb', Event count (approx.): 138 + 79.71% 79.71% 110 sshd [kernel.kallsyms] [k] consume_skb - 12.32% 12.32% 17 swapper [kernel.kallsyms] [k] consume_skb + 7.97% 7.97% 11 ping [kernel.kallsyms] [k] consume_skb 以ping为例, 这里可以看到整个ping的调用路径下面再来看看sshd的路径最有意思的是swapper, 它的路径是: idle进程, arch_cpu_idle() CPU执行路径被硬件中断切换(下图反选部分开始),ret_from_intr在arch/x86/kernel/entry_64.S do_IRQ(), 在arch/x86/kernel/irq.c, 这里会处理中断, 不在我们的调用栈里, 暂且不表 irq_exit(), 在kernel/softirq.c, 为什么在softirq.c呢? 因为这个函数发的目的是退出硬件中断, 如果需要的话, 处理软中断 invoke_softirq() 这里有两个分支, 一个是直接调__do_softirq(), 另一个是wakeup_softirqd() 因为现在还是在硬中断里, 直接调的话就失去了软中断的意义 所以这里据我考证, force_irqthreads=1, 应该是调: wakeup_softirqd() 这里先获取这个软中断进程的tsk指针 struct task_struct *tsk = __this_cpu_read(ksoftirqd); 这个软中断tsk不在运行的话, 就把它唤醒 if (tsk && tsk->state != TASK_RUNNING) wake_up_process(tsk); 软中断进程 run_ksoftirqd() local_irq_disable() 不要误以为所有的软中断都在禁止中断下运行的, 因为这个软中断处理函数里, 也会打开中断. __do_softirq(), 这个函数会根据sofiqr在哪个bit, 来调相应的回调函数h->action(h).在这里是: net_rx_action(), 下面就是处理协议栈了 local_irq_enable() core 0执行start_kernel() 各种init() rest_init() kernel_thread(kernel_init, NULL, CLONE_FS); kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES); //注意下面的函数可能被其他core执行, 本质上都是idle进程 cpu_startup_entry(CPUHP_ONLINE); cpu_idle_loop(); while(1) //先禁止本地中断 local_irq_disable() cpuidle_idle_call() if (need_resched()) local_irq_enable(); return; //x86实现的 arch_cpu_idle() 其他core被core 0唤醒后执行start_secondary() cpu_startup_entry(CPUHP_ONLINE); 下面再来看perf对调用的统计结果里面有什么规律, 能够体现什么原理: 首先, perf hit到事件, 就记录一次调用路径,比如这个swapper里, 有17次hit到事件, 根据这17次的调用路径, perf能够给出不同分支的分布.甚至这个调用路径能够回溯到系统初始化的代码, 因为中断的关系, 中断接替了idle来完成某次的运行路径.从而能让我们看到idle进程的由来. 比如上面的图里, 每个core都有自己的idle, 并且core 0的idle是从rest_init路径来的.而其他core是从start_secondary路径来的. 分布大约为3:1, 这也符合了我电脑的4核配置. 动态跟踪 需要打开内核选项CONFIG_KPROBES=y CONFIG_KPROBE_EVENTS=y CONFIG_FRAME_POINTER=y CONFIG_UPROBES=y CONFIG_UPROBE_EVENTS=y CONFIG_DEBUG_INFO=y Kernel: tcp_sendmsg() 比如我想知道tcp_sendmsg()的调用情况 GG bin # ./perf probe --add tcp_sendmsg Added new event: probe:tcp_sendmsg (on tcp_sendmsg) You can now use it in all perf tools, such as: perf record -e probe:tcp_sendmsg -aR sleep 1 record./perf record -e probe:tcp_sendmsg -ag -- sleep 5 因为是tcp, 我在另外一个窗口执行wget www.baidu.com, 下面是统计结果: 一共有37次tcp_sendmsg(准吗? 如果perf是根据中断计数的, 可能不准; 如果是直接在函数地址打断点之类的, 应该准) 其中sshd有35次, 我刚执行的wget有2次, 都显示了整个调用路径. 但有些符号没发解析?是哪些共享库吗? 用完以后, 要删除这个eventGG bin # ./perf probe --del tcp_sendmsg 带size的Kernel: tcp_sendmsg() 先看看这个函数有哪些参数 GG bin # ./perf probe -V tcp_sendmsg Available variables at tcp_sendmsg @ size_t size struct kiocb* iocb struct msghdr* msg struct sock* sk 带size GG bin # ./perf probe --add 'tcp_sendmsg size' Added new event: probe:tcp_sendmsg (on tcp_sendmsg with size) You can now use it in all perf tools, such as: perf record -e probe:tcp_sendmsg -aR sleep 1 记录 GG bin # ./perf record -e probe:tcp_sendmsg -a ^C[ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.063 MB perf.data (~2738 samples) ] 用perf script分析, 可以看到每次传的size GG bin # ./perf script sshd 954 [000] 17352.267210: probe:tcp_sendmsg: (ffffffff814b4790) size=0x108 sshd 954 [003] 17353.576942: probe:tcp_sendmsg: (ffffffff814b4790) size=0x1028 sshd 954 [003] 17353.577130: probe:tcp_sendmsg: (ffffffff814b4790) size=0x3b8 sshd 954 [003] 17353.591157: probe:tcp_sendmsg: (ffffffff814b4790) size=0x148 sshd 954 [003] 17353.769294: probe:tcp_sendmsg: (ffffffff814b4790) size=0x108 sshd 954 [003] 17357.122057: probe:tcp_sendmsg: (ffffffff814b4790) size=0x48 sshd 954 [001] 17357.740253: probe:tcp_sendmsg: (ffffffff814b4790) size=0x48 sshd 954 [001] 17357.805653: probe:tcp_sendmsg: (ffffffff814b4790) size=0x48 sshd 954 [002] 17358.029329: probe:tcp_sendmsg: (ffffffff814b4790) size=0x48 列出源码, 一起如何捕捉变量?GG bin # ./perf probe -L tcp_sendmsg # perf probe -L tcp_sendmsg 比如要看第81行有哪些变量 GG bin # ./perf probe -V tcp_sendmsg:81 Available variables at tcp_sendmsg:81 @ int copy size_t seglen size_t size struct msghdr* msg struct sk_buff* skb struct sock* sk unsigned char* from @ int copy size_t seglen size_t size struct msghdr* msg struct sk_buff* skb struct sock* sk 先把前面的东东删掉 GG bin # ./perf probe --list GG bin # ./perf probe --del tcp_sendmsg Removed event: probe:tcp_sendmsg 现在我们要跟踪第81行的seglen变量 GG bin # ./perf probe --add 'tcp_sendmsg:81 seglen' Added new events: probe:tcp_sendmsg (on tcp_sendmsg:81 with seglen) probe:tcp_sendmsg_1 (on tcp_sendmsg:81 with seglen) You can now use it in all perf tools, such as: perf record -e probe:tcp_sendmsg_1 -aR sleep 1 GG bin # ./perf record -e probe:tcp_sendmsg -a ^C[ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.062 MB perf.data (~2709 samples) ] GG bin # ./perf script sshd 954 [003] 18319.361428: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x1028 sshd 954 [003] 18319.361499: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x1001b7ee6 sshd 954 [003] 18319.361508: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x286 sshd 954 [003] 18319.361572: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x4b8 sshd 954 [003] 18319.375664: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x148 sshd 954 [003] 18319.739989: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x108 sshd 954 [003] 18321.071952: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x58 sshd 954 [003] 18321.510444: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x58 sshd 954 [003] 18322.692657: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x38 sshd 954 [002] 18322.696197: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x78 sshd 954 [002] 18322.696414: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x68 sshd 954 [002] 18322.724547: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x68 sshd 954 [002] 18322.727263: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x1d8 wget 2354 [003] 18322.736503: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x6f sshd 954 [002] 18322.736773: probe:tcp_sendmsg: (ffffffff814b4a0a) seglen=0x98 更高端的用法是perf probe的filter功能 --filter=FILTER (Only for --vars and --funcs) Set filter. FILTER is a combination of glob pattern, see FILTER PATTERN for detail. Default FILTER is \"!k???tab_* & !crc_*\" for --vars, and \"!_*\" for --funcs. If several filters are specified, only the last filter is used. 用户态的malloc 比如我想跟踪malloc先检查以下比如ls命令的共享库 GG bin # ldd /bin/ls linux-vdso.so.1 (0x00007ffccce4e000) libacl.so.1 => /lib64/libacl.so.1 (0x00007fc9c8901000) libc.so.6 => /lib64/libc.so.6 (0x00007fc9c8565000) libattr.so.1 => /lib64/libattr.so.1 (0x00007fc9c835f000) /lib64/ld-linux-x86-64.so.2 (0x00007fc9c8b0a000) 然后列出来libc到底有哪些函数这就用到了perf probe的-x 执行路径和-F参数来列出来一个可执行文件的所有函数 GG bin # ./perf probe -x /lib64/libc.so.6 -F | grep malloc malloc malloc@plt malloc_atfork malloc_check malloc_consolidate malloc_hook_ini malloc_info malloc_init_state malloc_printerr mallochook ptmalloc_init ptmalloc_init.part.8 ptmalloc_lock_all ptmalloc_unlock_all ptmalloc_unlock_all2 tr_mallochook 下面我要开始跟踪malloc了, 需要注意的是, 这是个使用频率非常高的函数, 所以在perf的时候over head会很高. 但是这里我遇到点问题虽然-F能够列出malloc, 但不能add GG bin # ./perf probe -x /lib64/libc-2.20.so --add malloc Probe point 'malloc' not found. Error: Failed to add events. 有人也遇到了这个问题, 并给出了补丁, 似乎是debug info和alias函数有关 https://lkml.org/lkml/2015/3/2/269 下面是我的方法: GG bin # readelf /lib64/libc.so.6 -a | grep malloc 5514: 000000000007d960 385 FUNC GLOBAL DEFAULT 11 malloc 5524: 000000000007fba0 1368 FUNC GLOBAL DEFAULT 11 malloc_info 5634: 000000000007f560 703 FUNC WEAK DEFAULT 11 malloc_trim 6172: 0000000000396630 8 OBJECT WEAK DEFAULT 29 __malloc_hook 6261: 000000000007e720 200 FUNC WEAK DEFAULT 11 malloc_usable_size 6735: 000000000007dc00 546 FUNC WEAK DEFAULT 11 malloc_get_state 6752: 000000000007d960 385 FUNC GLOBAL DEFAULT 11 __libc_malloc 6920: 000000000007f940 490 FUNC WEAK DEFAULT 11 malloc_stats 可以看到malloc和libc_malloc的地址是一样的现在加libc_malloc试试, OK了. GG bin # ./perf probe -x /lib64/libc.so.6 --add __libc_malloc Added new event: probe_libc:__libc_malloc (on __libc_malloc in /lib64/libc-2.20.so) You can now use it in all perf tools, such as: perf record -e probe_libc:__libc_malloc -aR sleep 1 perf一下: GG bin # ./perf record -e probe_libc:__libc_malloc -a -- sleep 3 结果: GG bin # ./perf report -n Samples: 5K of event 'probe_libc:__libc_malloc', Event count (approx.): 5098 Overhead Samples Command Shared Object Symbol 99.39% 5067 tmux libc-2.20.so [.] malloc 0.61% 31 sleep libc-2.20.so [.] malloc 证明效果是一样的, 相当于跟踪malloc, 因为libc会把malloc alias到__libc_malloc 这里把网上例子抄过来 # perf probe -x /lib/x86_64-linux-gnu/libc-2.15.so --add malloc Added new event: probe_libc:malloc (on 0x82f20) You can now use it in all perf tools, such as: perf record -e probe_libc:malloc -aR sleep 1 # perf record -e probe_libc:malloc -a ^C[ perf record: Woken up 12 times to write data ] [ perf record: Captured and wrote 3.522 MB perf.data (~153866 samples) ] # perf report -n [...] # Samples: 45K of event 'probe_libc:malloc' # Event count (approx.): 45158 # # Overhead Samples Command Shared Object Symbol # ........ ............ ............... ............. .......... # 42.72% 19292 apt-config libc-2.15.so [.] malloc 19.71% 8902 grep libc-2.15.so [.] malloc 7.88% 3557 sshd libc-2.15.so [.] malloc 6.25% 2824 sed libc-2.15.so [.] malloc 6.06% 2738 which libc-2.15.so [.] malloc 4.12% 1862 update-motd-upd libc-2.15.so [.] malloc 3.72% 1680 stat libc-2.15.so [.] malloc 1.68% 758 login libc-2.15.so [.] malloc 1.21% 546 run-parts libc-2.15.so [.] malloc 1.21% 545 ls libc-2.15.so [.] malloc [...] ping的usage跟踪 我对ping的usage()函数做跟踪, 在另外一个窗口执行ping --help GG bin # ./perf probe -x /bin/ping -F GG bin # ./perf probe -x /bin/ping --add usage Added new event: probe_ping:usage (on usage in /bin/ping) You can now use it in all perf tools, such as: perf record -e probe_ping:usage -aR sleep 1 GG bin # ./perf record -e probe_ping:usage -ag ^C[ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.058 MB perf.data (~2516 samples) ] GG bin # ./perf report 带变量的ping GG bin # ./perf probe -x /bin/ping -V pr_addr GG bin # ./perf probe -x /bin/ping --add 'pr_addr addr' Added new event: probe_ping:pr_addr (on pr_addr in /bin/ping with addr) You can now use it in all perf tools, such as: perf record -e probe_ping:pr_addr -aR sleep 1 GG bin # ./perf record -e probe_ping:pr_addr -ag ^C[ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.059 MB perf.data (~2595 samples) ] GG bin # ./perf script ping 2482 [002] 21670.501019: probe_ping:pr_addr: (402b40) addr=0x4af85070 402b40 pr_addr (/bin/ping) 4063f2 main_loop (/bin/ping) 40225b main (/bin/ping) 7f9844bf0adf __libc_start_main (/lib64/libc-2.20.so) 4029c5 _start (/bin/ping) ping 2482 [000] 21671.423816: probe_ping:pr_addr: (402b40) addr=0x4af85070 402b40 pr_addr (/bin/ping) 4063f2 main_loop (/bin/ping) 40225b main (/bin/ping) 7f9844bf0adf __libc_start_main (/lib64/libc-2.20.so) 4029c5 _start (/bin/ping) ping 2482 [003] 21672.347853: probe_ping:pr_addr: (402b40) addr=0x4af85070 402b40 pr_addr (/bin/ping) 4063f2 main_loop (/bin/ping) 40225b main (/bin/ping) 7f9844bf0adf __libc_start_main (/lib64/libc-2.20.so) 4029c5 _start (/bin/ping) 查看系统调用次数 一次ls有58次系统调用 GG bin # ./perf stat -e raw_syscalls:sys_enter ls index.html index.html.1 index.html.2 index.html.3 index.html.4 index.html.5 perf perf.data perf.data.old src trace vmlinux vmlinux.gz Performance counter stats for 'ls': 58 raw_syscalls:sys_enter 0.001785210 seconds time elapsed 而sleep 1秒和2秒都是33次系统调用 GG bin # ./perf stat -e raw_syscalls:sys_enter sleep 1 Performance counter stats for 'sleep 1': 33 raw_syscalls:sys_enter 1.000840210 seconds time elapsed GG bin # ./perf stat -e raw_syscalls:sys_enter sleep 2 Performance counter stats for 'sleep 2': 33 raw_syscalls:sys_enter 2.002338440 seconds time elapsed 这里要说明以下, perf的参数里面, 如果带-a, 是说要对整个系统做perf. 不带-a只针对后面的command; 以上两个实验都是不带-a的. 如果带-a, 则统计的结果多很多 GG bin # ./perf stat -e raw_syscalls:sys_enter -a ls index.html index.html.1 index.html.2 index.html.3 index.html.4 index.html.5 perf perf.data perf.data.old src trace vmlinux vmlinux.gz Performance counter stats for 'system wide': 76 raw_syscalls:sys_enter 0.002150540 seconds time elapsed GG bin # ./perf stat -e raw_syscalls:sys_enter -a -- sleep 1 Performance counter stats for 'system wide': 266 raw_syscalls:sys_enter 1.002717980 seconds time elapsed 好了, 现在我们来看看详细的ls的系统调用情况 GG bin # ./perf record -e raw_syscalls:sys_enter -g ls Lowering default frequency rate to 800. Please consider tweaking /proc/sys/kernel/perf_event_max_sample_rate. index.html index.html.1 index.html.2 index.html.3 index.html.4 index.html.5 perf perf.data perf.data.old src trace vmlinux vmlinux.gz [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.014 MB perf.data (~591 samples) ] GG bin # ./perf report -n 下图是report结果, 可以看到, 一个ls有mmap64 close open64 access等系统调用 misc 比如在动态probe里面, 可以对某个函数的某行来做一个trace点 添加probe点的语法格式为: 1) Define event based on function name [EVENT=]FUNC[@SRC][:RLN|+OFFS|%return|;PTN] [ARG ...] 2) Define event based on source file with line number [EVENT=]SRC:ALN [ARG ...] 3) Define event based on source file with lazy pattern [EVENT=]SRC;PTN [ARG ...] 举例: EXAMPLES Display which lines in schedule() can be probed: ./perf probe --line schedule Add a probe on schedule() function 12th line with recording cpu local variable: ./perf probe schedule:12 cpu or ./perf probe --add='schedule:12 cpu' this will add one or more probes which has the name start with \"schedule\". Add probes on lines in schedule() function which calls update_rq_clock(). ./perf probe 'schedule;update_rq_clock*' or ./perf probe --add='schedule;update_rq_clock*' Delete all probes on schedule(). ./perf probe --del='schedule*' Add probes at zfree() function on /bin/zsh ./perf probe -x /bin/zsh zfree or ./perf probe /bin/zsh zfree Add probes at malloc() function on libc ./perf probe -x /lib/libc.so.6 malloc or ./perf probe /lib/libc.so.6 malloc perf wiki里的例子 $ ./perf record -e sched:sched_stat_sleep -e sched:sched_switch -e sched:sched_process_exit -g -o ~/perf.data.raw ~/foo $ ./perf inject -v -s -i ~/perf.data.raw -o ~/perf.data $ ./perf report --stdio --show-total-period -i ~/perf.data "},"notes/profiling_ftrace和trace-cmd记录.html":{"url":"notes/profiling_ftrace和trace-cmd记录.html","title":"ftrace和trace-cmd记录","keywords":"","body":" trace实例 kernelshark图解2个VM通过OVS互相ping 重点总结 场景 Host OS的运行情况 VM1的运行情况 strace看VM的四个核 VM里加用户态应用 vm和host上查看vm内的用户态进程 VM内perf能看到bash进程 在host上查不到任何在VM空间上跑的代码 HOST里加用户态应用 ovs的pmd进程 为什么没有捕捉到eventfd_write(其实有的) kernelshark 安装 使用 trace-cmd 使用举例 哪个进程, 在什么时间, 调用了irqfd_inject()函数 一个内核函数都调用了哪些函数, 及其子函数 trace所有event function tracer也能反映调用关系, 甚至更直观 看哪个中断的latency最大 统计模式 record命令 report命令 profile命令 动态probe支持 ftrace使用记录 函数跟踪 function tracer function tracer和function_graph tracer 只看一个cpu filter 对进程号tracing 静态探测点, 也称预定义event 动态跟踪 kprobe, 动态event 局部变量 时延tracer 跟踪分支 其他功能 trace实例 kernelshark图解2个VM通过OVS互相ping 重点总结 trace-cmd的overhead不大, 其运行期间系统CPU使用率稍稍上升 arch_timer提供的100HZ(和kernel 选项CONFIG_HZ=100一致)的时钟中断依旧是进程调度的触发点. ftrace类的kernel trace能看到哪个任务, 什么时间, 执行了多久, 但看不到虚拟机内部的执行情况, 也看不到用户态的执行情况. host上的perf top也看不到虚拟机里面的执行情况, 但perf能看host的用户态进程. 因为其原理是采样, 而ftrace是基于tracing. 从任务被唤醒到被调度是有延迟的, 即从sched_wakeup到sched_switch所用的时间, 一般是us数量级. 在本系统(48core aarch64)上, 某次进程迁移用了97us, 一次普通的时钟中断处理用了17us. 进程在cpu间的迁移并不是想象中那么昂贵 用trace-cmd --profile时, 默认只有部分事件触发, 加-e all能扩大触发事件范围 场景 两个VM使用virtio-net的kernel驱动, 其后端是OVS的vhostuserclient PMD, pmd进程运行在13 15号cpu上. #系统通过isolcpus=2-23,26-47 只保留4个核给系统(centos 7.5)使用. HOST: 0 1 24 25 #OVS的pmd进程跑在四个核上 OVS: 12 13 14 15 #两个VM分别pin了4个CPU VM1: 26 27 28 29 VM2: 40 41 42 43 通过htop看到, 除了OVS的2个pmd线程, 系统负载很轻 #开始捕捉系统活动30秒 sudo trace-cmd record --profile sleep 30 #拷贝trace.data到我的ubuntu虚拟机, 运行kernelshark, 打开这个trace文件 注意到运行trace-cmd期间, OS的4个核的CPU使用率稍稍上升了一点, 在3% - 10%左右, trace-cmd最后要合并临时文件, CPU使用率能到30%. Host OS的运行情况 首先看看OS的4个核(0, 1, 24, 25)的情况, 初看起来挺饱满的, 似乎CPU很忙, 和htop观察到的CPU利用率并不match. 真实情况是, CPU没有那么忙, 但因为软件绘图时要着色的原因, 看起来比较饱满. 放大其中一部分观察, 这样看起来CPU利用率就显得苗条多了.观察CPU 0的其中一段, 连续红色着色的地方是kworker在执行, 后面青色的很长一段是htop在执行. 下面事件部分, 高亮的那行表示发生了sched_switch事件, 导致从kworker切换到htop.trace-cmd record --profile默认会在每次任务被换出时, 触发一次kernel_stack事件, 打印kernel调用栈. 本次htop被调度执行约0.17秒, 但中间还是被其他进程打断了几次. VM1的运行情况 VM1绑定运行在26 27 28 29四个核上 主要有几个进程会跑在这几个核上 host的idle进程 host的kworker进程, watchdog进程 qemu-system-aarch64的子线程, 显示为CPUN/KVM线程, N为0到3, 代表了4个核 下图显示了一次典型路径: 某个时刻CPU处于idle状态, 此时host的arch_timer触发中断, 并唤醒并切换到kworker进程, 后者唤醒了CPU 1/KVM进程, 从图中可以看到, 从sched_wakeup到sched_switch用了36.1 us. 接下来CPU 1/KVM做的工作是, 调用kvm_timer和kvm_vgic等函数来更新vm的状态, 随后又进入idle, 这个过程花了151 us. 看下来core 27 28 29的情况类似, 大部分是由arch_timer触发的更新vm时间的操作. 在本例中, VM1是在不断的ping VM2的, 没有其他应用在跑. 那么估计有用的操作都在core 26上了. 下面来看一下core 26, 对应的qemu进程是CPU 0/KVM图中显示, 这次SyS_ioctl()共执行了29 us左右. 这次CPU 0/KVM共持续了10 ms, 在此期间, SyS_ioctl()出现了多次, 后再次进入idle. 根据这个进程的用户态的调用栈来看, qemu用ioctl()系统调用, 来让host执行VM的代码. #0 0x0000ffff8b4665bc in ioctl () from /lib64/libc.so.6 #1 0x0000000000498f30 in kvm_vcpu_ioctl (cpu=0xffff876f0010, type=44672) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:2093 #2 0x0000000000498828 in kvm_cpu_exec (cpu=0xffff876f0010) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:1930 #3 0x0000000000460b74 in qemu_kvm_cpu_thread_fn (arg=0xffff876f0010) at /home/bai/share/repo/hxt/qemu/cpus.c:1215 #4 0x0000000000a91468 in qemu_thread_start (args=0x279f03e0) at util/qemu-thread-posix.c:504 #5 0x0000ffff8b527bb8 in start_thread () from /lib64/libpthread.so.0 #6 0x0000ffff8b46fb50 in thread_start () from /lib64/libc.so.6 strace看VM的四个核 VM的4个核, 其实是qemu的四个线程. 在本例中, 它们是38610 38612 38613 38614, 其余几个线程是管理辅助线程. #38610 38612 38614这几个线程类似, strace都停在KVM_RUN不动 $ sudo strace -tt -p 38610 strace: Process 38610 attached 19:21:44.113743 ioctl(30, KVM_RUN #38613线程一直有输出, 平均0.1ms就有一次ioctl调用. sudo strace -tt -p 38613 ... 19:16:03.422829 writev(12, [{\"6\", 1}], 1) = 1 19:16:03.423031 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.423105 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423175 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423237 writev(12, [{\"4\", 1}], 1) = 1 19:16:03.423304 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.423358 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423418 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423476 writev(12, [{\" \", 1}], 1) = 1 19:16:03.423536 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.423587 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423649 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423708 writev(12, [{\"b\", 1}], 1) = 1 19:16:03.423767 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.423819 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423878 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423937 writev(12, [{\"y\", 1}], 1) = 1 19:16:03.423995 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.424046 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424106 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424165 writev(12, [{\"t\", 1}], 1) = 1 19:16:03.424224 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.424274 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424334 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424393 writev(12, [{\"e\", 1}], 1) = 1 19:16:03.424451 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.424502 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424560 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424620 writev(12, [{\"s\", 1}], 1) = 1 ... 经过实验发现, 38613线程一直有ioctl调用, 是因为我用virtmanager来显示VM的console, 相当于VM的\"串口\", 这里的ioctl调用是ping的输出导致的. 在ssh session里面ping, 就看不到KVM_IRQ_LINE类型的ioctl, 此时4个vm的core对应的线程都是处于系统调用\"不活跃\"的状态. 仔细看strace的输出, writev()调用每次有一个字节的入参, 连起来就是ping的输出\"64 bytes from ...\" 下图就是其中一次ioctl的kernel trace VM里加用户态应用 前面的例子中, VM里只运行ping命令, 这个命令大部分时间也是在VM的内核协议栈里跑. 为了加大vm里面用户态的cpu使用率, 在VM里运行: while true; do :; done会把其中一个core跑满100%用户态. 这在VM里面看是core 3, 在host上看是core 28. 整体看是这个样子的, 绝大部分都是CPU 2/KVM在跑 pstack看到这个进程的stack并没有什么变化 Thread 3 (Thread 0xffff99afede0 (LWP 38613)): #0 0x0000ffff9d9165bc in ioctl () from /lib64/libc.so.6 #1 0x0000000000498f30 in kvm_vcpu_ioctl (cpu=0xffff99b00010, type=44672) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:2093 #2 0x0000000000498828 in kvm_cpu_exec (cpu=0xffff99b00010) at /home/bai/share/repo/hxt/qemu/accel/kvm/kvm-all.c:1930 #3 0x0000000000460b74 in qemu_kvm_cpu_thread_fn (arg=0xffff99b00010) at /home/bai/share/repo/hxt/qemu/cpus.c:1215 #4 0x0000000000a91468 in qemu_thread_start (args=0x13b84e60) at util/qemu-thread-posix.c:504 #5 0x0000ffff9d9d7bb8 in start_thread () from /lib64/libpthread.so.0 #6 0x0000ffff9d91fb50 in thread_start () from /lib64/libc.so.6 取其中一处放大, 发现在上一个funcgraph_exit和下一个funcgraph_enter之间, 有7ms时间, CPU还是被\"CPU 2/KVM\"进程占用, 但在host kernel并没有footprint. 几乎都是类似的情况 偶尔能看到kworker 在kernelshark显示的整个trace中, 找不到ioctl. 但用下面的命令, 可以显示kernel的调用栈, 其最开始就是SyS_ioctl调用. 其原理是在每次上下文切换的时候, trace-cmd会记录当前进程的kernel调用栈. 说明VM里面的代码, 不管是kernel态还是用户态, 都是通过调用host的ioctl来完成执行的. trace-cmd report --cpu 28 trace-ovs-2vm-ping-while1.dat CPU 2/KVM-38613 [028] 356963.892719: sched_wakeup: kworker/28:1:322 [120] success=1 CPU:028 CPU 2/KVM-38613 [028] 356963.892722: kernel_stack: => try_to_wake_up (ffff00000810cef8) => wake_up_process (ffff00000810d1c4) => wake_up_worker (ffff0000080f6814) => insert_work (ffff0000080f860c) => __queue_work (ffff0000080f8794) => queue_work_on (ffff0000080f8ae4) => dbs_irq_work (ffff000008780d20) => irq_work_run_list (ffff0000081ee390) => irq_work_run (ffff0000081ee3ec) => handle_IPI (ffff00000809121c) => gic_handle_irq (ffff000008081958) => el1_irq (ffff000008082f84) => __do_softirq (ffff000008081a50) => irq_exit (ffff0000080e43b4) => __handle_domain_irq (ffff00000814665c) => gic_handle_irq (ffff000008081870) => el1_irq (ffff000008082f84) => kvm_arch_vcpu_ioctl_run (ffff0000080b3980) => kvm_vcpu_ioctl (ffff0000080a8f6c) => do_vfs_ioctl (ffff0000082c5434) => SyS_ioctl (ffff0000082c5d04) => __sys_trace_return (ffff0000080837ac) vm和host上查看vm内的用户态进程 在vm里执行taskset -c 3 bash -c 'while true; do :; done', 然后用perf分别在VM内和host上查看: VM内perf能看到bash进程 sudo perf top -g --no-children -C 3 在host上查不到任何在VM空间上跑的代码 VM的cpu3, 对应host上cpu 29, 在host上看来, 运行的是qemu的一个线程 注意htop默认的CPU编号从1开始, 但我这里设置了从0开始, 和perf等命令保持一致. 在host上只能看到自己在内核态的代码:sudo perf top -g --no-children -C 29 HOST里加用户态应用 在host上执行while true; do :; done, 整个系统tracing如下:bash这个进程几乎100%占用CPU运行, 从图中可以看出, 该进程在0 24 1上发生过迁移.在其执行期间, 会被arch_timer周期性打断, 随后触发softirq的action为timer或rcu. 除了被中断打断能在kernel空间留下痕迹, bash用户态代码在内核空间不能被跟踪到, 内核只知道这段时间是bash在运行. 下面是bash进程从cpu24迁移到cpu1的过程, 本次迁移用了97 us. 作为对比, 一次普通的时钟中断处理, 为17us. 另外, 还捕捉到bash被urcu2进程抢占执行了178us. ovs的pmd进程 本例中, ovs有16个线程, 它们共享pid号, 但tid号不一样. kernelshark里面, 用tid号作为区分. pmd8和pmd9分别运行在core 15和core 13上, 对应tid: 38725和38726 这里重点看core 15: sudo perf top -g --no-children -C 15 虽然perf通过采样的方式, 能看到用户态和内核态的踪迹, 但trace-cmd目前只能跟踪内核事件, 如下图:它只能看到pmd8一直占用core 15, 偶尔被100 HZ的中断打断, 进而偶尔执行kworker等进程很短一段时间, 但其用户态进程没有跟踪到. 为什么没有捕捉到eventfd_write(其实有的) 之前分析过OVS的vhost代码, 用的是dpdk librte_vhost/virtio_net.c里的收发包函数. 按理说对于这种OVS是pmd进程收发包, 而VM是内核态驱动收发包的场景, VM内核驱动应该是中断, 那么根据代码要走eventfd读写流程的, 为什么kernelshark没显示相关信息呢? 用sudo trace-cmd stream -e syscalls -P 38725能够看到这个pmd进程的系统调用: 是有write系统调用的, 但在上面用sudo trace-cmd record --profile sleep 30就没有捕捉到系统调用. 原来--profile并不是所有event都是触发源, 要加-e all sudo trace-cmd record -e all --profile sleep 30现在有了: 这里过滤了其他的event, 只保留syscalls 这里显示, 大概每秒2次write系统调用, 和两个VM互相ping的操作(默认1秒1次), 是匹配的.在代码里, rte_vhost_enqueue_burst()是OVS通过vhost往VM发包的函数 这里面, 会调用eventfd_write() 再用perf top抓一下调用栈: 确认是sys_write()系统调用, 再调用内核里的eventfd_write() kernelshark kernelshark是trace-cmd的gui版本 安装 新版本的kernelshark实用qt作为显示框架 git clone git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/trace-cmd.git sudo apt install libjson-c-dev doxygen freeglut3-dev make gui sudo make install_gui #开启 kernelshark 使用 我是在ubuntu上安装的kernelshark sudo apt install kernelshark, 可以单独安装kernelshark, 并不依赖trace-cmd.然后把trace.data拷到ubuntu下面, 在其目录下执行kernelshark 显示内容还是比较丰富, 默认是按照CPU的时间来显示, 有CPU task event等filter来过滤显示. trace-cmd 需要安装trace-cmd.aarch64 trace-cmd是个集合, 包括 commands: #见下面详细解释 record - record a trace into a trace.dat file #start stop extract连用, start的参数和record一样, extract是把kernel的buffer保存为trace.data文件. start - start tracing without recording into a file extract - extract a trace from the kernel stop - stop the kernel from recording trace data restart - restart the kernel trace data recording #show kernel buffer, 可以用-c来限定cpu号. show - show the contents of the kernel tracing buffer #全部关闭, 清理buffer. 很实用 reset - disable all kernel tracing and clear the trace buffers report - read out the trace stored in a trace.dat file #和start然后show效果差不多. 用户体验并不友好, 输出太多的时候很难ctrl+c停止 stream - Start tracing and read the output directly profile - Start profiling and read the output directly hist - show a historgram of the trace.dat information stat - show the status of the running tracing (ftrace) system split - parse a trace.dat file into smaller file(s) options - list the plugin options available for trace-cmd report listen - listen on a network socket for trace clients list - list the available events, plugins or options restore - restore a crashed record snapshot - take snapshot of running trace stack - output, enable or disable kernel stack tracing check-events - parse trace event formats trace-cmd还支持嵌入式的client server模式. 使用举例 哪个进程, 在什么时间, 调用了irqfd_inject()函数 sudo trace-cmd stream -p function -l irqfd_inject sleep 10 #结果显示, 这些进程都是kworker进程 -22636 [000] 1210302.576857: function: irqfd_inject kworker/15:1-310 [015] 1210302.051545: function: irqfd_inject kworker/13:1-308 [013] 1210302.051652: function: irqfd_inject -33928 [024] 1210302.228918: function: irqfd_inject -33928 [024] 1210302.256864: function: irqfd_inject kworker/13:1-308 [013] 1210302.468840: function: irqfd_inject kworker/15:1-310 [015] 1210302.468931: function: irqfd_inject -33928 [024] 1210302.576854: function: irqfd_inject -22636 [000] 1210302.869004: function: irqfd_inject -34497 [001] 1210303.011720: function: irqfd_inject kworker/15:1-310 [015] 1210303.091628: function: irqfd_inject kworker/13:1-308 [013] 1210303.091726: function: irqfd_inject kworker/13:1-308 [013] 1210303.508968: function: irqfd_inject kworker/15:1-310 [015] 1210303.509060: function: irqfd_inject ... #带调用栈 sudo trace-cmd record -p function -l irqfd_inject -e sched -T sleep 10 sudo trace-cmd report #irqfd_inject调用栈如下: => swake_up_locked (ffff000008129bd8) => swake_up (ffff000008129c40) => kvm_vcpu_kick (ffff0000080a9f24) => vgic_queue_irq_unlock (ffff0000080c0f3c) => vgic_its_trigger_msi (ffff0000080c893c) => vgic_its_inject_msi (ffff0000080cae0c) => kvm_set_msi (ffff0000080c25e4) => kvm_set_irq (ffff0000080cbee8) => irqfd_inject (ffff0000080aef58) => process_one_work (ffff0000080f94e8) => worker_thread (ffff0000080f9784) => kthread (ffff0000081003b8) => ret_from_fork (ffff000008084d70) 一个内核函数都调用了哪些函数, 及其子函数 比如查看do_notify_resume都干了什么 #调试中发现很多任务切换发生在这个函数执行过程中, 想了解一下这个函数都干了什么, 调用了哪些函数: #-g表示要跟踪这个函数的所有子函数调用, 只用-p function_graph默认好像只有一层显示. sudo trace-cmd stream -p function_graph -g do_notify_resume sleep 1 #结果里面有do_notify_resume的调用图 do_notify_resume() { task_work_run() { _raw_spin_lock_irq(); ____fput() { __fput() { _cond_resched(); __fsnotify_parent(); fsnotify(); locks_remove_file(); mutex_lock() { _cond_resched(); } free_pages(); kfree() { __free_pages() { __free_pages_ok() { free_one_page() { _raw_spin_lock(); __mod_zone_page_state(); pfn_valid() { memblock_is_map_memory(); } pfn_valid() { memblock_is_map_memory(); } pfn_valid() { memblock_is_map_memory(); } } } } } mutex_unlock(); security_file_free(); module_put(); put_pid(); call_rcu_sched() { __call_rcu() { rcu_segcblist_enqueue(); } } dput() { _cond_resched(); } mntput() { mntput_no_expire(); } } } _cond_resched(); _raw_spin_lock_irq(); } mem_cgroup_handle_over_high(); } do_notify_resume() { fpsimd_restore_current_state() { __local_bh_enable_ip(); } } do_notify_resume() { schedule() { rcu_note_context_switch() { rcu_sched_qs(); } _raw_spin_lock(); update_rq_clock(); pick_next_task_fair() { update_curr(); check_cfs_rq_runtime(); pick_next_entity() { clear_buddies(); } put_prev_entity() { update_curr() { update_min_vruntime(); cpuacct_charge(); } check_cfs_rq_runtime(); check_spread.isra.64(); __enqueue_entity(); __update_load_avg_se.isra.36(); __update_load_avg_cfs_rq.isra.37(); } put_prev_entity() { update_curr(); check_cfs_rq_runtime(); check_spread.isra.64(); __enqueue_entity(); __update_load_avg_se.isra.36(); __update_load_avg_cfs_rq.isra.37(); } set_next_entity() { __update_load_avg_se.isra.36(); __update_load_avg_cfs_rq.isra.37(); } } fpsimd_thread_switch(); hw_breakpoint_thread_switch(); uao_thread_switch(); qqos_sched_in(); finish_task_switch(); } } #在上面命令的基础上, -e sched和-T联用, 在sched* 事件触发下, 记录kernel调用栈 sudo trace-cmd record -p function_graph -g do_notify_resume -e sched -T sleep 1 sudo trace-cmd report --cpu 0 #比如: 一般是work_pending调用的do_notify_resume => try_to_wake_up (ffff00000810cef8) => default_wake_function (ffff00000810d2a0) => autoremove_wake_function (ffff00000812987c) => __wake_up_common (ffff00000812903c) => __wake_up_common_lock (ffff0000081291d4) => __wake_up (ffff000008129254) => rb_wake_up_waiters (ffff0000081c0a8c) => irq_work_run_list (ffff0000081ee390) => irq_work_run (ffff0000081ee3ec) => handle_IPI (ffff00000809121c) => gic_handle_irq (ffff000008081958) => el1_irq (ffff000008082f84) => trace_graph_entry (ffff0000081d5460) => prepare_ftrace_return (ffff000008091e0c) => ftrace_graph_caller (ffff000008091f58) => call_rcu_sched (ffff000008157b70) => __fput (ffff0000082b29e0) => ____fput (ffff0000082b2af8) => task_work_run (ffff0000080fe0e0) => do_notify_resume (ffff0000080896a4) => work_pending (ffff0000080835d8) trace所有event sudo trace-cmd record -e all ls > /dev/null function tracer也能反映调用关系, 甚至更直观 #使用function tracer, 也能反映调用关系, 甚至比function_graph还直观些. #加-F是指只trace \"ls\", 否则所有的调用都有. sudo trace-cmd record -F -p function -e sched_switch ls > /dev/null #function_graph的好处是有funcgraph_entry和irq_handler_entry, 好统计函数耗时 效果示意: 看哪个中断的latency最大 trace-cmd record -p function_graph -e irq_handler_entry -l do_IRQ sleep 10 trace-cmd report 统计模式 #默认用一层的function_graph sudo trace-cmd record --profile sleep 1 #这里--comm没有找到地方说明, 但加上的效果就是除了sleep进程名, 过滤了其他进程的event sudo trace-cmd report --profile --comm sleep | less --profile默认只打开下面的event, 要trace所有event, 使用: sudo trace-cmd record -e all --profile sleep 1 record命令 man trace-cmd record trace-cmd record [OPTIONS] [command] 记录command执行期间的ftrace数据. 为每个CPU创建一个tracer进程来把per cpu的ring buffer写到临时文件, 结束的时候把所有的临时文件汇总到trace.data 部分选项: -M cpumask 只对cpumask里的cpu trace -p plugin 可以是function, function_graph, preemptirqsoff, irqsoff, preemptoff, wakeup -e event 静态探测点, 是subsystem:event-name的形式 可以简写, 比如-e sched_switch -e sched 可以带通配符, 比如*stat*表示所有带stat的event -e all 表示全部event 可以多个-e联用 -T 对每个event使能stacktrace -f filter 对前面那一个event做filter 可以对filed(查format)进行这些操作: ==, >=, , report命令 trace-cmd report [OPTIONS] [input-file] -i input文件, 通常是trace.data -f 列出数据文件里的所有函数 -E 列出数据文件里可能的事件 --events 列出数据文件里的event格式 --event regex 只打印match的event 比如trace-cmd report --event sys:read, 是匹配system name有sys并且event name含有read的事件 也可以直接写trace-cmd report --event read, 表示匹配system name或event name含有read的事件 -t 时间戳使用ns, 默认是us -F filter ':' = SYSTEM'/'EVENT | SYSTEM | EVENT | ',' = EVENT_FIELD | '&&' | '||' | '(' ')' | '!' = '==' | '!=' | '>=' | '' | ' = NUM | STRING | EVENT_FIELD 比如 #EVENT_FIELD能从event的format查出来, 用--events可以列出来 #bogus不是任何event的EVENT_FIELD, 所以bogus==1为假, 但后面的common_pid == 2会生效 -F 'sched : bogus == 1 || common_pid == 2' -F 'sched/sched_switch : prev_state==0' #.*是正则, 表示所有event; 下面的filter表示除去trace-cmd进程的event -F '.*:COMM != \"trace-cmd\"' -I 不打印有HARDIRQ标记的事件, 即不打印中断上下文的事件. -S 不打印有SOFTIRQ标记的事件 -v 反选, 后面跟-F filter --profile 统计模式, 显示哪里任务最耗时, 哪里任务呗阻塞最久, 唤醒的时延在哪里 -G 和--profile联用, 指定中断事件是全局的, 即CPU相关而不是默认的task相关 -R raw模式 -l 显示latency标记 标记依次表示: CPU号 irqs-off need-resched hardirq/softirq preempt-depth kernel-lock -0 0d.h1. 106467.859747: function: ktime_get profile命令 trace-cmd profile [OPTIONS] [command]是record --profile和report --profile的组合, 选项和record基本一致. trace部分event和使用function graph tracer #-F表示只trace后面的command, -c表示follow fork sudo trace-cmd profile -F -c sleep 1 #输出 #这个event是sched_switch, 表示在running的状态下, 被抢占(preempted)了2次, 一共是234559 ns, 平均是117279 ns(总时间/被抢占次数), 以及最大的一次和最小的一次被抢占的延时 Event: sched_switch:R (2) Total: 234559 Avg: 117279 Max: 129886 Min:104673 #后面的调用栈显示在哪里被抢占了 #被调度到S状态(INTERRUPTIBLE)下的时间为1000513242 ns, 约为1s, 也就是sleep 1的时间 Event: sched_switch:S (1) Total: 1000513242 Avg: 1000513242 Max: 1000513242 Min:10005132 #再后面是function trace类型的event, 默认使用depth为1的function_graph tracer #括号里的数字是hit的次数 #overhead很高, 虽然depth为1表示只trace第一层的函数调用, 但所有内核函数的_mcount()还是会被触发(trigger) #可以用-l来限制触发event的函数, 比如-l 'sys_*' -l 'SyS_*' Event: func: SyS_open() (3) Total: 23615 Avg: 7871 Max: 10535 Min:4743 #再后面是, sys_enter:35的35是系统调用号 Event: sys_enter:35 (1) Total: 1000599765 Avg: 1000599765 Max: 1000599765 Min:1000599765 #其他 page_fault_user 用户进程发生page fault softirq_raise 后面的时间表示soft irq从raise到执行花的时间 softirq_entry 后面的时间表示soft irq执行花费的时间 #-S表示不trace默认的全部event, 只trace后面显式指定的. #-l '*kmalloc*:stacktrace'表示打开kmalloc*的stack trace trace-cmd profile -S -p function_graph -l '*kmalloc*' -l '*kmalloc*:stacktrace' sleep 1 动态probe支持 trace-cmd本身并没有支持kprobe的语法, 但可以分两步来完成 #第一步, 先定义kprobe echo 'r:md_make_request_probe md_make_request $retval' >> kprobe_events echo 'r:mwait_probe mwait_idle $retval' >> kprobe_events #第二部, 用trace-cmd来跟踪 trace-cmd record -p function_graph -o graph_md.dat -g md_make_request -e md_make_request_probe -e mwait_probe -F ./block_hasher -d /dev/md0 -b 1048576 -t10 -n100 ftrace使用记录 ftrace以debugfs目录为接口, 是kprobe和uprobe的前端. 有两类的probe 静态tracepoint提前在代码里声明的 动态的probe event/sys/kernel/debug/tracing/kprobe_events 查看所有的tracer方法cat available_tracers以及所有的静态tracepoint cat available_events [root@rep2-130 tracing]# cat available_tracers blk function_graph wakeup_dl wakeup_rt wakeup function nop [root@rep2-130 tracing]# cat available_events nfs* sunrpc* xfs* syscall* raw_syscalls ipi* irq* signal* workqueue* sched* timer* alarmtimer* task* migrate* kvm* clk* iommu* cpuhp* percpu* regmap* random* module* cgroup* power* drm* bpf* oom* vmscan* kmem* huge_memory* writeback* filelock* block* scsi* libata* spi* mdio* xhci* usb* i2c* smbus* thermal* dma* net* sock* skb* mlx5* 函数跟踪 function tracer kernel编译的时候加了-pg选项, 功能是给每个内核函数插入特殊的_mcount()函数. kernel编译完成后, 每个函数都有_mcount()调用, 但kernel启动过程中, 会把bl _mcount指令替换成nop; 当function tracer使能函数跟踪的时候, 再替换成bl ftrace_caller 相比动态probe需要把指定函数地址替换为breakpoint指令的方式, _mcount()方法更简单高效. cd /sys/kernel/debug/tracing/ #选择function tracing echo function > current_tracer #选择调用图 echo function_graph > current_tracer #注意: >>== current_tracer打开后, 马上就有overhead; #二者overhead都很明显, function_graph比function大 == current_tracer #下面操作通用 #开始tracing, >>== overhead在前面的基础上, 再次明显增大. == tracing_on #关闭tracing echo 0 > tracing_on #查看trace log, 不过滤的话会有很多 cat trace #清空trace log cat trace_pipe > /dev/null function tracer和function_graph tracer 下图是在ping localhost过程中, 打开ftrace的overhead实例: 其中, 1是echo function_graph > current_tracer; 2是echo 1 > tracing_on; 3是echo 0 > tracing_on 补充一下, 最后echo nop > current_tracer, ping延迟恢复到0.026ms. 下图是function tracer的实例, 当前函数和其caller都能显示 下图是function_graph tracer实例, 可以和上面的function tracer对比. 相对来说, function_graph更直观一些: 有函数的调用关系, 函数的执行时间; 中间被中断打断的话, 还有箭头提示; 任务切换有横线分割; 只看一个cpu 经常多个CPU混杂在一起显示, 就不容易看懂:此时可以用per_cpu目录 cd /sys/kernel/debug/tracing cat per_cpu/cpu42/trace | less filter 实际观察如果不加filter, trace log的东西太多太杂, 很难阅读和理解; 比如所有的CPU都混杂在一起显示 在kernel cmdline里面可以加通配符ftrace=function ftrace_filter=mydrv_*ftrace=function_graph ftrace_graph_notrace=rcu*,*lock,*spin* 在运行时, 也可以用通配符, 比如value* *value* *value value1*value2等形式 比如 #只看sys_开头的函数 echo sys_* > set_ftrace_filter #清空filter echo > set_ftrace_filter #filter支持更通用的格式: function:command[:count] #mod: enable or disable function per module #traceon/traceoff: enable or disable trace #snapshot: preserves current trace buffer and swap to spare buffer #enable_event/disable_event: enable or disable event #stacktrace: enable stack trace #dump: dump all trace buffers #cpudump: only dump buffer for current CPU #例如: 只看modlue sha512_generic的函数 echo ':mod:sha512_generic' > set_ftrace_filter #例如: 在segment fault时停止tracing, 防止后续的trace log把有用信息淹没 echo 'do_bad_area.part.1:traceoff' > set_ftrace_filter 对进程号tracing #可以限制只对几个进程做tracing echo '123 244 1' >> set_ftrace_pid #对当前线程tracing echo $$ >> set_ftrace_pid #清除限定的pid echo > set_ftrace_pid 下面的脚本用来trace用户命令, 用法: ./traceme.sh ls && ./traceme.sh stop #!/bin/sh DEBUGFS=/sys/kernel/debug if test \"$1\" != \"stop\"; then #对当前线程tracing echo $$ > $DEBUGFS/tracing/set_ftrace_pid #选择tracing方法 #echo function > $DEBUGFS/tracing/current_tracer echo function_graph > $DEBUGFS/tracing/current_tracer #开始tracing echo 1 > $DEBUGFS/tracing/tracing_on #执行被trace的命令 #用exec不会改变进程号, 但会替换掉当前shell exec $* else #关闭tracing echo 0 > $DEBUGFS/tracing/tracing_on #清除限定的pid echo > $DEBUGFS/tracing/set_ftrace_pid #show trace log cat $DEBUGFS/tracing/trace #清空trace log cat $DEBUGFS/tracing/trace_pipe > /dev/null fi 用trace-cmd更简单: sudo trace-cmd record -F -p function ls > /dev/null sudo trace-cmd report | less 静态探测点, 也称预定义event 静态探测点是kernel代码里面预定义的探测点, 比如: #配置trace任务调度 echo 1 > events/sched/enable echo 1 > tracing_on #也可以用set_event文件 echo sched:* > set_event #以上两个方法都会在set_event里面生成sched:*的tracepoint #也可以单独为某个静态probe点设置 echo sched_wakeup >> set_event echo 1 > events/sched/sched_wakeup/enable #或者全部enable echo *:* > set_event echo 1 > events/enable #等待一段时间 echo 0 > tracing_on #查看输出 cat trace #查看输出的格式定义 cat events/sched/sched_wakeup/format #过滤语法可以对某个probe进行过滤, 比如: prev_comm是从format里面查到的变量, ~表示通配符匹配 echo 'prev_comm ~ \"test_task*\"' > events/sched/sched_switch/filter #或者匹配pid echo 'common_pid == 0' > events/sched/filter #指定pid也可以用 echo 123 244 1 >> set_event_pid #triger文件可以用来触发规定的动作, 这些动作有: traceon/traceoff: enable or disable trace snapshot: preserves current trace buffer and swap to spare buffer enable_event/disable_event: enable or disable event stacktrace: enable stack trace hist: aggregates event hits into for histogram #比如:当main进程退出的时候, 停止trace echo 'traceoff if comm ~ \"main\"' > events/sched/sched_process_exit/trigger 动态跟踪 kprobe, 动态event kprobe(和kretprobe)提供的是动态probe功能 基本原理是用break指令替换掉被probe的指令 #增加一个kprobe event, 会在events/kprobes生成一个目录 echo \"p:myprobe do_sys_open\" > kprobe_events #这个目录下有类似的format, trigger等文件 ls events/kprobes/ enable filter myprobe ls events/kprobes/myprobe/ enable filter format id trigger #去掉myprobe echo \"-:myprobe\" >> kprobe_events #或者全部清空, 执行后myprobe目录就消失了 echo > kprobe_events #开始 echo 1 > events/kprobes/myprobe/enable echo 1 > tracing_on cat trace #带参数, 在aarch64上, x0到x7表示前8个参数 echo \"p:my_probe update_min_vruntime cfs=%x0:x64\" > kprobe_events echo 1 > events/kprobes/my_probe/enable echo 1 > tracing_on 局部变量 如果仅仅用动态probe来probe函数地址, 就太大材小用了. 动态probe的真正意义在于可以probe任何程序地址(函数名+偏移量), 这也是它比function trace更灵活的地方, 比如: echo \"p:my_e1 update_min_vruntime+0x1c %x2:u64\" > kprobe_events echo 1 > events/kprobes/my_e1/enable 再比如两次引用指针: 注: 需要对照汇编代码来找到变量保存在哪个寄存器, 不容易操作.还是systemtap的kernel.statement(PATTERN)和process(\"PATH\").statement(\"*@FILE.c:123\")比较容易使用 时延tracer 关于时延的tracer有: irqsoff, preemptoff, preemptirqoff: 跟踪关闭中断/抢占的最大时间 wakeup, wakeup_rt, wakeup_dl: 跟踪任务在wakeup后到被调度的时延 需要kernel config支持 跟踪分支 kernel选项配了CONFIG_PROFILE_ANNOTATED_BRANCHES CONFIG_PROFILE_ALL_BRANCHES后有branch类型的tracer 其他功能 trace_printk(): 打印到buffer, overhead小 trace_marker: 写到trace_marker里面的内容会出现在trace buffer里, 用于同步标识echo Hello World > trace_marker option目录下面可以用来配置功能 [root@rep2-130 tracing]# for f in options/*; do printf \"$f: \"; cat $f; done | grep -v \": 0\" options/annotate: 1 options/context-info: 1 options/funcgraph-cpu: 1 options/funcgraph-duration: 1 options/funcgraph-irqs: 1 options/funcgraph-overhead: 1 options/function-trace: 1 options/graph-time: 1 options/irq-info: 1 options/markers: 1 options/overwrite: 1 options/print-parent: 1 options/record-cmd: 1 options/sleep-time: 1 options/trace_printk: 1 "},"notes/profiling_ftrace使用实例.html":{"url":"notes/profiling_ftrace使用实例.html","title":"ftrace使用实例","keywords":"","body":" ftrace使用实例 ftrace nand cmdline使用ftrace: 看open系统调用都干了什么 function_graph最大的作用是看一个函数的子函数调用 调试libc.so.6找不到问题 一次没有找到文件的open 找到文件的open 实际能找到, 为什么不能load? ftrace使用实例 ftrace nand ftrace说明文件 /sys/kernel/debug/tracing/README ftrace已经打开, 但需要手动mount mount -t debugfs none /sys/kernel/debug cd /sys/kernel/debug/tracing/ #默认就是tracing on的, 执行下面的语句就开始tracing了 echo function_graph > current_tracer echo ubifs* > set_ftrace_filter echo octeon_nand* >> set_ftrace_filter echo cvmx_nand* >> set_ftrace_filter cat trace_pipe > /dev/null echo 1 > tracing_on echo 0 > tracing_on cat trace 在另外窗口 /mnt/nand-persistent # dd if=/dev/zero of=test bs=4M count=1 1+0 records in 1+0 records out /mnt/nand-persistent # sync /mnt/nand-persistent # ls test testnand cmdline使用ftrace: 看open系统调用都干了什么 调试open系统调用, 用function_graph看open都干了什么 loglevel=8 ftrace=function_graph ftrace_graph_filter=do_sys_open ftrace_dump_on_oops tp_printk=1 octeon-wdt.disable=1 #这个不是很好用, printk是没有了, 但其子函数还有; 而且右括号还在 ftrace_graph_notrace=*printk*,_raw_spin* #下面这个好点, 不能有空格! 不能有空格! 不能有空格! ftrace_notrace=__*,*printk*,_raw_spin*,*console*,prom_putchar,*8250*,wait_for_xmitr,preempt_count* #看代码应该管用, 实际也管用! 默认如果函数被irq打断, 也会trace irq; 加这句不trace irq trace_options=nofuncgraph-irqs #如果boot支持max_graph_depth 就好了, 起来以后debug文件系统支持 max_graph_depth ftrace_graph_filter=do_sys_open : 和 /sys/kernel/debug/tracing # echo do_sys_open > set_graph_function 一样 和ftrace=function_graph连用, 只跟踪do_sys_open及其子函数 ftrace_dump_on_oops : 在kernel oop时打印ftrace buffer; 实测panic也会打 tp_printk=1 : 效果未知 octeon-wdt.disable=1 : 关闭nmi wdt trace里面夹杂着很多中断函数的调用, 很讨厌. 在debugfs里可以关掉 # funcgraph-irqs - When disabled, functions that happen inside an interrupt will not be traced. root@yingjieb-VirtualBox /sys/kernel/debug/tracing/options Linux Mint 19.1 Tessa # cat funcgraph-irqs 1 #用trace_options文件 /sys/kernel/debug/tracing hide: echo nofuncgraph-cpu > trace_options show: echo funcgraph-cpu > trace_options function_graph最大的作用是看一个函数的子函数调用 #打开function_graph, 看一个函数都调用了哪些函数 /sys/kernel/debug/tracing # cat current_tracer function_graph #这个set_graph_function和kernel cmdline的ftrace_graph_filter是一样的 /sys/kernel/debug/tracing # echo do_sys_open > set_graph_function /sys/kernel/debug/tracing # cat set_graph_function do_sys_open /sys/kernel/debug/tracing # cat set_ftrace_filter #### all functions enabled #### /sys/kernel/debug/tracing # echo 1 > tracing_on # 此时能看到所有do_sys_open的子函数及其子函数再往下所有的调用 /sys/kernel/debug/tracing # cat trace set_graph_function和kernel cmdline的ftrace_graph_filter是一样的 set_graph_function和set_ftrace_filter不冲突 set_graph_function: 要看哪个函数的子函数调用 set_ftrace_filter: 应该是更底层的控制, 哪些函数能被trace. 想知道一个函数的行为, 用function_graph max_graph_depth : 可以控制最大深度, 很有用! 调试libc.so.6找不到问题 使用新的GCC7.3编译版本运行, 在kernel启动到init的时候, 提示: [ 39.572368] (c01 1 swapper/0) This architecture does not have kernel memory protection. /bin/sh: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory [ 39.713984] (c00 1 init) Kernel panic - not syncing: Attempted to kill init! exitcode=0x00007f00 为什么会找不到libc.so呢? 在open的时候加点打印:这个时用trace_printk()的版本, 打印到trace buffer里 直接用printk会有如下调试信息: Open : /etc/ld.so.cache Open return: -2 Open : /lib32-fp/tls/octeon3/libc.so.6 Open return: -2 Open : /lib32-fp/tls/libc.so.6 Open return: -2 Open : /lib32-fp/octeon3/libc.so.6 Open return: -2 Open : /lib32-fp/libc.so.6 Open return: 3 Open : /usr/lib32-fp/tls/octeon3/libc.so.6 Open return: -2 Open : /usr/lib32-fp/tls/libc.so.6 Open return: -2 Open : /usr/lib32-fp/octeon3/libc.so.6 Open return: -2 Open : /usr/lib32-fp/libc.so.6 Open return: -2 可以看到 系统会搜索很多path来找libc libc.so.6能被找到, 但为什么还说找不到呢? 用下面的ftrace分析一下do_sys_open都干了什么, 注意, 我把一些函数过滤掉了:和trace_printk()搭配是绝配! 一次没有找到文件的open do_sys_open() /* Open : /usr/lib32-fp/tls/libc.so.6 */ //对应上面代码 getname() get_unused_fd_flags() //返回struct file * do_filp_open() path_openat() get_empty_filp() path_init() link_path_walk() terminate_walk() put_filp() put_unused_fd() putname() 找到文件的open do_sys_open() /* Open : /usr/lib32-fp/tls/libc.so.6 */ //对应上面代码 getname() get_unused_fd_flags() //返回struct file * do_filp_open() path_openat() get_empty_filp() path_init() link_path_walk() complete_walk() may_open() vfs_open() open_check_o_direct() terminate_walk() put_filp() put_unused_fd() //从这里就能看出来, 走的是找到文件的分支 fsnotify() fd_install() putname() 实际能找到, 为什么不能load? 经过咨询gcc专家, 解释是: Because there was a bug in glibc where it rejects libc for hard float mode. A mismatch between glibc and binutils linker. A problem due to porting the patch from the previous toolchain and updating only one side. 就是说在使用hard float时, 代码bug导致的libc和binutils不兼容.编译的时候不加octeon3标记就会使用soft float, 对应的库是lib32, 而不是lib32-fp 以下是patch: diff --git a/elf/elf.h b/elf/elf.h index 8244872f41..3ed1d32aa1 100644 --- a/elf/elf.h +++ b/elf/elf.h @@ -1611,8 +1611,8 @@ typedef struct #define EF_MIPS_64BIT_WHIRL 16 #define EF_MIPS_ABI2 32 #define EF_MIPS_ABI_ON32 64 -#define EF_MIPS_HARD_FLOAT 0x00000200 -#define EF_MIPS_SINGLE_FLOAT 0x00000400 +#define EF_MIPS_HARD_FLOAT 0x00000800 +#define EF_MIPS_SINGLE_FLOAT 0x00001000 #define EF_MIPS_FP64 512 /* Uses FP64 (12 callee-saved). */ #define EF_MIPS_NAN2008 1024 /* Uses IEEE 754-2008 NaN encoding. */ #define EF_MIPS_ARCH 0xf0000000 /* MIPS architecture level. */ -- 2.17.1 "},"notes/profiling_systemtap实例.html":{"url":"notes/profiling_systemtap实例.html","title":"systemtap实例","keywords":"","body":" systemtap使用举例 记录用户态函数调用 systemtap不能probe ovs-vswitchd的用户态函数? systemtap可以probe \"普通\"用户进程 问题在哪? 打印格式化标题头 关于调度相关的alias, 及其脚本可见变量 列出静态trace点 查看内核函数运行情况, 变量, 内核调用栈, 用户调用栈 看运行时内核局部变量改进版, 带调用栈 看内核函数irqfd_inject的入参 -- systemtap版本 看内核函数irqfd_inject的入参 -- perf版本 统计变量和timer相关 查看tcp的connect连接 简单例子 代码定位, 比grep方便, 但需要debug符号表? 查看函数指针(虚函数)运行时对应的实际函数 查看调用栈 跟踪应用执行流程 优点 跟踪内核执行流程 判断代码分支路径 systemtap和dtrace语法对比 systemtap实例 latencytap.stp libvirtd调试dead lock systemtap使用举例 记录用户态函数调用 在分析ovs转发路径耗时时, 用的是perf # 收包函数 $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd --add netdev_rxq_recv # 发包函数, 这里的%return表示要probe这个函数返回点 $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd --add netdev_send%return # 记录30秒, -R表示记录所有打开的counter(默认是tracepoint counters) $ sudo perf record -e probe_ovs:netdev_rxq_recv -e probe_ovs:netdev_send -R -t 38726 -- sleep 30 发现两个VM ping的场景下, ping延时从0.18ms升到了0.20ms 下面看看systemtap做相同的功能, 性能损失是多少 systemtap不能probe ovs-vswitchd的用户态函数? 奇怪的是, 下面的stap能编译成KO, 也加载了, 但一直就没输出; 难道是stap在arm64上还不支持probe用户态进程? sudo stap -e 'probe process(38726).function(\"netdev_rxq_recv\"), process(38726).function(\"netdev_send\").return {printf(\"%-16s %6d [%03d] %s %24s\\n\",execname(),tid(),cpu(),usecs_to_string(gettimeofday_us()),probefunc())}' -vv 对ovs-vswitchd的非pmd线程probe也不行. 是systemtap的bug吗? $ sudo /usr/bin/stap -e 'probe process(1989).function(\"ovs_mutex_lock_at\") {printf(\"hhhhhh\\n\")}' -vv 没输出 systemtap可以probe \"普通\"用户进程 而probe其他用户进程, 比如libvirtd, 就能触发打印.$ sudo stap -e 'probe process(38304).function(\"daemonStreamMessageFinished\") {printf(\"hhhhh\\n\")}' -vv 或者对ovsdb-server的用户态函数probe也可以$ sudo /usr/bin/stap -e 'probe process(1936).function(\"shash_find__\") {printf(\"hhhhhh\\n\")}' -vv 问题在哪? 既然perf 可以做到动态probe ovs-vswitchd, 而systemtap对有的进程又是可以probe的, 但对ovs-vswitchd的probe就有问题, 那么应该是ovs-vswitchd进程有什么特殊的地方. 我目前的结论是: 可能是使用了大页内存的关系, 导致systemtap对其支持不好. 打印格式化标题头 printf(\"%-16s %6d [%03d] %s %24s \",execname(),tid(),cpu(),usecs_to_string(gettimeofday_us()),probefunc()) 关于调度相关的alias, 及其脚本可见变量 #正在进行任务切换 probe scheduler.ctxswitch = kernel.trace(\"sched_switch\") name: name of the probe point prev_task = $prev_p next_task = $next_p prev_pid: The PID of the process to be switched out next_pid: The PID of the process to be switched in prev_tid: The TID of the process to be switched out next_tid: The TID of the process to be switched in prev_task_name: The name of the process to be switched out next_task_name: The name of the process to be switched in prev_priority: The priority of the process to be switched out next_priority: The priority of the process to be switched in prevtsk_state: the state of the process to be switched out nexttsk_state: the state of the process to be switched in #任务正开始在CPU上执行 probe scheduler.cpu_on = kernel.function(\"finish_task_switch\") name: name of the probe point task_prev: the process that was previously running on this cpu idle:- boolean indicating whether current is the idle process #任务将要在CPU上停止执行 probe scheduler.cpu_off = kernel.trace(\"sched_switch\") name: name of the probe point task_prev: the process leaving the cpu (same as current) task_next: the process replacing current idle: boolean indicating whether current is the idle process #任务正在被唤醒 probe scheduler.wakeup = kernel.trace(\"sched_wakeup\") name: name of the probe point task = $p task_pid: PID of the task being woken up task_priority: priority of the task being woken up task_cpu: cpu of the task being woken up task_state: state of the task being woken up task_tid: tid of the task being woken up #任务在进行CPU间的切换 probe scheduler.migrate = kernel.trace(\"sched_migrate_task\") name: name of the probe point task: the process that is being migrated pid: PID of the task being migrated priority: priority of the task being migrated cpu_from: the original cpu cpu_to: the destination cpu #任务在发送signal probe scheduler.signal_send = kernel.trace(\"sched_signal_send\") name: name of the probe point pid: pid of the process sending signal signal_number: signal number #调度器开始在一个进程上等待 probe scheduler.process_wait = kernel.trace(\"sched_process_wait\") name: name of the probe point pid: PID of the process scheduler is waiting on #等待一个进程变成inactive probe scheduler.wait_task = kernel.trace(\"sched_wait_task\") name: name of the probe point task_pid: PID of the task the scheduler is waiting on task_priority: priority of the task 列出静态trace点 $ stap -l 'kernel.trace(\"kvm*\")' kernel.trace(\"kvm:kvm_access_fault\") kernel.trace(\"kvm:kvm_ack_irq\") kernel.trace(\"kvm:kvm_age_hva\") kernel.trace(\"kvm:kvm_age_page\") kernel.trace(\"kvm:kvm_arm_clear_debug\") kernel.trace(\"kvm:kvm_arm_set_dreg32\") kernel.trace(\"kvm:kvm_arm_set_regset\") kernel.trace(\"kvm:kvm_arm_setup_debug\") kernel.trace(\"kvm:kvm_entry\") kernel.trace(\"kvm:kvm_exit\") kernel.trace(\"kvm:kvm_fpu\") kernel.trace(\"kvm:kvm_guest_fault\") kernel.trace(\"kvm:kvm_halt_poll_ns\") kernel.trace(\"kvm:kvm_handle_sys_reg\") kernel.trace(\"kvm:kvm_hvc_arm64\") kernel.trace(\"kvm:kvm_irq_line\") kernel.trace(\"kvm:kvm_mmio\") kernel.trace(\"kvm:kvm_mmio_emulate\") kernel.trace(\"kvm:kvm_set_guest_debug\") kernel.trace(\"kvm:kvm_set_irq\") kernel.trace(\"kvm:kvm_set_spte_hva\") kernel.trace(\"kvm:kvm_set_way_flush\") kernel.trace(\"kvm:kvm_test_age_hva\") kernel.trace(\"kvm:kvm_timer_update_irq\") kernel.trace(\"kvm:kvm_toggle_cache\") kernel.trace(\"kvm:kvm_unmap_hva\") kernel.trace(\"kvm:kvm_unmap_hva_range\") kernel.trace(\"kvm:kvm_userspace_exit\") kernel.trace(\"kvm:kvm_vcpu_wakeup\") kernel.trace(\"kvm:kvm_wfx_arm64\") $ sudo stap -l 'kernel.trace(\"sched*\")' kernel.trace(\"sched:sched_kthread_stop\") kernel.trace(\"sched:sched_kthread_stop_ret\") kernel.trace(\"sched:sched_migrate_task\") kernel.trace(\"sched:sched_move_numa\") kernel.trace(\"sched:sched_pi_setprio\") kernel.trace(\"sched:sched_process_exec\") kernel.trace(\"sched:sched_process_exit\") kernel.trace(\"sched:sched_process_fork\") kernel.trace(\"sched:sched_process_free\") kernel.trace(\"sched:sched_process_hang\") kernel.trace(\"sched:sched_process_wait\") kernel.trace(\"sched:sched_stat_blocked\") kernel.trace(\"sched:sched_stat_iowait\") kernel.trace(\"sched:sched_stat_runtime\") kernel.trace(\"sched:sched_stat_sleep\") kernel.trace(\"sched:sched_stat_wait\") kernel.trace(\"sched:sched_stick_numa\") kernel.trace(\"sched:sched_swap_numa\") kernel.trace(\"sched:sched_switch\") kernel.trace(\"sched:sched_wait_task\") kernel.trace(\"sched:sched_wake_idle_without_ipi\") kernel.trace(\"sched:sched_wakeup\") kernel.trace(\"sched:sched_wakeup_new\") kernel.trace(\"sched:sched_waking\") 查看内核函数运行情况, 变量, 内核调用栈, 用户调用栈 sudo stap ~/repo/save/debug/see_kfunc_run.stp irqfd_wakeup irqfd_inject #!/usr/bin/stap # Take up to 4 arguments as kernel functions for which stp shows their runtime status including visiable variables on the current probe point. # Each time the probed event occurs, log the kernel stack and print the same callstack records once when this script exits. # User space stack traces are also logged and printed. # eg. # sudo stap ~/repo/save/debug/see_kfunc_run.stp irqfd_wakeup irqfd_inject # 关联数组 global kstack, kstack_log global ustack, ustack_log probe begin { println(\"========begining========\\n\") } # 宏定义 @define do_probe %( ln = sprintf(\"%16s %6d [%03d] %s %s => %s\",execname(),tid(),cpu(),usecs_to_string(gettimeofday_us()),probefunc(),$$vars) println(ln) new_kstack = callers(-1) if (kstack[tid()] != new_kstack) { kstack[tid()] = new_kstack kstack_log[ln] = new_kstack } new_ustack = ucallers(-1) if (ustack[tid()] != new_ustack) { ustack[tid()] = new_ustack ustack_log[ln] = new_ustack } %) # 预处理语法 # 尝试过把命令行输入参数解析到变量, 在用变量代替@1等, 但probe语法不过; 估计probe是在预处理阶段就要定下来; 而变量要到运行阶段了. %( $# == 1 %? probe kernel.function(@1) { @do_probe } %) %( $# == 2 %? probe kernel.{function(@1),function(@2)} { @do_probe } %) %( $# == 3 %? probe kernel.{function(@1),function(@2),function(@3)} { @do_probe } %) %( $# == 4 %? probe kernel.{function(@1),function(@2),function(@3),function(@4)} { @do_probe } %) #脚本退出时调用, 比如CTRL_C probe end { println(\"========ended========\\n\") println(\"Kernel space callstack:\\n\") foreach ([ln] in kstack_log) { println(ln) print_syms(kstack_log[ln]) println() } println(\"User space callstack:\\n\") foreach ([ln] in ustack_log) { println(ln) print_usyms(ustack_log[ln]) println() } } 看运行时内核局部变量改进版, 带调用栈 # $$vars表示当前probe点上所有可见变量被sprintf到一个字符串里. sudo stap -e 'probe kernel.function(\"irqfd_wakeup\") {printf(\"%16s %6d [%03d] %s %s => \",execname(),tid(),cpu(),usecs_to_string(gettimeofday_us()),probefunc()) printf(\"%s\\n\", $$vars)}' # 带调用栈 sudo stap -e 'probe kernel.function(\"irqfd_wakeup\") {printf(\"%16s %6d [%03d] %s %s => \",execname(),tid(),cpu(),usecs_to_string(gettimeofday_us()),probefunc()) printf(\"%s\\n\", $$vars) print_syms(callers(-1))}' 看内核函数irqfd_inject的入参 -- systemtap版本 # 应该首先检查一下kernel的内核符号表是否包含irqfd_inject, 因为代码里它是static声明的, 编译器可能将其inline # 经检查是有的 $ cat /proc/kallsyms | grep irqfd_inject ffff0000080aeef0 t irqfd_inject ffff00000869e1cc t virqfd_inject # 查看内核函数irqfd_inject的源代码位置 $ stap -l 'kernel.function(\"irqfd_inject\")' kernel.function(\"irqfd_inject@virt/kvm/eventfd.c:48\") # 查看irqfd_inject的可见变量 $ stap -L 'kernel.function(\"irqfd_inject\")' kernel.function(\"irqfd_inject@virt/kvm/eventfd.c:48\") $work:struct work_struct* $ stap -L 'kernel.statement(\"irqfd_inject@virt/kvm/eventfd.c:62\")' kernel.statement(\"irqfd_inject@virt/kvm/eventfd.c:62\") $work:struct work_struct* $kvm:struct kvm* # 打印入参: work的地址 # 内置函数usecs_to_string()返回一个适合阅读的时间字符串, 其代码在tapset/linux/task_time.stp $ sudo stap -e 'probe kernel.function(\"irqfd_inject\") {printf(\"%16s %6d [%03d] %s %s => \",execname(),tid(),cpu(),usecs_to_string(gettimeofday_us()),probefunc()) printf(\"work:%x\\n\",$work)}' 看内核函数irqfd_inject的入参 -- perf版本 perf也可以probe内核函数 # 查看源码, 与stap不同的是, perf probe -L 从函数名开始记行数 $ sudo perf probe -L irqfd_inject # 查看变量 $ sudo perf probe -V irqfd_inject:6 # 添加probe点 sudo perf probe --add 'irqfd_inject+36 work:x64 kvm:x64' # 根据提示, 开始record sudo perf record -e probe:irqfd_inject -a sleep 10 # 解析记录 sudo perf script | less 可以说, 相比systemtap, perf probe列出来的代码是经过解析的, 显示更友好, 比如蓝色部分在编译的时候并没有产生代码, 比如irqfd变量被编译器优化掉了. 最后的输出结果, 相对于systemtap的可以自定义输出内容, perf probe方式的输出格式是固定的, 但基本也满足要求, 使用更简单. 另外, 可以这样访问结构体成员:sudo perf probe --add 'irqfd_inject+36 work:x64 kvm->userspace_pid:x64' 统计变量和timer相关 #oneshot probe sudo stap -e 'global x probe oneshot { for(i=1;i CONFIG_HZ=%d\\n\", count_jiffies, count_ms, hz) exit () } EOF jiffies:ms ratio 11:123 => CONFIG_HZ=89 查看tcp的connect连接 #查看tcp的connect连接, tcp_connections.stp是systemtap带的example, 需要安装systemtap-docs $ cat /usr/share/systemtap/examples/network/tcp_connections.stp #!/usr/bin/stap probe begin { printf(\"%6s %16s %6s %6s %16s\\n\", \"UID\", \"CMD\", \"PID\", \"PORT\", \"IP_SOURCE\") } probe kernel.{function(\"tcp_accept\"),function(\"inet_csk_accept\")}.return? { sock = $return if (sock != 0) printf(\"%6d %16s %6d %6d %16s\\n\", uid(), execname(), pid(), inet_get_local_port(sock), inet_get_ip_source(sock)) } #上面用到的函数inet_get_local_port()是/usr/share/systemtap/tapset/linux/inet_sock.stp提供的, tapset/linux/ip.stp有ip级的库方法. #运行这个脚本 $ sudo stap /usr/share/systemtap/examples/network/tcp_connections.stp UID CMD PID PORT IP_SOURCE 0 sshd 1357 22 10.64.16.123 0 sshd 1357 22 10.64.16.123 简单例子 begin, end, end refers to the startup and normal shutdown of the session. In this case, the handler would run once during startup and twice during shutdown. timer.jiffies(1000).randomize(200) refers to a periodic interrupt, every 1000 +/- 200 jiffies. kernel.function(\"*init*\"), kernel.function(\"*exit*\") refers to all kernel functions with \"init\" or \"exit\" in the name. kernel.function(\"*@kernel/time.c:240\") refers to any functions within the \"kernel/time.c\" file that span line 240. Note that this is not a probe at the statement at that line number. Use the kernel.statement probe instead. kernel.trace(\"sched_*\") refers to all scheduler-related (really, prefixed) tracepoints in the kernel. kernel.mark(\"getuid\") refers to an obsolete STAP_MARK(getuid, ...) macro call in the kernel. module(\"usb*\").function(\"*sync*\").return refers to the moment of return from all functions with \"sync\" in the name in any of the USB drivers. kernel.statement(0xc0044852) refers to the first byte of the statement whose compiled instructions include the given address in the kernel. kernel.statement(\"*@kernel/time.c:296\") refers to the statement of line 296 within \"kernel/time.c\". kernel.statement(\"bio_init@fs/bio.c+3\") refers to the statement at line bio_init+3 within \"fs/bio.c\". kernel.data(\"pid_max\").write refers to a hardware breakpoint of type \"write\" set on pid_max syscall.*.return refers to the group of probe aliases with any name in the third position 代码定位, 比grep方便, 但需要debug符号表? #找到kernel系统调用open的源文件位置 $ stap -l 'kernel.function(\"sys_open\")' kernel.function(\"SyS_open@fs/open.c:1072\") #也可以找用户进程的符号, 可以带通配符 $ stap -l 'process(\"/usr/local/sbin/ovs-vswitchd\").function(\"dp_netdev_input*\")' process(\"/usr/local/sbin/ovs-vswitchd\").function(\"dp_netdev_input@lib/dpif-netdev.c:4958\") process(\"/usr/local/sbin/ovs-vswitchd\").function(\"dp_netdev_input__@lib/dpif-netdev.c:4910\") 查看函数指针(虚函数)运行时对应的实际函数 参考https://blog.csdn.net/wangzuxi/article/details/42849053 #比如想知道下面的命令调用的read的具体实现 $ cat /proc/39817/stat 39817 (monitor) S 1 39817 39817 0 -1 4194368 49 0 0 0 0 0 0 0 10 -10 1 0 105153583 19595264 49 18446744073709551615 1 1 0 0 0 0 0 4096 0 0 0 0 17 24 0 0 0 0 0 0 0 0 0 0 0 0 0 #先看sys_read $ stap -l 'kernel.function(\"sys_read\")' kernel.function(\"SyS_read@fs/read_write.c:566\") #看代码, read调用vfs_read, 现在想知道这里的file->f_op->read指向谁 #可以gdb, 但搭环境太麻烦, 比如用qemu跑VM, gdb调试VM的kernel; 或者打印这个地址, 查符号表得到符号 ssize_t __vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos) { if (file->f_op->read) return file->f_op->read(file, buf, count, pos); else if (file->f_op->read_iter) return new_sync_read(file, buf, count, pos); else return -EINVAL; } 根据参考文章 用ftrace也可以查到这个函数指针的真正对象, 部分函数如下 查看调用栈 比如想看内核函数netif_receive_skb的调用栈 [root@rep2-130 debug]# cat netif_receive_skb.stp probe kernel.function(\"netif_receive_skb\") { printf(\"--------------------------------------------------------\\n\"); print_backtrace(); printf(\"--------------------------------------------------------\\n\"); } [root@rep2-130 debug]# stap netif_receive_skb.stp WARNING: Missing unwind data for a module, rerun with 'stap -d vhost_net' WARNING: Missing unwind data for a module, rerun with 'stap -d bridge' -------------------------------------------------------- 0xffff0000088220cc : netif_receive_skb+0x0/0xac [kernel] 0xffff000008670ff4 : tun_sendmsg+0x50/0x94 [kernel] 0xffff000005a9155c [vhost_net] 0x0 (inexact) -------------------------------------------------------- -------------------------------------------------------- 0xffff0000088220cc : netif_receive_skb+0x0/0xac [kernel] 0xffff00000553534c [bridge] 0x0 (inexact) -------------------------------------------------------- stap会先把netif_receive_skb.stp编译成内核模块, 然后插入内核运行, 首次编译要稍等一会没打印, 其实后台是在编译. 编译完成自动运行, 期间会出现stap打头的内核模块 bai@rep2-130 ~ $ lsmod | grep stap stap_3789144b44a61fc185145ac98a2b544_36760 6815744 2 跟踪应用执行流程 参考文章中跟踪ngnix的执行流程: https://blog.csdn.net/wangzuxi/article/details/42976577 root@jusse ~/systemtap# cat trace_nginx.stp probe process(\"/opt/nginx-dso/sbin/nginx\").function(\"*\").call { printf(\"%s -> %s\\n\", thread_indent(4), ppfunc()); } probe process(\"/opt/nginx-dso/sbin/nginx\").function(\"*\").return { printf(\"%s ngx_time_update 10 nginx(29774): ngx_event_process_posted 3 nginx(29774): ngx_event_expire_timers 3 nginx(29774): ngx_event_process_posted 2 nginx(29774): ngx_process_events_and_timers 5 nginx(29774): -> ngx_event_find_timer 8 nginx(29774): ngx_trylock_accept_mutex 16 nginx(29774): -> ngx_shmtx_trylock 19 nginx(29774): ngx_epoll_process_events 500591 nginx(29774): -> ngx_time_update 500604 nginx(29774): -> ngx_gmtime 500608 nginx(29774): ngx_vslprintf 500623 nginx(29774): -> ngx_sprintf_num …… 500655 nginx(29774): ngx_localtime 500689 nginx(29774): ngx_vslprintf 500699 nginx(29774): -> ngx_sprintf_num …… 500732 nginx(29774): ngx_vslprintf 500745 nginx(29774): -> ngx_sprintf_num …… 500784 nginx(29774): ngx_vslprintf 4 nginx(29774): -> ngx_sprintf_num …… 48 nginx(29774): ngx_vslprintf 5 nginx(29774): -> ngx_sprintf_num …… 25 nginx(29774): （省略号是删去一些重复的输出，下同）。前面的数字是这个函数调用开始或者结束时间，单位是微秒。 优点 根据systemtap的原理, 和uftrace相比, systemtap跟踪执行流程不需要重新用-pg来编译应用, 只需要debug信息, 这点非常好! 跟踪内核执行流程 以signal为例 参考: https://blog.csdn.net/wangzuxi/article/details/44901285 #先找到signal的内核代码 root@jusse ~# stap -l 'kernel.function(\"sys_signal\")' kernel.function(\"SyS_signal@/build/buildd/linux-lts-trusty-3.13.0/kernel/signal.c:3525\") root@jusse ~# stap -l 'kernel.function(\"sys_kill\")' kernel.function(\"SyS_kill@/build/buildd/linux-lts-trusty-3.13.0/kernel/signal.c:2909\") #在signal.c的所有函数上, 调用和返回的时候都调用probe root@jusse ~/systemtap# cat kernel_signal_process.stp probe begin { printf(\"begin\\n\") } probe kernel.function(\"*@/build/buildd/linux-lts-trusty-3.13.0/kernel/signal.c\").call { if (target() == pid()) { printf(\"%s -> %s\\n\", thread_indent(4), ppfunc()) } } probe kernel.function(\"*@/build/buildd/linux-lts-trusty-3.13.0/kernel/signal.c\").return { if (target() == pid()) { printf(\"%s 结果: 判断代码分支路径 参考: https://blog.csdn.net/wangzuxi/article/details/43856857 用systemtap的statement比如像看下面代码是否进了if分支, 注意这段代码一直到4805行都是在if的条件里 root@jusse ~/systemtap# cat tcp_ack_snd_check.stp probe begin { printf(\"begin\\n\"); } probe kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:*\") { printf(\"%s\\n\", pp()); } #运行 root@jusse ~/systemtap# stap tcp_ack_snd_check.stp begin kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4793\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4797\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4807\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4812\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4793\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4797\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4803\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4805\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4810\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4793\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4797\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4803\") kernel.statement(\"__tcp_ack_snd_check@/build/buildd/linux-lts-trusty-3.13.0/net/ipv4/tcp_input.c:4810\") 结果带行号, 很清楚能看到分支走向: 第一次走了if, 后面两次都是else分支. systemtap和dtrace语法对比 systemtap实例 libvirt里面有几个不错的例子 比如: libvirt-4.6.0/examples/systemtap/events.stp libvirt-4.6.0/examples/systemtap/lock-debug.stp latencytap.stp 源代码在/usr/share/systemtap/examples/profiling/latencytap.stp 这个脚本记录一个进程在kernel.trace(\"activate_task\")和kernel.trace(\"deactivate_task\")的时间差, 并在进程被调度的时候scheduler.cpu_on记录调用栈, 根据预定义的优先级表数组, 把这个进程因为什么原因睡眠, 占多少时间, 统计出来. 这个优先级表是个经验表: 通过逐条匹配调用栈的符号, 找到优先级最高的符号, 作为睡眠原因. libvirtd调试dead lock #!/usr/bin/stap --ldd -d /usr/sbin/libvirtd -c libvirtd # # Usage with installed libvirt daemon: # stap --ldd -d /usr/sbin/libvirtd -c libvirtd \\ # lock-debug.stp /usr/lib/libvirt.so # # If made executable; simple './lock-debug.stp' should work too. # # TODOs: # # Document usage with uninstalled daemon and libs. Assuming CWD is toplevel # source git directory, it should be only slight modification to the following: # # ./run stap --ldd -c src/libvirtd -d src/libvirtd # examples/systemtap/lock-debug.stp src/.libs/libvirt.so # # Debug RWLock mechanisms as well. # # Author: Martin Kletzander global mx_tolock global mx_locked function filter() { if (pid() != target()) return 1 return 0 } probe library = process( %( $# > 0 %? @1 %: \"/usr/lib/libvirt.so\" %) ) { if (filter()) next } probe lock = library.function(\"virMutexLock\") { lockname = usymdata($m) } probe unlock = library.function(\"virMutexUnlock\") { lockname = usymdata($m) } probe begin { %( $# > 1 %? println(\"error: Too many parameters\"); exit(); %: print(\"Started, press ^C when the process hangs\\n\"); %) } probe lock.call { mx_tolock[lockname, tid()] = sprint_usyms(ubacktrace()) } probe lock.return { if ([lockname, tid()] in mx_tolock) { mx_locked[lockname, tid()] = mx_tolock[lockname, tid()] delete mx_tolock[lockname, tid()] } else { printf(\"internal error: lock acquired unwillingly?\\n\") } } probe unlock.call { found = 0 foreach ([lock, tid] in mx_locked) { if (lock != lockname) continue if (tid != tid()) { printf(\"Warning: lock released on different thread that locked it.\\n\") printf(\"Lock trace:\\n%s\\n\", mx_locked[lock, tid]) printf(\"Unlock trace:\\n%s\\n\", sprint_usyms(ubacktrace())) } found = tid break } if (found && [lockname, found] in mx_locked) delete mx_locked[lockname, found] } probe end { tmp = 0 printf(\"\\n=============\\n\") foreach (bt1 = [lock1, tid1] in mx_tolock) { deadlock = 0 foreach (bt2 = [lock2, tid2] in mx_tolock) { if (lock1 == lock2) { if (!tmp++) printf(\"The following locks cannot be acquired:\\n\") if (!deadlock++) printf(\"Lock %s was locked in thread %d with this trace:\\n%s\\n\", lock1, tid1, bt1) printf(\"and is waiting to be locked by thread %d here:\\n%s\\n\", tid2, bt2) } } if (deadlock) printf(\"---\\n\") } if (!tmp) printf(\"No deadlocks found, sorry.\\n\") } "},"notes/profiling_Comparing_SystemTap_and_bpftrace.html":{"url":"notes/profiling_Comparing_SystemTap_and_bpftrace.html","title":"Comparing SystemTap and bpftrace(网摘)","keywords":"","body":"原文: https://lwn.net/Articles/852112/ 原文下面的评论也要看 Installation Program structure Features Real world uses Conclusions There are times when developers and system administrators need to diagnose problems in running code. The program to be examined can be a user-space process, the kernel, or both. Two of the major tools available on Linux to perform this sort of analysis are SystemTap and bpftrace. SystemTap has been available since 2005, while bpftrace is a more recent contender that, to some, may appear to have made SystemTap obsolete. However, SystemTap is still the preferred tool for some real-world use cases. Although dynamic instrumentation capabilities, in the form of KProbes, were added to Linux as early as 2004, the functionality was hard to use and not particularly well known. Sun released DTrace one year later, and soon that system became one of the highlights of Solaris. Naturally, Linux users started asking for something similar, and SystemTap quickly emerged as the most promising answer. But SystemTap was criticized as being difficult to get working, while DTrace on Solaris could be expected to simply work out of the box. While DTrace came with both kernel and user-space tracing capabilities, it wasn't until 2012 that Linux gained support for user-space tracing in the form of Uprobes. Around 2019, bpftrace gained significant traction, in part due to the general attention being paid to the various use cases for BPF. More recently, Oracle has been working on a re-implementation of DTrace, for Linux, based on the latest tracing facilities in the kernel, although, at this point, it may be too late for DTrace given the options that are already available in this space. The underlying kernel infrastructure used by both SystemTap and bpftrace is largely the same: KProbes, for dynamically tracing kernel functions, tracepoints for static kernel instrumentation, Uprobes for dynamic instrumentation of user-level functions, and user-level statically defined tracing (USDT) for static user-space instrumentation. Both systems allow instrumenting the kernel and user-space programs through a \"script\" in a high-level language that can be used to specify what needs to be probed and how. The important design distinction between the two is that SystemTap translates the user-supplied script into C code, which is then compiled and loaded as a module into a running Linux kernel. Instead, bpftrace converts the script to LLVM intermediate representation, which is then compiled to BPF. Using BPF has several advantages: creating and running a BPF program is significantly faster than building and loading a kernel module. Support for data structures consisting of key/value pairs can be easily added by using BPF maps. The BPF verifier ensures that BPF programs will not cause the system to crash, while the kernel module approach used by SystemTap implies the need for implementing various safety checks in the runtime. On the other hand, using BPF makes certain features hard to implement, for example, a custom stack walker, as we shall see later in the article. The following example shows the similarity between the two systems from the user standpoint. A simple SystemTap program to instrument the kernel function icmp_echo() looks like this: probe kernel.function(\"icmp_echo\") { println(\"icmp_echo was called\") } The equivalent bpftrace program is: kprobe:icmp_echo { print(\"icmp_echo was called\") } We will now look at the differences between SystemTap and bpftrace in terms of installation procedure, program structure, and features. Installation Both SystemTap and bpftrace are packaged by all major Linux distributions and can be installed easily using the familiar package managers. SystemTap requires the Linux kernel headers to be installed in order to work, while bpftrace does not, as long as the kernel has BPF Type Format (BTF) support enabled. Depending on whether the user wants to analyze a user-space program or the kernel, there might be additional requirements. For user-space software, both SystemTap and bpftrace require the debugging symbols of the software under examination. The details of how to install the symbol data depend on the distribution. On systems with elfutils 0.178 or later, SystemTap makes the process of finding and installing the right debug symbols fully automatic by using a remote debuginfod server. For example, on Debian systems: # export DEBUGINFOD_URLS=https://debuginfod.debian.net # export DEBUGINFOD_PROGRESS=1 # stap -ve 'probe process(\"/bin/ls\").function(\"format_user_or_group\") { println(pp()) }' Downloading from https://debuginfod.debian.net/ [...] This feature is not yet available for bpftrace. For kernel instrumentation, SystemTap requires the kernel debugging symbols to be installed in order to use the advanced features of the tool, such as looking up the arguments or local variables of a function, as well as instrumenting specific lines of code within the function body. In this case, too, a remote debuginfod server can be used to automate the process. Program structure Both systems provide an AWK-like language, inspired by DTrace's D, to describe predicates and actions. The bpftrace language is pretty much the same as D, and follows this general structure: probe-descriptions /predicate/ { action-statements } That is to say: when the probes fire, if the given (optional) predicate matches, perform the specified actions. The structure of SystemTap programs is slightly different: probe PROBEPOINT [, PROBEPOINT] { [STMT ...] } In SystemTap there is no support for specifying a predicate built into the language, but conditional statements can be used to achieve the same goal. For example, the following bpftrace program prints all mmap() calls issued by the process with PID 31316: uprobe:/lib/x86_64-linux-gnu/libc.so.6:mmap /pid == 31316/ { print(\"mmap by 31316\") } The SystemTap equivalent is: probe process(\"/lib/x86_64-linux-gnu/libc.so.6\").function(\"mmap\") { if (pid() == 31316) { println(\"mmap by 31316\") } } Data aggregation and reporting in bpftrace is done exactly the same way as it is done in DTrace. For example, the following program does a by-PID sum and aggregation of the number of bytes sent with the tcp_sendmsg() kernel function: $ sudo bpftrace -e 'kprobe:tcp_sendmsg { @bytes[pid] = sum(arg2); }' Attaching 1 probe... ^C @bytes[58832]: 75 @bytes[58847]: 77 @bytes[58852]: 857 Like DTrace, bpftrace defaults to automatically printing aggregation results when the program exits: no code had to be written to print the breakdown by PID above. The downside of this implicit behavior is that, to avoid automatic printing of all data structures, users have to explicitly clear() those that should not be printed. For instance, to change the script above and only print the top 5 processes, the bytes map must be cleared upon program termination. kprobe:tcp_sendmsg { @bytes[pid] = sum(arg2); } END { print(@bytes, 5); clear(@bytes); } Some powerful facilities for generating histograms are available too, allowing for terse scripts such as the following, which operates on the number of bytes read in calls to vfs_read(): $ sudo bpftrace -e 'kretprobe:vfs_read { @bytes = hist(retval); }' Attaching 1 probe... ^C @bytes: (..., 0) 169 |@@ | [0] 206 |@@@ | [1] 1579 |@@@@@@@@@@@@@@@@@@@@@@@@@@@ | [2, 4) 13 | | [4, 8) 9 | | [8, 16) 2970 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@| [16, 32) 45 | | [32, 64) 91 |@ | [64, 128) 108 |@ | [128, 256) 10 | | [256, 512) 8 | | [512, 1K) 69 |@ | [1K, 2K) 97 |@ | [2K, 4K) 37 | | [4K, 8K) 64 |@ | [8K, 16K) 24 | | [16K, 32K) 29 | | [32K, 64K) 80 |@ | [64K, 128K) 18 | | [128K, 256K) 0 | | [256K, 512K) 2 | | [512K, 1M) 1 | | Statistical aggregates are also available in SystemTap. The operator allows adding values to a statistical aggregate. SystemTap does not automatically print aggregation results when the program exits, so it needs to be done explicitly. global bytes probe kernel.function(\"vfs_read\").return { bytes Features A very useful feature of DTrace-like systems is the ability to obtain a stack trace to see which sequence of function calls lead to a given probe point. Kernel stack traces can be obtained in bpftrace as follows: kprobe:icmp_echo { print(kstack); exit() } Equivalently, with SystemTap: probe kernel.function(\"icmp_echo\") { print_backtrace(); exit() } An important problem affecting bpftrace is that it cannot generate user-space stack traces unless the program being traced was built with frame pointers. For the vast majority of cases, that means that users must recompile the software under examination in order to instrument it. SystemTap's user-space stack backtrace mechanism, instead, provides a full stack trace by making use of debug information to walk the stack. This means that no recompilation is needed. probe process(\"/bin/ls\").function(\"format_user_or_group\") { print_ubacktrace(); exit() } The script above produces a full backtrace, here shortened for readability: 0x55767a467f60 : format_user_or_group+0x0/0xc0 [/bin/ls] 0x55767a46d26a : print_long_format+0x58a/0x9f0 [/bin/ls] 0x55767a46d840 : print_current_files+0x170/0x3e0 [/bin/ls] 0x55767a465d8d : main+0x62d/0x1a00 [/bin/ls] The same feature is unlikely to be added to bpftrace, as it would need to be implemented either by the kernel or in BPF bytecode. Real world uses Consider the following example of a practical production investigation that could not proceed further with bpftrace due to the backtrace limitation, so SystemTap needed to be used to track it down. At Wikimedia we ran into an interesting problem with LuaJIT after observing high system CPU usage on behalf of Apache Traffic Server, we could confirm that it was due to mmap() being called unusually often: $ sudo bpftrace -e 'kprobe:do_mmap /pid == 31316/ { @[arg2]=count(); } interval:s:1 { exit(); }' Attaching 2 probes... @[65536]: 64988 That is where the investigation would have stopped, had it not been possible to generate user-space backtraces with SystemTap. Note that in this case the issue affected the Lua JIT component: rebuilding Apache Traffic Server with frame pointers to make bpftrace produce a stack trace would not have been sufficient, we would have had to rebuild LuaJIT too. Another important advantage of SystemTap over bpftrace is that it allows accessing function arguments and local variables by their name. With bpftrace, arguments can only be accessed by name when instrumenting the kernel, and specifically when using static kernel tracepoints or the experimental kfunc feature that is available for recent kernels. The kfunc feature is based on BPF trampolines and seems promising. When using regular kprobes, or when instrumenting user-space software, bpftrace can access arguments only by position (arg0, arg1, ... argN). SystemTap is also able to list available probe points by source file, and to match by filename in the definition of probes too. The feature can be used to focus the analysis only on specific areas of the code base. For instance, the following command can be used to list (-L) all of the functions defined in Apache Traffic Server's iocore/cache/Cache.cc: $ stap -L 'process(\"/usr/bin/traffic_server\").function(\"*@./iocore/cache/Cache.cc\") It is often necessary to probe a specific point somewhere in the body of a function, rather than limiting the analysis to the function entry point or to the return statement. This can be done in SystemTap using statement probes; the following will list the probe points available along with the variables available at each point: $ stap -L 'process(\"/bin/ls\").statement(\"format_user_or_group@src/ls.c:*\")' process(\"/bin/ls\").statement(\"format_user_or_group@src/ls.c:4110\")\\ $name:char const* $id:long unsigned int $width:int process(\"/bin/ls\").statement(\"format_user_or_group@src/ls.c:4115\")\\ $name:char const* $id:long unsigned int $width:int process(\"/bin/ls\").statement(\"format_user_or_group@src/ls.c:4116\")\\ $width_gap:int $name:char const* $id:long unsigned int $width:int process(\"/bin/ls\").statement(\"format_user_or_group@src/ls.c:4118\")\\ $pad:int $name:char const* $id:long unsigned int $width:int [...] process(\"/bin/ls\").statement(\"format_user_or_group@src/ls.c:4131\")\\ $name:char const* $id:long unsigned int $width:int $len:size_t The full output shows that there are ten different lines that can be probed inside the function format_user_or_group(), together with the various variables available in scope. By looking at the source code we can see which one exactly needs to be probed, and write the SystemTap program accordingly. To try to achieve the same goal with bpftrace we would need to disassemble the function and specify the right offset to the Uprobe based on the assembly instead, which is cumbersome at best. Additionally, bpftrace needs to be explicitly built with Binary File Descriptor (BFD) support for this feature to work. While all software is sooner or later affected by bugs, issues affecting debugging tools are particularly thorny. One specific issue affects bpftrace on systems with certain LLVM versions, and it seems worth mentioning. Due to an LLVM bug causing load/store instructions in the intermediate representation to be reordered when they should not be, valid bpftrace scripts can misbehave in ways that are difficult to figure out. Adding or removing unrelated code might work around or trigger the bug. The same underlying LLVM bug causes other bpftrace scripts to fail. The problem has recently been fixed in LLVM 12; bpftrace users should ensure they are running a recent LLVM version that is not affected by this issue. Conclusions SystemTap and bpftrace offer similar functionality, but differ significantly in their design choices by using loadable kernel module in one case and BPF in the other. The approach based on kernel modules offers greater flexibility, and allows implementing features that are hard if not impossible to do using BPF. On the other hand, BPF is an obviously good choice for tracing tools, as it provides a fast and safe environment to base observability tools on. For many use cases, bpftrace just works out of the box, while SystemTap generally requires installing additional dependencies in order to take full advantage of all of its features. Bpftrace is generally faster, and provides various facilities for quick aggregation and reporting that are arguably simpler to use than those provided by SystemTap. On the other hand, SystemTap provides several distinguishing features such as: generating user-space backtraces without the need for frame pointers, accessing function arguments and local variables by name, and the ability to probe arbitrary statements. Both would seem to have their place for diagnosing problems in today's Linux systems. "},"notes/system_analysis_bcc和ebpf.html":{"url":"notes/system_analysis_bcc和ebpf.html","title":"bcc和ebpf(starting)","keywords":"","body":"redhat写的简单易操作的文档, to be readhttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/network-tracing-using-the-bpf-compiler-collection_configuring-and-managing-networking "},"notes/as_title_os_perf.html":{"url":"notes/as_title_os_perf.html","title":"profiling和debugging实例","keywords":"","body":"如题 "},"notes/performance_ovs进程调查.html":{"url":"notes/performance_ovs进程调查.html","title":"OVS进程调查","keywords":"","body":" OVS进程调查 pmap 查看地址空间 lsof 查看打开的文件 strace 调查程序动态系统调用 pstack 抓调用栈 perf top到hot spot 情况有变化 后记 OVS进程调查 10.64.16.21机器上，41320号进程一直100%CPU，这个线程是： ovs-vswitchd --pidfile --detach --log-file ovs-vswitchd是由systemd启动的一组线程 下面我想查看这个线程在干什么，导致cpu占用100% pmap 查看地址空间 $ sudo pmap -p 41246 -x Address Kbytes RSS Dirty Mode Mapping 0000000000400000 1856 1856 0 r-x-- /usr/local/sbin/ovs-vswitchd 00000000005d0000 256 256 256 rw--- /usr/local/sbin/ovs-vswitchd 0000fff780000000 524288 0 0 rw-s- /dev/hugepages/rtemap_0 0000fff7a0000000 524288 0 0 rw-s- /dev/hugepages/rtemap_1 0000fff7c0000000 524288 0 0 rw-s- /dev/hugepages/rtemap_127 0000fff7e0000000 524288 0 0 rw-s- /dev/hugepages/rtemap_126 0000ffff983d0000 64 0 0 -w-s- /dev/infiniband/uverbs1 0000ffff983e0000 64 0 0 -w-s- /dev/infiniband/uverbs0 0000ffff998d0000 64 64 64 r---- /usr/lib64/libmlx5.so.1.3.16.0 0000ffff99b00000 64 64 64 r---- /usr/lib64/libc-2.17.so 0000ffff99f50000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_vhost.so.4.1 (deleted) 0000ffff99f60000 128 128 128 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_vhost.so.4.1 (deleted) 0000ffff99f80000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_timer.so.1.1 (deleted) 0000ffff99f90000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_timer.so.1.1 (deleted) 0000ffff99fa0000 128 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_table.so.3.1 (deleted) 0000ffff99fc0000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_table.so.3.1 (deleted) 0000ffff99fd0000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_security.so.1.1 (deleted) 0000ffff99fe0000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_security.so.1.1 (deleted) 0000ffff99ff0000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_sched.so.1.1 (deleted) 0000ffff9a000000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_sched.so.1.1 (deleted) 0000ffff9a010000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_ring.so.1.1 (deleted) 0000ffff9a020000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_ring.so.1.1 (deleted) 0000ffff9a030000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_reorder.so.1.1 (deleted) 0000ffff9a040000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_reorder.so.1.1 (deleted) 0000ffff9a050000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_power.so.1.1 (deleted) 0000ffff9a060000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_power.so.1.1 (deleted) 0000ffff9a080000 128 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_port.so.3.1 (deleted) 0000ffff9a0a0000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_port.so.3.1 (deleted) 0000ffff9a0b0000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_vmxnet3_uio.so.1.1 (deleted) 0000ffff9a0c0000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_vmxnet3_uio.so.1.1 (deleted) 0000ffff9a0d0000 128 128 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_virtio.so.1.1 (deleted) 0000ffff9a0f0000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_virtio.so.1.1 (deleted) 0000ffff9a100000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_vhost.so.2.1 (deleted) 0000ffff9a110000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_vhost.so.2.1 (deleted) 0000ffff9a120000 128 128 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_thunderx_nicvf.so.1.1 (deleted) 0000ffff9a140000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_thunderx_nicvf.so.1.1 (deleted) 0000ffff9a150000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_tap.so.1.1 (deleted) 0000ffff9a160000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_tap.so.1.1 (deleted) 0000ffff9a170000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_sw_event.so.1.1 (deleted) 0000ffff9a180000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_sw_event.so.1.1 (deleted) 0000ffff9a190000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_softnic.so.1.1 (deleted) 0000ffff9a1a0000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_softnic.so.1.1 (deleted) 0000ffff9a1b0000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_skeleton_event.so.1.1 (deleted) 0000ffff9a1c0000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_skeleton_event.so.1.1 (deleted) 0000ffff9a1d0000 64 64 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_ring.so.2.1 (deleted) 0000ffff9a1e0000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_ring.so.2.1 (deleted) 0000ffff9a1f0000 320 128 0 r-x-- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_qede.so.1.1 (deleted) 0000ffff9a240000 64 64 64 rw--- /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_pmd_qede.so.1.1 (deleted) 注：mmap的file并不是把文件内容复制到内存里，而是把文件映射到进程的虚拟地址空间，进程对这块地址空间读写的时候，OS会以page为单位， 根据情况来选择真正读文件/写文件的时机，比如你写了一段数据，这段数据所在的page在后面的某个时刻，OS决定换出，那么它会被OS真正写入 这个文件，而这个写入的动作可能又经历了文件系统 -> page caching -> IO驱动 -> 物理硬件的复杂过程。 lsof 查看打开的文件 $ sudo lsof -p 41246 ovs-vswit 41246 root txt REG 8,5 14113280 3527027728 /usr/local/sbin/ovs-vswitchd ovs-vswit 41246 root mem-R REG 0,40 536870912 968169 /dev/hugepages/rtemap_0 ovs-vswit 41246 root mem-R REG 0,40 536870912 968170 /dev/hugepages/rtemap_1 ovs-vswit 41246 root mem-R REG 0,40 536870912 968296 /dev/hugepages/rtemap_127 ovs-vswit 41246 root mem-R REG 0,40 536870912 968295 /dev/hugepages/rtemap_126 ovs-vswit 41246 root mem CHR 231,193 27211 /dev/infiniband/uverbs1 ovs-vswit 41246 root mem CHR 231,192 27210 /dev/infiniband/uverbs0 ovs-vswit 41246 root mem-w REG 0,23 208420 789242 /run/.rte_config ovs-vswit 41246 root 3w REG 8,5 283688 3527027762 /usr/local/var/log/openvswitch/ovs-vswitchd.log ovs-vswit 41246 root 15u CHR 10,196 0t0 2155 /dev/vfio/vfio ovs-vswit 41246 root 12u netlink 0t0 968157 ROUTE ovs-vswit 41246 root 21u a_inode 0,13 0 9112 [eventpoll] ovs-vswit 41246 root 36u CHR 10,200 0t0 2153 /dev/net/tun ovs-vswit 41246 root 47u unix 0xffff8017d901f300 0t0 946033 /usr/local/var/run/openvswitch/ovs-br0.snoop ovs-vswit 41246 root 51u unix 0xffff8017d6540000 0t0 953826 /usr/local/var/run/openvswitch/ovs-br0.mgmt ovs-vswit 41246 root 53u unix 0xffff8017d6540d80 0t0 953887 /usr/local/var/run/openvswitch/vhost-user0 ovs-vswit 41246 root 58u unix 0xffff8017d901f780 0t0 946094 /usr/local/var/run/openvswitch/vhost-user1 ovs-vswit 41246 root 59u unix 0xffff8017d6ee9480 0t0 953199 /usr/local/var/run/openvswitch/vhost-user2 ovs-vswit 41246 root 60u unix 0xffff8017d6ee5e80 0t0 953211 /usr/local/var/run/openvswitch/vhost-user3 注： 打开的有普通文件，内核设备，管道，unix socket，netlink socket， 这里看到有个log文件，这是调查的好入口 strace 调查程序动态系统调用 $ sudo strace -p 41246 strace: Process 41246 attached ppoll([{fd=11, events=POLLIN}, {fd=49, events=POLLIN}, {fd=10, events=POLLIN}, {fd=9, events=POLLIN}, {fd=12, events=POLLIN}, {fd=51, events=POLLIN}, {fd=36, events=POLLIN}, {fd=20, events=POLLIN}, {fd=7, eve nts=POLLIN}, {fd=30, events=POLLIN}, {fd=47, events=POLLIN}], 11, {3, 310381239}, NULL, 0) = 1 ([{fd=30, revents=POLLIN}], left {3, 247386583}) getrusage(0x1 /* RUSAGE_??? */, {ru_utime={41, 121831}, ru_stime={45, 446739}, ...}) = 0 read(30, \"\\0\", 512) = 1 recvfrom(11, 0x2409328, 264, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(10, 0xffffee67ea18, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) read(36, 0x24fbe36, 1518) = -1 EAGAIN (Resource temporarily unavailable) read(49, 0x24fbe36, 1518) = -1 EAGAIN (Resource temporarily unavailable) accept(51, 0xffffee68f7d0, 0xffffee68f7cc) = -1 EAGAIN (Resource temporarily unavailable) accept(47, 0xffffee68f7d0, 0xffffee68f7cc) = -1 EAGAIN (Resource temporarily unavailable) accept(9, 0xffffee68fac0, 0xffffee68fabc) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(20, 0xffffee67ea68, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(12, 0xffffee67ea58, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(12, 0xffffee67ea58, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(12, 0xffffee67ea58, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) ppoll([{fd=11, events=POLLIN}, {fd=49, events=POLLIN}, {fd=10, events=POLLIN}, {fd=9, events=POLLIN}, {fd=12, events=POLLIN}, {fd=51, events=POLLIN}, {fd=36, events=POLLIN}, {fd=20, events=POLLIN}, {fd=7, eve nts=POLLIN}, {fd=30, events=POLLIN}, {fd=47, events=POLLIN}], 11, {3, 247000000}, NULL, 0) = 1 ([{fd=30, revents=POLLIN}], left {2, 746806149}) getrusage(0x1 /* RUSAGE_??? */, {ru_utime={41, 122010}, ru_stime={45, 446739}, ...}) = 0 read(30, \"\\0\", 512) = 1 recvfrom(11, 0x2409328, 264, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(10, 0xffffee67ea18, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) read(36, 0x24fbe36, 1518) = -1 EAGAIN (Resource temporarily unavailable) read(49, 0x24fbe36, 1518) = -1 EAGAIN (Resource temporarily unavailable) accept(51, 0xffffee68f7d0, 0xffffee68f7cc) = -1 EAGAIN (Resource temporarily unavailable) accept(47, 0xffffee68f7d0, 0xffffee68f7cc) = -1 EAGAIN (Resource temporarily unavailable) accept(9, 0xffffee68fac0, 0xffffee68fabc) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(20, 0xffffee67ea68, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(12, 0xffffee67ea58, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(12, 0xffffee67ea58, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(12, 0xffffee67ea58, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) ppoll([{fd=11, events=POLLIN}, {fd=49, events=POLLIN}, {fd=10, events=POLLIN}, {fd=9, events=POLLIN}, {fd=12, events=POLLIN}, {fd=51, events=POLLIN}, {fd=36, events=POLLIN}, {fd=20, events=POLLIN}, {fd=7, eve nts=POLLIN}, {fd=30, events=POLLIN}, {fd=47, events=POLLIN}], 11, {2, 746000000}, NULL, 0) = 1 ([{fd=30, revents=POLLIN}], left {2, 246780821}) getrusage(0x1 /* RUSAGE_??? */, {ru_utime={41, 122186}, ru_stime={45, 446739}, ...}) = 0 read(30, \"\\0\", 512) = 1 recvfrom(11, 0x2409328, 264, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) recvmsg(10, 0xffffee67ea18, MSG_DONTWAIT) = -1 EAGAIN (Resource temporarily unavailable) read(36, 0x24fbe36, 1518) = -1 EAGAIN (Resource temporarily unavailable) read(49, 0x24fbe36, 1518) = -1 EAGAIN (Resource temporarily unavailable) accept(51, 0xffffee68f7d0, 0xffffee68f7cc) = -1 EAGAIN (Resource temporarily unavailable) accept(47, 0xffffee68f7d0, 0xffffee68f7cc) = -1 EAGAIN (Resource temporarily unavailable) accept(9, 0xffffee68fac0, 0xffffee68fabc) = -1 EAGAIN (Resource temporarily unavailable) 注： ppoll和pselect差不多，所谓pselect就是不被信号打断的select read recvfrom recvmsg都差不多，后两者多了些控制flag 这里不正常的地方在于，ppoll返回的fd，按说都应该能有数据，但是后面的read/recv等函数都读不到东西。--这就是100%的原因？ 似乎不对，应该是用户态收发包才对呀？ pstack 抓调用栈 $ sudo pstack 41320 #0 0x000000000053e91c in netdev_dpdk_vhost_rxq_recv (rxq=, batch=) at lib/netdev-dpdk.c:1943 #1 0x0000000000491d98 in netdev_rxq_recv (rx=, batch=0xffff767fe2c0, batch@entry=0xffff767fe300) at lib/netdev.c:701 #2 0x000000000046dd0c in dp_netdev_process_rxq_port (pmd=pmd@entry=0xffff94180010, rxq=0x24f4620, port_no=4) at lib/dpif-netdev.c:3279 #3 0x000000000046e0a8 in pmd_thread_main (f_=0xffff94180010) at lib/dpif-netdev.c:4146 #4 0x00000000004e452c in ovsthread_wrapper (aux_=) at lib/ovs-thread.c:348 #5 0x0000ffff99c47bb0 in start_thread () from /lib64/libpthread.so.0 #6 0x0000ffff99a6b4c0 in thread_start () from /lib64/libc.so.6 注：没看出啥来... perf top到hot spot $ sudo perf top Samples: 360K of event 'cycles:ppp', Event count (approx.): 42373494756 Overhead Shared Object Symbol 40.02% ovs-vswitchd [.] dp_netdev_process_rxq_port 25.65% [vdso] [.] monotonic 9.31% ovs-vswitchd [.] netdev_dpdk_vhost_rxq_recv 6.33% ovs-vswitchd [.] pmd_thread_main 4.65% ovs-vswitchd [.] netdev_rxq_recv 3.47% ovs-vswitchd [.] time_timespec__ 2.21% ovs-vswitchd [.] time_usec 1.67% libpthread-2.17.so [.] __pthread_once 1.65% libc-2.17.so [.] __clock_gettime 1.15% [vdso] [.] __kernel_clock_gettime #9382是这个线程的pid #进程可能重启了，变成了9382 $ sudo perf top -p 9382 Samples: 193K of event 'cycles:ppp', Event count (approx.): 38607322931 Overhead Shared Object Symbol 55.02% ovs-vswitchd [.] dp_netdev_process_rxq_port 21.43% librte_vhost.so.4.1 [.] rte_vhost_dequeue_burst 10.78% ovs-vswitchd [.] netdev_dpdk_vhost_rxq_recv 3.43% ovs-vswitchd [.] pmd_thread_main 2.92% librte_vhost.so.4.1 [.] get_device 2.82% [vdso] [.] monotonic 1.94% ovs-vswitchd [.] netdev_rxq_recv 0.41% ovs-vswitchd [.] rte_vhost_dequeue_burst@plt 0.31% ovs-vswitchd [.] time_timespec__ 0.26% ovs-vswitchd [.] time_usec 0.21% libc-2.17.so [.] __clock_gettime 0.12% libpthread-2.17.so [.] __pthread_once 0.11% [vdso] [.] __kernel_clock_gettime 0.03% ovs-vswitchd [.] clock_gettime@plt 0.02% ovs-vswitchd [.] pthread_once@plt 0.02% [kernel] [k] _raw_spin_unlock_irqrestore 0.01% libpthread-2.17.so [.] pthread_mutex_unlock 0.01% [kernel] [k] sys_futex 0.01% [kernel] [k] futex_wake 0.01% libc-2.17.so [.] _int_free 0.01% ovs-vswitchd [.] single_threaded 0.01% libc-2.17.so [.] __poll 注：以上两个，函数dp_netdev_process_rxq_port都出现在第一位 看一下汇编如下： Percent│ sub x2, x2, x3 │ add x2, x2, x1 │ str x2, [x0] │ cc: ldr x2, [x29,#88] │ str x0, [x19,#520] │ add x0, x20, #0x20 │ sub x1, x1, x2 │ → bl non_atomic_ullong_add │ ldr w0, [x19,#312] │ ↓ cbnz w0, 1d0 0.19 │ e8: str xzr, [x19,#248] │ mov w0, w23 │ ldp x19, x20, [sp,#16] 0.72 │ ldp x21, x22, [sp,#32] 0.08 │ ldp x23, x24, [sp,#48] │ ldr x25, [sp,#64] 0.77 │ ldp x29, x30, [sp],#384 │ ← ret 0.09 │108: ldr x0, [x23,#16] │ cmp x0, x22 │ ↓ b.ne 1b0 39.26 │ mrs x2, cntvct_el0 0.08 │ ldr x0, [x29,#104] 3.68 │ str x2, [x19,#512] │ ↓ cbz x0, 134 │ ldp x1, x3, [x0] │ sub x1, x1, x3 │ add x1, x1, x2 │ str x1, [x0] 0.14 │134: str x0, [x19,#520] 7.96 │ cmp w21, #0x5f │ mov w23, #0x0 // #0 │ ccmp w21, #0xb, #0x4, ne 36.43 │ ↑ b.eq e8 │ adrp x22, 5dc000 │ add x22, x22, #0x138 │ ldr w0, [x22,#36] │ cmp w0, #0x1 │ ↑ b.ls e8 │ ldr x0, [x20,#8] │ → bl netdev_rxq_get_name │ mov x20, x0 │ mov w0, w21 │ → bl ovs_strerror │ mov x4, x20 │ mov x5, x0 │ add x2, x22, #0x160 │ mov x0, x22 │ mov w1, #0x2 // #2 注意以下两条汇编占用很高 mrs x2, cntvct_el0 b.eq e8 根据ARM手册， cntvct_el0是个64bit的Virtual Timer Count register，只读的可参考https://patchwork.kernel.org/patch/9290801/ $ sudo perf stat -e cycles,stalled-cycles-frontend,stalled-cycles-backend,branch-misses,cache-references,cache-misses -p 9382 ^C Performance counter stats for process id '9382': 9,670,958,313 cycles 324,264,597 stalled-cycles-frontend # 3.35% frontend cycles idle 6,355,584,509 stalled-cycles-backend # 65.72% backend cycles idle 5,210,609 branch-misses 4,424,834,814 cache-references 48,121,592 cache-misses # 1.088 % of all cache refs 3.867804140 seconds time elapsed 这里stalled-cycles-backend很高，可能是data cache问题。复习一下data cache： cache有两种写策略： write through： 写透 写cache，同时更新下个level的mem。 write back： 写回 暂时不更新到下个level的mem，等到这个block被换出时一起更新。这样复杂点，但性能高 写也有cache miss，此时需要allocate一个cache line。 有人问写cache miss的时候，要先allocate cacheline，那么还要先把数据读到这个cache line里吗？ 答：需要。正因为cache的操作单位是cache line，比如64字节，但通常写个data，不会把这个64字节都更新， 那么就需要先读出这64字节，更新其中的一部分，再一起写回。 情况有变化 ovs-vswitchd的所有线程都100%，昨天还只有一个线程是... strace出来大部分都是futex系统调用 ifconfig也没看到有流量，用ip -s addr$ sudo pstack 27680 Thread 1 (process 27680): #0 0x0000ffffa29a048c in monotonic () at arch/arm64/kernel/vdso/gettimeofday.S:241 #1 0x0000ffffa15eeb28 in clock_gettime () from /lib64/libc.so.6 #2 0x000000000050f99c in xclock_gettime (ts=0xffff7969e3c0, id=) at lib/timeval.c:503 #3 time_timespec__ (c=0x690e70 , ts=0xffff7969e3c0) at lib/timeval.c:155 #4 0x000000000050fbf4 in time_usec__ (c=0x690e70 ) at lib/timeval.c:246 #5 time_usec () at lib/timeval.c:247 #6 0x000000000046e1dc in pmd_thread_ctx_time_update (pmd=0xffff796a0010) at lib/dpif-netdev.c:777 #7 pmd_thread_main (f_=0xffff796a0010) at lib/dpif-netdev.c:4156 #8 0x00000000004e452c in ovsthread_wrapper (aux_=) at lib/ovs-thread.c:348 #9 0x0000ffffa17b7bb0 in start_thread () from /lib64/libpthread.so.0 #10 0x0000ffffa15db4c0 in thread_start () from /lib64/libc.so.6 bai@CentOS-21 ~/tmp $ sudo pstack 27680 Thread 1 (process 27680): #0 0x0000000000491d90 in netdev_rxq_recv (rx=0xfff7ed43a100, batch=0xffff7969e2c0, batch@entry=0xffff7969e300) at lib/netdev.c:701 #1 0x000000000046dd0c in dp_netdev_process_rxq_port (pmd=pmd@entry=0xffff796a0010, rxq=0x1ea85bd0, port_no=4) at lib/dpif-netdev.c:3279 #2 0x000000000046e0a8 in pmd_thread_main (f_=0xffff796a0010) at lib/dpif-netdev.c:4146 #3 0x00000000004e452c in ovsthread_wrapper (aux_=) at lib/ovs-thread.c:348 #4 0x0000ffffa17b7bb0 in start_thread () from /lib64/libpthread.so.0 #5 0x0000ffffa15db4c0 in thread_start () from /lib64/libc.so.6 bai@CentOS-21 ~/tmp $ sudo pstack 27680 Thread 1 (process 27680): #0 0x0000ffffa1ad5420 in get_device () from /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_vhost.so.4.1 #1 0x0000ffffa1adab60 in rte_vhost_dequeue_burst () from /usr/src/dpdk-stable-17.11.3/arm64-armv8a-linuxapp-gcc/lib/librte_vhost.so.4.1 #2 0x000000000053e740 in netdev_dpdk_vhost_rxq_recv (rxq=, batch=0xffff7969e2c0) at lib/netdev-dpdk.c:1918 #3 0x0000000000491d98 in netdev_rxq_recv (rx=, batch=0xffff7969e2c0, batch@entry=0xffff7969e300) at lib/netdev.c:701 #4 0x000000000046dd0c in dp_netdev_process_rxq_port (pmd=pmd@entry=0xffff796a0010, rxq=0x1eb3c0e0, port_no=5) at lib/dpif-netdev.c:3279 #5 0x000000000046e0a8 in pmd_thread_main (f_=0xffff796a0010) at lib/dpif-netdev.c:4146 #6 0x00000000004e452c in ovsthread_wrapper (aux_=) at lib/ovs-thread.c:348 #7 0x0000ffffa17b7bb0 in start_thread () from /lib64/libpthread.so.0 #8 0x0000ffffa15db4c0 in thread_start () from /lib64/libc.so.6 后记 后来了解了一下ovs dpdk才知道, dpdk的pmd线程因为使用了轮询模式, 就是100% CPU占用的,. "},"notes/performance_ping流程和函数调用解析.html":{"url":"notes/performance_ping流程和函数调用解析.html","title":"ping流程和函数调用解析","keywords":"","body":" 环境及过程 记录并解析函数调用 流程分析 ping peer ping self ping localhost 结论 latency 本文主要分析ping命令的函数调用情况和不同场景下的区别 环境及过程 两个机器, Host A(5.5.5.11)和Host B(5.5.5.22), 物理网口(40G Mellanox CX4)直连. 在Host B上, 分三种情况 ping对端: ping 5.5.5.11, 即ping对端物理网卡的IP ping自己: ping 5.5.5.22 ping localhost ping localhost 然后考察函数调用的异同 记录并解析函数调用 sudo perf record -g ping localhost, 留稍长的时间给perf来采样, 这里有一两分钟就够了.sudo perf script | ~/repo/FlameGraph/stackcollapse-perf.pl | ~/repo/FlameGraph/flamegraph.pl > ping-localhost.svg, 解析用到flamegraph工具, 需要到github上下载. 流程分析 大致的过程是: ping先建立socket(sys_socket), 然后sys_sendto来发包, sys_recvmsg收包. 主要是内核协议栈在干活. 三个场景的火焰图见下: ping peer 核心的发送路径如下:可以看到从系统调用sys_sendto开始, 到调用实际物理网卡mlx5e_sq_xmit发送报文结束. 流程如下: Created with Raphaël 2.1.4Host BHost BHost AHost Apingsys_sendtosock_sendmsgip_finish_outputdev_hard_start_xmitmlx5e_sq_xmitICMP requestsleep on skb recive(on behalf of ping thread in kernel)handlingICMP replymlx driver recv packetwake up recive threadskb_recv_datagramsock_recvmsgsys_recvmsg(wake up ping thread)ping recive packet ping self 核心路径如下:这里也是从系统调用sys_sendto开始, 但到了ip_finish_output2, 协议栈知道这是发给自己的报文, 于是直接放到下半部,快速的走icmp的接收流程, 并且最后通过loopback_xmit发送icmp相应报文. 这个发送的报文通过loopback这个\"设备\"驱动来收包, 走标准的linux收包流程. 流程如下: Created with Raphaël 2.1.4Host B pingHost B pingHost B kernelHost B kernelpingsys_sendtosock_sendmsgip_finish_outputICMP requestdo_softirqICMP receive processip_rcvicmp_rcvicmp_echogo through ip send process againip_outputdev_hard_start_xmitsend packt through loopbackloopback_xmitICMP replysleep on skb recive(on behalf of ping thread in kernel)loopback driver recv packetwake up recive threadskb_recv_datagramsock_recvmsgsys_recvmsg(wake up ping thread)ping recive packet ping localhost 结论 三种方式的接收路径基本相同, ping调用sys_recvmsg接收报文, 导致内核在skb_recv_datagram路径上等待, 并主动让出CPU; 而从物理网口的driver接收报文很可能跑在其他CPU上(这个过程并不会被perf采样到, 因为我们是对进程号进行捕捉的), 收到并唤醒这里的等待, 进而接收报文路径返回. ping peer通过物理网卡驱动发包(好像这是废话...) ping localhost和ping self的发送流程类似, ICMP请求报文并没有真正\"发送\", 在ip层就直接走下半部接收流程, 直到ICMP回应报文通过\"localhost\"设备发出. latency ping latency ms (avg) delta % local 0.077 0 100% peer 0.134 0.057 174% peer方式比local方式的latency多了0.057ms, 结合以上分析, peer方式多了4次物理网卡驱动收发(Host B物理发, Host A物理收, Host A物理发, Host B物理收), 所以多的时间(57 us)就是这部分相关的硬件和驱动的处理时间. "},"notes/profiling_VM互相ping场景下的延迟分析.html":{"url":"notes/profiling_VM互相ping场景下的延迟分析.html","title":"VM互相ping场景下的延迟分析","keywords":"","body":" 2个VM互相ping场景下的延迟分析 OVS路径延迟 perf查看OVS转发路径执行时间 OVS转发延时数据 kernel路径延迟 eventfd和irqfd中断注入流程 kernel延迟数据 延迟图解 问题相关: 控制台输出 问题复现 抓ovs转发延迟 -- 没有发现 抓调度事件 用systemtap脚本对系统有影响, 导致高延迟现象消失 用ftrace静态probe点, 使用trace-cmd前端和使用perf前端 分析 改变VM的中断处理CPU 后续 2个VM互相ping场景下的延迟分析 场景是两个VM使用virtio-net的kernel驱动, 其后端是OVS的vhostuserclient PMD, pmd进程运行在13 15号cpu上. VM通过vhost接口与OVS连接 系统负载很轻 正常情况下, 两个VM的ping延迟在0.18ms, 但偶尔能看到超过1ms的情况, 延迟大了几乎一个数量级. 下文分析VM ping的路径延迟分布, 把总的延迟分解为可以量化的分阶段的延迟, 借此来分析异常高延迟的可能原因. BaiYingjie@rep ~/tmp $ uname -a Linux rep2-130 4.14.x.aarch64 SMP Wed Oct 17 05:03:39 UTC 2018 aarch64 aarch64 aarch64 GNU/Linux #系统通过isolcpus=2-23,26-47 只保留4个核给系统(centos 7.5)使用. HOST: 0 1 24 25 #OVS的pmd进程跑在四个核上 OVS: 12 13 14 15 #两个VM分别pin了4个CPU VM1: 26 27 28 29 VM2: 40 41 42 43 通过htop看到, 除了OVS的2个pmd线程, 系统负载很轻 OVS路径延迟 perf查看OVS转发路径执行时间 两个VM互相ping, 一秒一次. OVS走dpdk的vhost接口. 先复习一下OVS的关键路径, 要对代码熟悉:这个是OVS的pmd线程从vhost口(和VM相连)收包, 转发到dpdk物理口的流程 我们要看的其实是从vhost口收包, 再转发到vhost的流程, 只是后面不一样:netdev_send -> netdev_dpdk_vhost_send -> netdev_dpdk_vhost_send 那么转发时间, 也就是报文在OVS路径下的延迟, 是从netdev_rxq_recv收包, 到netdev_send结束的时间. 下面用perf记录这个时间: # 首先看看有没有这两个函数 $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd -F | grep netdev_send netdev_send netdev_send_wait $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd -F | grep netdev_rxq_recv netdev_rxq_recv # 有的, 增加动态probe点 # 收包函数 $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd --add netdev_rxq_recv # 发包函数, 这里的%return表示要probe这个函数返回点 $ sudo perf probe -x /usr/local/sbin/ovs-vswitchd --add netdev_send%return # 记录30秒, -R表示记录所有打开的counter(默认是tracepoint counters) $ sudo perf record -e probe_ovs:netdev_rxq_recv -e probe_ovs:netdev_send -R -t 38726 -- sleep 30 # 看结果 sudo perf script | less # 用这个命令计算netdev_send和netdev_rxq_recv的时间差 sudo perf script | grep -n1 netdev_send | awk '{print $5}' | awk -F\"[.:]\" 'NR%4==2 {print $2-t} {t=$2}' OVS转发延时数据 这个pmd线程不断轮询是否有报文netdev_rxq_recv()被不断调用, 频率约为3us一次.netdev_send()只有真正发报文才调用, 从图中看到, 从收包到发包完毕, 大约40us 在30秒的时间里, 一共有9百万行记录, 这两个函数的probe点触发频率为30万次/秒, 其中netdev_send只有2次/秒.差不多轮询15万次, 才有一次收包. $ sudo perf script | wc -l 9224453 kernel路径延迟 有时候发现VM之间ping的延时会高到2ms左右, 正常应该是0.2ms 查阅相关资料, 发现kvm提供的irqfd和eventfd联用, 来触发vm的中断. 在kvm_vm_ioctl()中, 增加KVM_ASSIGN_IRQFD, 调用kvm_assign_irqfd(), 把irqfd_wakeup()加到底下eventfd的回调里面, 通过workqueue调用irqfd_inject()完成中断注入 这里查看谁在什么时候调用了irqfd_inject(), 加-e sched是为了查看在中断注入后, 线程被唤醒的规律. sudo trace-cmd record -p function -l irqfd_inject -e sched sleep 10 sudo trace-cmd report 下面结果中, irqfd_inject后会紧跟sched_waking事件, 去唤醒KVM VCPU的线程 这里整个是说: core1运行的kworker线程, 因为irqfd_inject的触发, 去唤醒(sched_waking)qemu的vcpu线程(CPU 0/KVM), 目标是core26;46us后, core26上发生sched_switch事件, 从idle进程(swapper)切换到CPU 0/KVM进程. kworker/1:0-31764 [001] 1212277.097055: function: irqfd_inject kworker/1:0-31764 [001] 1212277.097059: sched_waking: comm=CPU 0/KVM pid=38610 prio=120 target_cpu=026 kworker/1:0-31764 [001] 1212277.097062: sched_wakeup: CPU 0/KVM:38610 [120] success=1 CPU:026 kworker/1:0-31764 [001] 1212277.097064: sched_stat_runtime: comm=kworker/1:0 pid=31764 runtime=14900 [ns] vruntime=9130727247869 [ns] kworker/1:0-31764 [001] 1212277.097068: sched_switch: kworker/1:0:31764 [120] t ==> swapper/1:0 [120] -0 [026] 1212277.097105: sched_switch: swapper/26:0 [120] R ==> CPU 0/KVM:38610 [120] CPU 0/KVM-38610 [026] 1212277.097141: sched_waking: comm=CPU 3/KVM pid=38614 prio=120 target_cpu=029 ... record命令加-e sched -T命令可以得到调用栈:调用栈也显示, irqfd_inject里, 在kvm_vcpu_kick时, 会swake_up某进程, 结合上面, 应该是KVM VCPU进程. => swake_up_locked (ffff000008129bd8) => swake_up (ffff000008129c40) => kvm_vcpu_kick (ffff0000080a9f24) => vgic_queue_irq_unlock (ffff0000080c0f3c) => vgic_its_trigger_msi (ffff0000080c893c) => vgic_its_inject_msi (ffff0000080cae0c) => kvm_set_msi (ffff0000080c25e4) => kvm_set_irq (ffff0000080cbee8) => irqfd_inject (ffff0000080aef58) => process_one_work (ffff0000080f94e8) => worker_thread (ffff0000080f9784) => kthread (ffff0000081003b8) => ret_from_fork (ffff000008084d70) 实际上, 这里有几个不同的irq被注入, 如下图 本例中的2个pmd分别运行在core13和core15中, 触发中断注入的是OVS pmd线程的写eventfd事件, 进而kworker13和kworker15执行irqfd_inject:以kworker/13:1为例, 它在29.2s 29.8s 30.2s 30.8s... 每秒2次, 都会被调度执行irqfd_inject, 这个符合两个VM互相ping的场景. eventfd和irqfd中断注入流程 最后, 整理整个流程如下: qemu创建eventfd, 调用ioctl到内核态 根据ioctl(), kernel执行kvm_irqfd_assign(), 创建struct kvm_kernel_irqfd irqfd, 关联eventfd到这个irqfd, 把irqfd_wakeup()注册到irqfd->wait的回调函数, 当irqfd被信号唤醒的时候, 调用irqfd_wakeupkvm_irqfd_assign@linux/virt/kvm/eventfd.c qemu通过控制socket把eventfd描述符传递给OVS, 走vhost-user协议 OVS的pmd线程, 在往VM发包函数rte_vhost_enqueue_burst()里, 写这个eventfd, 产生系统调用到内核态 因为之前的关联, irqfd_wakeup()被调用, 它再调用schedule_work(), 让kworker进程执行irqfd_inject irqfd_inject()完成中断注入, 并唤醒VCPU(CPU x/KVM)线程 代表VCPU的qemu线程(CPU x/KVM)被调度执行 kernel延迟数据 下面是systemtap捕捉到的相关内容: 见脚本see_kfunc_run.stp 1m37.267113s cpu15上的pmd8进程(即OVS的pmd进程)调用eventfd_write, 进而调用irqfd_wakeup 1m37.267211s pmd8唤醒kworker/15:1 1m37.267218s pmd8切换到kworker/15:1 1m37.267224s kworker/15:1调用irqfd_inject 1m37.267267s kworker/15:1唤醒\"CPU 0/KVM\", 目标CPU是40 1m37.267272s kworker/15:1切换回pmd8进程 1m37.267294s 在CPU40上, swapper/40(idle进程)切换到\"CPU 0/KVM\" 1m37.267374s 在CPU40上, \"CPU 0/KVM\"切换回swapper/40(idle进程) pmd8的调用栈, 显示了syswrite到eventfd_write到irqfd_wakeup的过程.![](img/profiling调试和分析记录2_20221017234701.png) 延迟图解 正常情况下, 两个vm的ping延时在0.18ms左右; 但做profiling会有overhead, kernel路径和OVS转发路径的overhead如下: kernel: overhead比较大在systemtap运行时, 每次irqfd_wakeup和irqfd_inject以及调度事件被捕捉到, 其运行时的ping延时有0.46ms. OVS: overhead比较小在perf record期间, ping的延迟增加大约15%, 从0.18ms到0.20ms 算上两个profile的overhead, ping的延迟在0.5ms左右 ping在VM A和VM B上的往返时间里(0.5ms), 在OVS路径上, 包括OVS转发和中断注入和唤醒的时间, 共消耗大约440us(40+181+40+181) 之前做过实验, ping localhost大约为0.077ms. 综上, 以ping延迟0.18ms计算, 在VM上的耗时共计大约60us, 在OVS转发路径下消耗120us, 其中用户态大概60us, kernel态60us. 根据以上数据, 得出大概的延迟图(ping共计耗时0.5ms, 算上2个profiling tool的overhead): Created with Raphaël 2.1.4VM_AVM_AHost_OVS(pmd)Host_OVS(pmd)VM_BVM_Bpingsys_sendtoip stack and dev_hard_start_xmitICMP requestsleep on skb receive(on behalf of ping thread in kernel)netdev_rxq_recv消耗40usnetdev_send and eventfd_writepacket forward唤醒本core的kworker执行中断注入, 唤醒VM B消耗181uswaked updriver recv packetip stack and dev_hard_start_xmit in kernelICMP replyping process receive packet, 打印时间戳netdev_rxq_recv消耗40usnetdev_sendpacket forward and eventfd_write唤醒本core的kworker执行中断注入, 唤醒VM A消耗181uswaked updriver recv packetip stack and deliver to ping processping process receive packet, 打印时间戳 问题相关: 控制台输出 VM的4个核, 其实是qemu的四个线程. 在本例中, 它们是38610 38612 38613 38614, 其余几个线程是管理辅助线程. #38610 38612 38614这几个线程类似, strace都停在KVM_RUN不动 $ sudo strace -tt -p 38610 strace: Process 38610 attached 19:21:44.113743 ioctl(30, KVM_RUN #38613线程一直有输出, 平均0.1ms就有一次ioctl调用. sudo strace -tt -p 38613 ... 19:16:03.422829 writev(12, [{\"6\", 1}], 1) = 1 19:16:03.423031 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.423105 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423175 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423237 writev(12, [{\"4\", 1}], 1) = 1 19:16:03.423304 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.423358 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423418 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423476 writev(12, [{\" \", 1}], 1) = 1 19:16:03.423536 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.423587 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423649 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423708 writev(12, [{\"b\", 1}], 1) = 1 19:16:03.423767 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.423819 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423878 ioctl(32, KVM_RUN, 0) = 0 19:16:03.423937 writev(12, [{\"y\", 1}], 1) = 1 19:16:03.423995 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.424046 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424106 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424165 writev(12, [{\"t\", 1}], 1) = 1 19:16:03.424224 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.424274 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424334 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424393 writev(12, [{\"e\", 1}], 1) = 1 19:16:03.424451 ioctl(15, KVM_IRQ_LINE, 0xffff99afe1b0) = 0 19:16:03.424502 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424560 ioctl(32, KVM_RUN, 0) = 0 19:16:03.424620 writev(12, [{\"s\", 1}], 1) = 1 ... 经过实验发现, 38613线程一直有ioctl调用, 是因为我用virtmanager来显示VM的console, 相当于VM的\"串口\", 这里的ioctl调用是ping的输出导致的. 在ssh session里面ping, 就看不到KVM_IRQ_LINE类型的ioctl, 此时4个vm的core对应的线程都是处于系统调用\"不活跃\"的状态. 仔细看strace的输出, writev()调用每次有一个字节的入参, 连起来就是ping的输出\"64 bytes from ...\" 下图就是其中一次ioctl的kernel trace 问题复现 VM1和VM2互相ping, 每秒一次ping报文. 但VM1得到的延迟异常, 为1.3ms. 抓ovs转发延迟 -- 没有发现 sudo perf record -e probe_ovs:netdev_rxq_recv -e probe_ovs:netdev_send -R -t 6965 -- sleep 30 sudo perf record -e probe_ovs:netdev_rxq_recv -e probe_ovs:netdev_send -R -t 6966 -- sleep 30 #没有发现, 转发耗时在20 ~ 60 us之间; 两个pmd都查了 sudo perf script | grep -n1 netdev_send | awk '{print $5}' | awk -F\"[.:]\" 'NR%4==2 {print $2-t} {t=$2}' 49 25 43 23 43 23 41 ... 抓调度事件 用systemtap脚本对系统有影响, 导致高延迟现象消失 sudo stap ~/repo/save/debug/see_kfunc_run.stp -d /usr/local/sbin/ovs-vswitchd irqfd_wakeup irqfd_inject -o stap-cpu13.log -x 6965 see_kfunc_run.stp是我写的systemtap脚本, 基本功能是记录内核函数的执行情况和相关任务的调度情况. 可以看到systemtap脚本的overhead还是比较大, 改变了系统的行为, 导致问题不复现了. 用ftrace静态probe点, 使用trace-cmd前端和使用perf前端 使用ftrace的静态probe可以在不破坏问题的复现的条件下, 记录系统调度信息 #只看这几个核 $ bitmask 13,14,26-29,40-43 f003c006000 #-M要在-e前面, 好像每个-e前面都可以有不同的-M sudo trace-cmd record -M f003c006000 -e sched #只看两个pmd核, 和VM1 VM2的核 sudo trace-cmd report --cpu 13,14,26-29,40-43 运行静态probe的方式, 对系统影响比较小 另外, 使用perf前端的效果和trace-cmd类似, 同样没有破坏问题的复现, overhead也一样. 只是perf命令更现代一点. sudo perf record -e sched:* -a -o perf-sched.data sudo perf script -i perf-sched.data | vi - 分析 #CPU26(VM1)开始工作 318 -0 [026] 117106.895215: sched_switch: swapper/26:0 [120] R ==> CPU 0/KVM:6853 [120] #CPU13(pmd)转发了一个报文, 调用eventfd_write, 唤醒本CPU的kworker 319 -6965 [013] 117106.895265: sched_waking: comm=kworker/13:1 pid=309 prio=120 target_cpu=013 320 -6965 [013] 117106.895267: sched_wakeup: kworker/13:1:309 [120] success=1 CPU:013 #CPU13(kworker)注入中断, 唤醒CPU40(VPU0 in VM2) 325 kworker/13:1-309 [013] 117106.895276: sched_waking: comm=CPU 0/KVM pid=6912 prio=120 target_cpu=040 #CPU26(VM1)进入idle 324 CPU 0/KVM-6853 [026] 117106.895273: sched_switch: CPU 0/KVM:6853 [120] S ==> swapper/26:0 [120] 326 kworker/13:1-309 [013] 117106.895279: sched_wakeup: CPU 0/KVM:6912 [120] success=1 CPU:040 #CPU40(VPU0 in VM2)开始工作 329 -0 [040] 117106.895320: sched_switch: swapper/40:0 [120] R ==> CPU 0/KVM:6912 [120] #CPU40一直工作, 不断唤醒CPU25的kworker来输出ping的字符 #期间CPU26从idle到kworker再到VCPU状态 #CPU13又转发了一个报文, 调用eventfd_write 360 pmd8-6965 [013] 117106.895821: sched_waking: comm=kworker/13:1 pid=309 prio=120 target_cpu=013 362 pmd8-6965 [013] 117106.895822: sched_wakeup: kworker/13:1:309 [120] success=1 CPU:013 365 pmd8-6965 [013] 117106.895824: sched_switch: pmd8:6965 [110] R ==> kworker/13:1:309 [120] # >>== 这次中断注入后, 不需要唤醒CPU40, 因为CPU40已经在运行 == pmd8:6965 [110] #CPU26重新idle 369 CPU 0/KVM-6853 [026] 117106.895838: sched_switch: CPU 0/KVM:6853 [120] S ==> swapper/26:0 [120] #期间CPU40一直处于活动状态, 不断的从VM陷入HOST, 唤醒CPU25的kworker来输出ping的字符 # >>== 这里出了问题, 从CPU13完成中断注入开始算起(117106.895827),到这里已经有1372us了! == kworker/14:1:308 [120] #CPU14唤醒CPU26(VM1 VCPU0) 462 kworker/14:1-308 [014] 117106.897204: sched_waking: comm=CPU 0/KVM pid=6853 prio=120 target_cpu=026 463 kworker/14:1-308 [014] 117106.897206: sched_wakeup: CPU 0/KVM:6853 [120] success=1 CPU:026 465 kworker/14:1-308 [014] 117106.897208: sched_switch: kworker/14:1:308 [120] t ==> pmd10:6966 [110] #CPU26(VM1 VCPU0)开始工作 468 -0 [026] 117106.897248: sched_switch: swapper/26:0 [120] R ==> CPU 0/KVM:6853 [120] #CPU26(VM1 VCPU0)开始屏幕输出 472 CPU 0/KVM-6853 [026] 117106.897346: sched_waking: comm=kworker/u92:1 pid=21632 prio=120 target_cpu=025 log看下来, 发现: OVS的pmd线程在转发报文后, 注入中断, 如果 目标VM的VCPU是idle态, 则唤醒目标VM的VCPU, 这个VCPU执行后面的处理.这种情况下ping延迟是正常的. 目标VM的VCPU正在活动, 不断的陷入HOST唤醒另一个HOST CPU, 根据分析应该是往屏幕打印(上次)ping的输出;这种情况下, ping的延迟会到1.3ms.后面perf的时候再加上KVM:*也许会看到更多的信息.而且, 这种情况下, 相应网络中断的是CPU40, 而不断陷入HOST来输出打印信息(一个字符一次)的也是CPU40, 会不会是频繁陷入HOST有影响? 改变VM的中断处理CPU 找到网卡eth1对应的中断, irq57, cat /proc/interrupts看到所有的中断都是CPU0来处理, cat /proc/irq/57/smp_affinity也显示默认的中断处理CPU是0中断处理CPU改成1 cd /proc/irq/57 [root@localhost 57]# echo 1 > smp_affinity_list [root@localhost 57]# cat smp_affinity_list 1 后面看到现在eth1的中断由CPU 1来处理了, 再看ping延迟的现象也\"消失\"了. 为什么要引号\"消失\"呢? 过一会ping延迟的现象又出现了, 仔细观察发现:现在改成VM里的CPU1来处理网卡中断, 因为VM和HOST的CPU是一对一pin的, 对应HOST的CPU41, 再次抓kernel路径发现, 负责输出字符的CPU, 也从40变为了41, 所以VM网络处理和ping的控制台输出, 还是在同一个CPU上争抢CPU时间, 导致问题现象卷土重来. 后续 目前debug到此, 基本上找到原因. 解决的话, 还要调查一下控制台输出的细节: 为什么qemu/kvm倾向于调度同一个CPU来完成两件事? 理解调度函数如何找到下一个被调度的任务/CPU pick_next_task_fair 或者简单一点, 不在控制台ping, 改为ssh到VM再ping, 这样输出不在控制台, 不走频繁陷入HOST流程. 另外, 在控制台ping, 也不是问题必现的, 因为ping每秒1次, VM要在收到并处理ICMP request报文的过程中, \"同时\"进行控制台字符输出, 需要一定的时间上的配合. "},"notes/debugging_ftrace_谁创建了bond0设备.html":{"url":"notes/debugging_ftrace_谁创建了bond0设备.html","title":"谁创建了bond0设备: ftrace kprobe uprobe perf综合使用","keywords":"","body":" bond0设备的例子 背景 -- 是谁创建了bond0设备 使用ftrace打印调用栈 -- MIPS上不成功 打开stacktrace选项 使用function trace 初步结果 使用笨方法 bond代码: bond驱动会默认创建bond0设备 打印调用栈 -- MIPS上不成功 补充网上成功的例子 使用perf probe perf支持的栈帧记录模式 带dwarf解析的输出 不带dwarf解析的输出 例子 但boot time怎么办? probe方式看用户态进程的调用栈 ftrace方法 -- 不成功 先回顾以下kprobe用法 再看uprobe 实际操作 -- 最后enable没成功 perf probe方法 -- 成功 bond0设备的例子 背景 -- 是谁创建了bond0设备 用kernel4.9时, 多了bond0设备, 导致其默认路由配置错误, 以及mtu配置错误, 给NT LT通信带来了不小的麻烦. 因为新kernel配置了bonding驱动, 但这个驱动会自动创建bond0设备, 还是启动脚本里创建的, 不得而知. 搜索启动脚本, 有bond相关操作的脚本有: echo broadcast > /sys/class/net/bond0/bonding/mode ifconfig bond0 up echo +eth-nta > /sys/class/net/bond0/bonding/slaves vconfig add bond0 $vlan 似乎没有哪里创建bond0设备 使用ftrace打印调用栈 -- MIPS上不成功 网络驱动会调用register_netdevice来创建网络设备. 思路是使用ftrace, 在register_netdevice被调用的时候, 打印调用栈; 通过调用栈, 来进一步寻找谁创建了netdev. 打开stacktrace选项 要用到trace_options的stacktrace功能, 默认是关掉的.带前缀no的是关闭的选项. /sys/kernel/debug/tracing # cat trace_options print-parent nosym-offset nosym-addr noverbose noraw nohex nobin noblock trace_printk annotate nouserstacktrace nosym-userobj noprintk-msg-only context-info nolatency-format record-cmd overwrite nodisable_on_free irq-info markers noevent-fork function-trace nodisplay-graph nostacktrace notest_nop_accept notest_nop_refuse 相关的选项说明如下: userstacktrace - 记录当前线程的用户态调用栈, 默认关闭 stacktrace - 记录调用栈, 默认关闭 要打开stacktrace, 可以:echo stacktrace > trace_options也可以在启动cmdline里加 trace_options=stacktrace参考: Documentation/trace/ftrace.txt 使用function trace 本文这里, bond0在系统启动时就生成了, 如果用debugfs来调试就太完了. 所以我用cmdline来使能ftrace调试. 在kernel cmdline里, ftrace=function ftrace_filter=register_netdevice trace_options=stacktrace ftrace= 效果和debugfs里面的current_tracer一样 ftrace_filter= 效果和debugfs里面的set_ftrace_filter一样 参考: kernel/trace/trace.c, 搜索__setup kernel/trace/ftrace.c, 搜索__setup 初步结果 看来没有stacktrace的内容, 经过验证, cmdline里面的trace_options=stacktrace没有作用. function tracer默认打印一级的调用关系 一个比较笨的思路是, 因为function trace默认有上一级函数调用, 那么可以用ftrace filter反复trace多次, 一步步的找到调用源头.但这个方法对boot time的函数跟踪很不友好, 要反复重启N多次. 使用笨方法 #增加ftrace=function ftrace_filter=register_netdevice,bond_create,bonding_init,register_netdev bootoctlinux $(loadaddr) 'coremask=0x0f mem=0 ctxt=OSW isamversion=ZAM8AA62.990, prozone=0x4000000,0x80000000 logbuffer=uboot,0x20000,0x7be00000 bpsflag=0x4,0x7be20000 bpcommit=0,0x140000,0x160000 mtdparts=octeon_nand0:0x8000000@0x0(recovery),-@0x8000000(nand);octeon_nand1:0x40000000@0x0(nand1);bootflash:0x20000@0x140000(statusA),0x20000@0x160000(statusB),0x140000@0x180000(bootA),0x140000@0x2c0000(bootB),0x1900000@0x400000(linuxA),0x1900000@0x1d00000(linuxB),0x20000@0x120000(preferred_oswp),0x80000@0x3600000(management_a),0x80000@0x3680000(management_b),0x120000@0x0(bps) linux_fit_image=0x1e00000,0x7a000000 unpreferred_oswp=0 console=ttyS1,115200 ftrace=function ftrace_filter=register_netdevice,bond_create,bonding_init,register_netdev config_overlay=linux_shell_only=1,reboot=0' bond代码: bond驱动会默认创建bond0设备 bonding_init() ... //max_bonds默认是1 for (i = 0; i 打印调用栈 -- MIPS上不成功 再调查一下, 原来function tracer提供了自己的func_stack_trace选项. ftrace的options有两个形式: trace_options文件: 只有通用的option options目录: 除了通用的option, 还有各个tracer的特殊option function tracer使用func_stack_trace来打开调用栈 /sys/kernel/debug/tracing # cat options/func_stack_trace 0 试试看能不能在cmdline里面打开ftrace=function ftrace_filter=register_netdevice trace_options=func_stack_trace这次有调用栈, 但都是0, 似乎是MIPS的bug. 注: 即使打开了内核选项CONFIG_FRAME_POINTER=y也不行. MIPS的CONFIG_FRAME_POINTER不怎么管用. 参考: Secrets of the Ftrace function tracer 补充网上成功的例子 使用function tracer提供的options/func_stack_trace [tracing]# echo kfree > set_ftrace_filter [tracing]# cat set_ftrace_filter kfree [tracing]# echo function > current_tracer [tracing]# echo 1 > options/func_stack_trace [tracing]# cat trace | tail -8 => sys32_execve => ia32_ptregs_common cat-6829 [000] 1867248.965100: kfree => free_bprm => compat_do_execve => sys32_execve => ia32_ptregs_common [tracing]# echo 0 > options/func_stack_trace [tracing]# echo > set_ftrace_filter 使用perf probe perf probe和perf record的组合, 能够记录几乎任何函数的运行时调用关系. perf probe底层使用kprobe来实现对kernel的探测, 对用户态程序则使用uprobe(基于utrace框架). probe方式最基本的思想是把被probe的指令, 替换成breakpoint指令, 在hit的时候, 调用特殊的handler; handler作为一个同步异常, 工作在内核态, 能够访问当前线程的所有信息. 记录调用栈最关键的地方在于记录栈帧信息, 对octeon系列的MIPS板子来说, 内核态和用户态稍微有点区别: 内核态: 虽然MIPS内核默认没有CONFIG_FRAME_POINTER=y, 但内核的栈帧似乎被perf解析的很好.注: 即使打开CONFIG_FRAME_POINTER=y, 效果也没有更好. 用户态函数: 必须用dwarf方式记录调用栈. perf支持的栈帧记录模式 在perf record时候, 通过--call-graph参数可以指定record调用栈的方式: fp: 编译时使用栈指针. 如果编译时有--fomit-frame-pointer, 则对栈帧解析不准. dwarf : 编译时有--fomit-frame-pointer, 要使用dwarf格式, 需要libundwind或libdwdwarf是一种标准的debugging信息的格式, 包括了CFI (Call Frame Information).使用dwarf模式时, perf record会同时记录用户态栈帧 lbr : Intel硬件支持 --call-graph Setup and enable call-graph (stack chain/backtrace) recording, implies -g. Default is \"fp\". Allows specifying \"fp\" (frame pointer) or \"dwarf\" (DWARF's CFI - Call Frame Information) or \"lbr\" (Hardware Last Branch Record facility) as the method to collect the information used to show the call graphs. In some systems, where binaries are build with gcc --fomit-frame-pointer, using the \"fp\" method will produce bogus call graphs, using \"dwarf\", if available (perf tools linked to the libunwind or libdw library) should be used instead. Using the \"lbr\" method doesn't require any compiler options. It will produce call graphs from the hardware LBR registers. The main limitation is that it is only available on new Intel platforms, such as Haswell. It can only get user call chain. It doesn't work with branch stack sampling at the same time. When \"dwarf\" recording is used, perf also records (user) stack dump when sampled. Default size of the stack dump is 8192 (bytes). User can change the size by passing the size after comma like \"--call-graph dwarf,4096\". 带dwarf解析的输出 perf record -e probe:register_netdevice -aR -g --call-graph dwarf -- sleep 20注: eoe_filter被strip掉了debug信息, 所以这里显示unknown 不带dwarf解析的输出 perf record -e probe:register_netdevice -aR -g -- sleep 20 例子 比如我想看是谁调用了register_netdevice函数, 这是内核函数, 调用后会创建个netdev. #添加内核probe点 perf probe register_netdevice #记录20秒 perf record -e probe:register_netdevice -aR -g -- sleep 20 #使用dwarf还可以看到用户态调用 perf record -e probe:register_netdevice -aR -g --call-graph dwarf -- sleep 20 #在另外一个窗口操作, 会触发函数调用; 我敲了4次命令 /isam/user/eoe_filter -n lo -t tap0 -E #record完成后会有如下打印, 显示采样到4次. ~ # [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.087 MB perf.data (4 samples) ] #能够看调用栈 perf script 但boot time怎么办? perf probe底层使用kprobe的event接口, 虽然上面的例子里, 在系统起来后, 用perf probe可以看调用栈. 但kernel启动过程中, kprobe并没有提供相应的手段. Huawei的wang nan在2015年提交了一组patch: 没被接受?Early kprobe: enable kprobes at very early booting stage 19年Masami Hiramatsu提交的patch似乎更好: 使用了新的kernel cmdline方式: SKC(Supplemental Kernel Cmdline)tracing: skc: Boot-time tracing and Supplemental Kernel Cmdline以上两组patch改的很深入, 值得看看 Supplemental Kernel Cmdline: Supplemental kernel command line (SKC) allows admin to pass a tree-structured supplemental kernel commandline file (SKC file) when boot up kernel. This expands the kernel command line in efficient way. probe方式看用户态进程的调用栈 主要是用uprobe接口和callstack调试用户态perf probe找不到call frame问题 ftrace方法 -- 不成功 ~ # cp remote/root/tmp/a.out . ~ # perf probe -x a.out -V p -v Open Debuginfo file: /root/a.out Searching variables at p Matched function: p [325] Failed to get call frame on 0x10000600 Failed to find the address of p Error: Failed to show vars. Reason: No such file or directory (Code: -2) perf是在kernel目录树tools下面编译的, 是个用户态程序.错误打印出自: tools/perf/util/probe-finder.c的call_probe_finder()函数, 有好几个路径会调用它. 那么怎么知道出错时确切的调用路径呢?联想到, ftrace支持的filter格式中, 有stacktrace选项, 见ftrace和trace-cmd记录, 但那是内核的function trace提供的功能, kprobe/uprobe有吗? 先回顾以下kprobe用法 详见linux/Documentation/trace/kprobetrace.rst #用到的文件 /sys/kernel/debug/tracing/kprobe_events /sys/kernel/debug/tracing/events/kprobes//enable #语法 p[:[GRP/]EVENT] [MOD:]SYM[+offs]|MEMADDR [FETCHARGS] : Set a probe r[MAXACTIVE][:[GRP/]EVENT] [MOD:]SYM[+0] [FETCHARGS] : Set a return probe -:[GRP/]EVENT : Clear a probe #增加一个kprobe event, 会在events/kprobes生成一个目录 echo \"p:myprobe do_sys_open\" > kprobe_events #这个目录下有类似的format, trigger等文件 ls events/kprobes/ enable filter myprobe ls events/kprobes/myprobe/ enable filter format id trigger #去掉myprobe echo \"-:myprobe\" >> kprobe_events #或者全部清空, 执行后myprobe目录就消失了 echo > kprobe_events #开始 echo 1 > events/kprobes/myprobe/enable echo 1 > tracing_on cat trace #带参数, 在aarch64上, x0到x7表示前8个参数 echo \"p:my_probe update_min_vruntime cfs=%x0:x64\" > kprobe_events echo 1 > events/kprobes/my_probe/enable echo 1 > tracing_on 再看uprobe 详见linux/Documentation/trace/uprobetracer.rst filter和trigger等控制文件使用说明: linux/Documentation/trace/events.rst文档上说要打开CONFIG_UPROBE_EVENTS实际上, 新版本的kernel使用 ~ # zcat /proc/config.gz | grep -i UPROBE CONFIG_ARCH_SUPPORTS_UPROBES=y CONFIG_UPROBES=y CONFIG_UPROBE_EVENT=y 控制文件: /sys/kernel/debug/tracing/uprobe_events /sys/kernel/debug/tracing/events/uprobes//enable 格式 p[:[GRP/]EVENT] PATH:OFFSET [FETCHARGS] : Set a uprobe r[:[GRP/]EVENT] PATH:OFFSET [FETCHARGS] : Set a return uprobe (uretprobe) -:[GRP/]EVENT GRP : Group name. If omitted, \"uprobes\" is the default value. EVENT : Event name. If omitted, the event name is generated based on PATH+OFFSET. PATH : Path to an executable or a library. OFFSET : Offset where the probe is inserted. FETCHARGS : Arguments. Each probe can have up to 128 args. %REG : Fetch register REG @ADDR : Fetch memory at ADDR (ADDR should be in userspace) @+OFFSET : Fetch memory at OFFSET (OFFSET from same file as PATH) $stackN : Fetch Nth entry of stack (N >= 0) $stack : Fetch stack address. $retval : Fetch return value.(*) $comm : Fetch current task comm. +|-offs(FETCHARG) : Fetch memory at FETCHARG +|- offs address.(**) NAME=FETCHARG : Set NAME as the argument name of FETCHARG. FETCHARG:TYPE : Set TYPE as the type of FETCHARG. Currently, basic types 实际操作 -- 最后enable没成功 mount -t debugfs none /sys/kernel/debug cd /sys/kernel/debug/tracing/ #Failed to get call frame on是在call_probe_finder函数里打印的 #先要要先找到这个函数的地址, 因为uprobe只接受地址 yingjieb@FNSHA190 /repo/yingjieb/ms/buildroot73/output/build/linux-custom/tools/perf $ nm perf | grep call_probe_finder 10114b48 t call_probe_finder #按照格式, 创建一个uprobe echo 'p /usr/bin/perf:0x10114b48' > /sys/kernel/debug/tracing/uprobe_events #在events下面, 会生成一个新的目录 /sys/kernel/debug/tracing # ls events/uprobes/ enable filter p_perf_0x10114b48 #在新的probe目录下, 有filter和trigger控制文件, 这就是我想要的 /sys/kernel/debug/tracing/events/uprobes/p_perf_0x10114b48 # ls enable filter format id trigger #加stacktrace触发: 注意echo后面必须加引号 echo 'stacktrace' > /sys/kernel/debug/tracing/events/uprobes/p_perf_0x10114b48/trigger /sys/kernel/debug/tracing/events/uprobes/p_perf_0x10114b48 # cat trigger stacktrace:unlimited #开始, 提示错误!!! /sys/kernel/debug/tracing/events/uprobes/p_perf_0x10114b48 # echo 1 > enable [247440.579699] (c02 4595 sh) event trace: Could not enable event p_perf_0x10114b48 sh: write error: Invalid argument perf probe方法 -- 成功 要点: 在mips上, 必须用--call-graph dwarf才能看到调用栈; 默认的fp(fram pointer)方式不行 使用perf probe命令定义的动态probe点, 在/sys/kernel/tracing/events/uprobe下面是没有event的;但在这个目录下有: /sys/kernel/tracing/events/probe_perf;说明perf probe -x对用户态probe, 底层也是调用uprobe接口? 注: 同时看看/sys/kernel/tracing和/sys/kernel/debug/tracing # 用perf probe定义probe点 ~ # perf probe -x /root/perf call_probe_finder Failed to get call frame on 0x10114b48 Added new event: probe_perf:call_probe_finder (on call_probe_finder in /root/perf) You can now use it in all perf tools, such as: perf record -e probe_perf:call_probe_finder -aR sleep 1 # 记录 ~ # perf record -e probe_perf:call_probe_finder -aR --call-graph dwarf -- /root/perf probe -x a.out -V p -v Open Debuginfo file: /root/a.out Searching variables at p Matched function: p [325] Failed to get call frame on 0x10000600 Failed to find the address of p Error: Failed to show vars. Reason: No such file or directory (Code: -2) [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.102 MB perf.data (1 samples) ] #解析perf.data ~ # perf script perf 4779 [001] 248792.3977600534: probe_perf:call_probe_finder: (10114b48) 114b48 call_probe_finder+0xe0000000 (/root/perf) 117360 probe_point_search_cb+0xe0000278 (/root/perf) 173fc [unknown] (/usr/lib/libdw-0.176.so) 17360 [unknown] (/usr/lib/libdw-0.176.so) 15e18 dwarf_getfuncs+0x125aa184 (/usr/lib/libdw-0.176.so) 117560 debuginfo__find_probe_location.isra.8+0xe0000140 (/root/perf) 1177dc debuginfo__find_available_vars_at+0xe000011c (/root/perf) 10e75c show_available_vars+0xe00000fc (/root/perf) 3b1fc __cmd_probe.isra.4+0xe0000a54 (/root/perf) 3b9dc cmd_probe+0xe00000ac (/root/perf) 5f928 run_builtin+0xe0000098 (/root/perf) 83a8 main+0xe0000780 (/root/perf) 1de1c __libc_start_main+0x12cf40fc (/lib/libc-2.27.so) 84dc hlt+0xe0000000 (/root/perf) "},"notes/profiling_unixbench之filecopy分析.html":{"url":"notes/profiling_unixbench之filecopy分析.html","title":"unixbench之file copy分析","keywords":"","body":" 现象 复现与简化 初步分析 htop 用内存文件系统? 代码分析 cscope/global结合vim代码阅读 生成cscope.files 建立代码检索数据库 直接到main函数 用gdb tui模式看代码 代码逻辑 perf 对比试验 加长运行时间, 运行时观察 和存储设备无关? 4.9 vs 4.10 和2699对比 profiling perf flamegraph 结论 本文是一个入门的分析方法. 背景是对比ARM64 server芯片和Intel server芯片的性能在ES2评估过程中, 用unixbench测试, 文件系统性能低, 这里记录分析过程, 包括初步排查, 代码梳理, gdb代码跟踪, perf性能分析, 以借此演示一些基础工具的使用方法. AW: Qualcomm server SOC Amberwing(48 core AARCH64)hp380: Intel 2699v4 现象 unixbench的file copy项里, AW比Intel慢4到5倍 复现与简化 Unixbench编译省略, 运行只有一个Run脚本, 直接运行会按默认的配置跑完整个测试用例, 并给出得分(index).但这里我们只关心file copy, 所以第一步是先读这个perl脚本, 找到单独调用这个测试项的办法. 没有--help, 看来只有看脚本了:./Run fstime -i 1 -c 1运行fstime 1次(默认是10次), 1个copy, unixbench默认先跑1 copy, 再跑全copy(和核数相等)脚本里提到了fstime的传入参数 File Copy 1024 bufsize 2000 maxblocks -c -t 30 -d tmp -b 1024 -m 2000 File Copy 256 bufsize 500 maxblocks -c -t 30 -d tmp -b 256 -m 500 File Copy 4096 bufsize 8000 maxblocks -c -t 30 -d tmp -b 4096 -m 8000 那么可以直接调用: $ pgms/fstime -c -t 30 -d tmp -b 1024 -m 2000 Write done: 2448000 in 2.0006, score 305906 COUNT|305906|0|KBps TIME|2.0 Read done: 8954552 in 2.0000, score 1119313 COUNT|1119313|0|KBps TIME|2.0 Copy done: 28439512 in 30.0000, score 236995 COUNT|236995|0|KBps TIME|30.0 从打印来看, 是先写, 再读, 最后copyOK, 现在复现问题已经化简到一条更具体的命令. 初步分析 htop htop显示CPU8打满100%, 大部分在内核态(红色) 用内存文件系统? 运行时生成的测试文件只有两个, dummy0, dumm1, 大小只有2M. 那么这个file copy是否大部分在文件系统的buffer里, 和实际的物理介质关系不大呢?$ sudo mount -o size=2G -t tmpfs none tmp 还真不是, 换成tmpfs以后, 性能达到2倍以上. $ pgms/fstime -c -t 30 -d tmp -b 1024 -m 2000 Write done: 8056000 in 2.0012, score 1006395 COUNT|1006395|0|KBps TIME|2.0 Read done: 11411328 in 2.0000, score 1426408 COUNT|1426408|0|KBps TIME|2.0 Copy done: 66244060 in 30.0000, score 552033 COUNT|552033|0|KBps TIME|30.0 代码分析 题外话, 这个unixbench代码很少, 工程结构也比较简单. 但考虑到以后要分析更大的工程代码, 那么先找个通用的, 快速的代码分析方法. cscope/global结合vim代码阅读 需要先安装cscope和gnu global, 我个人推荐gnu global, 这两个工具都会生成代码的检索数据库, 方便代码阅读. 生成cscope.files gdb可以根据elf文件的debug信息(编译时加-g), 列出源文件列表. 这个列表里面的源文件是真正参与编译的, 对大工程推荐用这招, 减少冗余文件的干扰. mkcselffiles() { ${CROSS_COMPILE}gdb -ex=\"info sources\" -ex=\"quit\" $1 | sed -e '1,15d' -e 's/,/\\n/g' | sed -e '/^ *$/d' -e 's/^ *//g' > cscope.files.tmp1 #find -L `cat cscope.files.tmp1 | egrep \"/flat/\" | sed 's!\\(.*/flat/[^/]*\\).*!\\1!g' | sort -u` -iname \"*.h\" -o -iname \"*.hh\" -o -iname \"*.hpp\" > cscope.files.tmp2 #cat cscope.files.tmp1 cscope.files.tmp2 | sort -u > cscope.files cat cscope.files.tmp1 | sort -u > cscope.files rm -f cscope.files.tmp* } 或者简单点, 生成所有源文件列表 mkcsfiles() { #find -L $* -iname '*.[ch]' -o -iname '*.[ch]pp' -o -iname '*.cc' -o -iname '*.hh' -o -iname '*.s' | sort -u > cscope.files find -L $* -type f | sed -n '/\\.\\([chs]\\)\\1\\{0,1\\}\\(pp\\)\\?$/Ip' | sort -u > cscope.files } 这里我用 mkcselffiles pgms/fstime 建立代码检索数据库 有两种 cscopecscope -bkq -i cscope.files gnu global用global需要在当前工程的根目录运行gtags -f cscope.files 直接到main函数 vim -t main 用gdb tui模式看代码 边看边调, 参考GDB tui教程 gdb -tui pgms/fstime b main run -c -t 30 -d tmp -b 1024 -m 2000 代码逻辑 前面提到, 整个测试过程是先写, 再读, 再拷贝.按照这个过程, 代码逻辑如下:一个读文件(dummy0), 一个写文件(dummy1)在写的时候, 就是一直用底层的wirte函数写文件, 读类似, 拷贝是先读再写. buf是静态数组 计分是在固定时间内的拷贝的字节数 这是个单进程的测试程序 perf perf record pgms/fstime -c -t 30 -d tmp -b 1024 -m 2000 perf report 在SDP1上, 4.9.0.2 在SDP2上, 4.9.0.4注: 以上命令是在用户权限下看到的统计, 注意event标记是'cycles:u' sudo perf record pgms/fstime -c -t 30 -d tmp -b 1024 -m 2000 sudo perf report 在SDP1上, 4.9.0.2 在SDP2上, 4.9.0.4 对比试验 重点看SDP1上的perf, 迹象表明可能和auditd有关(老外提示), 比如selinux*高占用, __audit_sysall_entry等.那打开和关闭auditd的对比结果:在SDP1上, 4.9.0-2.el7.aarch64(64K): tmpfs情况下, 关闭auditd, 系统write效率提高大约24%, read提高17%, 拷贝提高20% 环境: 4.9.0-2.el7.aarch64(64K), tmpfs|auditd|read|write|copy |--|--|--|-- |auditd on|895906|1163064|502048 |auditd off|1055766|1445539|607435 普通ssd, 关闭auditd, 读提高6%, 写提高27%, 拷贝提高6% 环境: 4.9.0-2.el7.aarch64(64K), ssd(240G)|auditd|read|write|copy |--|--|--|-- |auditd on|260701|1158512|212135 |auditd off|278926|1478228|226238 综合以上, audit关闭对tmpfs提升大, 对普通的ssd提升较小.是不是因为有实际的磁盘读写, 相对慢的磁盘开销导致的? 加长运行时间, 运行时观察 pgms/fstime -c -t 300 -d tmp -b 1024 -m 2000 sudo perf top -p `pidof fstime` sudo iotop -p `pidof fstime` 磁盘利用率很低, 基本上要好几秒才会有实际的磁盘读写, 其他时间磁盘读写都是0这说明可能并不是磁盘读写, 而是xfs文件系统路径的复杂度拖慢了性能?perf top里面有很多关于xfs的调用. 和存储设备无关? 那如何证明呢? 建立tmpfssudo mount -o size=2G -t tmpfs none mytmp 在这个tmpfs里面dd一个文件, 并格式化成xfs dd if=/dev/zero of=mytmp/testdisk bs=1M count=1000 mkfs.xfs mytmp/testdisk mount 这个文件sudo mount mytmp/testdisk tmp OK, 现在测试目录tmp是xfs文件系统, 但实际上是在tmpfs下面的一个文件, 都是在内存里. 环境: 4.9.0-2.el7.aarch64(64K), xfs on file(which is in tmpfs)auditd on: r264033, w1153529, c211546 对比前面在SSD上的数据, 几乎没差别.环境: 4.9.0-2.el7.aarch64(64K), ssd(240G)auditd on: r260701, w1158512, c212135 这说明: 这个测试项, 性能和具体存储设备无关, 底层的文件系统直接决定了测试结果 4.9 vs 4.10 sdp2比sdp1, 读快了22%, 写慢了8%, 拷贝快了18% 环境: AWSDP, xfs on ssd(240G), audit off |kernel|read|write|copy |--|--|--|-- |4.9|278926|1478228|226238 |4.10|341409|1360577|267056 和2699对比 所以, 公平的对比要在: audit 同时on, 或者同时off 都是xfs 读是2699v4的35%, 写是44%, 拷贝是36% 环境: xfs on ssd, auditd on |CPU/server/kernel|read|write|copy |--|--|--|-- |AW/SDP/4.9|260701|1158512|212135 |2699v4/HP380/3.10|739714|2628781|574521 profiling perf SDP1: 2699v4: flamegraph 生成flamegraph sudo perf record -g pgms/fstime -c -t 30 -d tmp -b 1024 -m 2000 sudo perf script | ~/yingjieb/FlameGraph/stackcollapse-perf.pl | ~/yingjieb/FlameGraph/flamegraph.pl > fstime.svg on SDP2: on 2699v4: 结论 这个单进程的测试项基本可以认为反映了单核处理文件系统的能力. AW关闭auditd, 文件写提升27%, 拷贝提升6% AW 4.10(4K)比4.9(64K)在读上快22%, 但写慢了8%, 综合好18%. 和具体存储硬件无关, 是内核文件系统的执行效率问题, 对比2699v4, AW执行文件系统路径更慢, 或者说AW跑这些代码更慢. 整个文件系统路径如下: 系统调用读写->linux vfs读写->xfs读写->block设备驱动读写. ll_sc*, 以及block_write_begin_int等排名靠前的函数值得分析. 现在AW性能大概在2699的1/3, 结合经验, AW文件系统性能应该提高一倍, 达到2699的2/3才算正常. "},"notes/as_title_arch_perf.html":{"url":"notes/as_title_arch_perf.html","title":"CPU ARCH相关","keywords":"","body":"如题 "},"notes/performance_CPU_microarchiteture_pmu.html":{"url":"notes/performance_CPU_microarchiteture_pmu.html","title":"Top-down Microarchitecture Analysis Method(网摘)","keywords":"","body":" Top-down Microarchitecture Analysis Method Overview Top-Down Analysis Method with VTune Profiler Microarchitectural Tuning Methodology Tune for the Back-End Bound Category Tune for the Front-End Bound Category Tune for the Bad Speculation Category Tune for the Retiring Category Conclusion Use this recipe to know how an application is utilizing available hardware resources and how to make it take advantage of CPU microarchitectures. One way to obtain this knowledge is by using on-chip Performance Monitoring Units (PMUs). PMUs are dedicated pieces of logic within a CPU core that count specific hardware events as they occur on the system. Examples of these events may be Cache Misses or Branch Mispredictions. These events can be observed and combined to create useful high-level metrics such as Cycles per Instruction (CPI).A specific microarchitecture may make available hundreds of events through its PMU. However, it is frequently non-obvious to determine which events are useful in detecting and fixing specific performance issues. Often it requires an in-depth knowledge of both the microarchitecture design and PMU specifications to obtain useful information from raw event data. But you can benefit from using predefined events and metrics, and the top-down characterization method to convert the data into actionable information.Explore the PMU analysis recipe to learn the methodology and how it is used in the Intel® VTune Profiler: INGREDIENTS: Top-down Microarchitecture Analysis Method (TMA) overview- Top-Down Analysis Method with VTune Profiler- Microarchitectural Tuning Methodology DIRECTIONS: Tune for the Back-End Bound Category- Tune for the Front-End Bound Category- Tune for the Bad Speculation Category- Tune for the Retiring Category Related Cookbook Recipes Top-down Microarchitecture Analysis Method Overview Modern CPUs employ pipelining as well as techniques like hardware threading, out-of-order execution and instruction-level parallelism to utilize resources as effectively as possible. In spite of this, some types of software patterns and algorithms still result in inefficiencies. For example, linked data structures are commonly used in software, but cause indirect addressing that can defeat hardware prefetchers. In many cases, this behavior can create bubbles of idleness in the pipeline while data is retrieved and there are no other instructions to execute. Linked data structures could be an appropriate solution to a software problem, but may result in inefficiencies. There are many other examples at the software level that have implications on the underlying CPU pipelines. The Top-down Microarchitecture Analysis Method based on the Top-Down Characterization methodology aims to provide an insight into whether you have made wise choices with your algorithms and data structures. See the Intel® 64 and IA-32 Architectures Optimization Reference Manual, Appendix B.1 for more details on the Top-down Microarchitecture Analysis Method.The Top-Down Characterization is a hierarchical organization of event-based metrics that identifies the dominant performance bottlenecks in an application. Its aim is to show, on average, how well the CPU's pipeline(s) were being utilized while running an application. Previous frameworks for interpreting events relied on accounting for CPU clockticks - determining what fraction of CPU's clockticks was spent on what type of operations (retrieving data due to L2 cache misses, for example). This framework instead is based on accounting for the pipeline's resources. To understand the Top-Down Characterization, explore a few microarchitectural concepts below, at a high level. Many of the details of the microarchitecture are abstracted in this framework, enabling you to use and understand it without being a hardware expert.The pipeline of a modern high-performance CPU is quite complex. In the simplified view blow, the pipeline is divided conceptually into two halves, the Front-end and the Back-end. The Front-end is responsible for fetching the program code represented in architectural instructions and decoding them into one or more low-level hardware operations called micro-ops (uOps). The uOps are then fed to the Back-end in a process called allocation. Once allocated, the Back-end is responsible for monitoring when uOp's data operands are available and executing the uOp in an available execution unit. The completion of a uOp's execution is called retirement, and is where results of the uOp are committed to the architectural state (CPU registers or written back to memory). Usually, most uOps pass completely through the pipeline and retire, but sometimes speculatively fetched uOps may get cancelled before retirement - like in the case of mis-predicted branches. The Front-end of the pipeline on recent Intel microarchitectures can allocate four uOps per cycle, while the Back-end can retire four uOps per cycle. From these capabilities the abstract concept of a pipeline slot can be derived. A pipeline slot represents the hardware resources needed to process one uOp. The Top-Down Characterization assumes that for each CPU core, on each clock cycle, there are four pipeline slots available. It then uses specially designed PMU events to measure how well those pipeline slots were utilized. The status of the pipeline slots is taken at the allocation point (marked with a star in the figure above), where uOps leave the Front-end for the Back-end. Each pipeline slot available during an application’s runtime will be classified into one of four categories based on the simplified pipeline view described above. During any cycle, a pipeline slot can either be empty or filled with a uOp. If a slot is empty during one clock cycle, this is attributed to a stall. The next step needed to classify this pipeline slot is to determine whether the Front-end or the Back-end portion of the pipeline caused the stall. This is done using designated PMU events and formulas. The goal of the Top-Down Characterization is to identify dominant bottlenecks, hence the attribution of the stall to either the Front- or Back-end is a critical point of consideration. Generally, if the stall is caused by the Front-end’s inability to fill the slot with a uOp, it will be classified as a Front-End Bound slot for this cycle, meaning, the performance was limited by some bottleneck under the Front-End Bound category. In the case where the Front-end has a uOp ready but cannot deliver it because the Back-end is not ready to handle it, the empty pipeline slot will be classified as Back-End Bound. Back-end stalls are generally caused by the Back-end running out of some resource, for example, load buffers. However, if both the Front-end and the Back-end are stalled, then the slot will be classified as Back-End Bound. This is because, in that case, fixing the stall in the Front-end would most likely not help an application’s performance. The Back-end is the blocking bottleneck, and it would need to be removed first before fixing issues in the Front-end would have any effect. If the processor is not stalled then a pipeline slot will be filled with a uOp at the allocation point. In this case, the determining factor for how to classify the slot is whether the uOp eventually retires. If it does retire, the slot is classified as Retiring. If it does not, either because of incorrect branch predictions by the Front-end or a clearing event like a pipeline flush due to Self-Modifying-Code, the slot will be classified as Bad Speculation. These four categories make up the top level of the Top-Down Characterization. To characterize an application, each pipeline slot is classified into exactly one of these four categories: The distribution of pipeline slots in these four categories is very useful. Although metrics based on events have been possible for many years, before this characterization there was no approach for identifying which possible performance issues were the most impactful. When performance metrics are placed into this framework, you can see which issues need to be tackled first. The events needed to classify pipeline slots into the four categories are available beginning with Intel® microarchitecture code name Sandy Bridge – which is used in the 2nd Generation Intel Core processor family and the Intel Xeon® processor E5 family. Subsequent microarchitectures may allow further decomposition of these high-level categories into more detailed performance metrics. Top-Down Analysis Method with VTune Profiler Intel® VTune Profiler provides a Microarchitecture Exploration analysis type that is pre-configured to collect the events defined in the Top-Down Characterization starting with the Intel microarchitecture code name Ivy Bridge. Microarchitecture Exploration also collects the events required to calculate many other useful performance metrics. The results of a Microarchitecture Exploration analysis are displayed by default in the Microarchitecture Exploration viewpoint . Microarchitecture Exploration results are displayed in hierarchical columns to reinforce the top-down nature of the characterization. The Summary window gives the percentage of pipeline slots in each category for the whole application. You can explore results in multiple ways. The most common way to explore results is to view metrics at the function level: For each function, the fraction of pipeline slots in each category is shown. For example, the price_out_impl function, selected above, had 2.2% of its pipeline slots in the Front-End Bound category, 7.4% in Bad Speculation, 64.2% in Memory Bound, 8.4% in Core Bound, and 17.8% in the Retiring category. Each category can be expanded to view metrics underneath that category. Automatic highlighting is used to draw your attention to potential problem areas, in this case, to the high percentage of Memory Bound pipeline slots for price_out_impl. Microarchitectural Tuning Methodology When doing any performance tuning, it is important to focus on the top hotspots of the application. Hotspots are the functions taking the most CPU time. Focus on these spots will ensure that optimizations impact the overall application performance. VTune Profiler has a Hotspots analysis with two specific collection modes: user-mode sampling and hardware event-based sampling. Within the Microarchitecture Exploration viewpoint, hotspots can be identified by determining the functions or modules with the highest Clockticks event counts, which measures the number of CPU clockticks. To obtain maximum benefit from microarchitectural tuning, ensure that algorithmic optimizations such as adding parallelism have already been applied. Generally system tuning is performed first, then application-level algorithm tuning, then architectural and microarchitectural tuning. This process is also referred to as \"Top-Down\", as in the Top-Down software tuning methodology. It, as well as other important aspects of performance tuning like workload selection, are described in the De-Mystifying Software Performance Optimization article. Select a hotspot function (one with a large percentage of the application's total clockticks). Evaluate the efficiency of that hotspot using the Top-Down Method and the guidelines given below. If inefficient, drill down the category representing the primary bottleneck, and use the next levels of sub-bottlenecks to identify causes. Optimize the issue(s). VTune Profiler tuning guides contain specific tuning suggestions for many of the underlying performance issues in each category. Repeat until all significant hotspots have been evaluated. VTune Profiler automatically highlights metric values in the GUI if they are outside a predefined threshold and occur in a hotspot. VTune Profiler classifies a function as a hotspot if greater than 5% of the total clockticks for an application accrued within it. Determining whether a given fraction of pipeline slots in a particular category constitutes a bottleneck can be workload-dependent, but some general guidelines are provided in the table below: Expected Range of Pipeline Slots in This Category, for a Hotspot in a Well-Tuned: Category Client/Desktop Application Server/Database/Distributed application High Performance Computing (HPC) application --- --- --- --- Retiring 20-50% 10-30% 30-70% --- --- --- --- Back-End Bound 20-40% 20-60% 20-40% Front-End Bound 5-10% 10-25% 5-10% Bad Speculation 5-10% 5-10% 1-5% These thresholds are based on analysis of some workloads in labs at Intel. If the fraction of time spent in a category (other than Retiring) for a hotspot is on the high end or greater than the range indicated, an investigation might be useful. If this is true for more than one category, the category with the highest fraction of time should be investigated first. Note that it is expected that hotspots will have some fraction of time spent in each category, and that values within the normal range below may not indicate a problem. The important thing to realize about the Top-Down Method is that you do not need to spend time optimizing issues in a category that is not identified as a bottleneck - doing so will likely not lead to a significant performance improvement. Tune for the Back-End Bound Category The majority of un-tuned applications will be Back-End Bound. Resolving Back-end issues is often about resolving sources of latency, which cause retirement to take longer than necessary. On the Intel microarchitecture code name Sandy Bridge, VTune Profiler has Back-End Bound metrics to find the sources of high latency. For example, the LLC Miss (Last-Level Cache Miss) metric identifies regions of code that need to access DRAM for data, and the Split Loads and Split Stores metrics point out memory access patterns that can harm performance. For more details on Intel microarchitecture code name Sandy Bridge metrics, see the Tuning Guide. Starting with Intel microarchitecture code name Ivy Bridge (which is used in the 3rd Generation Intel Core processor family), events are available to breakdown the Back-End Bound classification into Memory Bound and Core Bound sub-metrics. A metric beneath the top 4 categories may use a domain other than the pipeline slots domain. Each metric will use the most appropriate domain based on underlying PMU events. For more details see the documentation for each metric or category. The Memory and Core Bound sub-metrics are determined using events corresponding to the utilization of the execution units - as opposed to the allocation stage used in the top-level classifications. Therefore, the sum of these metrics will not necessarily match the Back-End Bound ratio determined at the top-level (though they correlate well). Stalls in the Memory Bound category have causes related to the memory subsystem. For example, cache misses and memory accesses can cause Memory Bound stalls. Core Bound stalls are caused by a less-than-optimal use of the available execution units in the CPU during each cycle. For example, several multi-cycle divide instructions in a row competing for the divide units could cause Core Bound stalls. For this breakdown, slots are only classified as Core Bound if they are stalled AND there are no uncompleted memory accesses. For example, if there are pending loads, the cycle is classified as Memory Bound because the execution units are being starved while the loads have not returned data yet. PMU events were designed into the hardware to specifically allow this type of breakdown, which helps identify the true bottleneck in an application. The majority of Back-End Bound issues will fall into the Memory Bound category. Most of the metrics under the Memory Bound category identify which level of the memory hierarchy from the L1 cache through the memory is the bottleneck. Again, the events used for this determination were carefully designed. Once the Back-end is stalled the metrics try to attribute the stalls of pending loads to a particular level of cache or to in-flight stores. If a hotspot is bound at a given level, it means that most of its data is being retrieved from that cache- or memory-level. Optimizations should focus on moving data closer to the core. Store Bound is also called out as a sub-category, which can indicate dependancies - such as when loads in the pipeline depend on prior stores. Under each of these categories, there are metrics that can identify specific application behaviors resulting in Memory Bound execution. For example, Loads Blocked by Store Forwarding and 4k Aliasing are metrics that flag behaviors that can cause an application to be L1 Bound. Core Bound stalls are typically less common within Back-End Bound. These can occur when available computing resources are not sufficiently utilized and/or used without significant memory requirements. For example, a tight loop doing Floating Point (FP) arithmetic calculations on data that fits within cache. VTune Profiler provides some metrics to detect behaviors in this category. For example the Divider metric identifies cycles when divider hardware is heavily used and the Port Utilization metric identifies competition for discrete execution units. Grayed out metric values indicate that the data collected for this metric is unreliable. This may happen, for example, if the number of samples collected for PMU events is too low. You may either ignore this data, or rerun the collection with the data collection time, sampling interval, or workload increased. Tune for the Front-End Bound Category The Front-End Bound category covers several other types of pipeline stalls. It is less common for the Front-end portion of the pipelines to become the application's bottleneck; however there are cases where the Front-end can contribute in a significant manner to machine stalls. For example, JITed code and interpreted code can cause Front-end stalls because the instruction stream is dynamically created without the benefit of compiler code layout in advance. Improving performance in the Front-End Bound category will generally relate to code layout (co-locating hot code) and compiler techniques. For example, branchy code or code with a large footprint may highlight the Front-End Bound category. Techniques like code size optimization and compiler profile-guided optimization (PGO) are likely to reduce stalls in many cases. The Top-Down Method on Intel microarchitecture code name Ivy Bridge and beyond divides Front-End Bound stalls into 2 categories, Front-End Latency and Front-End Bandwidth. The Front-End Latency metric reports cycles in which no uops were issued by the Front-end in a cycle, while the Back-end was ready to consume them. Recall that the Front-end cluster can issue up to 4 uops per cycle. The Front-End Bandwidth metric reports cycles in which less than 4 uops were issued, representing an inefficient use of the Front-end's capability. Further metrics are identified below each of the categories. Branch mispredictions, which are mostly accounted for in the Bad Speculation category, could also lead to inefficiencies in the Front-end as denoted by the Branch Resteers bottleneck metric underneath Front-End Latency starting in the Intel microarchitecture code name Ivy Bridge. VTune Profiler lists metrics that may identify causes of Front-End Bound code. If any of these categories shows up significantly in the results, dig deeper into the metrics to determine the causes and how to correct them. For example, the ITLB Overhead (Instruction Translation Lookaside Buffer Overhead) and ICache Miss (Instruction Cache miss) metrics may point out areas suffering from Front-End Bound execution. For tuning suggestions see the VTune Profiler tuning guides. Tune for the Bad Speculation Category The third top-level category, Bad Speculation, denotes when the pipeline is busy fetching and executing non-useful operations. Bad Speculation pipeline slots are slots wasted by issued uops that never retired or stalled while the machine recovers from an incorrect speculation. Bad Speculation is caused by branch mispredictions and machine clears and less commonly by cases like Self-Modifying-Code. Bad Speculation can be reduced through compiler techniques such as Profile-Guided Optimization (PGO), avoiding indirect branches, and eliminating error conditions that cause machine clears. Correcting Bad Speculation issues may also help decrease the number of Front-End Bound stalls. For specific tuning techniques refer to the VTune Profiler tuning guide appropriate for your microarchitecture. Tune for the Retiring Category The last category at the top level is Retiring. It denotes when the pipeline is busy with typically useful operations. Ideally an application would have as many slots classified in this category as possible. However, even regions of code with a large portion of their pipeline slots retiring may have room for improvement. One performance issue that will fall under the retiring category is heavy use of the micro-sequencer, which assists the Front-end by generating a long stream of uops to address a particular condition. In this case, although there are many retiring uops, some of them could have been avoided. For example, FP Assists that apply in the event of Denormals can often be reduced through compiler options (like DAZ or FTZ). Code generation choices can also help mitigate these issues - for more details see the VTune Profiler tuning guides. In the Intel microarchitecture code name Sandy Bridge, Assists are identified as a metric under the Retiring category. In the Intel microarchitecture code name Ivy Bridge and beyond, the pipeline slots in the ideal category of retirement are broken into a sub-category called General Retirement, and Microcode Sequencer uops are identified separately. If not already done, algorithmic tuning techniques like parallelization and vectorization can help improve the performance of code regions that fall into the retiring category. Conclusion The Top-Down Method and its availability in VTune Profiler represent a new direction for performance tuning using PMUs. Developer time invested in becoming familiar with this characterization will be worth the effort, since support for it is designed into recent PMUs and, where possible, the hierarchy is further expanded on future Intel microarchitectures. For example, the characterization was significantly expanded between Intel microarchitecture code name Sandy Bridge and Intel microarchitecture code name Ivy Bridge. The goal of the Top-Down Method is to identify the dominant bottlenecks in an application performance. The goal of Microarchitecture Exploration analysis and visualization features in VTune Profiler is to give you actionable information for improving your applications. Together, these capabilities can significantly boost not only application performance, but also the productivity of your optimizations. 原文链接: https://www.intel.com/content/www/us/en/develop/documentation/vtune-cookbook/top/methodologies/top-down-microarchitecture-analysis-method.html#top-down-microarchitecture-analysis-method_GUID-FA8F07A1-3590-4A91-864D-CE96456F84D7 https://www.intel.com/content/www/us/en/developer/articles/guide/processor-specific-performance-analysis-papers.html "},"notes/as_title_cloud.html":{"url":"notes/as_title_cloud.html","title":"Cloud和容器相关","keywords":"","body":"如题 "},"notes/cloud_杂记.html":{"url":"notes/cloud_杂记.html","title":"云杂记","keywords":"","body":" nginx负载均衡 负载均衡mode server 权重 用DNS服务来更新server的IP nginx负载均衡 nginx可以做URL的静态路由和负载均衡, 分到不同的应用微服务: 比如下面的配置使用round robin方式负载均衡到两个服务器 http { upstream backend { server backend1.example.com; server backend2.example.com; server 192.0.0.1 backup; } server { location / { proxy_pass http://backend; } } } 负载均衡mode 默认是Round Robin, 还有least_conn, ip_hash, least_time, random等等, i.e. upstream backend { least_conn; server backend1.example.com; server backend2.example.com; } server 权重 可以配权重来反应不同server的性能配置, weight默认是1, 性能越好的, weight越高. upstream backend { server backend1.example.com weight=5; server backend2.example.com; server 192.0.0.1 backup; } 用DNS服务来更新server的IP nginx会周期的跟10.0.0.1 check, 更新backend1.example.com和backend2.example.com的ip地址, 如果更换server了, 只要域名不变, ip可以run time变更, 不用重启. http { resolver 10.0.0.1 valid=300s ipv6=off; resolver_timeout 10s; server { location / { proxy_pass http://backend; } } upstream backend { zone backend 32k; least_conn; # ... server backend1.example.com resolve; server backend2.example.com resolve; } } "},"notes/container_当代容器读书笔记.html":{"url":"notes/container_当代容器读书笔记.html","title":"当代容器读书笔记","keywords":"","body":" 背景 isolation platforms 隔离平台类型 hypervisors qemu/kvm Firecracker Cloud Hypervisor 容器平台 docker LXC 安全容器平台 Kata container gVisor Unikernels 性能对比 CPU bond 内存 FIO Networking 结论 本文是 A Fresh Look at the Architecture and Performance of Contemporary Isolation Platforms 的阅读笔记. 背景 虚拟化有两个大的需求: 隔离性 性能 这个文章就分析了现在主流的虚拟化平台在这两方面的表现: 容器平台: 比如docker 安全容器平台: 比如kata hypervisor平台: 比如qemu unikernel平台 qemu/kvm的技术是第一波主导虚拟化浪潮的技术, 随后容器技术以其轻量化的隔离技术提供了更多的实例, 创造了更多的效益, 从而逐渐更加流行. 所谓的安全容器就是容器+qemu/kvm的结合体, 分别取了kvm的隔离性和容器的便捷性, 但在性能上有一定的损失. isolation platforms 隔离平台类型 hypervisors 使用了硬件虚拟化技术, 有guest kernel和guest OS, 隔离性好, 但比较重. 比如ARMv8架构中: 其中EL2就是专门给hypervisor运行的mode一般的kernel运行在EL1 qemu/kvm 一个VM就是一个host上的进程, vCPU是进程里的thread, VM的内存就是用这个进程的内存作为VM的“物理”内存. Qemu的main loop是事件驱动的，等待fd事件, 比如tap设备, virtio设备的事件. main loop和/dev/kvm交互, 把对guest VM的操做交给KVM去做. 比如KVM会启动两个线程来模拟vCPU. 比如ioctl(KVM_RUN)产生一个VM_ENTRY, 把控制权交给guest, guest持续运行, 遇到某些操做在guest里面无法进行就会trap到QEMU. ARM KVM架构: Firecracker firecracker 和qemu类似, 也是event driven模式, 也是用kvm来创建和运行VM, 但是精简了qemu的设备模拟, 只保留virtio-net, virtio-blk等半虚拟设备, 以及鼠标键盘, 目标是精简. 另外fircracker也优化了linux kernel的启动, 直接启动64bit的非压缩的kernel, 而非kernel自解压再启动. github地址: https://github.com/firecracker-microvm/firecracker 从Chrome OS Virtual Machine Monitor 项目而来, 基于KVM, 大部分代码是Rust. crosvm是一个全新的VMM, 类似eqmu的功能. Firecracker在crosvm基础上, 减小了50%的代码, 没有考虑BIOS, 没有考虑PCI. 比QEMU少了96%的代码... 设计亮点: Simple Guest Model – Firecracker guests are presented with a very simple virtualized device model in order to minimize the attack surface: a network device, a block I/O device, a Programmable Interval Timer, the KVM clock, a serial console, and a partial keyboard (just enough to allow the VM to be reset). Process Jail – The Firecracker process is jailed using cgroups and seccomp BPF, and has access to a small, tightly controlled list of system calls. Static Linking – The firecracker process is statically linked, and can be launched from a jailer to ensure that the host environment is as safe and clean as possible. Cloud Hypervisor 被Firecracker支持的功能多点, 比qemu瘦点. 支持mem和vcpu的热插拔. 容器平台 docker 也就是runc LXC docker最开始是用liblxc做为基础库的, 后来改成自己写的libcontainer. 相对于docker, lxc试图比较完整的建立一个linux系统, 比如带systemd, 比如使用ZFS文件系统. lxc用了cgroup v2的特性支持以non-root启动container. 安全容器平台 Kata container Kata自己精简优化了guest kernel, 去掉了大部分驱动和模块. host上运行的Kata-runtime负责和hypervisor交互, hypervisor来启动VM. Kata有自己定制的mini OS, 基于Clear Linux, OS起来后马上用systemd启动Kata-agent Kata-agent和Kata-runtime通过vsock来交互, 使用ttRPC(一种gRPC的变种) 流程是host上docker run命令运行一个image, kata-runtime先通知hypervisor启动一个VM, 这个VM会使用Kata定制精简的kernel和同样定制精简的mini OS先启动, 然后马上启动Kata-agent, 然后Kata-agent通过ttRPC和Kata-runtime要到image, 然后在VM内部启动这个image. 后续host上的docker 命令, 会经过Kata-runtime的转发, 到Kata-agent来执行. gVisor gVisor是一个用户态的操做系统, 旨在减小attack surface. 在用户态实现了tcpip等协议栈, 但没有设备驱动. 网络还是通过传统的虚拟网络设备走host. app的系统调用会被gVisor拦截, 转到gVisor来处理. 拦截的办法有ptrace和KVM. ptrace性能低, KVM性能高. 在KVM模式下, gVisor运行在KVM VM里面. Gofer负责实际调用IO系统调用到host Unikernels 基本思想是把VM的kernel和OS全部用一个库来代替. 通过动态链接的方式, app的系统调用被这个lib接管, lib在同一个进程空间把kernel和OS的事情都给做了. 这样系统调用就是个简单的函数调用了. 类似vxworks没有用户态和内核态, 大家都在一个空间里, 所以也没有啥进程空间的概念, 也就不支持fork()和exec()等系统调用. 典型的代表是OSv, 从后面的测试来看, OSv必须运行在一个hypervisor上, 默认Qemu, 也可以运行在Firecracker上. 性能对比 CPU bond 对纯计算的应用, 各个平台性能差不多. 内存 Firecracker和cloud hypervisor的内存测试比较差, 可能是因为它们都是Rust实现的vm-memory. FIO qemu在bandwidth上和native/docker一样, 但latency还是大了好几倍. Kata表现的最差, 可能和9p文件系统有关. virtio-fs正在代替9p的使用. gVisor也差, 可能也受9p拖累, 而且它所有的IO请求都走Gofer, 也进一步拖慢了IO系统. Networking 使用iperf3进行client server测试. client跑在host, server在VM中. OSv: 其实是OSv运行在qemu上. 它甚至比直接运行在Qemu上的iperf3性能还好. 结论 containers(docker, lxc)性能几乎和native一样, 启动很快 hypervisors在networking和mem上都有overhead, 在io吞吐应该还好, 但* latency也有penalty. Kata性能一般, 可能和9p有关 gVisor性能差 OSv总的来说性能也不错, 调用的host的代码也最少 "},"notes/as_title_vdevice.html":{"url":"notes/as_title_vdevice.html","title":"CPU和device虚拟化","keywords":"","body":"如题 "},"notes/as_title_gvisor.html":{"url":"notes/as_title_gvisor.html","title":"gvisor","keywords":"","body":"如题 "},"notes/golang_gvisor代码_KVM.html":{"url":"notes/golang_gvisor代码_KVM.html","title":"gvisor KVM模式代码","keywords":"","body":" gvisor代码概览图: host mode和guest mode切换小结 go 汇编和arm64知识 ARM64页表和进程切换知识 ARM64异常处理 ARM64寄存器 gvisor介绍 原理简介 编译和调试 代码结构 abi sentry的内存管理 传统mm shared map private map 匿名映射 如何在用户态处理page fault signal方式 userfaultfd方式用户态page管理 sentry的mm 性能对比 netstask pkg/abi/linux pkg/abi/linux/seccomp.go pkg/hostarch/hostarch.go pkg/seccomp/seccomp.go pkg/sentry/arch/arch.go kernel task的状态机 runsc boot流程 platform 哪里调用了Switch() ptrace thread.setRegs 主要结构体 ptrace系统调用 ptrace可以做什么? kvm golang的汇编基础 arm64 exception level arm64内存基础 VM的地址空间 KVM基础 gvisor对CPU和ring0.kernel的抽象 machine和vCPU的定义 pagetable之虚拟地址region到物理地址region的map表 KVM_SET_USER_MEMORY_REGION mapPhysical()的调用路径之用户态page falut处理路径 mapPhysical的调用路径之seccompMmapHandler mapPhysical的调用路径之newMachine KVM新建一个VM newMachine arm64异常向量 EL0同步异常 EL0同步异常之SVC系统调用异常 kernelExitToEl1 顺便看一下kernelExitToEl0 EL1同步异常El1_sync EL1其他异常(irq, fiq, error)都走shutdown流程, 关闭guest KVM_CREATE_VCPU kernelAddr可以获取一个eface和func的内核地址 KVM_ARM_VCPU_INIT 入口代码 KVM的context实现 arm64的cpu.SwitchToUser bluepill()汇编函数 vCPU.CPU.SwitchToUser函数 补充 go linkname用法 gvisor代码概览图: host mode和guest mode切换小结 总的来说, 虽然用了kvm, 但gvisor巧妙地设计了从guest PA到host VA的映射, 从而让guest能读写host一样的地址空间, 而且gvisor会把所有的guest app也都映射到这个地址空间. 这样产生的现象是guest和host交替执行这个地址空间上的代码. 切换点在(*vCPU).SwitchToUser这个函数, 在bluepill前在host模式执行, 然后 切换到guest模式继续执行接下来的代码(包括EL1特权代码), 其中的关键函数是kernelExitToEl0, 效果是让geust执行guest的EL0代码, 直到遇到SVC系统调用指令 guest的SVC指令导致guest模式退出, ucontext被设置为guest执行SVC前的状态, 但交给host来接力执行. 接着host来执行SVC指令, 在host模式下触发syscall, 让host kernel来执行syscall go 汇编和arm64知识 伪寄存器: SB: Static base pointer 全局基地址. 比如foo(SB)就是foo这个symbol的地址 FP: 帧指针. 用来传参的 SP: 栈指针. 指向栈顶. 用于局部变量. 注意真寄存器叫RSP PC: 程序指针 函数格式: TEXT symbol(SB), [flags,] $framesize[-argsize] symbol: 函数名 SB: SB伪寄存器 flags: 可以是 NOSPLIT: 不让编译器插入栈分裂的代码 WRAPPER: 不增加函数帧计数 NEEDCTXT: 需要上下文参数, 一般用于闭包 $framesize: 局部变量大小, 包含要传给子函数的参数部分 -argsize: 参数+返回值的大小, 可以省略由编译器自己推导 ARM64页表和进程切换知识 每个进程都有自己的translation table, 这个table是kernel分配的, 把其物理地址配置到ttbr0寄存器. 上下文切换的时候, kernel会保存/恢复如下上下文: general-purpose registers X0-X30. Advanced SIMD and Floating-point registers V0 - V31. Some status registers. TTBR0_EL1 and TTBR0. Thread Process ID (TPIDxxx) Registers. Address Space ID (ASID). EL0和EL1有两个translation table, TTBR0_EL1负责bottom空间(用户空间), TTBR1_EL1负责top空间(kernel空间). 大家都用TTBR1_EL1做kernel空间, 所以进程切换的时候, TTBR1_EL1不用变, 所以kernel的映射不用变. ASID配置在TTBR0_EL1里 ASID(Address Space ID)寄存器用来标记页表entry所属的task, 由kernel分配.当TLB更新的时候, TLB entry除了保存地址翻译信息, 还会包括这个ASID.TLB查询的时候, 只有当前的ASID和TLB entry保存的ASID匹配的时候, 才算TLB命中. 所以上下文切换的时候不需要flush TLB.把ASID值放在TTBR0_EL1里的好处是, 一个指令就可以原子的更改ASID和页表. AARCH64支持虚拟内存的tag, 虚拟内存的最高8位是tag, 在地址翻译的时候会被忽略. PC, LR, SP, ELR里面都是VA AArch64有48位VA, 空间有256TB, 有两个range空间 0xFFFF_0000_0000_0000 到 0xFFFF_FFFF_FFFF_FFFF 基址寄存器是TTBR1, 内核态 或 0x0000_0000_0000_0000 到 0x0000_FFFF_FFFF_FFFF 基址寄存器是TTBR0, 用户态 IPA也是48位 PA也是48位, 并且secure和non-secure的物理地址空间是独立的 TTBR是地址转换表的基址寄存器, 这个表由硬件自动查, 并被缓存到TLB中; TTBR里面保存的是物理地址, 是给硬件MMU waker看的. 这个表最多有四级, 地址最多48位, 最大64KB一个映射 ARM64异常处理 异常发生的时候, CPU会自动的实施如下动作: 将PSTATE保存到SPSR_ELn 比如异常发生在EL0, 一般会在EL1处理. 那PSTATE会保存在SPSR_EL1 更新PSTATE以反映新的CPU状态, 比如已经进入EL1 硬件会将返回地址保存在ELR_Eln. 还是比如异常发生在EL0, 但在EL1处理, 那返回地址保存在ELR_EL1 eret指令用来从异常处理返回: 从SPSR_ELn恢复异常前的PSTATE 从ELR_ELn恢复PC 异常返回, 从恢复的PC和PSTATE继续执行 在发生异常时, 硬件会自动更新ELR, 根据情况, 返回地址有几种可能: 比如SVC指令触发的同步异常, ELR里保存的是其下一条指令 比如异步异常(即外部中断), ELR里保存的是下一个没被执行(或完全执行)的指令 ELR可以在异常处理程序里面被更改. 每个exception level都有独立的异常向量表 VBAR_EL3, VBAR_EL2 and VBAR_EL1向量表的虚拟地址配在VBAR寄存器里 arm64的sp寄存器每个EL都有, 但不一定都用: SPSel选择寄存器的0位, 来决定用哪个SP 默认每个EL使用自己的level对应的SP ARM64寄存器 In AArch64 state, the following registers are available: Thirty-one 64-bit general-purpose registers X0-X30, the bottom halves of which are accessible as W0-W30. Four stack pointer registers SP_EL0, SP_EL1, SP_EL2, SP_EL3. Three exception link registers ELR_EL1, ELR_EL2, ELR_EL3. Three saved program status registers SPSR_EL1, SPSR_EL2, SPSR_EL3. One program counter. For the purposes of function calls, the general-purpose registers are divided into four groups: r30(LR): The Link Register r29(FP): The Frame Pointer r19...r28: Callee-saved registers r18: The Platform Register, if needed; otherwise a temporary register. r17(IP1): The second intra-procedure-call temporary register r16(IP0): The first intra-procedure-call scratch register r9...r15: Temporary registers r8: Indirect result location register r0...r7: Parameter/result registers XZR是zero寄存器PC寄存器BL或ADL指令可以修改SP向下增长, 必须16字节对齐PSR寄存器: Process State, 反映一些比较操作的状态 gvisor介绍 gvisor是一个用户态操做系统, 自带一个runsc, 可以和conainterd等编排工具集成. gvisor主打安全特性. As outlined in the previous blog post, gVisor’s secure design principles are: Defense in Depth: each component of the software stack trusts each other component as little as possible. Least Privilege: each software component has only the permissions it needs to function, and no more. Attack Surface Reduction: limit the surface area of the host exposed to the sandbox. Secure by Default: the default choice for a user should be safe. 原理简介 原文: https://www.infoq.com/presentations/gvisor-os-go/ 编译和调试 clone gvisor后, 切换到go分支, 在runsc下面go build编译出来的runsc可以直接使用.修改daemon.json并且systemctl reload docker就可以使用runsc了: docker run --cpus=2 -m 2g --rm --runtime=runsc -it --name=test centos:7 bash 顺利的话就进入container里面了 debug的log在/tmp/runsc, 其中比如runsc.log.20220315-022030.789081.boot就是主进程的log docker run后, 找到对应的sndbox进程runsc-sandbox, 就可以用dlv attach pid来调试. 官方github里面install提到的containerd-shim-runsc-v1是没用到的.实际docker使用的是/usr/bin/containerd-shim-runc-v2 如果在arm64的raspberry pi上, 需要修改/boot/firmware/cmdline.txt, 增加cgroup_enable=memory net.ifnames=0 dwc_otg.lpm_enable=0 console=serial0,115200 console=tty1 root=LABEL=writable rootfstype=ext4 elevator=deadline rootwait fixrtc cgroup_enable=memory 代码结构 gvisor的依赖包很多, 有containerd和docker等容器的, 有k8s的, 有protobuf和grpc的, 有报文解析相关的gopacket, 有更加基础的btree库... 很多gvisor有很多自己实现的基础库https://github.com/google/gvisor/tree/master/pkg可以说, gvisor的这些基础库本身, 就是系统级linux基础的golang实现的参考库.比如eventfd包就包装了linux的enventfd系统调用.比如在标准库基础上的sync包 abi gvisor提供了兼容linux的abi: https://github.com/google/gvisor/tree/master/pkg/abi/linux就是说gvisor支持大部分的c的abi: 比如aio, bpf, elf, epoll, fcntl, fs, ipc, mm, netlink, netfilter, ptrace, sched, time, uio, socket等等 sentry的内存管理 sentry是gvisor用户态kernel的核心. 传统mm mmap的任务是在一个进程里, map一个文件到一个虚拟地址范围. 当这个虚拟地址被访问的时候, 没有PTE的时候会产生page fault异常, kernel才分配物理页, 从文件copy实际内容到这个物理页. page是有cache的, 使用Least Recently Used (LRU)策略换出不经常使用的page. 当dirty page超过一个ratio, kernel会flush脏页. Read-ahead技术预加载page从而避免缺页异常的产生. madvise系统调用可以告知kernel app对内容范围的期望. shared map linux的mmap系统调用, 比如: mmap( /* addr = */ 0x400000, /* length = */ 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, /* fd = */ 3, /* offset = */ 0); 创建一个从fd 3到virtual memory areas (VMAs)的mapping.这个mapping从VA 0x400000开始, 长度为0x1000字节, offset是0.假设fd 3对应的文件是/tmp/foo内核中这个mapping表示为: VMA: VA:0x400000 -> /tmp/foo:0x0 创建VMA的时候并没有分配PA, 因为这个时候linux还没有准备物理地址来保存/tmp/foo的内容. 直到应用读VA地址0x400000, 产生缺页异常, 才分配物理页, 然后copy文件内容到这个物理页. 比如kernel选择了PA:0x2fb000, 此时VMA是这样的: VMA: VA:0x400000 -> /tmp/foo:0x0 Filemap: /tmp/foo:0x0 -> PA:0x2fb000 这里的Filemap对应kernel的struct address_space 这个时候kernel使用page table entry (PTE)来做VA到PA的转换表. VMA: VA:0x400000 -> /tmp/foo:0x0 Filemap: /tmp/foo:0x0 -> PA:0x2fb000 PTE: VA:0x400000 -----------------> PA:0x2fb000 注意, VMA和Filemap是相对独立的东西, 而PTE受二者的影响, 比如: 这个应用调用了munmap系统调用, 这就解除了VMA: VA:0x400000 -> /tmp/foo:0x0的映射, 进而解除了PTE: VA:0x400000 -----------------> PA:0x2fb000. 但是, Filemap: /tmp/foo:0x0 -> PA:0x2fb000不一定就解除了, 因为从文件/tmp/foo:0x0到物理地址PA:0x2fb000的映射以后还能用得上. 这个应用也可能调用ftruncate来invalidate这个文件的内容. 这就解除了Filemap: /tmp/foo:0x0 -> PA:0x2fb000, 进而解除了PTE: VA:0x400000 -----------------> PA:0x2fb000; 而VMA: VA:0x400000 -> /tmp/foo:0x0就不需要改变, 因为PTE解除了, VA:0x400000需要另一个缺页异常来分配新的物理页, 所以VA仍然反应了文件内容的改变. private map 对private map来说, 读和写都可能会有缺页异常. 首次读产生的缺页异常会产生一个只读的物理页: VMA: VA:0x400000 -> /tmp/foo:0x0 (private) Filemap: /tmp/foo:0x0 -> PA:0x2fb000 PTE: VA:0x400000 -----------------> PA:0x2fb000 (read-only) 此时如果是shared map, 写操做也会写到PA:0x2fb000. 但private map会产生另外一个缺页异常, kernel另外选择一个物理页(比如0x5ea000), 拷贝之前的物理页内容到这个新分配的物理页, 然后更新map: VMA: VA:0x400000 -> /tmp/foo:0x0 (private) Filemap: /tmp/foo:0x0 -> PA:0x2fb000 PTE: VA:0x400000 -----------------> PA:0x5ea000 匿名映射 flag里面如果有MAP_ANONYMOUS就使用匿名映射, 就是不需要文件的映射. 匿名也分shared和private. shared模式下面, 会产生一个临时的零字节的文件, 大家都map到这个文件. private模式下面, 就没有这个临时文件了. 而是一开始都用一个固定的readonly的全零的页, 直到copy on write新分配一个可写的物理页. 如何在用户态处理page fault signal方式 参考: https://lwn.net/Articles/550555/ 一般的, 可以使用mprotect(PROT_NONE)来产生SIGSEGV, 然后在SIGSEGV的handler里面在用户态处理page fault. #include int mprotect(void *addr, size_t len, int prot); int pkey_mprotect(void *addr, size_t len, int prot, int pkey); The SIGBUS signal handler's job is to handle the page fault by mapping a real page to the faulting address. That can be done in current kernels with the mremap() system call. The problem with mremap() is that it works by splitting the virtual memory area (VMA) structure used to describe the memory range within the kernel. Frequent mremap() calls will result in the kernel having to manage a large number of VMAs, which is an expensive proposition. mremap() will also happily overwrite existing memory mappings, making it harder to detect errors (or race conditions) in user-space handlers. For these reasons, mremap() is not an ideal solution to the problem. 还可以选madvise(MADV_USERFAULT), 似乎更好 Perhaps I'm misunderstanding something here, but I don't understand how MADV_USERFAULT is different/superior from doing an mprotect(PROT_NONE) and then handling the SIGSEGV. Can someone help me out? For one there is the uglyness of properly handling SIGSEGVs which requires sigaltstack et al. which is far from easy. For another, if you would go that way you would need to call mmap() for every single page fault which would probably end up being horrendously expensive since you would end up with thousands of different mmap()s setup which is rather expensive. With the patchset, as far as I understand it, there's just one memory region setup in the kernel and just when it cannot find backing memory it falls back to the userspace page fault handler. userfaultfd方式用户态page管理 上面描述的是使用SIGSEGV信号及其handler在用户态处理page fault, 简称umap而这篇文章: https://arxiv.org/ftp/arxiv/papers/1910/1910.07566.pdf提到了使用userfaultfd的umap技术, 在用户app的单独线程里处理page fault. Page faults in the address ranges are delivered asynchronously so that the faulting process is blocked instead of idling, allowing other processes to be scheduled to proceed. 参考: man userfaultfd userfaultfd() creates a new userfaultfd object that can be used for delegation of page-fault handling to a user-space application, and returns a file descriptor that refers to the new object. The new userfaultfd object is configured using ioctl(2). Once the userfaultfd object is configured, the application can use read(2) to receive userfaultfd notifications. The reads from userfaultfd may be blocking or non-blocking, depending on the value of flags used for the creation of the userfaultfd or subsequent calls to fcntl(2). 另外参考: https://www.kernel.org/doc/html/latest/admin-guide/mm/userfaultfd.html The real advantage of userfaults if compared to regular virtual memory management of mremap/mprotect is that the userfaults in all their operations never involve heavyweight structures like vmas (in fact the userfaultfd runtime load never takes the mmap_lock for writing). Vmas are not suitable for page- (or hugepage) granular fault tracking when dealing with virtual address spaces that could span Terabytes. Too many vmas would be needed for that. The userfaultfd once opened by invoking the syscall, can also be passed using unix domain sockets to a manager process, so the same manager process could handle the userfaults of a multitude of different processes without them being aware about what is going on (well of course unless they later try to use the userfaultfd themselves on the same region the manager is already tracking, which is a corner case that would currently return -EBUSY). The userland application should set the feature flags it intends to use when invoking the UFFDIO_API ioctl, to request that those features be enabled if supported. Once the userfaultfd API has been enabled the UFFDIO_REGISTER ioctl should be invoked (if present in the returned uffdio_api.ioctls bitmask) to register a memory range in the userfaultfd by setting the uffdio_register structure accordingly. The uffdio_register.mode bitmask will specify to the kernel which kind of faults to track for the range. The UFFDIO_REGISTER ioctl will return the uffdio_register.ioctls bitmask of ioctls that are suitable to resolve userfaults on the range registered. Not all ioctls will necessarily be supported for all memory types (e.g. anonymous memory vs. shmem vs. hugetlbfs), or all types of intercepted faults. Userland can use the uffdio_register.ioctls to manage the virtual address space in the background (to add or potentially also remove memory from the userfaultfd registered range). This means a userfault could be triggering just before userland maps in the background the user-faulted page. 这个page fault处理线程使用UFFDIO_COPY ioctl来解决page fault, 这个ioctl的好处是保证file内容被完全拷贝到新分配的物理内容, 才会唤醒app进程. 这个用户态处理page fault的过程, 可以使用app的knowledge, 比如可以设置page size, 预取和换出策略. 这些\"定制化\"的策略只影响这个app, 其他的app可以选择不同的策略. 这个灵活性是kernel无法提供的. 同时, 写进物理页的内容也可以是从其他非文件的地方来, 比如远程的数据服务器. 总的来说, Umap在用户态实现了page的管理. 在用户app的虚拟地址空间, umap管理深蓝色部分. 产生的page faults入队列, 由filler们把数据从不同的存储实体(stroe object)里填充到内部buffer, 如果buffer满了, 就触发eviction机制, 由evictor把脏页写回到存储实体里. 为了提高并发, fillers和evictors都是IO线程池. umap可以让app自己配置page size, 这个对性能提高帮助很大. 也可以让app配置灵活的prefetching策略. 硬件的prefetching往往不够灵活, 因为现实的预取的pattern很复杂, 很难有一个通用的策略. 内核代码可以使用prefetch相关的函数来直到硬件预取, 但一般的实现里用户态没有相关的api. umap的API类似mmap: int fd = open(fname , O_RDWR); void* base_addr = umap(NULL, totalbytes, PROT_READ|PROT_WRITE, UMAP_PRIVATE, fd, 0); //Select two non -contiguous pages to prefetch std::vector pfi; umap_prefetch_item p0 = { .page_base_addr = &base[5 *psize] }; pfi.push_back(p0); umap_prefetch_item p1 = { .page_base_addr = &base[15* psize] }; pfi.push_back(p1); umap_prefetch(num_prefetch_pages , &pfi[0]); computation(); //release resources uunmap(base_addr , totalbytes); umap的性能在page size为4K的时候还是低于mmap的, 但从64K开始, 已经开始超越:这说明基本上, umap的性能超越来自于page size可以修改, 而不是来自于其本身的框架. 因为userfaultfd的机制通知用户态本身就有不小的overhead. sentry的mm 参考: https://xhfu.me/files/ad5e3bbbdb2e7f4dbb5dc19c121e89a9/cse291_project_final_report.pdf 对sentry来说, app的mmap会被sentry拦截并解析(使用ptrace或kvm), 创建sentry的VMA到这个file的映射, 然后使用pgalloc包创建这个file到一个host临时文件的映射: Create sentry VMA: Maps virtual address to offset of file in sentry (instead of host kernel). After triggered by a sentry page fault (VA accessed for 1st time) Create sentry filemap: pgalloc is used to map file and offset in sentry to file and offset on host. Create host VMA: Maps virtual address from 1. to file and offset on host from 2. by calling the host mmap syscall. After triggered by a host page fault (VA acessed for 2nd time) Create host filemap: Maps file and offset on host to physical address. Create PTE: Maps virtual address from 1. to physical address from 4. 性能对比 netstask netstask是gvisor的用户态kernel sentry的tcp/ip协议栈. 代码在https://github.com/google/gvisor/tree/master/pkg/sentry/socket/netstack tcp/ip协议栈的核心实现在https://github.com/google/gvisor/tree/master/pkg/tcpip gvisor支持NetworkSandbox和NetworkHost: 前者是默认的, 用的是gvisor自己实现的协议栈; 后者是直接使用host的syscall. const ( // NetworkSandbox uses internal network stack, isolated from the host. NetworkSandbox NetworkType = iota // NetworkHost redirects network related syscalls to the host network. NetworkHost // NetworkNone sets up just loopback using netstack. NetworkNone ) pkg/abi/linux 这个包提供了很多系统级宏定义, 结构体定义, 按功能.go文件来组织的, 比较清楚, 相对全面, 比如 mm.go ip.go fs.go epoll_arm64.go 等等, 很多. 下面是其中一个seccomp.go的举例: pkg/abi/linux/seccomp.go 看起来都是手动根据系统头文件整理的, 比如: // Seccomp constants taken from . const ( SECCOMP_MODE_NONE = 0 SECCOMP_MODE_FILTER = 2 SECCOMP_RET_ACTION_FULL = 0xffff0000 SECCOMP_RET_ACTION = 0x7fff0000 SECCOMP_RET_DATA = 0x0000ffff SECCOMP_SET_MODE_FILTER = 1 SECCOMP_FILTER_FLAG_TSYNC = 1 SECCOMP_GET_ACTION_AVAIL = 2 ) // BPFAction is an action for a BPF filter. type BPFAction uint32 // BPFAction definitions. const ( SECCOMP_RET_KILL_PROCESS BPFAction = 0x80000000 SECCOMP_RET_KILL_THREAD BPFAction = 0x00000000 SECCOMP_RET_TRAP BPFAction = 0x00030000 SECCOMP_RET_ERRNO BPFAction = 0x00050000 SECCOMP_RET_TRACE BPFAction = 0x7ff00000 SECCOMP_RET_ALLOW BPFAction = 0x7fff0000 ) pkg/hostarch/hostarch.go host只依赖标准库\"encoding/binary\"和unix系统库\"golang.org/x/sys/unix\", 它描述了host的地址空间host包很简单, 主要是pagesize, 目前只支持4K的页. func init() { // Make sure the page size is 4K on arm64 platform. if size := unix.Getpagesize(); size != PageSize { panic(\"Only 4K page size is supported on arm64!\") } } 还有rwx的读写执行的属性定义. pkg/seccomp/seccomp.go seccomp用于产生seccomp filter. 看起来是用的比较原始的bpf汇编. pkg/sentry/arch/arch.go 依赖 \"gvisor.dev/gvisor/pkg/abi/linux\" \"gvisor.dev/gvisor/pkg/cpuid\" \"gvisor.dev/gvisor/pkg/hostarch\" \"gvisor.dev/gvisor/pkg/log\" \"gvisor.dev/gvisor/pkg/marshal\" \"gvisor.dev/gvisor/pkg/sentry/arch/fpu\" \"gvisor.dev/gvisor/pkg/sentry/limits\" arch目前支持amd64和arm64, 里面定义了上下文的接口, 用接口来抽象: Context provides architecture-dependent information for a specific thread. 每个线程都有个context, 里面有系统调用相关的, 栈相关的, 寄存器恢复相关的. 为了通用性, 值都用uintptr来表示, 比如: type SyscallArgument struct { // Prefer to use accessor methods instead of 'Value' directly. Value uintptr } // SyscallArguments represents the set of arguments passed to a syscall. type SyscallArguments [6]SyscallArgument context有个方法, get和set所有寄存器 // PtraceGetRegs implements ptrace(PTRACE_GETREGS) by writing the // general-purpose registers represented by this Context to dst and // returning the number of bytes written. PtraceGetRegs(dst io.Writer) (int, error) // PtraceSetRegs implements ptrace(PTRACE_SETREGS) by reading // general-purpose registers from src into this Context and returning the // number of bytes read. PtraceSetRegs(src io.Reader) (int, error) 寄存器有统一的抽象 比如pkg/sentry/arch/arch_aarch64.go中, 定义的寄存器: // State contains the common architecture bits for aarch64 (the build tag of this // file ensures it's only built on aarch64). // // +stateify savable type State struct { // The system registers. Regs Registers // Our floating point state. fpState fpu.State `state:\"wait\"` // FeatureSet is a pointer to the currently active feature set. FeatureSet *cpuid.FeatureSet // OrigR0 stores the value of register R0. OrigR0 uint64 } 注意这里的Registers其实用的是linux.PtraceRegs // Registers represents the CPU registers for this architecture. // // +stateify savable type Registers struct { linux.PtraceRegs // TPIDR_EL0 is the EL0 Read/Write Software Thread ID Register. TPIDR_EL0 uint64 } 这个linux.PtraceRegs在pkg/abi/linux/ptrace_arm64.go中定义, 这是个只在arm64上编译的文件, 属于linux abi的一部分. // PtraceRegs is the set of CPU registers exposed by ptrace. Source: // syscall.PtraceRegs. // // +marshal // +stateify savable type PtraceRegs struct { Regs [31]uint64 Sp uint64 Pc uint64 Pstate uint64 } 上面的State实现了部分context接口的函数, 被pkg/sentry/arch/arch_arm64.go的context64使用: // context64 represents an ARM64 context. // // +stateify savable type context64 struct { State sigFPState []fpu.State // fpstate to be restored on sigreturn. } 这个context64就实现了全部的context要求的接口, 这要求有对底层寄存器的ABI的知识, 比如: // General purpose registers usage on Arm64: // R0...R7: parameter/result registers. // R8: indirect result location register. // R9...R15: temporary rgisters. // R16: the first intra-procedure-call scratch register. // R17: the second intra-procedure-call scratch register. // R18: the platform register. // R19...R28: callee-saved registers. // R29: the frame pointer. // R30: the link register. kernel package kernel里面实现了基础的内核组件: task 调度 signal等等... 标准的kernel的调度对象是线程, 而gvisor的调度对象是goroutine 在标准的kernel下, 一个线程可以在如下情况下被调度出去: 线程自己让出执行, 或被抢占, 线程仍然runnable, 但已经不在执行 线程退出. sentry里面, 从Task.run里退出就可以了 线程进入可打断的sleep, 线程可以被自己唤醒或收到信号. sentry里面是用blocking这个模式, 所有事件block在go channel的select, 所以可以被打断. 在task_block.go 线程进入不可打断的睡眠, 只有用户自己定义的wakeup条件达到才能唤醒. 大体上, sentry还是用了checkpoints技术在关键点设置调度代码. task的状态机 runsc boot流程 比如如下命令: docker run --cpus=2 -m 2g --rm --runtime=runsc -it --name=test centos:7 bash 会导致containerd-shim调用runsc boot命令 runsc --root=/var/run/docker/runtime-runsc/moby --debug=true --log=/run/containerd/io.containerd.runtime.v1.linux/moby/8142acd62c66b0847eddee55c7c247a05a04e91b0e4a0db2c6942075ceb75f2e/log.json --log-format=json --debug-log=/tmp/runsc/ --platform=kvm --strace=true --log-fd=3 --debug-log-fd=4 boot --bundle=/run/containerd/io.containerd.runtime.v1.linux/moby/8142acd62c66b0847eddee55c7c247a05a04e91b0e4a0db2c6942075ceb75f2e --controller-fd=5 --mounts-fd=6 --spec-fd=7 --start-sync-fd=8 --io-fds=9 --io-fds=10 --io-fds=11 --io-fds=12 --device-fd=13 --pidns=true --setup-root --stdio-fds=14 --stdio-fds=15 --stdio-fds=16 --cpu-num 24 --total-memory76005576704 8142acd62c66b0847eddee55c7c247a05a04e91b0e4a0db2c6942075ceb75f2e] kernel就是runsc boot进程, 这个进程也叫sandbox进程 比如在vm里ls或者ps, 用户程序代码段会被map到runsc boot进程空间, 用pmap能看到 所有用户态程序都是用goroutine运行的 所有的用户态程序都是在runsc boot进程空间的, 对host来说都是一个runsc boot进程, 如果看CPU占用率就会看到都是runsc boot进程在占用CPU. KVM的platform的内存映射的核心逻辑是通过kVM的KVM_SET_USER_MEMORY_REGION 来配置VM的PA到host进程的VA, 使得VM的VA被map到host进程的对应VA, 从而使用host进程的PA. 比如在VM里面执行ls命令, kernel(即runsc boot)发现是syscall的exec, 所以退出VM, 在host进程 空间load这个ls的elf, 做好memory映射 //runsc boot: launch a sandbox process //@runsc/cli/main.go Main() //@runsc/cmd/boot.go (b *Boot) Execute() //Setting up sandbox chroot in \"/tmp\" setUpChroot() //原理上是调用unix.Mount()准备chroot的目录, 比如/tmp /proc等 //然后调用pivot_root系统调用做chroot pivotRoot(\"/tmp\") //使用controller-fd mounts-fd spec-fd start-sync-fd io-fds io-fds device-fd和上层交互 //使用上面的信息Create the loader. bootArgs := boot.Args{ ID: f.Arg(0), Spec: spec, Conf: conf, ControllerFD: b.controllerFD, Device: os.NewFile(uintptr(b.deviceFD), \"platform device\"), GoferFDs: b.ioFDs.GetArray(), StdioFDs: b.stdioFDs.GetArray(), NumCPU: b.cpuNum, TotalMem: b.totalMem, UserLogFD: b.userLogFD, ProfileBlockFD: b.profileBlockFD, ProfileCPUFD: b.profileCPUFD, ProfileHeapFD: b.profileHeapFD, ProfileMutexFD: b.profileMutexFD, TraceFD: b.traceFD, } //新建一个kvm类型的VM做为flatform, 新建一个临时文件做为backed memory file //map VDSO, 新建一个timekeeper, 新建rootNetwork, 注册文件系统, 作为kernel //用上面的kernel做一个loader, 启动service响应socket的control请求 l, err := boot.New(bootArgs) // l是loader //createPlatform在kvm实现下是新建一个vm //在Restore流程里也会调用createPlatform, 从当前的kernel恢复到新建的kernel //@runsc/boot/loader.go p, err := createPlatform(args.Conf, args.Device) p, err := platform.Lookup(\"kvm\") //这里是kvm的New //@pkg/sentry/platform/kvm/kvm.go p.New(deviceFile) updateGlobalOnce() updateSystemValues(int(fd)) sz, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(fd), _KVM_GET_VCPU_MMAP_SIZE, 0) runDataSize = int(sz) hasGuestPCID = true //@pkg/ring0/lib_arm64.go ring0.Init() //目前为空 physicalInit() //重要全局变量, 记录了这个进程状态下的VA和虚拟物理地址(或者叫IPA)的映射. //把host空间用mmap填满 physicalRegions = computePhysicalRegions(fillAddressSpace()) //ioctl创建一个vm vm, _, errno = unix.Syscall(unix.SYS_IOCTL, fd, _KVM_CREATE_VM, 0) //@pkg/sentry/platform/kvm/machine.go machine, err := newMachine(int(vm)) m := &machine{fd: vm} maxVCPUs, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_CHECK_EXTENSION, _KVM_CAP_MAX_VCPUS) m.vCPUsByTID = make(map[uint64]*vCPU) m.vCPUsByID = make([]*vCPU, m.maxVCPUs) //这个kernel不是kernel.Kernel, 而是ring0.kernel, 是给kvm用的 m.kernel.Init(m.maxVCPUs) //24个核是24个VCPU maxSlots, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_CHECK_EXTENSION, _KVM_CAP_MAX_MEMSLOTS) //509个 hasTSCControl, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_CHECK_EXTENSION, _KVM_CAP_TSC_CONTROL) //我这里是false // Create the upper shared pagetables and kernel(sentry) pagetables. m.upperSharedPageTables = pagetables.New(newAllocator()) m.mapUpperHalf(m.upperSharedPageTables) m.upperSharedPageTables.Allocator.(*allocator).base.Drain() m.upperSharedPageTables.MarkReadOnlyShared() m.kernel.PageTables = pagetables.NewWithUpper(newAllocator(), m.upperSharedPageTables, ring0.KernelStartAddress) // 配置seccomp为trap mmap, 其他allow; mmap会触发SIGSYS信号 seccompMmapRules(m) // Map everything in the lower half. //即physicalRegions里的每个region都生成一个PTE(page table entry), 都在lower half m.kernel.PageTables.Map() // 把当前host进程已经map的虚拟地址空间(\"/proc/self/maps\")映射进VM for line in open(\"/proc/self/maps\") if vr.accessType.Execute //有代码被加到VM的页表 m.kernel.PageTables.Map() m.mapPhysical(physical, length, physicalRegions) // 用KVM的SET_USER_MEMORY_REGION ioctl来配置kvm的第二次翻译 // map host va和VM pa handleBluepillFault(m, physical, phyRegions) virtualStart, physicalStart, length, pr := calculateBluepillFault(physical, phyRegions) m.setMemoryRegion(int(slot), physicalStart, length, virtualStart, flags) unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_SET_USER_MEMORY_REGION, uintptr(unsafe.Pointer(&userRegion))) //@pkg/sentry/platform/kvm/machine_arm64_unsafe.go m.initArchState() unix.RawSyscall(_KVM_ARM_PREFERRED_TARGET) for maxVCPU //对每个vCPU //@pkg/sentry/platform/kvm/machine.go m.createVCPU(i) //KVM ioctl _KVM_CREATE_VCPU fd, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_CREATE_VCPU, uintptr(id)) c := &vCPU{ id: id, fd: int(fd), machine: m, } c.CPU.Init(&m.kernel, c.id, c) // Set the kernel stack pointer(virtual address). c.registers.Sp = uint64(c.StackTop()) //这个kernel的stack是给中断用的 m.vCPUsByID[c.id] = c c.setSignalMask() //Defines which signals are blocked during execution of KVM_RUN //按理说应该是每个线程来配置, 但这里没有开新go routine unix.RawSyscall(_KVM_SET_SIGNAL_MASK) //runData是mmap来的, 是用户态和KVM交互用的 runData, err := mapRunData(int(fd)) c.runData = runData //@pkg/sentry/platform/kvm/machine_arm64_unsafe.go c.initArchState() //KVM_ARM_VCPU_INIT会把cpu重置为初始值. 如果没有这一步, KVM_RUN就会错误 unix.RawSyscall(_KVM_ARM_VCPU_INIT) //用setOneRegister()设置如下寄存器: //tcr_el1: Translation Control Register //mair_el1: Multiprocessor Affinity Register //ttbr0_el1: 用户态页表基地址, 这里对应sentry, 代码里也对应c.SetTtbr0Kvm(uintptr(data)) //ttbr1_el1: 内核态页表基地址, 这里是upper空间 //sp_el1: 内核态的sp, 处理中断的 //pc: 初始指向ring0.Start() //r8: r8是platform专用寄存器, 指向c.CPU //vbar_el1: 异常vector基地址, 是ring0.Vectors, 这是代码段 //给这个vCPU设置时间 //在arm64上是setOneRegister _KVM_ARM64_REGS_TIMER_CNT c.setSystemTime() // m代表的machine是垃圾回收的, 用SetFinalizer机制来调用m.Destroy清理 runtime.SetFinalizer(m, (*machine).Destroy) //这里new一个kernel.Kernel, 包括了一个新建的kvm platform的实例 k := &kernel.Kernel{ Platform: p, } //使用一个叫\"runsc-memory\"的临时文件做为backed memory file mf, err := createMemoryFile() memfd, err := memutil.CreateMemFD(\"runsc-memory\", 0) memfile := os.NewFile(uintptr(memfd), memfileName) //@pkg/sentry/pgalloc/pgalloc.go mf, err := pgalloc.NewMemoryFile(memfile, pgalloc.MemoryFileOpts{}) k.SetMemoryFile(mf) //VDSO vdso, err := loader.PrepareVDSO(k) //这个vdsodata.Binary是个自动生成的[]byte数组 //好像是从vdso_bin里读出来的 //在@pkg/sentry/loader/vdsodata/vdso_arm64.go vdsoFile := &byteFullReader{data: vdsodata.Binary} //检测VDSO的elf头, 返回一个elfInfo结构体 info, err := validateVDSO(nil, vdsoFile, uint64(len(vdsodata.Binary))) size, ok := hostarch.Addr(len(vdsodata.Binary)).RoundUp() mf := mfp.MemoryFile() //给VDSO申请页 vdso, err := mf.Allocate(uint64(size), pgalloc.AllocOpts{Kind: usage.System}) ims, err := mf.MapInternal(vdso, hostarch.ReadWrite) _, err = safemem.CopySeq(ims, safemem.BlockSeqOf(safemem.BlockFromSafeSlice(vdsodata.Binary))) //再申请一个参数页 paramPage, err := mf.Allocate(hostarch.PageSize, pgalloc.AllocOpts{Kind: usage.System}) //新建一个timekeeper. 包括Monotonic和Realtime两种 //@pkg/sentry/time/calibrated_clock.go tk := kernel.NewTimekeeper(k, vdso.ParamPage.FileRange()) tk.SetClocks(time.NewCalibratedClocks()) // Create root network namespace/stack. netns, err := newRootNetworkNamespace(args.Conf, tk, k) case config.NetworkHost: inet.NewRootNamespace(hostinet.NewStack(), nil) case config.NetworkNone, config.NetworkSandbox: s, err := newEmptySandboxNetworkStack(clock, uniqueID, conf.AllowPacketEndpointWrite) creator := &sandboxNetstackCreator{ clock: clock, uniqueID: uniqueID, allowPacketEndpointWrite: conf.AllowPacketEndpointWrite, } inet.NewRootNamespace(s, creator) // Create capabilities. caps, err := specutils.Capabilities(args.Conf.EnableRaw, args.Spec.Process.Capabilities) // Create credentials. creds := auth.NewUserCredentials( auth.KUID(args.Spec.Process.User.UID), auth.KGID(args.Spec.Process.User.GID), extraKGIDs, caps, auth.NewRootUserNamespace()) //初始化kernel k.Init(kernel.InitKernelArgs{ FeatureSet: cpuid.HostFeatureSet().Fixed(), Timekeeper: tk, RootUserNamespace: creds.UserNamespace, RootNetworkNamespace: netns, ApplicationCores: uint(args.NumCPU), Vdso: vdso, RootUTSNamespace: kernel.NewUTSNamespace(args.Spec.Hostname, args.Spec.Hostname, creds.UserNamespace), RootIPCNamespace: kernel.NewIPCNamespace(creds.UserNamespace), RootAbstractSocketNamespace: kernel.NewAbstractSocketNamespace(), PIDNamespace: kernel.NewRootPIDNamespace(creds.UserNamespace), }) registerFilesystems(k) adjustDirentCache(k) procArgs, err := createProcessArgs(args.ID, args.Spec, creds, k, k.RootPIDNamespace()) err := initCompatLogs(args.UserLogFD) mountHints, err := newPodMountHints(args.Spec) eid := execID{cid: args.ID} //loader包括kernel.Kernel l := &Loader{ k: k, watchdog: dog, sandboxID: args.ID, processes: map[execID]*execProcess{eid: {}}, mountHints: mountHints, root: info, stopProfiling: stopProfiling, } sighandling.IgnoreChildStop() //Create the control server using the provided FD. ctrl, err := newController(args.ControllerFD, l) ctrl := &controller{} //创建基于unix socket urpc server ctrl.srv, err = server.CreateFromFD(fd) //注册控制接口 ctrl.srv.Register(ctrl.manager) ctrl.srv.Register(net) ctrl.srv.Register(&control.Events{}) ctrl.srv.Register(&control.Fs{Kernel: l.k}) ctrl.srv.Register(&control.Lifecycle{Kernel: l.k}) ctrl.srv.Register(&control.Logging{}) ctrl.srv.Register(&control.Usage{Kernel: l.k}) ctrl.srv.Register(&control.Proc{Kernel: l.k}) ctrl.srv.Register(&control.State{Kernel: l.k}) ctrl.srv.Register(&debug{}) l.ctrl = ctrl //起一个go routine accept socket连接 ctrl.srv.StartServing() //通知父进程sandbox已经启动完毕, 已经准备好controller服务 // Wait for the start signal from runsc. //等待runsc发start信号 l.WaitForStartSignal() // Run runs the root container. // Run the application and wait for it to finish. l.Run() //installs sandbox seccomp filters with the host. l.installSeccompFilters() //如果不是restore场景, 就新起一个进程load user mode程序 // Create the root container init task. It will begin running // when the kernel is started. l.createContainerProcess(true, l.sandboxID, &l.root) // Create the FD map, which will set stdin, stdout, and stderr. createFDTable(ctx, info.spec.Process.Terminal, info.stdioFDs, info.spec.Process.User) //起一个监测gofer的routine l.startGoferMonitor(cid, int32(info.goferFDs[0].FD())) mntr := newContainerMounter(info, l.k, l.mountHints, kernel.VFS2Enabled) if root mntr.processHints(info.conf, info.procArgs.Credentials) //set up the file system for all containers //即所有的container都看到同一份/目录, mount的信息放在&info.procArgs setupContainerFS(ctx, info.conf, mntr, &info.procArgs) //@pkg/sentry/kernel/kernel.go // CreateProcess creates a new task in a new thread group with the given // options. The new task has no parent and is in the root PID namespace. //新的task在root pid 空间里, 没有parent l.k.CreateProcess(info.procArgs) //准备mount namespace //建一个空的thread group tg := k.NewThreadGroup(mntns, args.PIDNamespace, NewSignalHandlers(), linux.SIGCHLD, args.Limits) // Create a fresh task context. remainingTraversals := args.MaxSymlinkTraversals loadArgs := loader.LoadArgs{ Opener: opener, RemainingTraversals: &remainingTraversals, ResolveFinal: true, Filename: args.Filename, File: args.File, CloseOnExec: false, Argv: args.Argv, Envv: args.Envv, Features: k.featureSet, } // LoadTaskImage loads a specified file into a new TaskImage. //@pkg/sentry/kernel/task_image.go image, se := k.LoadTaskImage(ctx, loadArgs) //新建一个空的memoryManager with no mappings and 1 user //@pkg/sentry/mm/lifecycle.go m := mm.NewMemoryManager(k, k, k.SleepForAddressSpaceActivation) //@pkg/sentry/loader/loader.go //Load loads args.File into a MemoryManager. os, ac, name, err := loader.Load(ctx, args, k.extraAuxv, k.vdso) loaded, ac, file, newArgv, err := loadExecutable(ctx, args) //看文件头, 分elf(\"\\x7fELF\")和script(\"#!\") case \"\\x7fELF\": // loadELF loads args.File into the Task address space. //@pkg/sentry/loader/elf.go loaded, ac, err := loadELF(ctx, args) //加载args.File到args.MemoryManager, 底层调用MemoryManager.MMap建立一个内存区域 bin, ac, err := loadInitialELF(ctx, args.MemoryManager, args.Features, args.File) if 解释器 interp, err = loadInterpreterELF(ctx, args.MemoryManager, intFile, bin) case \"#!\": //返回解释器的路径 args.Filename, args.Argv, err = parseInterpreterScript(ctx, args.Filename, args.File, args.Argv) // Lookup our new syscall table. st, ok := LookupSyscallTable(os, ac.Arch()) // Create the task. config := &TaskConfig{ Kernel: k, ThreadGroup: tg, TaskImage: image, FSContext: fsContext, FDTable: args.FDTable, Credentials: args.Credentials, NetworkNamespace: k.RootNetworkNamespace(), AllowedCPUMask: sched.NewFullCPUSet(k.applicationCores), UTSNamespace: args.UTSNamespace, IPCNamespace: args.IPCNamespace, AbstractSocketNamespace: args.AbstractSocketNamespace, MountNamespaceVFS2: mntnsVFS2, ContainerID: args.ContainerID, UserCounters: k.GetUserCounters(args.Credentials.RealKUID), } t, err := k.tasks.NewTask(ctx, config) t, err := ts.newTask(cfg) t.traceExecEvent(image) // Simulate exec for tracing. l.watchdog.Start() //45s周期, 3m超时 l.k.Start() k.started = true k.cpuClockTicker = ktime.NewTimer(k.timekeeper.monotonicClock, newKernelCPUClockTicker(k)) k.cpuClockTicker.Swap(ktime.Setting{ Enabled: true, Period: linux.ClockTick, }) // Start task goroutines. for t, tid := range k.tasks.Root.tids { t.Start(tid) //@pkg/sentry/kernel/task_run.go go t.run(uintptr(tid)) // Activate our address space. t.Activate() //调用platform的NewAddressSpace() for t.doStop() //等待下一次运行 //注意这个结构, 一行就支持状态机的迁移 //因为多个状态都有execute函数, 执行后返回下一个状态 //不同的状态: //runSyscallAfterPtraceEventClone //runSyscallAfterVforkStop //runSyscallAfterExecStop //runExit //runExitMain //runExitNotify //runApp //runInterrupt //runInterruptAfterSignalDeliveryStop //runSyscallAfterPtraceEventSeccomp //runSyscallAfterSyscallEnterStop //runSyscallAfterSysemuStop //runSyscallReinvoke //runSyscallExit //runVsyscallAfterPtraceEventSeccomp //初始状态是(*runApp): t.runState = t.runState.execute(t) if t.interrupted() return (*runInterrupt)(nil) //下个状态是interrupt //在返回用户态之前, 执行task work if atomic.LoadInt32(&t.taskWorkCount) > 0 queue := t.taskWork for _, work := range queue work.TaskWork(t) //处理可能的SyscallReturn //处理可能的SavedSignalMask //调用platform的switch info, at, err := t.p.Switch(t, t.MemoryManager(), t.Arch(), t.rseqCPU) switch err { case nil: return t.doSyscall() //默认就是syscall case platform.ErrContextInterrupt: //被platform.Context.Interrupt()打断 return (*runApp)(nil) case platform.ErrContextSignal: //被信号打断 t.MemoryManager().HandleUserFault() //处理用户pagefault if 是同步signal t.SendSignal(info) else t.k.sendExternalSignal(info, \"application\") return (*runApp)(nil) case platform.ErrContextCPUPreempted: t.rseqPreempted = true return (*runApp)(nil) } } l.WaitExit() l.Destroy() platform kernel包括了platform, 比如在初始化的时候: p, err := createPlatform(cm.l.root.conf, deviceFile) //Kernel包括platform k := &kernel.Kernel{ Platform: p, } platform抽象了一个platform的能力, 主要是描述调度能力, 地址空间能力. 比如MemoryManager是在地址空间之上的一种抽象: // MemoryManager represents an abstraction above the platform address space // which manages memory mappings and their contents. type MemoryManager interface { //usermem.IO provides access to the contents of a virtual memory space. usermem.IO // MMap establishes a memory mapping. MMap(ctx context.Context, opts memmap.MMapOpts) (hostarch.Addr, error) // AddressSpace returns the AddressSpace bound to mm. AddressSpace() AddressSpace } 地址空间 // AddressSpace represents a virtual address space in which a Context can // execute. type AddressSpace interface { // MapFile creates a shared mapping of offsets fr from f at address addr. // Any existing overlapping mappings are silently replaced. // // If precommit is true, the platform should eagerly commit resources (e.g. // physical memory) to the mapping. The precommit flag is advisory and // implementations may choose to ignore it. // // Preconditions: // * addr and fr must be page-aligned. // * fr.Length() > 0. // * at.Any() == true. // * At least one reference must be held on all pages in fr, and must // continue to be held as long as pages are mapped. MapFile(addr hostarch.Addr, f memmap.File, fr memmap.FileRange, at hostarch.AccessType, precommit bool) error // Unmap unmaps the given range. // // Preconditions: // * addr is page-aligned. // * length > 0. Unmap(addr hostarch.Addr, length uint64) // Release releases this address space. After releasing, a new AddressSpace // must be acquired via platform.NewAddressSpace(). Release() // PreFork() is called before creating a copy of AddressSpace. This // guarantees that this address space will be in a consistent state. PreFork() // PostFork() is called after creating a copy of AddressSpace. PostFork() // AddressSpaceIO methods are supported iff the associated platform's // Platform.SupportsAddressSpaceIO() == true. AddressSpaces for which this // does not hold may panic if AddressSpaceIO methods are invoked. AddressSpaceIO } platform包括了Context抽象, 包括上下文切换: // Switch resumes execution of the thread specified by the arch.Context // in the provided address space. This call will block while the thread // is executing. // 正常应该是成功调用一个系统调用. // 如果正在执行这个系统调用的时候有signal, 返回ErrContextSignal // 如果调用了Interrupt()则返回ErrContextInterrupt Switch(ctx context.Context, mm MemoryManager, ac arch.Context, cpu int32) (*linux.SignalInfo, hostarch.AccessType, error) // PullFullState() pulls a full state of the application thread. PullFullState(as AddressSpace, ac arch.Context) // Interrupt interrupts a concurrent call to Switch(), causing it to return // ErrContextInterrupt. Interrupt() // Release() releases any resources associated with this context. Release() 根据g3doc/architecture_guide/platforms.md, gvisor需要平台实现系统调用的拦截, 上下文切换, 和memory map. 这些需求是以interface的形式来体现的: type Platform interface { NewAddressSpace() (AddressSpace, error) NewContext() Context } type Context interface { Switch(as AddressSpace, ac arch.Context) (..., error) } type AddressSpace interface { MapFile(addr hostarch.Addr, f File, fr FileRange, at hostarch.AccessType, ...) error Unmap(addr hostarch.Addr, length uint64) } 现在有ptrace方式和KVM方式: ptrace方式更通用, 但性能差; KVM方式需要硬件虚拟化支持, 性能好点. 哪里调用了Switch() 是kernel调的: 在pkg/sentry/kernel/task_run.go // run runs the task goroutine. // // threadID a dummy value set to the task's TID in the root PID namespace to // make it visible in stack dumps. A goroutine for a given task can be identified // searching for Task.run()'s argument value. func (t *Task) run(threadID uintptr) { atomic.StoreInt64(&t.goid, goid.Get()) // Construct t.blockingTimer here. We do this here because we can't // reconstruct t.blockingTimer during restore in Task.afterLoad(), because // kernel.timekeeper.SetClocks() hasn't been called yet. blockingTimerNotifier, blockingTimerChan := ktime.NewChannelNotifier() t.blockingTimer = ktime.NewTimer(t.k.MonotonicClock(), blockingTimerNotifier) defer t.blockingTimer.Destroy() t.blockingTimerChan = blockingTimerChan // Activate our address space. t.Activate() // The corresponding t.Deactivate occurs in the exit path // (runExitMain.execute) so that when // Platform.CooperativelySharesAddressSpace() == true, we give up the // AddressSpace before the task goroutine finishes executing. // If this is a newly-started task, it should check for participation in // group stops. If this is a task resuming after restore, it was // interrupted by saving. In either case, the task is initially // interrupted. t.interruptSelf() for { // Explanation for this ordering: // // - A freshly-started task that is stopped should not do anything // before it enters the stop. // // - If taskRunState.execute returns nil, the task goroutine should // exit without checking for a stop. // // - Task.Start won't start Task.run if t.runState is nil, so this // ordering is safe. t.doStop() t.runState = t.runState.execute(t) //本质上是在循环调用这个t.runState.execute, 结合后面的分析, 这个函数触发一个syscall cycle的执行. if t.runState == nil { t.accountTaskGoroutineEnter(TaskGoroutineNonexistent) t.goroutineStopped.Done() t.tg.liveGoroutines.Done() t.tg.pidns.owner.liveGoroutines.Done() t.tg.pidns.owner.runningGoroutines.Done() t.p.Release() // Deferring this store triggers a false positive in the race // detector (https://github.com/golang/go/issues/42599). atomic.StoreInt64(&t.goid, 0) // Keep argument alive because stack trace for dead variables may not be correct. runtime.KeepAlive(threadID) return } } } // The runApp state checks for interrupts before executing untrusted // application code. type runApp struct{} func (app *runApp) execute(t *Task) taskRunState { //先检查是否需要处理interrupt //在执行用户代码之前, 执行里面的taskWork for _, work := range queue { work.TaskWork(t) } if t.haveSyscallReturn { } if t.haveSavedSignalMask { } if t.rseqPreempted { } if t.hasTracer() { } //下面就调用了t.p.Switch info, at, err := t.p.Switch(t, t.MemoryManager(), t.Arch(), t.rseqCPU) switch err { case nil: // 最常见的case // Handle application system call. // 这里就是处理用户的syscall return t.doSyscall() case platform.ErrContextInterrupt: case platform.ErrContextSignalCPUID: case platform.ErrContextSignal: case platform.ErrContextCPUPreempted: default: } } 这里是执行syscall的地方 @pkg/sentry/kernel/task_syscall.go // doSyscall is the entry point for an invocation of a system call specified by // the current state of t's registers. // // The syscall path is very hot; avoid defer. func (t *Task) doSyscall() taskRunState { t.Arch().SyscallSaveOrig() sysno := t.Arch().SyscallNo() args := t.Arch().SyscallArgs() if t.syscallFilters.Load() != nil { } return t.doSyscallEnter(sysno, args) } func (t *Task) doSyscallEnter(sysno uintptr, args arch.SyscallArguments) taskRunState { if next, ok := t.ptraceSyscallEnter(); ok { return next } return t.doSyscallInvoke(sysno, args) } func (t *Task) doSyscallInvoke(sysno uintptr, args arch.SyscallArguments) taskRunState { rval, ctrl, err := t.executeSyscall(sysno, args) if ctrl != nil { if !ctrl.ignoreReturn { t.Arch().SetReturn(rval) } if ctrl.next != nil { return ctrl.next } } else if err != nil { t.Arch().SetReturn(uintptr(-ExtractErrno(err, int(sysno)))) t.haveSyscallReturn = true } else { t.Arch().SetReturn(rval) } return (*runSyscallExit)(nil).execute(t) } //这里的executeSyscall就是gvisor拦截syscall后, 真正处理这个syscall的地方 func (t *Task) executeSyscall(sysno uintptr, args arch.SyscallArguments) (rval uintptr, ctrl *SyscallControl, err error) { s := t.SyscallTable() //查表到具体的syscall fn := s.Lookup(sysno) if fn != nil { // Call our syscall implementation. rval, ctrl, err = fn(t, args) //调用fn } else { // Use the missing function if not found. rval, err = t.SyscallTable().Missing(t, sysno, args) } } ptrace gvisor试用ptrace来执行用户代码, 但不允许其执行系统调用. ptrace有context的实现实例:主要是实现了Switch()方法 // Switch runs the provided context in the given address space. func (c *context) Switch(ctx pkgcontext.Context, mm platform.MemoryManager, ac arch.Context, cpu int32) (*linux.SignalInfo, hostarch.AccessType, error) { as := mm.AddressSpace() s := as.(*subprocess) //这里的效果是让这个subprocess执行到下一次syscall, 然后停下来 isSyscall := s.switchToApp(c, ac) //有syscall和signal两种可能 ...保存faultSP, faultAddr, faultIP 如果是syscall, 就返回nil, hostarch.NoAccess, nil 如果有SIGSEGV信号, 就返回&si, hostarch.NoAccess, platform.ErrContextSignal 最后根据条件返回&si, at, platform.ErrContextSignalCPUID } Switch()的基础逻辑是执行一个syscall sycle, 然后停在下一个syscall. 这里面关键是switchToApp // switchToApp is called from the main SwitchToApp entrypoint. // // This function returns true on a system call, false on a signal. func (s *subprocess) switchToApp(c *context, ac arch.Context) bool { // Lock the thread for ptrace operations. runtime.LockOSThread() defer runtime.UnlockOSThread() // Grab our thread from the pool. currentTID := int32(procid.Current()) t := s.sysemuThreads.lookupOrCreate(currentTID, s.newThread) // Reset necessary registers. regs := &ac.StateData().Regs // 从ac里面读出regs t.resetSysemuRegs(regs) // Set registers. 要先设置寄存器, 寄存器的值来自于上次保存的值 if err := t.setRegs(regs); err != nil { panic(fmt.Sprintf(\"ptrace set regs (%+v) failed: %v\", regs, err)) } // 这里看似像是个主循环, 但实际只是想执行一次system call for { // Start running until the next system call. if isSingleStepping(regs) { if _, _, errno := unix.RawSyscall6( unix.SYS_PTRACE, unix.PTRACE_SYSEMU_SINGLESTEP, uintptr(t.tid), 0, 0, 0, 0); errno != 0 { panic(fmt.Sprintf(\"ptrace sysemu failed: %v\", errno)) } } else { if _, _, errno := unix.RawSyscall6( unix.SYS_PTRACE, unix.PTRACE_SYSEMU, //通常是走这里, 每次都用ptrace系统调用来设置, 让tracee在下一次syscall之前停住 uintptr(t.tid), 0, 0, 0, 0); errno != 0 { panic(fmt.Sprintf(\"ptrace sysemu failed: %v\", errno)) } } // Wait for the syscall-enter stop. sig := t.wait(stopped) //注释说的很明确, wait syscall-enter stop(下面会有说明) if sig == unix.SIGSTOP { // SIGSTOP was delivered to another thread in the same thread // group, which initiated another group stop. Just ignore it. continue } t.getRegs(regs) //wait()返回走到这里说明这个tracee的线程已经停在syscall了. 马上保存寄存器到t, 以便下次恢复. t.getFPRegs(fpState, uint64(fpLen), useXsave) t.getTLS(&tls) ac.SetTLS(uintptr(tls)) // Is it a system call? 如果是syscall, 就return true了, 直接从本函数返回. if sig == (syscallEvent | unix.SIGTRAP) { s.arm64SyscallWorkaround(t, regs) // Ensure registers are sane. updateSyscallRegs(regs) return true } //下面是signal的处理流程 t.getSignalInfo(&c.signalInfo) //处理的时候看这个signal是kernel发的还是自己进程发的, 其他进程发的被忽略. } } thread.setRegs 上面switchToApp()函数中, 上来就从adress space里读出保存的寄存器, 用thread.setRegs()恢复到cpu中去. 这个函数就用到了下面会提到的ptrace的PTRACE_SETREGSET命令 // setRegs sets the general purpose register set. func (t *thread) setRegs(regs *arch.Registers) error { iovec := unix.Iovec{ Base: (*byte)(unsafe.Pointer(regs)), Len: uint64(unsafe.Sizeof(*regs)), } _, _, errno := unix.RawSyscall6( unix.SYS_PTRACE, unix.PTRACE_SETREGSET, uintptr(t.tid), linux.NT_PRSTATUS, uintptr(unsafe.Pointer(&iovec)), 0, 0) if errno != 0 { return errno } return nil } 主要结构体 子进程拥有线程池 // subprocess is a collection of threads being traced. type subprocess struct { platform.NoAddressSpaceIO // requests is used to signal creation of new threads. requests chan chan *thread // sysemuThreads are reserved for emulation. sysemuThreads threadPool // syscallThreads are reserved for syscalls (except clone, which is // handled in the dedicated goroutine corresponding to requests above). syscallThreads threadPool // mu protects the following fields. mu sync.Mutex // contexts is the set of contexts for which it's possible that // context.lastFaultSP == this subprocess. contexts map[*context]struct{} } 线程 // thread is a traced thread; it is a thread identifier. // // This is a convenience type for defining ptrace operations. type thread struct { tgid int32 tid int32 cpu uint32 // initRegs are the initial registers for the first thread. // // These are used for the register set for system calls. initRegs arch.Registers } thread有个syscall方法, 用于执行一个系统调用的cycle. 注意这里用了cycle一词, 执行过程和switchToApp()很像: 也是先load寄存器, 调用ptrace的PTRACE_CONT, 然后wait(), 待tracee的线程停住后保存寄存器. 看注释这个syscall不是给app用的, 而是某种条件下inject到远程上下文的. // syscall executes a system call cycle in the traced context. // // This is _not_ for use by application system calls, rather it is for use when // a system call must be injected into the remote context (e.g. mmap, munmap). // Note that clones are handled separately. func (t *thread) syscall(regs *arch.Registers) (uintptr, error) { // Set registers. if err := t.setRegs(regs); err != nil { panic(fmt.Sprintf(\"ptrace set regs failed: %v\", err)) } for { // Execute the syscall instruction. The task has to stop on the // trap instruction which is right after the syscall // instruction. if _, _, errno := unix.RawSyscall6(unix.SYS_PTRACE, unix.PTRACE_CONT, uintptr(t.tid), 0, 0, 0, 0); errno != 0 { panic(fmt.Sprintf(\"ptrace syscall-enter failed: %v\", errno)) } sig := t.wait(stopped) if sig == unix.SIGTRAP { // Reached syscall-enter-stop. break } else { // Some other signal caused a thread stop; ignore. if sig != unix.SIGSTOP && sig != unix.SIGCHLD { log.Warningf(\"The thread %d:%d has been interrupted by %d\", t.tgid, t.tid, sig) } continue } } // Grab registers. if err := t.getRegs(regs); err != nil { panic(fmt.Sprintf(\"ptrace get regs failed: %v\", err)) } return syscallReturnValue(regs) } 线程池 // threadPool is a collection of threads. type threadPool struct { // mu protects below. mu sync.RWMutex // threads is the collection of threads. // // This map is indexed by system TID (the calling thread); which will // be the tracer for the given *thread, and therefore capable of using // relevant ptrace calls. threads map[int32]*thread } ptrace系统调用 要理解这部分, 需要看man ptrace ptrace是个系统调用, 可以让一个进程(tracer)观察和控制另一个进程(tracee)的内存和寄存器.主要是用来实现断点和系统调用跟踪用的. tracee需要先被attach到tracer上. 这个attach是以线程为单位的: 一个进程的多个线程可以分别被attach到不同的tracer上, 没有被attach的线程就还是照常执行. 记住ptrace是对一个线程的. #include //下面的pid是tid, 即线程id long ptrace(enum __ptrace_request request, pid_t pid, void *addr, void *data); 两种情况下一个进程可以发起trace到另外一个进程: fork一个子进程, 然后在子进程里调用ptrace PTRACE_TRACEME, 然后再用execve. 这样这个子进程就可以被trace了 用PTRACE_ATTACH or PTRACE_SEIZE来attach到另一个线程. tracee在有signal的时候会停下来, 即使ignore的signal也会停住. 这个时候, tracer的wait()系统调用会返回, 然后tracer可以通过ptrace命令来对已经停住的tracee做检查和修改. 然后tracer让tracee继续运行. 最后用PTRACE_DETACH来解除trace状态, 让tracee可以继续正常运行. ptrace可以做什么? ptrace的command很丰富, command就是传给ptrace系统调用的request The value of request determines the action to be performed: PTRACE_TRACEME: 唯一一个tracee调用的command, 用于把自身置为tracee PTRACE_PEEKTEXT, PTRACE_PEEKDATA: 查看tracee的内存, 在linux下面, text和data是一个空间, 效果一样. PTRACE_POKETEXT, PTRACE_POKEDATA: 修改tracee的内存. PTRACE_PEEKUSER: 读tracee的USER区域, 这是个含有寄存器和其他信息的区域(见) PTRACE_POKEUSER: 修改tracee的USER区域 PTRACE_GETREGS, PTRACE_GETFPREGS: 读tracee的通用寄存器或者浮点寄存器 PTRACE_SETREGS, PTRACE_SETFPREGS: 修改tracee寄存器 PTRACE_GETREGSET: 读CPU特殊寄存器 PTRACE_SETREGSET: 修改CPU寄存器 PTRACE_GETSIGINFO: 读取signal的信息. 这个signal导致了tracee的stop PTRACE_PEEKSIGINFO PTRACE_SETSIGINFO: 修改signal信息给tracee看 PTRACE_GETSIGMASK PTRACE_SETSIGMASK PTRACE_SETOPTIONS: 使用data来区分接下来的具体option, 用于控制ptrace行为 PTRACE_CONT: 继续tracee PTRACE_SYSCALL, PTRACE_SINGLESTEP: 也是继续tracee, 但让它在下一个syscall的时候停下来, 或者是在下一个指令时停下来. PTRACE_SYSEMU, PTRACE_SYSEMU_SINGLESTEP: 和上面差不多 PTRACE_LISTEN: 也是让tracee继续, 但是不执行(不会被调度到)... PTRACE_KILL: kill tracee PTRACE_INTERRUPT: 让tracee停下来 PTRACE_ATTACH: attach tracee. 需要相应权限 PTRACE_SEIZE: 和attach类似, 但不会导致tracee停止 PTRACE_DETACH: dettach tracee有stop和running状态, 虽然在blocking的系统调用的时候tracee被阻塞了, 但实际还在内核态运行, 是running的状态. stop状态统称ptrace-stop, 当tracee进入ptrace-stop状态时, 会通过waitpid()通知tracer. 所以tracer要在循环里等待 pid = waitpid(pid_or_minus_1, &status, __WALL); ptrace-stop有几种可能: Signal-delivery-stop: 当一个进程收到signal时, 除了sigkill, kernel会选择这个进程的某个线程来handle signal(但如果这个signal是tgkill产生的, 那目标线程可以被caller指定). 如果这个被选中的线程是tracee, 就会进入signal-delivery-stop. 这个时候, signal还没有到达tracee, 而是先被tracer知道: 如果tracer不区supress这个signal, tracer的下一次ptrace restart命令会inject这个signal到tracee. Group-stop: stop signal会导致整个进程的全部线程stop. 那tracee会进入group-stop. 注意这个stop signal也是经过了Signal-delivery-stop然后由tracer inject到tracee的, 进而导致了整个进程stop. PTRACE_EVENT stops: ptrace事件导致的stop, 比如PTRACE_EVENT_FORK, PTRACE_EVENT_CLONE Syscall-stops: 这个场景是因为之前ptrace设置了PTRACE_SYSCALL or PTRACE_SYSEMU命令, 会导致tracee在调用syscall之前stop, 这个就是Syscall-stop PTRACE_EVENT_SECCOMP stops: 和seccomp有关 PTRACE_SINGLESTEP stops: 单步 注意, 因为ptrace大量用了waitpid(), 而真正的tracee的父进程一般也调用wait()来等待子进程退出, 那tracer会先收到waitpid的通知, 然后再通知tracee的父进程. kvm 目录在gvisor/pkg/sentry/platform/kvm golang的汇编基础 官方文档: https://go.dev/doc/asm FP: Frame pointer: arguments and locals. PC: Program counter: jumps and branches. SB: Static base pointer: global symbols. SP: Stack pointer: the highest address within the local stack frame. 某些CPU指令集, 比如arm64, 数据是从右到左: MRS Move System register to general-purpose register MSR Move general-purpose register to System register MRS R0,CPSR ; delivery CPSR Content to R0 MSR CPSR,R0 ; delivery R0 Content to CPSR 比如在kernel代码里: mrs x0, tpidrro_el0 // 特殊寄存器tpidrro_el0赋值给x0 msr sctlr_el1, x0 // x0赋值给特殊寄存器sctlr_el1 但注意, go里面MOV的方向是从左向右: MOVL g(CX), AX // Move g into AX. MOVL g_m(AX), BX // Move g.m into BX. 所以, ARM64的MRS/MSR在golang里是反的: 它们依然遵循从左到右的原则: MSR R1, MDSCR_EL1 // access to the DCC from EL0 MRS TTBR1_EL1, R1 再强调一次: 在go的汇编里, MSR和MRS的使用和ARM官方文档的方向相反 arm64 exception level 详见: https://developer.arm.com/documentation/102412/0102/Privilege-and-Exception-levels 寄存器都是带EL后缀的. 低EL不能访问高EL的寄存器; 强行访问会异常, 类别应该是指令异常 高EL可以访问低EL的寄存器, 但除了虚拟化场景, 其它场景并不常用? arm64内存基础 基础概念: AARCH64支持虚拟内存的tag, 虚拟内存的最高8位是tag, 在地址翻译的时候会被忽略. PC, LR, SP, ELR里面都是VA AArch64有48位VA, 空间有256TB, 有两个range空间 0xFFFF_0000_0000_0000 到 0xFFFF_FFFF_FFFF_FFFF 基址寄存器是TTBR1, 内核态 或 0x0000_0000_0000_0000 到 0x0000_FFFF_FFFF_FFFF 基址寄存器是TTBR0, 用户态 IPA也是48位 PA也是48位, 并且secure和non-secure的物理地址空间是独立的 TTBR是地址转换表的基址寄存器, 这个表由硬件自动查, 并被缓存到TLB中; TTBR里面保存的是物理地址, 是给硬件MMU waker看的. 这个表最多有四级, 地址最多48位, 最大64KB一个映射 一个VA怎么找到PA? VM的地址空间 用KVM启动的VM, 从VM看来, 它的物理地址空间就是其所在的host的qemu进程(或gvisor进程)的进程空间.第一步先用VM里的VA通过TTBR寄存器指向的page table, 查到IPA. 这个IPA其实就是启动VM的进程中某个地址.第二步拿着IPA通过VTTBR寄存器指向的page table来查PA. 这个VTTBR是在EL2里的hypervisor配置的. 再说一遍, VM看到的物理地址, 就是VM所在的进程地址. KVM基础 用户态是通过open(\"/dev/kvm\")然后做ioctl来和内核的kvm模块交互的. KVM支持的IOCTL是: // KVM ioctls. // // Only the ioctls we need in Go appear here; some additional ioctls are used // within the assembly stubs (KVM_INTERRUPT, etc.). // 这里面包括创建VM, 创建VCPU, 设置寄存器, 读取寄存器等等 const ( _KVM_CREATE_VM = 0xae01 _KVM_GET_VCPU_MMAP_SIZE = 0xae04 _KVM_CREATE_VCPU = 0xae41 _KVM_SET_TSS_ADDR = 0xae47 _KVM_RUN = 0xae80 _KVM_NMI = 0xae9a _KVM_CHECK_EXTENSION = 0xae03 _KVM_GET_TSC_KHZ = 0xaea3 _KVM_SET_TSC_KHZ = 0xaea2 _KVM_INTERRUPT = 0x4004ae86 _KVM_SET_MSRS = 0x4008ae89 _KVM_SET_USER_MEMORY_REGION = 0x4020ae46 _KVM_SET_REGS = 0x4090ae82 _KVM_SET_SREGS = 0x4138ae84 _KVM_GET_MSRS = 0xc008ae88 _KVM_GET_REGS = 0x8090ae81 _KVM_GET_SREGS = 0x8138ae83 _KVM_GET_SUPPORTED_CPUID = 0xc008ae05 _KVM_SET_CPUID2 = 0x4008ae90 _KVM_SET_SIGNAL_MASK = 0x4004ae8b _KVM_GET_VCPU_EVENTS = 0x8040ae9f _KVM_SET_VCPU_EVENTS = 0x4040aea0 ) // KVM exit reasons. const ( _KVM_EXIT_EXCEPTION = 0x1 _KVM_EXIT_IO = 0x2 _KVM_EXIT_HYPERCALL = 0x3 _KVM_EXIT_DEBUG = 0x4 _KVM_EXIT_HLT = 0x5 _KVM_EXIT_MMIO = 0x6 _KVM_EXIT_IRQ_WINDOW_OPEN = 0x7 _KVM_EXIT_SHUTDOWN = 0x8 _KVM_EXIT_FAIL_ENTRY = 0x9 _KVM_EXIT_INTERNAL_ERROR = 0x11 _KVM_EXIT_SYSTEM_EVENT = 0x18 _KVM_EXIT_ARM_NISV = 0x1c ) // KVM capability options. const ( _KVM_CAP_MAX_MEMSLOTS = 0x0a _KVM_CAP_MAX_VCPUS = 0x42 _KVM_CAP_ARM_VM_IPA_SIZE = 0xa5 _KVM_CAP_VCPU_EVENTS = 0x29 _KVM_CAP_ARM_INJECT_SERROR_ESR = 0x9e _KVM_CAP_TSC_CONTROL = 0x3c ) // KVM limits. const ( _KVM_NR_MEMSLOTS = 0x100 _KVM_NR_VCPUS = 0xff _KVM_NR_INTERRUPTS = 0x100 _KVM_NR_CPUID_ENTRIES = 0x100 ) // KVM kvm_memory_region::flags. const ( _KVM_MEM_LOG_DIRTY_PAGES = uint32(1) gvisor对CPU和ring0.kernel的抽象 注意kernel这个词在gvisor里有两个不同的意思: kernel.kernel: 是sentry的用户态kernel ring0.kernel: 专门管虚拟化的 pkg/ring0/defs.go 一个kernel可以被多个CPU共享 // Kernel is a global kernel object. // // This contains global state, shared by multiple CPUs. type Kernel struct { // PageTables are the kernel pagetables; this must be provided. PageTables *pagetables.PageTables KernelArchState } CPU // CPU is the per-CPU struct. type CPU struct { // self is a self reference. // // This is always guaranteed to be at offset zero. self *CPU // 持有对自己的引用, 第一次见这么用的... // kernel is reference to the kernel that this CPU was initialized // with. This reference is kept for garbage collection purposes: CPU // registers may refer to objects within the Kernel object that cannot // be safely freed. kernel *Kernel // CPUArchState is architecture-specific state. CPUArchState // registers is a set of registers; these may be used on kernel system // calls and exceptions via the Registers function. registers arch.Registers // floatingPointState holds floating point state. floatingPointState fpu.State // hooks are kernel hooks. hooks Hooks } 对arm64来说: // KernelArchState contains architecture-specific state. type KernelArchState struct { } // CPUArchState contains CPU-specific arch state. type CPUArchState struct { // stack is the stack used for interrupts on this CPU. stack [128]byte //中断栈只有128字节 // errorCode is the error code from the last exception. errorCode uintptr // errorType indicates the type of error code here, it is always set // along with the errorCode value above. // // It will either by 1, which indicates a user error, or 0 indicating a // kernel error. If the error code below returns false (kernel error), // then it cannot provide relevant information about the last // exception. errorType uintptr // faultAddr is the value of far_el1. faultAddr uintptr // el0Fp is the address of application's fpstate. el0Fp uintptr // ttbr0Kvm is the value of ttbr0_el1 for sentry. ttbr0Kvm uintptr // ttbr0App is the value of ttbr0_el1 for applicaton. ttbr0App uintptr // exception vector. vecCode Vector // application context pointer. appAddr uintptr // lazyVFP is the value of cpacr_el1. lazyVFP uintptr // appASID is the asid value of guest application. appASID uintptr } machine和vCPU的定义 machine就是一个kvm new出来的一个VM, 包括一个kernel和多个vCPU // machine contains state associated with the VM as a whole. type machine struct { // fd is the vm fd. fd int // machinePoolIndex is the index in the machinePool array. machinePoolIndex uint32 // nextSlot is the next slot for setMemoryRegion. // // This must be accessed atomically. If nextSlot is ^uint32(0), then // slots are currently being updated, and the caller should retry. nextSlot uint32 // upperSharedPageTables tracks the read-only shared upper of all the pagetables. upperSharedPageTables *pagetables.PageTables // kernel is the set of global structures. kernel ring0.Kernel // mu protects vCPUs. mu sync.RWMutex // available is notified when vCPUs are available. available sync.Cond // vCPUsByTID are the machine vCPUs. // // These are populated dynamically. vCPUsByTID map[uint64]*vCPU // vCPUsByID are the machine vCPUs, can be indexed by the vCPU's ID. vCPUsByID []*vCPU // maxVCPUs is the maximum number of vCPUs supported by the machine. maxVCPUs int // maxSlots is the maximum number of memory slots supported by the machine. maxSlots int // tscControl checks whether cpu supports TSC scaling tscControl bool // usedSlots is the set of used physical addresses (not sorted). usedSlots []uintptr // nextID is the next vCPU ID. nextID uint32 // machineArchState is the architecture-specific state. machineArchState } 注意这里的vCPU其实是和KVM的KVM_CREATE_VCPU对应的 // vCPU is a single KVM vCPU. type vCPU struct { // CPU is the kernel CPU data. // // This must be the first element of this structure, it is referenced // by the bluepill code (see bluepill_amd64.s). ring0.CPU // vCPU包含了CPU的抽象 // id is the vCPU id. id int // fd is the vCPU fd. fd int // tid is the last set tid. tid uint64 // userExits is the count of user exits. userExits uint64 // guestExits is the count of guest to host world switches. guestExits uint64 // faults is a count of world faults (informational only). faults uint32 // state is the vCPU state. // // This is a bitmask of the three fields (vCPU*) described above. state uint32 // runData for this vCPU. runData *runData // machine associated with this vCPU. machine *machine // active is the current addressSpace: this is set and read atomically, // it is used to elide unnecessary interrupts due to invalidations. active atomicAddressSpace // vCPUArchState is the architecture-specific state. vCPUArchState // dieState holds state related to vCPU death. dieState dieState } pagetable之虚拟地址region到物理地址region的map表 pkg/sentry/platform/kvm/physical_map.go对虚拟地址和物理地址的定义: type region struct { virtual uintptr length uintptr } type physicalRegion struct { region physical uintptr } // 用全局变量表示所有的region // region从虚拟地址0开始, 到ring0.MaximumUserAddress // 物理地址从physical := uintptr(reservedMemory)开始. // 这个reservedMemory在arm64上是0, 在amd64上是0x100000000 //比如在x86机器上 // physicalRegion: virtual [1000,3f5b92bd1000) => physical [100001000,3f5c92bd1000) // physicalRegion: virtual [7f5d12bd1000,7ffffffff000) => physical [3f5c92bd1000,3fff7ffff000) // 这里的physical地址从100001000(4G+1000)开始, 因为在x86上reservedMemory = 0x100000000 //在arm64上(a53) // region: virtual [fef5ca6000,ffff75ca6000) // physicalRegion: virtual [1000,10000) => physical [1000,10000) // physicalRegion: virtual [10000,abf000) => physical [10000,abf000) // physicalRegion: virtual [abf000,ac0000) => physical [abf000,ac0000) // physicalRegion: virtual [ac0000,181f000) => physical [ac0000,181f000) // physicalRegion: virtual [181f000,fef5ca6000) => physical [181f000,fef5ca6000) // physicalRegion: virtual [ffff75ca6000,ffff9cb58000) => physical [fef5ca6000,ff1cb58000) // physicalRegion: virtual [ffff9cb58000,ffff9cb59000) => physical [ff1cb58000,ff1cb59000) // physicalRegion: virtual [ffff9cb59000,ffff9cb5a000) => physical [ff1cb59000,ff1cb5a000) // physical_map.go:177] physicalRegion: virtual [ffff9cb5a000,fffffffff000) => physical [ff1cb5a000,ff7ffff000) var physicalRegions []physicalRegion pkg/sentry/platform/kvm/virtual_map.go type virtualRegion struct { region accessType hostarch.AccessType shared bool offset uintptr filename string } 初始的physicalRegions基本上是个virtual 和physical 1:1的映射根据host地址空间和target的VM物理地址空间计算而来, applyPhysicalRegions()函数会遍历physicalRegions以此来更改pagetable, 比如pkg/sentry/platform/kvm/machine_arm64.go中: func (m *machine) mapUpperHalf(pageTable *pagetables.PageTables) { applyPhysicalRegions(func(pr physicalRegion) bool { //增加一个pageTable映射 pageTable.Map( //这里对UpperHalf来说, 加上了ffff000000000000, 即虚拟地址=物理地址+ffff000000000000 hostarch.Addr(ring0.KernelStartAddress|pr.virtual), pr.length, pagetables.MapOpts{AccessType: hostarch.AnyAccess, Global: true}, pr.physical) return true // Keep iterating. }) } 后面会看到 vCPU初始化的时候, 会配置用户态的TTBR0和内核态的TTBR1 注意ttbr配置的是物理地址. 对VM来说, 其物理地址就是所在qemu进程(或gvisor进程)的ptes变量的虚拟地址. // ttbr0_el1 data = c.machine.kernel.PageTables.TTBR0_EL1(false, 0) reg.id = _KVM_ARM64_REGS_TTBR0_EL1 if err := c.setOneRegister(&reg); err != nil { return err } c.SetTtbr0Kvm(uintptr(data)) // ttbr1_el1 data = c.machine.kernel.PageTables.TTBR1_EL1(false, 0) reg.id = _KVM_ARM64_REGS_TTBR1_EL1 if err := c.setOneRegister(&reg); err != nil { return err } TTBR0和TTBR1分别是 PageTables下面的两个地址: rootPhysical和archPageTables.rootPhysical // TTBR0_EL1 returns the translation table base register 0. // //go:nosplit func (p *PageTables) TTBR0_EL1(noFlush bool, asid uint16) uint64 { return uint64(p.rootPhysical) | (uint64(asid)&ttbrASIDMask) TTBR0的地址是在pkg/ring0/pagetables/pagetables.go的PageTables的Init里面赋值的: // Init initializes a set of PageTables. // // +checkescape:hard,stack //go:nosplit func (p *PageTables) Init(allocator Allocator) { p.Allocator = allocator p.root = p.Allocator.NewPTEs() p.rootPhysical = p.Allocator.PhysicalFor(p.root) } 而TTBR1的地址在pkg/ring0/pagetables/pagetables_arm64.go // InitArch does some additional initialization related to the architecture. // // +checkescape:hard,stack //go:nosplit func (p *PageTables) InitArch(allocator Allocator) { if p.upperSharedPageTables != nil { p.cloneUpperShared() } else { p.archPageTables.root = p.Allocator.NewPTEs() p.archPageTables.rootPhysical = p.Allocator.PhysicalFor(p.archPageTables.root) } } 有两种Allocator, pkg/ring0/pagetables/allocator.go 这是base的allocator, 具体实现在pkg/ring0/pagetables/allocator_unsafe.go, 就是普通的new一个PTE table的结构体, 它的PhysicalFor函数实际上获取的是这个PTE的虚拟地址, uintptr(unsafe.Pointer(ptes)); 这个allocator是给kernel用的, 因为kernel运行在vm外面; 从外面看, vm的物理地址就是外面的虚拟地址. pkg/sentry/platform/kvm/bluepill_allocator.go KVM的bluepill的实现是用base的allocator申请PTE表, 再用bluepill进入guest模式一次, 然后退出guest.func (a *allocator) NewPTEs() *pagetables.PTEs { ptes := a.base.NewPTEs() // escapes: bluepill below. if a.cpu != nil { bluepill(a.cpu) } return ptes } // PhysicalFor returns the physical address for a set of PTEs. // translateToPhysical()这个函数就是从全局表physicalRegions里面匹配虚拟地址找到物理地址. // 纯软件查表, 没有硬件参与. func (a *allocator) PhysicalFor(ptes *pagetables.PTEs) uintptr { virtual := a.base.PhysicalFor(ptes) physical, _, ok := translateToPhysical(virtual) if !ok { panic(fmt.Sprintf(\"PhysicalFor failed for %p\", ptes)) // escapes: panic. } return physical } 这个是给里面用的. KVM_SET_USER_MEMORY_REGION kvm大概支持512个slot, gvisor定义了faultBlockSize为2G. setMemoryRegion()函数调用了kvm的KVM_SET_USER_MEMORY_REGION API: // setMemoryRegion initializes a region. // // This may be called from bluepillHandler, and therefore returns an errno // directly (instead of wrapping in an error) to avoid allocations. // //go:nosplit func (m *machine) setMemoryRegion(slot int, physical, length, virtual uintptr, flags uint32) unix.Errno { userRegion := userMemoryRegion{ slot: uint32(slot), flags: uint32(flags), guestPhysAddr: uint64(physical), memorySize: uint64(length), userspaceAddr: uint64(virtual), } // Set the region. _, _, errno := unix.RawSyscall( unix.SYS_IOCTL, uintptr(m.fd), _KVM_SET_USER_MEMORY_REGION, uintptr(unsafe.Pointer(&userRegion))) return errno } 注意这个API实际上是告诉KVM, 发生在EL2的第二阶段的地址该如何翻译: 初始化VM: ioctl KVM_SET_USER_MEMORY_REGION : 决定了VM_PA --> host_VA 第一阶段: VM_VA --> VM_PA 第二阶段: VM_PA --> host_VA --> host_PA mapPhysical()调用了setMemoryRegion()函数从全局表physicalRegions, 按照入参物理地址和长度, 调用kvm的ioctl来映射USER_MEMORY_REGION // mapPhysical的入参phyRegions都是传入全局变量physicalRegions //比如在x86机器上 // physicalRegion: virtual [1000,3f5b92bd1000) => physical [100001000,3f5c92bd1000) // physicalRegion: virtual [7f5d12bd1000,7ffffffff000) => physical [3f5c92bd1000,3fff7ffff000) // 这里的physical地址从100001000(4G+1000)开始, 因为在x86上reservedMemory = 0x100000000 (m *machine) mapPhysical(physical, length uintptr, phyRegions []physicalRegion, flags uint32) //传入physical, 从phyRegions里查表, 并考虑对齐, 返回virtualStart, physicalStart, length _, physicalStart, length, ok := calculateBluepillFault(physical, phyRegions) //如果没map if !m.hasSlot(physicalStart) handleBluepillFault(m, physical, phyRegions, flags) //又来一次 virtualStart, physicalStart, length, pr := calculateBluepillFault(physical, phyRegions) //因为kvm是以slot来管理内存的, 这里用atomic和for循环来自己做互斥 slot := atomic.SwapUint32(&m.nextSlot, ^uint32(0)) for slot == ^uint32(0) { //在这里等待atomic换出的value不是ffffffff yield() // Race with another call. slot = atomic.SwapUint32(&m.nextSlot, ^uint32(0)) } m.setMemoryRegion(int(slot), physicalStart, length, virtualStart, flags) //用KVM的_KVM_SET_USER_MEMORY_REGION ioctl把 unix.RawSyscall(unix.SYS_IOCTL, ..._KVM_SET_USER_MEMORY_REGION...) mapPhysical()的调用路径之用户态page falut处理路径 //pkg/sentry/mm (mm *MemoryManager) handleASIOFault() mm.mapASLocked() (mm *MemoryManager) HandleUserFault() mm.mapASLocked() (mm *MemoryManager) populateVMA() mm.mapASLocked() (mm *MemoryManager) populateVMAAndUnlock() mm.mapASLocked() (mm *MemoryManager) MLock() mm.mapASLocked() (mm *MemoryManager) MLockAll() mm.mapASLocked() //pkg/sentry/mm/address_space.go (mm *MemoryManager) mapASLocked() //pkg/sentry/platform/kvm/address_space.go mm.as.MapFile() as.mapLocked() //pkg/sentry/platform/kvm/machine.go as.machine.mapPhysical() //pkg/sentry/platform/kvm/bluepill_fault.go handleBluepillFault() //pkg/sentry/platform/kvm/machine_unsafe.go m.setMemoryRegion() unix.RawSyscall(unix.SYS_IOCTL, _KVM_SET_USER_MEMORY_REGION) mapPhysical的调用路径之seccompMmapHandler 在new一个VM的时候, host进程的mmap会被seccomp拦截, 转由seccompMmapHandler()来处理准确的说, 在newMachine()里面, seccompMmapRules()用sigsysHandler来处理sigsys // seccompMmapRules adds seccomp rules to trap mmap system calls that will be // handled in seccompMmapHandler. func seccompMmapRules(m *machine) { // 只在host进程执行一次 seccompMmapRulesOnce.Do(func() { // 调用unix.RawSyscall6(unix.SYS_RT_SIGACTION, ...)替换handler sighandling.ReplaceSignalHandler(unix.SIGSYS, addrOfSigsysHandler(), &savedSigsysHandler) rules := []seccomp.RuleSet{} rules = append(rules, []seccomp.RuleSet{ // Trap mmap system calls and handle them in sigsysGoHandler { Rules: seccomp.SyscallRules{ unix.SYS_MMAP: { { seccomp.MatchAny{}, seccomp.MatchAny{}, seccomp.MaskedEqual(unix.PROT_EXEC, 0), /* MAP_DENYWRITE is ignored and used only for filtering. */ seccomp.MaskedEqual(unix.MAP_DENYWRITE, 0), }, }, }, Action: linux.SECCOMP_RET_TRAP, }, }...) instrs, err := seccomp.BuildProgram(rules, linux.SECCOMP_RET_ALLOW, linux.SECCOMP_RET_ALLOW) }) ... 根据sigaction的配置, sigsys被触发的时候, sigsysHandler(int sig, siginfo_t *info, void *ucontext)会被调用 // The arguments are the following: // // R0 - The signal number. // R1 - Pointer to siginfo_t structure. // R2 - Pointer to ucontext structure. // TEXT ·sigsysHandler(SB),NOSPLIT,$0 // si_code should be SYS_SECCOMP. MOVD SIGINFO_CODE(R1), R7 CMPW $1, R7 BNE fallback CMPW $SYS_MMAP, R8 BNE fallback MOVD R2, 8(RSP) BL ·seccompMmapHandler(SB) // Call the handler. RET fallback: // Jump to the previous signal handler. MOVD ·savedHandler(SB), R7 B (R7) // 取上面函数的地址 // func addrOfSighandler() uintptr TEXT ·addrOfSigsysHandler(SB), $0-8 MOVD $·sigsysHandler(SB), R0 MOVD R0, ret+0(FP) RET 需要注意的是, 在初始化的时候, host进程的地址空间有很多已经mmap成了不可访问, 那么后面的mmap得到的虚拟地址, 应该是落在和目标VM物理地址范围(40bit)的连续地址空间. 在这个signal处理函数里面, 调用了seccompMmapHandler // seccompMmapHandler is a signal handler for runtime mmap system calls // that are trapped by seccomp. // // It executes the mmap syscall with specified arguments and maps a new region // to the guest. // //go:nosplit func seccompMmapHandler(context unsafe.Pointer) { //从ucontext上下文恢复当时mmap的入参, 并重新调用mmap addr, length, errno := seccompMmapSyscall(context) //类似这样: unix.RawSyscall6(uintptr(ctx.Regs[8]), uintptr(ctx.Regs[0]), ...) for 在machinePool里所有host进程管理的VM physical, length, ok := translateToPhysical(virtual) //用KVM的SET_USER_MEMORY_REGION ioctl来配置kvm的第二次翻译 m.mapPhysical(physical, length, physicalRegions) } 注: man sigaction sigaction系统调用接受两种形式的handler struct sigaction { void (*sa_handler)(int); void (*sa_sigaction)(int, siginfo_t *, void *); sigset_t sa_mask; int sa_flags; void (*sa_restorer)(void); }; 这里我们用的是void (*sa_sigaction)(int, siginfo_t *, void *) If SA_SIGINFO is specified in sa_flags, then sa_sigaction (instead of sa_handler) specifies the signal-handling function for signum. This function receives three arguments, as described below. void handler(int sig, siginfo_t *info, void *ucontext) { ... } These three arguments are as follows sig The number of the signal that caused invocation of the handler. info A pointer to a siginfo_t, which is a structure containing further information about the signal, as described below. ucontext This is a pointer to a ucontext_t structure, cast to void *. The structure pointed to by this field contains signal context information that was saved on the user-space stack by the kernel; for details, see sigreturn(2). Further information about the ucontext_t structure can be found in getcontext(3). Commonly, the handler function doesn't make any use of the third argument. The siginfo_t data type is a structure with the following fields: siginfo_t { int si_signo; /* Signal number */ int si_errno; /* An errno value */ int si_code; /* Signal code */ int si_trapno; /* Trap number that caused hardware-generated signal (unused on most architectures) */ pid_t si_pid; /* Sending process ID */ uid_t si_uid; /* Real user ID of sending process */ int si_status; /* Exit value or signal */ clock_t si_utime; /* User time consumed */ clock_t si_stime; /* System time consumed */ sigval_t si_value; /* Signal value */ int si_int; /* POSIX.1b signal */ void *si_ptr; /* POSIX.1b signal */ int si_overrun; /* Timer overrun count; POSIX.1b timers */ int si_timerid; /* Timer ID; POSIX.1b timers */ void *si_addr; /* Memory location which caused fault */ long si_band; /* Band event (was int in glibc 2.3.2 and earlier) */ int si_fd; /* File descriptor */ short si_addr_lsb; /* Least significant bit of address (since Linux 2.6.32) */ void *si_lower; /* Lower bound when address violation occurred (since Linux 3.19) */ void *si_upper; /* Upper bound when address violation occurred (since Linux 3.19) */ int si_pkey; /* Protection key on PTE that caused fault (since Linux 4.6) */ void *si_call_addr; /* Address of system call instruction (since Linux 3.5) */ int si_syscall; /* Number of attempted system call (since Linux 3.5) */ unsigned int si_arch; /* Architecture of attempted system call (since Linux 3.5) */ } 注2: man sigretrun这也是个系统调用, 但不能被用户直接调用, 而是被libc调用做为signal trampoline(蹦床).kernel在返回用户态的时候, 看到有signal pending, 就把当前上下文ucontext保存在该进程的用户空间, 然后调用用户态的sighandler, 当这个handler返回的时候, 返回到libc的signal trampoline代码, sigreturn在这个trampoline代码里被调用, 这是个系统调用, 内核会把之前的为调用sighandler的准备工作undo, 从ucontext里面恢复上下文, 恢复原来进程的执行. If the Linux kernel determines that an unblocked signal is pending for a process, then, at the next transition back to user mode in that process (e.g., upon return from a sys‐tem call or when the process is rescheduled onto the CPU), it creates a new frame on the user-space stack where it saves various pieces of process context (processor status word, registers, signal mask, and signal stack settings). The kernel also arranges that, during the transition back to user mode, the signal handler is called, and that, upon return from the handler, control passes to a piece of user-space code commonly called the \"signal trampoline\". The signal trampoline code in turn calls sigreturn(). This sigreturn() call undoes everything that was done—changing the process's signal mask, switching signal stacks (see sigaltstack(2))—in order to invoke the signal handler. Using the information that was earlier saved on the user-space stack sigreturn() restores the process's signal mask, switches stacks, and restores the process's context (processor flags and registers, including the stack pointer and instruction pointer), so that the process resumes execution at the point where it was interrupted by the signal. mapPhysical的调用路径之newMachine 见下面 KVM新建一个VM New一个kvm实例就是 func New() { deviceFile, err := os.OpenFile(\"/dev/kvm\", unix.O_RDWR, 0) fd := deviceFile.Fd() // 注意这里, 不管New多少个VM, updateGlobalOnce只被调用一次 // // Ensure global initialization is done. globalOnce.Do(func() { globalErr = updateGlobalOnce(int(fd)) physicalInit() //这个函数在host的地址空间内, 调用多次mmap, 使其地址空间充满unix.PROT_NONE属性(即不可访问)的mmap映射, 最后只保留一个小于40bit的host地址空间 //正如其名称所示, 用不可访问的地址映射填充host的大部分地址空间, 只保留一个小于40bit(ARM64)的地址空间. //这么做的目的是让host地址空间可以injective fillAddressSpace() // 这个函数返回已经map了的地址区域, 这些区域是excludedRegions //将上面excludedRegions逐个addValidRegion到全局表physicalRegions, 物理地址从reservedMemory(arm64上是0)开始递增 //将剩下的host地址空间(即上面保留的40bit大小的空间)addValidRegion到全局表physicalRegions //故全局表physicalRegion不是1:1的物理到虚拟地址的映射 //而是物理地址从0开始的, 多个到host地址空间的映射; 前面的多组映射实际上是不能访问的, 只有最后一个映射才是VM的物理地址空间到host的地址空间的一个 //比如在x86机器上 // physicalRegion: virtual [1000,3f5b92bd1000) => physical [100001000,3f5c92bd1000) // physicalRegion: virtual [7f5d12bd1000,7ffffffff000) => physical [3f5c92bd1000,3fff7ffff000) // 这里的physical地址从100001000(4G+1000)开始, 因为在x86上reservedMemory = 0x100000000 computePhysicalRegions() updateSystemValues(int(fd)) ring0.Init() }) for { //直接的syscall调用要自己用个for来处理unix.EINTR, man 7 signal说的很清楚, 像read/write/ioctl等系统调用作用在slow的device上, 如果在还没有transfer数据前被signal打断, 根据SA_RESTART的情况, 可以返回EINTR vm, _, errno = unix.Syscall(unix.SYS_IOCTL, fd, _KVM_CREATE_VM, 0) if errno == unix.EINTR { continue } if errno != 0 { return nil, fmt.Errorf(\"creating VM: %v\", errno) } break } // Create a VM context. machine, err := newMachine(int(vm)) // All set. return &KVM{ machine: machine, }, nil } // NewContext returns an interruptible context. func (k *KVM) NewContext() platform.Context { return &context{ machine: k.machine, } } newMachine // newMachine returns a new VM context. func newMachine(vm int) (*machine, error) { // Create the machine. m := &machine{fd: vm} // Pull the maximum vCPUs. // CPU虚拟化: 最大的vCPU个数 m.getMaxVCPU() // Pull the maximum slots. // 内存虚拟化: 最大的内存slot数, 大概512个 unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_CHECK_EXTENSION, _KVM_CAP_MAX_MEMSLOTS) // Check TSC Scaling unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_CHECK_EXTENSION, _KVM_CAP_TSC_CONTROL) // Create the upper shared pagetables and kernel(sentry) pagetables. m.upperSharedPageTables = pagetables.New(newAllocator()) // 应该是对应ffff000000000000空间(kernel) // map 虚拟地址=物理地址+ffff000000000000 m.mapUpperHalf(m.upperSharedPageTables) m.upperSharedPageTables.MarkReadOnlyShared() //新建一个pagetable, 并把upper pagetable设为上面的m.upperSharedPageTables m.kernel.PageTables = pagetables.NewWithUpper(newAllocator(), m.upperSharedPageTables, ring0.KernelStartAddress) // KernelStartAddress是ffff000000000000 // Install seccomp rules to trap runtime mmap system calls. They will // be handled by seccompMmapHandler. // 配置seccomp为trap mmap, 其他allow; mmap会触发SIGSYS信号 // 当newMachine的进程(非guest)调用mmap时, seccompMmapHandler()会被调用(位于pkg/sentry/platform/kvm/machine_unsafe.go) // 当host进程(包括VM的进程)mmap的时候, mmap会触发SIGSYS信号, handler里面 // 为每个VM, 都使用m.mapPhysical(physical, length, physicalRegions, _KVM_MEM_FLAGS_NONE) seccompMmapRules(m) // Apply the physical mappings. Note that these mappings may point to // guest physical addresses that are not actually available. These // physical pages are mapped on demand, see kernel_unsafe.go. applyPhysicalRegions(func(pr physicalRegion) bool { // Map everything in the lower half. m.kernel.PageTables.Map( hostarch.Addr(pr.virtual), pr.length, pagetables.MapOpts{AccessType: hostarch.ReadWrite}, pr.physical) return true // Keep iterating. }) // 把当前host进程已经map的虚拟地址空间映射进VM // 这个函数的作用是把host进程的已经建立的虚拟地址region映射进VM, 让VM看到和host进程相同的虚拟地址, 并对应到相同的物理地址, 即VM能够执行host进程的代码. 比如在VM执行host中的funcx, VM中直接使用funcx在host中的虚拟地址, VM中自己维护的页表把这个VA转到PA, 这个PA是VM的\"物理地址\", 因为这个PA和host的VA已经在KVM中建立了映射(ioctl KVM_SET_USER_MEMORY_REGION), 经过KVM的第二次地址翻译, 这个host的VA就被翻译成真正的PA. // 其实就是相当于经过VM的两个阶段的MMU翻译, 结果就是VM能直接访问host空间的代码. applyVirtualRegions() f, err := os.Open(\"/proc/self/maps\") //用regex解析每行得到虚拟地址start和length //对每个virtual region for 每个entry //1. 从host虚拟地址查到VM的物理地址 physical, length, ok := translateToPhysical(virtual) //2. 如果属性有execute, 即host进程的代码, install到VM的kernel pagetable里 //只有代码被加到VM的页表 m.kernel.PageTables.Map( hostarch.Addr(virtual), length, pagetables.MapOpts{AccessType: vr.accessType}, physical) //3. 每个entry都会调用kvm的ioctl KVM_SET_USER_MEMORY_REGION, 让VM可以访问host进程的地址空间区域 // Ensure the physical range is mapped. m.mapPhysical(physical, length, physicalRegions) // 做个_KVM_ARM_PREFERRED_TARGET的KVM ioctl m.initArchState() //在里面newVCPU(), 最多maxvcpu个 for int(m.nextID) arm64异常向量 https://developer.arm.com/documentation/100933/0100/AArch64-exception-vector-table 每个exception level都有独立的异常向量表 向量表的虚拟地址配在VBAR寄存器里 The virtual address of each table base is set by the Vector Based Address Registers: VBAR_EL3, VBAR_EL2 and VBAR_EL1. 每个vector table有16个entry, 每个entry固定128个字节(可以有32个指令). 硬件会根据情况找到对应的entry. The type of exception (SError, FIQ, IRQ, or Synchronous) If the exception is being taken at the same Exception level, the stack pointer to be used (SP0 or SPn) If the exception is being taken at a lower Exception level, the Execution state of the next lower level (AArch64 or AArch32). 解释一下, arm64的sp寄存器每个EL都有, 但不一定都用: SPSel选择寄存器的0位, 来决定用哪个SP SP Meaning 0b0 Use SP_EL0 at all Exception levels. 0b1 Use SP_ELx for Exception level ELx. 默认是1, 就是说默认每个EL都用自己的SP寄存器. 举例来说: If kernel code is executing at EL1 and an IRQ interrupt is signaled, an IRQ exception occurs. This particular interrupt is not associated with the hypervisor or secure environment and is also handled within the kernel, and the SPSel bit is set, so SP_EL1 is used. Execution takes place, therefore, from address VBAR_EL1 + 0x280. gvisor里面, 这个向量表在pkg/ring0/entry_arm64.s TEXT ·Vectors(SB),NOSPLIT,$0 PCALIGN $2048 // gvisor并不支持SP0, 就是说这里`SPSel`是1 B ·El1_sync_invalid(SB) PCALIGN $128 B ·El1_irq_invalid(SB) PCALIGN $128 B ·El1_fiq_invalid(SB) PCALIGN $128 B ·El1_error_invalid(SB) // 接下来的4个entry是El1触发的 PCALIGN $128 B ·El1_sync(SB) PCALIGN $128 B ·El1_irq(SB) PCALIGN $128 B ·El1_fiq(SB) PCALIGN $128 B ·El1_error(SB) // 这4个entry是64位的EL0触发的 PCALIGN $128 B ·El0_sync(SB) PCALIGN $128 B ·El0_irq(SB) PCALIGN $128 B ·El0_fiq(SB) PCALIGN $128 B ·El0_error(SB) // 这4个entry是32位的EL0触发的. 就是说这里不支持32位用户态模式 PCALIGN $128 B ·El0_sync_invalid(SB) PCALIGN $128 B ·El0_irq_invalid(SB) PCALIGN $128 B ·El0_fiq_invalid(SB) PCALIGN $128 B ·El0_error_invalid(SB) 这个地址ring0.Vectors被配置到_KVM_ARM64_REGS_VBAR_EL1 // vbar_el1 reg.id = _KVM_ARM64_REGS_VBAR_EL1 // ring0.Vectors的虚拟地址 vectorLocation := reflect.ValueOf(ring0.Vectors).Pointer() // 虚拟地址加上KernelStartAddress(ffff000000000000) data = uint64(ring0.KernelStartAddress | vectorLocation) if err := c.setOneRegister(&reg); err != nil { return err } VBAR里配的是虚拟地址, 前面讲过, kernel的虚拟地址upperhalf的虚拟地址=物理地址+ffff000000000000, 而对于VM来说, 这个物理地址就是其所在VM进程的虚拟地址.所以发生异常时, 因为是EL1, 如果TLB没命中, CPU的MMU先找到TTBR1, walk PTE(page table entry), 找到物理地址; 这个物理地址就是ring0.Vectors的虚拟地址; MMU知道现在是2 stage translate模式, 就去找VTTBR(KVM配置的)的page table基地址去找物理地址, 这次找到真正的物理地址. EL0同步异常 异常入口先保存用户上下文的寄存器, 然后比较异常原因寄存器跳转到相应处理 // El0_sync is the handler for El0_sync. TEXT ·El0_sync(SB),NOSPLIT,$0 KERNEL_ENTRY_FROM_EL0 MRS ESR_EL1, R25 // read the syndrome register LSR $ESR_ELx_EC_SHIFT, R25, R24 // exception class CMP $ESR_ELx_EC_SVC64, R24 BEQ el0_svc // SVC in 64-bit state CMP $ESR_ELx_EC_DABT_LOW, R24 BEQ el0_da // data abort in EL0 CMP $ESR_ELx_EC_IABT_LOW, R24 BEQ el0_ia // instruction abort in EL0 CMP $ESR_ELx_EC_FP_ASIMD, R24 BEQ el0_fpsimd_acc // FP/ASIMD access CMP $ESR_ELx_EC_SVE, R24 BEQ el0_sve_acc // SVE access CMP $ESR_ELx_EC_FP_EXC64, R24 BEQ el0_fpsimd_exc // FP/ASIMD exception CMP $ESR_ELx_EC_SP_ALIGN, R24 BEQ el0_sp_pc // stack alignment exception CMP $ESR_ELx_EC_PC_ALIGN, R24 BEQ el0_sp_pc // pc alignment exception CMP $ESR_ELx_EC_UNKNOWN, R24 BEQ el0_undef // unknown exception in EL0 CMP $ESR_ELx_EC_BREAKPT_LOW, R24 BEQ el0_dbg // debug exception in EL0 CMP $ESR_ELx_EC_SYS64, R24 BEQ el0_sys // configurable trap CMP $ESR_ELx_EC_WFx, R24 BEQ el0_wfx // WFX trap B el0_invalid RSP: stack pointer // RSV_REG is a register that holds el1 information temporarily. #define RSV_REG R18_PLATFORM // RSV_REG_APP is a register that holds el0 information temporarily. #define RSV_REG_APP R19 KERNEL_ENTRY_FROM_EL0()保存用户app的上下文 // KERNEL_ENTRY_FROM_EL0 is the entry code of the vcpu from el0 to el1. #define KERNEL_ENTRY_FROM_EL0 \\ SUB $16, RSP, RSP; \\ // step1, save r18, r19 into kernel temporary stack. STP (RSV_REG, RSV_REG_APP), 16*0(RSP); \\ WORD $0xd538d092; \\ // MRS TPIDR_EL1, R18 MOVD CPU_APP_ADDR(RSV_REG), RSV_REG_APP; \\ // step2, load app context pointer. //保存r0到r31等通用寄存器 REGISTERS_SAVE(RSV_REG_APP, 0); \\ // step3, save app context. MOVD RSV_REG_APP, R20; \\ LDP 16*0(RSP), (RSV_REG, RSV_REG_APP); \\ ADD $16, RSP, RSP; \\ STP (RSV_REG, RSV_REG_APP), PTRACE_R18(R20); \\ // 更新r18 r19到上下文 MRS TPIDR_EL0, R3; \\ MOVD R3, PTRACE_TLS(R20); \\ //TPIDR_EL0保存到TLS WORD $0xd5384003; \\ // MRS SPSR_EL1, R3 MOVD R3, PTRACE_PSTATE(R20); \\ //状态寄存器保存到PSTATE MRS ELR_EL1, R3; \\ MOVD R3, PTRACE_PC(R20); \\ //ELR_EL1保存到PC WORD $0xd5384103; \\ // MRS SP_EL0, R3 MOVD R3, PTRACE_SP(R20); //SP_EL0保存到SP EL0同步异常之SVC系统调用异常 // System call vectors. const ( Syscall Vector = El0SyncSVC PageFault Vector = El0SyncDa VirtualizationException Vector = El0ErrBounce ) el0_svc: WORD $0xd538d092 //MRS TPIDR_EL1, R18 MOVD $0, CPU_ERROR_CODE(RSV_REG) // Clear error code. MOVD $1, R3 MOVD R3, CPU_ERROR_TYPE(RSV_REG) // Set error type to user. //Syscall是El0SyncSVC MOVD $Syscall, R3 // 相当于c.vecCode = El0SyncSVC MOVD R3, CPU_VECTOR_CODE(RSV_REG) //如果直接eret, 则会返回EL0, 因为是在EL0触发的异常 //但这里是要返回EL1 B ·kernelExitToEl1(SB) 这里的CPU_VECTOR_CODE是c.vecCode在c(就是CPU那个结构体)的偏移量 RSV_REG是R18_PLATFORM kernelExitToEl1 kernelExitToEl1()可以在EL0_SVC里面调用, 也可以在EL1_SVC里面调用. 恢复寄存器, 最后调用eret从异常返回. 即从EL1的异常态返回到EL1的正常态. // kernelExitToEl1 is the entrypoint for sentry in guest_el1. // Prepare the vcpu environment for sentry. TEXT ·kernelExitToEl1(SB),NOSPLIT,$0 WORD $0xd538d092 //MRS TPIDR_EL1, R18 //是vCPU结构体? MOVD CPU_REGISTERS+PTRACE_PSTATE(RSV_REG), R1 WORD $0xd5184001 //MSR R1, SPSR_EL1 //恢复保存的SPSR_EL1 //设置ELR为之前保存的PC MOVD CPU_REGISTERS+PTRACE_PC(RSV_REG), R1 MSR R1, ELR_EL1 //恢复保存的ELR // restore sentry's tls. MOVD CPU_REGISTERS+PTRACE_TLS(RSV_REG), R1 MSR R1, TPIDR_EL0 //用的是TPIDR_EL0, 说明sEntry也是在VM的用户态 MOVD CPU_REGISTERS+PTRACE_SP(RSV_REG), R1 MOVD R1, RSP //恢复保存的SP REGISTERS_LOAD(RSV_REG, CPU_REGISTERS) SWITCH_TO_KVM_PAGETABLE() //使用c.ttbr0Kvm 页表, 这个好像是和sentry对应的. 也配在ttbr0基址寄存器里; 相应的, 还有个SWITCH_TO_APP_PAGETABLE, 是使用c.ttbr0App页表 MRS TPIDR_EL1, RSV_REG MOVD CPU_REGISTERS+PTRACE_R19(RSV_REG), RSV_REG_APP ERET() kernelExitToEl1返回El1, 并切换到KVM页表(或者说sEntry页表), KVM页表也是用ttbr0来配的, 算用户页表(或者说下半部页表). 这里的关键点是从TPIDR_EL1拿到vCPU的结构体, 这个应该是指VM的\"kernel\"的信息, 从里面恢复SPSR_EL1, 而SPSR_EL1里面是保存的pstate寄存器(是kernel的吗?), eret要\"退回\"的EL级别是从SPSR_EL1里查的.用当时记录的PC(CPU_REGISTERS+PTRACE_PC(RSV_REG))恢复到ELR_EL1, 那么这个ELR带我们到哪里呢? 注: SPSR_EL1是Saved Program Status Register, Holds the saved process state when an exception is taken to EL1. 当异常被EL1处理时, SPSR_EL1持有保存的状态寄存器 顺便看一下kernelExitToEl0 kernelExitToEl0这个函数在(c *CPU) SwitchToUser里面被调用, // kernelExitToEl0 is the entrypoint for application in guest_el0. // Prepare the vcpu environment for container application. TEXT ·kernelExitToEl0(SB),NOSPLIT,$0 // Step1, save sentry context into memory. MRS TPIDR_EL1, RSV_REG REGISTERS_SAVE(RSV_REG, CPU_REGISTERS) MOVD RSV_REG_APP, CPU_REGISTERS+PTRACE_R19(RSV_REG) MRS TPIDR_EL0, R3 MOVD R3, CPU_REGISTERS+PTRACE_TLS(RSV_REG) WORD $0xd5384003 // MRS SPSR_EL1, R3 MOVD R3, CPU_REGISTERS+PTRACE_PSTATE(RSV_REG) MOVD R30, CPU_REGISTERS+PTRACE_PC(RSV_REG) MOVD RSP, R3 MOVD R3, CPU_REGISTERS+PTRACE_SP(RSV_REG) MOVD CPU_REGISTERS+PTRACE_R3(RSV_REG), R3 // Step2, switch to temporary stack. LOAD_KERNEL_STACK(RSV_REG) // Step3, load app context pointer. MOVD CPU_APP_ADDR(RSV_REG), RSV_REG_APP // Step4, prepare the environment for container application. // set sp_el0. MOVD PTRACE_SP(RSV_REG_APP), R1 WORD $0xd5184101 //MSR R1, SP_EL0 // set pc. MOVD PTRACE_PC(RSV_REG_APP), R1 MSR R1, ELR_EL1 // set pstate. MOVD PTRACE_PSTATE(RSV_REG_APP), R1 WORD $0xd5184001 //MSR R1, SPSR_EL1 // need use kernel space address to excute below code, since // after SWITCH_TO_APP_PAGETABLE the ASID is changed to app's // ASID. WORD $0x10000061 // ADR R1, do_exit_to_el0 ORR $0xffff000000000000, R1, R1 JMP (R1) do_exit_to_el0: // RSV_REG & RSV_REG_APP will be loaded at the end. REGISTERS_LOAD(RSV_REG_APP, 0) MOVD PTRACE_TLS(RSV_REG_APP), RSV_REG MSR RSV_REG, TPIDR_EL0 // switch to user pagetable. LDP PTRACE_R18(RSV_REG_APP), (RSV_REG, RSV_REG_APP) SUB $STACK_FRAME_SIZE, RSP, RSP STP (RSV_REG, RSV_REG_APP), 16*0(RSP) STP (R0, R1), 16*1(RSP) WORD $0xd538d092 //MRS TPIDR_EL1, R18 SWITCH_TO_APP_PAGETABLE() LDP 16*1(RSP), (R0, R1) LDP 16*0(RSP), (RSV_REG, RSV_REG_APP) ADD $STACK_FRAME_SIZE, RSP, RSP ERET() EL1同步异常El1_sync 在El1状态下的同步异常入口下面, 通过读取ESR_EL1, 异常可分为: 数据错误, 代码错误, 栈对齐错误, pc对齐错误等错误异常. svc异常, 系统调用用的 -- 但这里已经是el1了, 还有啥系统调用? 其他异常// El1_sync is the handler for El1_sync. TEXT ·El1_sync(SB),NOSPLIT,$0 KERNEL_ENTRY_FROM_EL1 // 保存sentry上下文寄存器, load kernel stack MRS ESR_EL1, R25 // read the syndrome register LSR $ESR_ELx_EC_SHIFT, R25, R24 // exception class CMP $ESR_ELx_EC_DABT_CUR, R24 BEQ el1_da // data abort in EL1 CMP $ESR_ELx_EC_IABT_CUR, R24 BEQ el1_ia // instruction abort in EL1 CMP $ESR_ELx_EC_FP_ASIMD, R24 BEQ el1_fpsimd_acc // FP/ASIMD access CMP $ESR_ELx_EC_SVE, R24 BEQ el1_sve_acc // SVE access CMP $ESR_ELx_EC_SP_ALIGN, R24 BEQ el1_sp_pc // stack alignment exception CMP $ESR_ELx_EC_PC_ALIGN, R24 BEQ el1_sp_pc // pc alignment exception CMP $ESR_ELx_EC_UNKNOWN, R24 BEQ el1_undef // unknown exception in EL1 CMP $ESR_ELx_EC_SVC64, R24 BEQ el1_svc // SVC in 64-bit state CMP $ESR_ELx_EC_BREAKPT_CUR, R24 BEQ el1_dbg // debug exception in EL1 B el1_invalid 一般的错误异常 最后调用HaltEl1ExceptionAndResume // HaltEl1ExceptionAndResume calls Hooks.KernelException and resume. TEXT ·HaltEl1ExceptionAndResume(SB),NOSPLIT,$0-8 WORD $0xd538d092 // MRS TPIDR_EL1, R18 MOVD CPU_SELF(RSV_REG), R3 // Load vCPU. MOVD R3, 8(RSP) // First argument (vCPU). MOVD vector+0(FP), R3 MOVD R3, 16(RSP) // Second argument (vector). CALL ·kernelException(SB) // Call the trampoline. B ·kernelExitToEl1(SB) // Resume. 而svc异常调用HaltEl1SvcAndResume, 它也会调用钩子函数处理系统调用:kernelSyscall el1_svc: B ·HaltEl1SvcAndResume(SB) // HaltEl1SvcAndResume calls Hooks.KernelSyscall and resume. TEXT ·HaltEl1SvcAndResume(SB),NOSPLIT,$0 WORD $0xd538d092 // MRS TPIDR_EL1, R18 MOVD CPU_SELF(RSV_REG), R3 // Load vCPU. MOVD R3, 8(RSP) // First argument (vCPU). CALL ·kernelSyscall(SB) // Call the trampoline. B ·kernelExitToEl1(SB) // Resume. EL1其他异常(irq, fiq, error)都走shutdown流程, 关闭guest // El1_irq is the handler for El1_irq. TEXT ·El1_irq(SB),NOSPLIT,$0 B ·Shutdown(SB) // El1_fiq is the handler for El1_fiq. TEXT ·El1_fiq(SB),NOSPLIT,$0 B ·Shutdown(SB) // El1_error is the handler for El1_error. TEXT ·El1_error(SB),NOSPLIT,$0 B ·Shutdown(SB) // Shutdown stops the guest. TEXT ·Shutdown(SB),NOSPLIT,$0 // PSCI EVENT. MOVD $0x84000009, R0 HVC $0 // hypervisor call KVM_CREATE_VCPU 比如在pkg/sentry/platform/kvm/machine.go中, 有个函数newVCPU就使用了_KVM_CREATE_VCPU // newVCPU creates a returns a new vCPU. // // Precondition: mu must be held. func (m *machine) newVCPU() *vCPU { // Create the vCPU. id := int(atomic.AddUint32(&m.nextID, 1) - 1) fd, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_CREATE_VCPU, uintptr(id)) if errno != 0 { panic(fmt.Sprintf(\"error creating new vCPU: %v\", errno)) } c := &vCPU{ id: id, fd: int(fd), machine: m, } c.CPU.Init(&m.kernel, c.id, c) c.self = c // Set self reference. c.kernel = k // Set kernel reference. c.init(cpuID) // Perform architectural init. // Set the kernel stack pointer(virtual address). c.registers.Sp = uint64(c.StackTop()) c.floatingPointState = fpu.NewState() // hooks是 c.hooks = hooks 或者 c.hooks = defaultHooks{} m.vCPUsByID[c.id] = c // Ensure the signal mask is correct. if err := c.setSignalMask(); err != nil { panic(fmt.Sprintf(\"error setting signal mask: %v\", err)) } // Map the run data. // 就是把fd mmap成内存, 用于kvm向用户态传递信息 runData, err := mapRunData(int(fd)) if err != nil { panic(fmt.Sprintf(\"error mapping run data: %v\", err)) } c.runData = runData // Initialize architecture state. // 相当于 if err := c.initArchState(); err != nil { panic(fmt.Sprintf(\"error initialization vCPU state: %v\", err)) } return c // Done. } 比如设置initArchState中, 就调用了setOneRegister很多次, 用_KVM_SET_ONE_REG配置寄存器, 比如ttbr0 func (c *vCPU) setOneRegister(reg *kvmOneReg) error { if _, _, errno := unix.RawSyscall( unix.SYS_IOCTL, uintptr(c.fd), _KVM_SET_ONE_REG, uintptr(unsafe.Pointer(reg))); errno != 0 { return fmt.Errorf(\"error setting one register: %v\", errno) } return nil } // 这个init是运行在用户态的, 所以需要借助KVM的`_KVM_SET_ONE_REG`功能, 通过ioctl系统调用的方式, 让KVM来设置寄存器 // initArchState initializes architecture-specific state. func (c *vCPU) initArchState() error { var ( reg kvmOneReg data uint64 regGet kvmOneReg dataGet uint64 ) reg.addr = uint64(reflect.ValueOf(&data).Pointer()) regGet.addr = uint64(reflect.ValueOf(&dataGet).Pointer()) // 先是KVM的ioctl的_KVM_ARM_VCPU_INIT vcpuInit.features[0] |= (1 kernelAddr可以获取一个eface和func的内核地址 package ring0 // eface mirrors runtime.eface. type eface struct { typ uintptr data unsafe.Pointer } // kernelAddr returns the kernel virtual address for the given object. // //go:nosplit func kernelAddr(obj interface{}) uintptr { e := (*eface)(unsafe.Pointer(&obj)) return KernelStartAddress | uintptr(e.data) } // kernelFunc returns the address of the given function. // KernelStartAddress是ffff000000000000 func kernelFunc(fn uintptr) uintptr { return KernelStartAddress | fn } 对arm64来说 type Vector uintptr // System call vectors. const ( Syscall Vector = El0SyncSVC PageFault Vector = El0SyncDa VirtualizationException Vector = El0ErrBounce ) // VirtualAddressBits returns the number bits available for virtual addresses. // 虚拟地址是48位, 和实际一样 func VirtualAddressBits() uint32 { return 48 } // PhysicalAddressBits returns the number of bits available for physical addresses. func PhysicalAddressBits() uint32 { return 40 } var ( // UserspaceSize is the total size of userspace. UserspaceSize = uintptr(1) KVM_ARM_VCPU_INIT 在ioctl KVM_CREATE_VCPU之后, 需要KVM_ARM_VCPU_INIT This tells KVM what type of CPU to present to the guest, and what optional features it should have. This will cause a reset of the cpu registers to their initial values. If this is not called, KVM_RUN will return ENOEXEC for that vcpu. KVM_ARM_VCPU_INIT会把cpu重置为初始值. 如果没有这一步, KVM_RUN就会错误. The initial values are defined as: Processor state: AArch64: EL1h, D, A, I and F bits set. All other bits are cleared. AArch32: SVC, A, I and F bits set. All other bits are cleared. General Purpose registers, including PC and SP: set to 0 FPSIMD/NEON registers: set to 0 SVE registers: set to 0 System registers: Reset to their architecturally defined values as for a warm reset to EL1 (resp. SVC) Note that because some registers reflect machine topology, all vcpus should be created before this ioctl is invoked. Userspace can call this function multiple times for a given vcpu, including after the vcpu has been run. This will reset the vcpu to its initial state. All calls to this function after the initial call must use the same target and same set of feature flags, otherwise EINVAL will be returned. 入口代码 在vCPU初始化的时候, 入口代码ring0.Start()用KVM的ioctl被写入PC // pc reg.id = _KVM_ARM64_REGS_PC data = uint64(reflect.ValueOf(ring0.Start).Pointer()) //这里的实现很关键!!!! ring0.Start是个函数, 取这个函数当作入口地址, 配到PC中. if err := c.setOneRegister(&reg); err != nil { return err } 这个Start()函数在 pkg/ring0/entry_arm64.s 应该是运行在EL1 // Start is the CPU entrypoint. TEXT ·Start(SB),NOSPLIT,$0 // Init. WORD $0xd508871f // __tlbi(vmalle1) DSB $7 // dsb(nsh) MOVD $1 KVM的context实现 上面提到platform要实现context方法: // Switch resumes execution of the thread specified by the arch.Context // in the provided address space. This call will block while the thread // is executing. // 正常应该是成功调用一个系统调用. // 如果正在执行这个系统调用的时候有signal, 返回ErrContextSignal // 如果调用了Interrupt()则返回ErrContextInterrupt Switch(ctx context.Context, mm MemoryManager, ac arch.Context, cpu int32) (*linux.SignalInfo, hostarch.AccessType, error) 按照ptrace的实现, Switch函数是执行一个sycle的用户代码直到下一个syscall. 那么kvm实现怎么做到syscall级别的呢? // Switch runs the provided context in the given address space. func (c *context) Switch(ctx pkgcontext.Context, mm platform.MemoryManager, ac arch.Context, _ int32) (*linux.SignalInfo, hostarch.AccessType, error) { as := mm.AddressSpace() localAS := as.(*addressSpace) // Grab a vCPU. // machine.Get()返回一个vCPU, vCPU是和TID绑定的, 先查表找已经绑定的; 没有就检查每个vCPU的状态, 尝试从vCPUReady转到vCPUUser状态; 最后实在不行就m.getNewVCPU()一个 cpu := c.machine.Get() // Enable interrupts (i.e. calls to vCPU.Notify). if !c.interrupt.Enable(cpu) { c.machine.Put(cpu) // Already preempted. return nil, hostarch.NoAccess, platform.ErrContextInterrupt } // Set the active address space. // // This must be done prior to the call to Touch below. If the address // space is invalidated between this line and the call below, we will // flag on entry anyways. When the active address space below is // cleared, it indicates that we don't need an explicit interrupt and // that the flush can occur naturally on the next user entry. cpu.active.set(localAS) // Prepare switch options. switchOpts := ring0.SwitchOpts{ Registers: &ac.StateData().Regs, FloatingPointState: ac.FloatingPointData(), PageTables: localAS.pageTables, Flush: localAS.Touch(cpu), FullRestore: ac.FullRestore(), } // Take the blue pill. 这里是关键 at, err := cpu.SwitchToUser(switchOpts, &c.info) // Clear the address space. cpu.active.set(nil) // Increment the number of user exits. atomic.AddUint64(&cpu.userExits, 1) // Release resources. c.machine.Put(cpu) // All done. c.interrupt.Disable() return &c.info, at, err } arm64的cpu.SwitchToUser 在pkg/sentry/platform/kvm/machine_arm64_unsafe.go中 // SwitchToUser unpacks architectural-details. func (c *vCPU) SwitchToUser(switchOpts ring0.SwitchOpts, info *linux.SignalInfo) (hostarch.AccessType, error) { // Check for canonical addresses. if regs := switchOpts.Registers; !ring0.IsCanonical(regs.Pc) { return nonCanonical(regs.Pc, int32(unix.SIGSEGV), info) } else if !ring0.IsCanonical(regs.Sp) { return nonCanonical(regs.Sp, int32(unix.SIGSEGV), info) } // Assign PCIDs. if c.PCIDs != nil { var requireFlushPCID bool // Force a flush? switchOpts.UserASID, requireFlushPCID = c.PCIDs.Assign(switchOpts.PageTables) switchOpts.Flush = switchOpts.Flush || requireFlushPCID } var vector ring0.Vector ttbr0App := switchOpts.PageTables.TTBR0_EL1(false, 0) c.SetTtbr0App(uintptr(ttbr0App)) //这里配置了用户态的基址寄存器 // Full context-switch supporting for Arm64. // The Arm64 user-mode execution state consists of: // x0-x30 // PC, SP, PSTATE // V0-V31: 32 128-bit registers for floating point, and simd // FPSR, FPCR // TPIDR_EL0, used for TLS appRegs := switchOpts.Registers c.SetAppAddr(ring0.KernelStartAddress | uintptr(unsafe.Pointer(appRegs))) entersyscall() //go:linkname entersyscall runtime.entersyscall, 这里更像是告诉go的runtime, 我要进入syscall了, 要阻塞了 bluepill(c) //bluepill enters guest mode, 进入guest模式; 下面会提到, bluepill会触发非法指令异常, 程序会在这里卡住, 等待bluepillHandler执行一次ioctl的_KVM_RUN vector = c.CPU.SwitchToUser(switchOpts) exitsyscall() switch vector { case ring0.Syscall: // Fast path: system call executed. return hostarch.NoAccess, nil case ring0.PageFault: return c.fault(int32(unix.SIGSEGV), info) case ring0.El0ErrNMI: return c.fault(int32(unix.SIGBUS), info) case ring0.Vector(bounce): // ring0.VirtualizationException. return hostarch.NoAccess, platform.ErrContextInterrupt case ring0.El0SyncUndef: return c.fault(int32(unix.SIGILL), info) case ring0.El0SyncDbg: *info = linux.SignalInfo{ Signo: int32(unix.SIGTRAP), Code: 1, // TRAP_BRKPT (breakpoint). } info.SetAddr(switchOpts.Registers.Pc) // Include address. return hostarch.AccessType{}, platform.ErrContextSignal case ring0.El0SyncSpPc: *info = linux.SignalInfo{ Signo: int32(unix.SIGBUS), Code: 2, // BUS_ADRERR (physical address does not exist). } return hostarch.NoAccess, platform.ErrContextSignal case ring0.El0SyncSys, ring0.El0SyncWfx: return hostarch.NoAccess, nil // skip for now. default: panic(fmt.Sprintf(\"unexpected vector: 0x%x\", vector)) } } bluepill()汇编函数 在上面的SwitchToUser()函数中, 先是entersyscall(), 然后调用bluepill()进入guest模式 // See bluepill.go. TEXT ·bluepill(SB),NOSPLIT,$0 begin: MOVD vcpu+0(FP), R8 MOVD $VCPU_CPU(R8), R9 ORR $0xffff000000000000, R9, R9 // Trigger sigill. // In ring0.Start(), the value of R8 will be stored into tpidr_el1. // When the context was loaded into vcpu successfully, // we will check if the value of R10 and R9 are the same. // 注意原注释里的MRS TPIDR_EL1, R10, 因为在EL0级别使用了高级别的TPIDR_EL1, 会触发指令异常 // 大部分系统寄存器都不能在EL0访问, 个别的可以, 比如TPIDR_EL0. 注意这里的后缀是EL0 // Any access from EL0 to a System register with the access right disabled causes the instruction to behave as UNDEFINED // 这个非法指令异常应该会导致当前CPU跳转到kernel配置好的异常向量 // 然后kernel会路由这个signal到下面的sighandler, 在返回用户态后执行. WORD $0xd538d08a // MRS TPIDR_EL1, R10 // 因为当前线程在执行下面的sighandler, 暂时还不会走到这里. 待 sighandler执行完成后, 再次返回内核, 内核再次调度这个线程, 才跑到这里. check_vcpu: CMP R10, R9 BEQ right_vCPU wrong_vcpu: CALL ·redpill(SB) B begin right_vCPU: RET // 异常会在这里被处理. 但问题是为什么非要触发个sigill再来处理? // sighandler: see bluepill.go for documentation. // // The arguments are the following: // // R0 - The signal number. // R1 - Pointer to siginfo_t structure. // R2 - Pointer to ucontext structure. // TEXT ·sighandler(SB),NOSPLIT,$0 // si_signo should be sigill. MOVD SIGINFO_SIGNO(R1), R7 CMPW $4, R7 BNE fallback MOVD CONTEXT_PC(R2), R7 CMPW $0, R7 BEQ fallback MOVD R2, 8(RSP) BL ·bluepillHandler(SB) // Call the handler. RET 下面就来到了bluepillHandler()这个函数, 在pkg/sentry/platform/kvm/bluepill_unsafe.go粗看下来, 这个函数核心是调用一次unix.RawSyscall(unix.SYS_IOCTL, uintptr(c.fd), _KVM_RUN, 0), 然后等待guest VM因为异常等原因退出这一次的RUN我认为在KVM_RUN syscall期间是阻塞的, 那么这里就是在signal处理函数里阻塞. func bluepillHandler(context unsafe.Pointer) { c := bluepillArchEnter(bluepillArchContext(context)) //这里返回一个*vCPU for { _, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(c.fd), _KVM_RUN, 0) // escapes: no. //检查errno是否是被打断等 //检查exitReason, 每个分支都return(除了中断注入), 也就是说for只管一次 // exitReason是kvm通过mmap传出来的值 switch c.runData.exitReason { } } } 对应一次_KVM_RUN让VM运行, stop VM是通过ioctl的_KVM_SET_VCPU_EVENTS来实现的. // bluepillStopGuest is reponsible for injecting sError. // //go:nosplit func bluepillStopGuest(c *vCPU) { // vcpuSErrBounce is the event of system error for bouncing KVM. vcpuSErrBounce := &kvmVcpuEvents{ exception: exception{ sErrPending: 1, }, } if _, _, errno := unix.RawSyscall( // escapes: no. unix.SYS_IOCTL, uintptr(c.fd), _KVM_SET_VCPU_EVENTS, uintptr(unsafe.Pointer(vcpuSErrBounce))); errno != 0 { throw(\"bounce sErr injection failed\") } } 这个runData定义是, 估计是kvm标准定义的 type runData struct { requestInterruptWindow uint8 _ [7]uint8 exitReason uint32 readyForInterruptInjection uint8 ifFlag uint8 _ [2]uint8 cr8 uint64 apicBase uint64 // This is the union data for exits. Interpretation depends entirely on // the exitReason above (see vCPU code for more information). data [32]uint64 } 为什么runData的exitReason会反应VM的退出码?因为在newVCPU()里面, 用_KVM_CREATE_VCPU出来的fd, mmap成了*runData func (m *machine) newVCPU() *vCPU { fd, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(m.fd), _KVM_CREATE_VCPU, uintptr(id)) // Map the run data. runData, err := mapRunData(int(fd)) c.runData = runData } // mapRunData maps the vCPU run data. func mapRunData(fd int) (*runData, error) { r, _, errno := unix.RawSyscall6( unix.SYS_MMAP, 0, uintptr(runDataSize), unix.PROT_READ|unix.PROT_WRITE, unix.MAP_SHARED, uintptr(fd), 0) if errno != 0 { return nil, fmt.Errorf(\"error mapping runData: %v\", errno) } return (*runData)(unsafe.Pointer(r)), nil } vCPU.CPU.SwitchToUser函数 位于pkg/ring0/kernel_arm64.go // SwitchToUser performs an eret. // // The return value is the exception vector. // // +checkescape:all // //go:nosplit func (c *CPU) SwitchToUser(switchOpts SwitchOpts) (vector Vector) { storeAppASID(uintptr(switchOpts.UserASID)) storeEl0Fpstate(switchOpts.FloatingPointState.BytePointer()) if switchOpts.Flush { LocalFlushTlbByASID(uintptr(switchOpts.UserASID)) } regs := switchOpts.Registers regs.Pstate &= ^uint64(PsrFlagsClear) regs.Pstate |= UserFlagsSet fpDisableTrap := CPACREL1() if fpDisableTrap != 0 { FPSIMDEnableTrap() } kernelExitToEl0() fpDisableTrap = CPACREL1() if fpDisableTrap != 0 { SaveFloatingPoint(switchOpts.FloatingPointState.BytePointer()) } vector = c.vecCode return } 补充 go linkname用法 pkg/sentry/platform/kvm/machine_unsafe.go中, 引用了2个runtime的小写函数: //go:linkname entersyscall runtime.entersyscall func entersyscall() //go:linkname exitsyscall runtime.exitsyscall func exitsyscall() The //go:nosplit directive must be followed by a function declaration. It specifies that the function must omit its usual stack overflow check. This is most commonly used by low-level runtime code invoked at times when it is unsafe for the calling goroutine to be preempted. //go:linkname localname [importpath.name] This special directive does not apply to the Go code that follows it. Instead, the //go:linkname directive instructs the compiler to use “importpath.name” as the object file symbol name for the variable or function declared as “localname” in the source code. If the “importpath.name” argument is omitted, the directive uses the symbol's default object file symbol name and only has the effect of making the symbol accessible to other packages. Because this directive can subvert the type system and package modularity, it is only enabled in files that have imported \"unsafe\". go:linkname有三种用法: export: localname是本地(pkgA)的有函数体的实现, export成pkgB.name. pkgB没有name的实现 import: localname是本地(pkgA)的没有函数体的声明, 它import pkgB.name的具体实现. 同名export: 不指定importpath.name, 让本来小写的符号能被外部访问. runtime.entersyscall在src/runtime/proc.go中定义 // Standard syscall entry used by the go syscall library and normal cgo calls. // // This is exported via linkname to assembly in the syscall package. // //go:nosplit //go:linkname entersyscall func entersyscall() { reentersyscall(getcallerpc(), getcallersp()) } // reentersyscall比较晦涩, 这里只贴了注释 // The goroutine g is about to enter a system call. // Record that it's not using the cpu anymore. // This is called only from the go syscall library and cgocall, // not from the low-level system calls used by the runtime. // // Entersyscall cannot split the stack: the gosave must // make g->sched refer to the caller's stack segment, because // entersyscall is going to return immediately after. // // Nothing entersyscall calls can split the stack either. // We cannot safely move the stack during an active call to syscall, // because we do not know which of the uintptr arguments are // really pointers (back into the stack). // In practice, this means that we make the fast path run through // entersyscall doing no-split things, and the slow path has to use systemstack // to run bigger things on the system stack. // // reentersyscall is the entry point used by cgo callbacks, where explicitly // saved SP and PC are restored. This is needed when exitsyscall will be called // from a function further up in the call stack than the parent, as g->syscallsp // must always point to a valid stack frame. entersyscall below is the normal // entry point for syscalls, which obtains the SP and PC from the caller. // // Syscall tracing: // At the start of a syscall we emit traceGoSysCall to capture the stack trace. // If the syscall does not block, that is it, we do not emit any other events. // If the syscall blocks (that is, P is retaken), retaker emits traceGoSysBlock; // when syscall returns we emit traceGoSysExit and when the goroutine starts running // (potentially instantly, if exitsyscallfast returns true) we emit traceGoStart. // To ensure that traceGoSysExit is emitted strictly after traceGoSysBlock, // we remember current value of syscalltick in m (_g_.m.syscalltick = _g_.m.p.ptr().syscalltick), // whoever emits traceGoSysBlock increments p.syscalltick afterwards; // and we wait for the increment before emitting traceGoSysExit. // Note that the increment is done even if tracing is not enabled, // because tracing can be enabled in the middle of syscall. We don't want the wait to hang. // //go:nosplit func reentersyscall(pc, sp uintptr) { ... } "},"notes/golang_gvisor调试.html":{"url":"notes/golang_gvisor调试.html","title":"gvisor调试","keywords":"","body":" 环境 gvisor编译 gvisor编译 go分支编译 master分支编译 gvisor调试 配置文件 运行 kvm模式运行非常缓慢 调试 调试前准备 dlv调试 signal返回到哪了? sighandler做桥, host和guest跳转过程 一次blupill流程 另外一次blupill流程 问题 断点记录: 问题1 以上修改不管用 bluepillGuestExit跳转到哪里了? 本文在raspberry pi 4上调试gvisor的kvm模式, 目的是理解gvisor的kvm运行机制. 环境 环境 说明 硬件 raspberry pi 4B OS ubuntu server 21.10 for arm64 kernel Linux ubuntu 5.13.0-1022-raspi #24-Ubuntu SMP PREEMPT Wed Mar 16 07:19:33 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux gvisor编译 gvisor信息 说明 版本 https://github.com/google/gvisor.git 分支 master 编译 make copy TARGETS=runsc DESTINATION=bin/ gvisor编译 go分支编译 gvisor有两个分支: master分支, 用bazel编译 go分支, 用go build编译 使用go分支可以编译, 但无法正常运行, 提示类似/proc/self/exe: no such file or directory的错误. 调查发现gvisor需要在当前进程下把uid和guid都设置成nobody, 然后执行/proc/self/exe再次启动自身. 但这一步出错: 当前进程下存在/proc/self/exe, 它是个软连接, 指向正确的runsc的路径 代码在runsc/cmd/cmd.go的callSelfAsNobody()函数 在这个函数里, 虽然能够找到/proc/self/exe, 但执行会报错. 具体原因不明, 我加了点调试代码, 发现这个slef exe可以执行, 但报错提示/dev/null不存在.func callSelfAsNobody(args []string) error { ... binPath := \"/proc/self/exe\" dest, _ := os.Readlink(binPath) output, err := exec.Command(dest, \"-h\").CombinedOutput() log.Infof(\"error: %v\", errr) //这里提示/dev/null不存在 } gvisor的启动顺序是runsc create再runsc gofer再runsc boot再runsc start再runsc delete. 我怀疑是在runsc create阶段对这个namespace的创建不完整 master分支编译 master分支可以编译, 可以\"正常\"运行. 但在raspberry上编译极其慢, 我大概等了两三个小时. gvisor调试 下面是以master分支编译出来的runsc来进行调试 配置文件 使用runsc需要在docker里配置runtime, 如下: ubuntu@ubuntu ~ $ cat /etc/docker/daemon.json { \"runtimes\": { \"runsc\": { \"path\": \"/home/ubuntu/repo/bbb/gvisor/bin/runsc\", \"runtimeArgs\": [ \"--cpu-num-from-quota\", \"--platform=kvm\", \"--debug-log=/tmp/runsc/\", \"--debug\" ] } } } 更改配置文件需要 sudo systemctl reload docker 运行 用docker运行centos, 需要指定--runtime=runsc, --cpus=1 -m 2g指定了cpu和mem的限制:docker run --cpus=1 -m 2g --rm --runtime=runsc -it --name=test centos:7 bash kvm模式运行非常缓慢 正常docker run命令应该很快返回, 但kvm模式下观察到CPU占用非常高, 多核100%运行, docker run命令迟迟不返回. 调试 调试前准备 除了需要大概了解gvisor的代码和执行流程外, 在调试前, 需要 找到runsc boot进程的pid 我这里显示runsc-sandbox boot进程的pid是4989 反汇编runsc go tool objdump -S runsc > runsc.objdump dlv调试 $ sudo /home/ubuntu/go/bin/dlv attach 4989 (dlv) b *0x951d5c (dlv) c (dlv) bt 0 0x0000000000951d5c in gvisor.dev/gvisor/pkg/ring0.(*CPU).SwitchToUser 1 0x000000000095a02c in gvisor.dev/gvisor/pkg/sentry/platform/kvm.(*vCPU).SwitchToUser 2 0x000000000095564c in gvisor.dev/gvisor/pkg/sentry/platform/kvm.(*context).Switch 3 0x00000000005420c8 in gvisor.dev/gvisor/pkg/sentry/kernel.(*runApp).execute 4 0x000000000054152c in gvisor.dev/gvisor/pkg/sentry/kernel.(*Task).run 5 0x000000000007e154 in runtime.goexit (dlv) si Stopped at: 0x951fa0 注:0x951d5c是调用ring0.kernelExitToEl0函数的地方, 在gvisor.dev/gvisor/pkg/ring0.(*CPU).SwitchToUser里:0x951d5c 94000091 CALL gvisor.dev/gvisor/pkg/ring0.kernelExitToEl0(SB) 而0x951fa0就是kernelExitToEl0的代码: 0x951fa0 d538d092 MRS $18052, R18 //其实是MRS TPIDR_EL1, R18 这个TPIDR_EL1是thread id寄存器, 只能在EL1访问; 但这里是EL0, 于是发生指令异常:在dlv里si并没有执行0x951fa4 a90e0640 STP (R0, R1), 224(R18)这句代码而是跳转到了0x95c700就能看出来: 指令异常被host kernel deliver到触发异常的线程(即当前线程), 交由提前注册的sighandler在用户态处理.0x95c700是SIGILL的处理函数: 这个sighandler是kvm实现里非常关键的函数, 它负责执行依次KVM_RUN. _, _, errno := unix.RawSyscall(unix.SYS_IOCTL, uintptr(c.fd), _KVM_RUN, 0) 我们一路向下单步执行, 来到了这个函数的返回点: (dlv) si Stopped at: 0x95c730 => 1: no source available (dlv) si Stopped at: 0x95c734 //这句正好是kvm.sighandler的RET指令 => 1: no source available 0x95c734处的指令是kvm.sighandler的RET指令, 继续向下, 我们来到了一个很奇怪的地址: (dlv) si Stopped at: 0xffffb00ba7dc => 1: no source available (dlv) si Stopped at: 0xffffb00ba7e0 => 1: no source available 这里的0xffff开头的地址, 比普通的代码段地址高很多, objdump里还查不到相应的地址...那看看dlv的反汇编指令: (dlv) disass -a 0x0000ffffb00ba780 0x0000ffffb00ba800 这个svc指令是arm64的系统调用指令, 系统调用号放在r8里:x8的0x8b就是代码里的$139, 这个调用是: rt_sigreturn rt_sigreturn系统调用是signal处理机制的trampoline的一部分, 当用户态的sighandler返回的时候, 实际上返回的是signal的trampoline的代码, 后者再调用系统调用rt_sigreturn()让内核帮助恢复用户态上下文, 再返回到用户态被signal打断的代码.我们这里的sigill是同步异常, 那么一般情况下就应该还是这个指令; 但我们这里实际上不是 因为ucontext改变了, 见下文: signal返回到哪了? 调用kvm.sighandler时, 代表被打断的上下文的ucontext被放到R2中: // sighandler: see bluepill.go for documentation. // // The arguments are the following: // // R0 - The signal number. // R1 - Pointer to siginfo_t structure. // R2 - Pointer to ucontext structure. // TEXT ·sighandler(SB),NOSPLIT,$0 // si_signo should be sigill. MOVD SIGINFO_SIGNO(R1), R7 CMPW $4, R7 BNE fallback MOVD CONTEXT_PC(R2), R7 CMPW $0, R7 BEQ fallback MOVD R2, 8(RSP) BL ·bluepillHandler(SB) // Call the handler. RET 这里的0x95c700就是kvm.sighandler(SB)这里的R2(就是X2)为0x4000460e20这实际上是个指向arch.UContext64的指针: dlv显示Pc: 95c6d8, 这正好是触发SIGILL的指令地址. 直到调用kvm.bluepillHandler(SB)之前, 这个PC都是95c6d8 注意看这里的Regs[30]为95a004, 我们知道r30是LR寄存器, 95a004就是调用完kvm.bluepill后的地址 sighandler做桥, host和guest跳转过程 一次blupill流程 0x95c6c0 kvm.bluepill 0x95c6d8 MRS TPIDR_EL1, R10 //触发sigill 0x95c700 kvm.sighandler // with Pc: 95c6d8, 也就是前面的指令 0x954f10 kvm.bluepillHandler() 0x954e20 bluepillGuestExit // with Pc: 95c6d8 ret with Pc: 951d5c 0x95c734 ret // with Pc: 951d5c 0xffffb00ba7dc 0xffffb00ba7e0 svc // rt_sigreturn 0x951d5c CALL kernelExitToEl0() // 单步执行就到了这里, 因为前面ucontext的pc就是951d5c 0x951fa0 // in kernelExitToEl0() 第一个指令就是特权指令 0x95c700 kvm.sighandler // with Pc: 951fa0, 也就是前面的指令 0x954f10 kvm.bluepillHandler() 0x954e20 bluepillGuestExit // with Pc: 951fa0 ret with Pc: 7f41c 0x95c734 ret // with Pc: 7f41c 0xffffb00ba7dc 0xffffb00ba7e0 svc // rt_sigreturn 0x7f41c svc指令 // in runtime.futex(SB) src/runtime/sys_linux_arm64.s 0x432e8 runtime.futexsleep() 0x95c6c0 kvm.bluepill // 再次进入bluepill ... 本次实验常用地址索引: 0x951d5c CALL gvisor.dev/gvisor/pkg/ring0.kernelExitToEl0(SB) 0x951fa0 ring0.kernelExitToEl0 0x95c6c0 kvm.bluepill(SB) 0x95c700 kvm.sighandler(SB) 0x95c728 CALL gvisor.dev/gvisor/pkg/sentry/platform/kvm.bluepillHandler(SB) 0x95c734 RET 如果只打断点0x95c734, 得到跳转地址如下: 0x7f41c svc in runtime.futex(SB) 0x9272c svc in syscall.Syscall(SB) 0x95c6d8 MRS in kvm.bluepill 0x7f41c svc in runtime.futex(SB) 0x7f41c 0x7f41c 过程中观察到有多个context: p %x *(*arch.UContext64)(0x4000460e20) p %x *(*arch.UContext64)(0x4000188e20) p %x *(*arch.UContext64)(0x400014ce20) p %x *(*arch.UContext64)(0x4000008e20) p %x *(*arch.UContext64)(0x400006ce20) 另外一次blupill流程 这次是打了bluepillGuestExit断点, continue等待断点时间较长, 从host观察到CPU一直在VM状态下, 持续高负载运行, 现象是htop显示黄色(CPU stolen), 直到trigger断点此时观察到ucontext的PC是0x95c6d8, 就是blupill触发sigill的指令地址: 问题 如果断点打在ring0.(*CPU).SwitchToUser(), 会导致bluepillGuestExit里, 误认为VM退出时候的PC是ring0.(*CPU).SwitchToUser(), 导致继续运行不正确. 断点记录: b ring0.(*CPU).SwitchToUser b *0x95c700 //sigill handler入口 b *0x95c734 //sigill handler返回前 b *0x95c6c0 //bluepill入口 b kvm.bluepillHandler b kvm.bluepillGuestExit b ring0.(*CPU).SwitchToUser 问题1 每一轮的进入guest模式是从bluepill开始的, 但每轮要在guest态执行很久, CPU100%很久, 才能退出guest状态, 此时看到guest里面的调用栈非常非常深: 0x7f41c svc in runtime.futex(SB)这里的调用栈显示有太多的runtime.morestack的栈帧.这里只显示50层, 但实际上bt命令支持depth参数, 即使加到10000层也显示不完... 0x7bf70地址实际上是runtime·morestack(SB)的最后一条指令, 理论上执行不到: @src/runtime/asm_arm64.s TEXT runtime·morestack(SB),NOSPLIT|NOFRAME,$0-0 ... BL runtime·newstack(SB) // Not reached, but make sure the return PC from the call to newstack // is still in this function, and not the beginning of the next. UNDEF 怀疑和BL地址有关, 把bl改为jmp指令试试 //也可以这样显示 (dlv) p %x *(*uint32)(0x7bf6c) //改为0x17ff9ee8 set *(*uint32)(0x7bf6c) = 0x17ff9ee9 即修改前:修改后: 可以看到, call指令换成了jmp指令后, 每轮的bluepill还是会执行很久, 但调用栈似乎已经正常: 有的时候即使调用栈不深也要执行很久 以上修改不管用 还是会出现很多0x000000000007bf5c in runtime.morestack那么guest在干什么导致CPU占用这么高呢? 用sudo perf kvm --guest --guestvmlinux path/to/runsc top可以看vm的符号观察到:对应的代码:pmap显示: sudo pmap 7556 ... 000000fee2e31000 273806262272K ----- [ anon ] 0000ffff62e31000 4K rw-s- memfd:memory-usage (deleted) 0000ffff62e32000 38212K rw--- [ anon ] 0000ffff65383000 512K ----- [ anon ] 0000ffff65403000 4K rw--- [ anon ] 0000ffff65404000 523836K ----- [ anon ] 0000ffff85393000 4K rw--- [ anon ] 0000ffff85394000 65476K ----- [ anon ] 0000ffff89385000 4K rw--- [ anon ] 0000ffff89386000 8180K ----- [ anon ] 0000ffff89b83000 4K rw--- [ anon ] 0000ffff89b84000 1020K ----- [ anon ] 0000ffff89c83000 384K rw--- [ anon ] 0000ffff89ce3000 8K r---- [ anon ] 0000ffff89ce5000 4K r-x-- [ anon ] 0000ffffe2bc3000 132K rw--- [ stack ] total 273808069472K 总共273T, 其中大部分都在一个段里. bluepillGuestExit跳转到哪里了? 比如: 在0x95c734, sigill handler马上要返回了, 此时ucontext的PC已经变成了927ac, 经过signal的trampoline系统调用后, kernel恢复了从927ac开始执行的上下文:这是个fdnotifier.epollWait在执行的syscall系统调用, 是guest task执行的, 因为这个上下文是依次KVM_RUN后, 从guest VCPU里得到的. 说明guest vCPU在遇到svc指令的时候, 会退出guest mode:退出后, host继续运行0x927ac处的代码(即svc指令). 这样host就\"代表\"guest来向host kernel发出了syscall动作. 比如:0x9272c处的指令是syscall.Syscall的svc指令 比如:0x7f41c是tuntime.futex的svc指令 比如: 比如: "},"notes/golang_gvisor_ptrace.html":{"url":"notes/golang_gvisor_ptrace.html","title":"gvisor ptrace模式介绍(网摘)","keywords":"","body":"原文: https://blog.csdn.net/M2l0ZgSsVc7r69eFdTj/article/details/82754587 传统的Container由于隔离性差而不适合作为Sandbox运行不受信工作负载，VM可以提供很好隔离但却额外消耗较多的内存。Google开源的gVisor为我们提供另外一种选择：在牺牲掉一定性能的情况下，它只额外消耗非常少量的内存，却可以提供了类似等级的隔离性。在本文里我们深入gVisor，最后了解一下我们增强gVisor以支持资源控制的方案。 gVisor是什么 gVisor为在Container中运行不受信代码提供了新的解决思路，gVisor是一个Sandbox方案和实现。 gVisor尝试解决什么问题 虽然Container上可以通过Namespace和Cgroup做资源的限制，但Container里的应用程序依然可以访问很多系统资源。事实上跟没有跑在Container里的应用程序一样，Container里的应用程序可以直接通过Linux内核的系统调用陷入到内核。任何一个被允许（通过Seccomp过滤系统调用）的系统调用的缺陷都可以被恶意的应用程序利用。 主流的Sandbox基于VM虚拟机的方案，将潜在恶意的应用程序隔离在独立的虚拟机中，例如Kata Linux，该项目与Docker和Kubernetes都有集成。基于VM的方案提高了很好的隔离，但相应额外消耗的内存会多一些。在有需要运行大量Container的场景下的额外资源消耗不能被忽略。 gVisor提供了另外一种Sandbox思路，gVisor非常轻量级，额外的内存消耗非常小，但同时提供了和VM方案相当隔离等级。该分享里介绍的基于Ptrace的gVisor，系统调用的性能比较差，应用程序的兼容性也差一些。gVisor可以和Docker很好的集成，但和Kubernetes的集成还处于实验阶段。在和Docker集成的时候，gVisor遵循了OCI（Open Containers Initiative）标准，所以可以作为Docker的一个Runtime执行。 gVisor如何工作 以非特权用户运行的gVisor通过截获应用程序的系统调用，将应用程序和内核之间完全隔离。gVisor没有简单的把应用程序发出的系统调用直接作用到内核，而是实现了大多数的系统调用，通过对系统调用模拟，让应用程序间接的访问到系统资源。gVisor模拟系统调用本身时对操作系统执行系统调用，通过使用Seccomp对这些系统调用做过滤。那么gVisor是如何截获应用程序的系统调用的呢？ gVisor截获系统调用 gVisor存在两种运行模式，这次分享只介绍了基于Ptrace的gVisor。 为了理解gVisor如何拦截系统调用，需要先了解一下Ptrace：Ptrace是Linux提供的一个系统调用接口，通过Ptrace，可以在两个进程之间建立Tracer和Tracee之间的关系。Tracer可以控制Tracee，例如当Tracee收到信号的时候主动进入stopped状态，此时Tracer可以选择是否对Tracee做一些操作（比如设置Tracee的寄存器上下文或者内存中内容等），在操作执行后，Tracer可以选择是否让Tracee继续执行。 Tracee除了可以在接受到信号时候进入stopped状态外，也可以被Tracer告知在即将进入系统调用时或者即将离开系统调用时进入stopped状态。具体说Ptrace可以通过PTRACE_SYSEMU控制Tracee在即将进去系统调动时stop。gVisor也正是通过该命令来截获应用程序的系统调用。 Sentry是通过Ptrace来控制应用程序，那么应用程序是如何变为Tracee的呢？ 将应用程序变成Tracee的 下图是gVisor控制应用程序的进程关系： 当gVisor以Docker的Runtime启动的时候，可以看到类似的进程间关系：docker-containerd-shim是容器的启动器；sentry是gVisor用于截获系统调用模拟内核的程序，他也正是Tracer。Stub可以暂时不用理会，stub的子进程正是我们想要放到Sandbox里的应用程序。Sentry创建stub，随后stub创建应用程序进程，sentry通过Ptrace attach到了stub和应用程序上。当应用程序在将要执行系统调用的时候会主动stop，此时也正是sentry拦截和模拟系统调用的点。 这跟用gdb调试程序C/C++程序类似，通过命令行给gdb指定一个要调试的目标程序的时候，该程序会以子进程的方式运行，gdb作为Tracer attach到应用程序上来对应用程序进行控制。类似gdb，sentry也以类似的方式启动应用程序，只是sentry先启动了一个stub，然后让stub以它的子进程方式启动了应用程序。 应用程序被创建并变为Tracee后，接下来就是sentry如何完成应用程序的启动流程了。 启动应用程序 应用程序被初始attach到sentry后，sentry负责启动应用程序。 在操作系统启动应程序场景里，应用程序的二进制文件由操作系统加载，譬如分配虚拟内存空间用来存放二进制中的代码段、数据段、共享库或者初始化应用程序的栈空间。gVisor启动应用程序的场景下，类似的过程由sentry完成： 为了了解sentry是如何初始化应用程序的虚拟内存空间，需要先了解一下上文提到的stub进程。 Stub进程的一个重要的作用是作为应用程序的初始模板，以该模板创建应用程序。事实上stub作为sentry的子进程，在启动后会主动将虚拟内存地址空间里几乎所有的memory region（通过查看/proc/${pid}/maps查看一个进程虚拟内存地址空间里的所有memory region）甚至将代码段和数据段也unmmap掉了。只保留两个memory region：其中第一个region存放了最简化的代码，即上图中第一个段，执行该代码甚至不需要栈空间，所以连栈段也被unmap掉了。 这样的空洞的虚拟内容地址空间正好可以作为一个模板虚拟内存地址空间。当stub以子进程的方式启动应用程序后，应用程序的虚拟内存地址空间的layout与stub的一样。应用程序在创建出来后会立即被sentry attach，此时正是sentry做应用程序初始化的过程：sentry初始设置应用程序的RIP（指令执行寄存器）的初始值为应用程序二进制中读出来的应用程序入口地址，该地址一般位于应用程序虚拟地址空间的较低位置，并通过PTRACE_SYSEMU指示应用程序开始执行，直到遇到以下两种事件的时候进入stop状态： 将要执行一个系统调用 收到了来自内核或者进程发给它的信号 因为应用程序的初始执行位置在用户态的虚拟内存地址里没有对应的memory region，所以应用程序会收到来自内核发来的SIGSEGV信号（段错误）。这里的场景非常类似通常的page fault，当一个应用程序试图访问的地址位于某个虚拟内存地址段内，但该段没有对应物理内存页的时候，操作系统会因此陷入page fault，在page fault的handler中为该虚拟内存地址段映射物理页。 事实上，在sentry在启动应用程序运行环境之前，已经应用程序“分配”了一个虚拟内存地址段（这个分配并不是使用mmap或者brk真正的在应用程序的虚拟地址空间中分配地址段，该分配是一个提前占位）。上面说到当应用程序执行指令地址上因为没有实际分配虚拟内存地址段，所以收到来自内核的SIGSEGV，并且进入stopped状态，此时sentry会通过mmap在该地址上真实分配一个虚拟地址内存段（类比操作系统为虚拟内存地址段上分配物理页），并且因为mmap的源文件是二进制文件本身，所以当sentry在处理完SIGSEGV指示应用程序继续执行的后，应用程序将实际执行到该二进制中的代码。 至此应用程序就已经启动起来了。接下来需要了解就是sentry如何控制应用程序的执行了。 应用程序的执行 应用程序被启动起来后，在执行的过程中可能会陆续遇到新的SIGSEGV（譬如程序读写地址段，或者栈空间的扩展），或者执行系统调用。 在“应用程序如何被启动”里实际上已经描述了SIGSEGV信号处理的一种场景，即只读地址且有映射文件的场景，其他的场景譬如匿名地址段或者栈空间的区别在于该地址段没有mmap实际的文件，而是mmap了一个sentry提前准备好的“空白”文件中。 在“gVisor如何拦截系统调用”中描述了系统调用的拦截，当应用程序在进入系统调用之前会自动进入stopped状态，此时sentry读取应用程序的系统调用号以及系统调用入参，试图模拟该系统调用。以文件的读sys_read为例，sys_read的作用是找到指定的文件，打开并读取文件内容，并将内存写入到应用程序系统调用参数指定的虚拟内存地址上。Sentry在接到这个的系统调用时，会将文件读取请求通过9p协议发给之前提到的gofer进程（sentry和gofer之间有建立socket pair传输9p协议），由gofer进程执行真正的文件读取且将读到的内容通过9p协议返回给sentry。sentry把读取到的文件内容写入到应用程序的虚拟内存中（如果该地址没有对应的虚拟内存地址段，则分配后再复制），随后sentry将系统调用的实际模拟结果写入到应用程序的寄存器中，然后让应用程序继续执行。恢复执行后的应用程序因为得到了系统调用的结果，所以在应用程序在分不清实际上系统调用是直接由操作系统执行了还是由sentry做的模拟的情况下，系统调用得到了满足。 应用程序的访问控制 “应用程序的执行”中对于文件读系统调用处理的描述实际上也描述了对应用程序文件系统访问的控制，实际上在“应用程序启动”中为了省略的根文件系统挂载的描述，在根文件系统挂载的模拟中也涉及到了通过9p协议对文件系统的访问。文件写的处理也非常类似。 除了文件读写外，还有很多其他的系统调用，譬如共享内存或其他IPC，锁，创建线程或者进程，发送信号，socket，execv，epoll，eventfd，pid namespace等，gVisor都进行了模拟，涉及到了操作系统的方方面面。这里仅仅介绍了socket相关的系统调用： Sentry在用户态实现了基本的TCP/IP协议栈，在启动应用程序之前，gVisor会启动一个临时的start进程，在start进程会进入到docker创建的network namespace，start进程从该network namespace中获取veth pair中属于gVisor的一端的veth设备，创建AF_PACKET的socket绑定到该veth设备上来接管该设备上的网络流量（同时也将ip从该设备上去掉了），并将该socket传递给sentry进程。后续当sentry截获了应用程序的socket系统调用后，最终通过该socket将网络包实际从veth设备上发送出去；从该veth设备上接收到的网络包在经过sentry网络协议栈后递交给应用程序的socket层。 gVisor当前缺少资源限制 gVisor具备沙盒的能力，但是缺少Cgroup提供的资源使用量的限制的功能。在官方的Roadmap中计划提供Cgroup支持。但此时为了能够使用gVisor运行工作负载，需要让gVisor具备资源使用量限制的功能。 Docker通过Runtime支持资源使用量限制，gVisor则是Docker的另外一个Runtime名叫runsc。通过了解Docker原生的Runtime即runC，可以为gVisor中支持Cgroup提供思路。Docker通过OCI（Open Containers Initiative）spec跟Runtime之间进行交互，符合该标准的Runtime可以通过Docker的命令来启动。OCI spec里规范了应用程序启动资源使用量的限制的描述，Docker在启动Runtime的时候，将OCI spec内容传递给Runtime，由Runtime负责给应用程序应用这些资源限制： runC由docker shim启动，首先会创建一个init进程，该进程最终会通过execv转变为我们希望启动的应用程序，在init进程执行execv之前，runC会为init进程创建Cgroup，将实际上init进程放入到该Cgroup中。此后init进程通过execv变为应用程序，应用程序以及后来由它创建的子进程也都会进入到该Cgroup中，从而达到资源限制的功能。 gVisor的启动流程中类似也可以嵌入类似的逻辑：runsc启动流程中首先会创建gofer和boot进程，在boot进程真正启动应用程序之前，runsc为boot进程创建新的Cgroup，并将boot进程放入到该Cgroup中，后续boot进程以及被它Ptrace的应用程序就都会处于该Cgroup中，从而达到资源限制的效果。 版权声明：本文为CSDN博主「Docker_」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/M2l0ZgSsVc7r69eFdTj/article/details/82754587 "},"notes/rust_vmm_brief.html":{"url":"notes/rust_vmm_brief.html","title":"rust VMM(virtual machine monitor)","keywords":"","body":"介绍Firecracker Cloud-hypervisor以及virtio基础概念. 转录自我的PPT VMM brief Virtio devices MMIO based virtio devices PCI based virtio devices Memory Manager in cloud-hypervisor Device Manager Virtio Net example VMM brief Virtio devices Virtio is a protocol that defines how guest drivers talk to virtual devices. See the spec v1.2.Virtio devices can be exposed by PCI or MMIO PCI: a device with PCI vendor ID 0x1AF4 is a virtio device, device configuration structures are mapped to PCI configuration header BAR 0 Common configuration: feature bits, queue num, queue size, queue select, queue address Notifications: driver writes to notification address triggers an event to device ISR Status: not used when msi-x is enabled Device-specific configuration: different virtio types(net, block…) have different layouts PCI configuration access: provide an alternative way to access above registers other than BAR MMIO: a region of predefined register layout starting at base address, with compatible = \"virtio,mmio“ in DTS, which can be “discovered” by guest driver. All registers are little endian MMIO based virtio devices PCI based virtio devices Memory Manager in cloud-hypervisor Defines VM physical memory layout, just like a new SOC Uses BtreeMap to record memory ranges Uses KVM_SET_USER_MEMORY_REGION ioctl to map the layout to VMM virtual memory. (VM_PA to HOST_VA) When guest VM access memory, 2 stages translate happens(e.g. AARCH64): VM_VA -> VM_PA HOST_VA -> HOST_PA Mainly focus on PCI MMIO space PCI MMCONFIG space AARCH64 VM_PA layout Device Manager Manages all PCI devices Virtio PCI devices VFIO PCI devices Normally has 2 PCI segments Segment 0 is default Each PCI segment has PCI root, vendor ID intel, device ID VIRT_PCIE_HOST Uses HashMap to map bdf to PciDevice A pci config mmio BusDevice to route mm config access to corresponding PciDevice A MMIO address Allocator And many VirtioPciDevices Virtio Net example Virtio net has at least 3 virtqueues Transmitq Receiveq Controlq Driver sends and receives packet driver puts a packet into transmitq Notifies device by writing the notification address of the queue Kvm delivers the notification VMM handles the packet, typically by forwarding it to tap VMM receives the reply packet from tap VMM injects interrupt through KVM Guest irq handler receives the packet Guest driver handles the received packet and hands over it to upper network stack. The content in the virtqueue is virtio_net_hdr + packet data "},"notes/rust_vmm_简介.html":{"url":"notes/rust_vmm_简介.html","title":"rust-vmm简介","keywords":"","body":"https://github.com/rust-vmm Rust-Vmm 是一个开源工程，是一个可以自由定制的 VMM（virtual machine monitor）虚拟机管理器，用户可以按照自己的方式订制它。它是基于 Rust 语言实现的 VMM，有着 Rust 语言带来的优点和特性。 首先，Rust 语言一个内存安全的语言，相比于用 C 或者 C++ 会频繁遇到的各种内存的问题，比如内存的溢出、空指针、野指针、越界访问等等，更进一步会造成安全的问题、性能的问题，以及各种崩溃的问题。Rust 语言很好地解决了这一点，从它的语法、编译规则等杜绝了内存级别访问的漏洞以及风险，所以用 Rust 写的 Rust-Vmm 天然的就是内存安全的。 第二，Rust-Vmm 是不易被攻击的，Rust-VMM 是从零开始的，它是从最小的硬件虚拟化出发的，最小的硬件虚拟化意味着它有着最小的攻击面，被攻击的面就非常少，所以它会很安全。 第三，Rust-Vmm 能够很灵活的定制。Rust-VMM 可以灵活定制它的每一个组件，所有的对于设备的模拟或者关键特性的处理都是封装成了一个一个的 Rust-Vmm crates 包，比如有 VCPU，有 linuxloader，vm-virtIO 等等。其中 crates 是 Rust 语言中的包管理工具，可以理解 JAVA 或 golang 里面的 package，它是以发行不同的包或者库的形式对外发布它的 feature。 第四，Rust-Vmm 有非常高的性能，基于 Rust 语言的 without garbage collection 特性，它是没有 GC 回收检查机制的，不像 JAVA 或者其他更高级的语言会有一个 runtime，Rust-Vmm 的性能上会更好，同时基于 KVM 实现的虚拟化方案也是性能的保证。 简单介绍一下 Rust-Vmm 的一个历史，它是由谷歌首先实现的，谷歌首先实现一个 Rust based 的轻量级的 VMM，它叫做 crosVM，大家也可以从链接里面看到，它是一个为 chrome 浏览器做的一个微内核。然后 AWS，亚马逊基于谷歌开源出来的 crosVM，实现了自己的基于 rust 的 VMM 叫 Firecracker。两个项目的开发人员会发现做这两个项目的时候，会有很多重复的重叠的通用的代码，很自然的把可以开源的、通用的部分结合到一块，就有了 Rust-Vmm 的项目。 使用rust vmm crosvm 使用rust vmm 参考https://opensource.com/article/19/3/rust-virtual-machine要自己搭一个vmm, 可以使用rust rmm提供的各种模块, 这些模块都是独立的项目, rust里面叫crate. KVM interface: Creating our VMM on top of KVM requires an interface that can invoke KVM functionality from Rust. The kvm-bindings crate represents the Rust Foreign Function Interface (FFI) to KVM kernel headers. Because headers only include structures and defines, we also have wrappers over the KVM ioctls (kvm-ioctls) that we use for opening dev/kvm, creating a VM, creating vCPUs, and so on. Virtio devices and rate limiting: Virtio has a frontend-backend architecture. Currently in rust-vmm, the frontend is implemented in the virtio-devices crate, and the backend lies in the vhost package. Vhost has support for both user-land and kernel-land drivers, but users can also plug virtio-devices to their custom backend. The virtio-bindings are the bindings for Virtio devices generated using the Virtio Linux headers. All devices in the virtio-devices crate are exported independently as modules using conditional compilation. Some devices, such as block, net, and vsock support rate limiting in terms of I/O per second and bandwidth. This can be achieved by using the functionality provided in the rate-limiter crate. The kernel-loader is responsible for loading the contents of an ELF kernel image in guest memory. rust-vmm的github上, 各个模块是单独成库的: crosvm crosvm是google的为chrome OS上运行的VMM, 安全性好, Rust写的. Rust也是静态binary 每个虚拟设备是个进程,fork出来的, 但不exec. 这个进程用了minijail做沙盒处理, 应该类似seccomp. 基于KVM 使用Rust 支持x86, aarch64 virtual device使用socket和VM通信 设备模型的核心是bus, 一个读/写操做到bus上, bus会按这个读或写的地址, 找到对应的BusDevice, 转发读/写请求到该BusDevice. 一个地址上只能有一个BusDevice BusDevice可能出现在多个地址 每个BusDevice都自带mutex, 所以BusDevice的实现里就不需要再加锁了. "},"notes/rust_firecracker_代码.html":{"url":"notes/rust_firecracker_代码.html","title":"firecracker代码","keywords":"","body":"firecracker是最终的可执行文件: run_without_api流程 build_microvm_from_json event manager aarch64 物理内存layout devices::Bus 底层serial 用法 Serial结构体 三个Trait 自定义Error 用NoEvents的实例化的Serial driver在哪里读写? SerialWrapper 从stdin读输入发给guest流程 实现了BusDevice的按地址读写的trait attach_virtio_device vm.register_ioevent(queue_evt, &io_addr, i as u32) vm.register_irqfd kvm的irq相关API KVM_CREATE_IRQCHIP KVM_SET_GSI_ROUTING KVM_IRQFD KVM_CREATE_DEVICE 都有哪些可以被create ARM gic v3 KVM_DEV_ARM_VGIC_GRP_ADDR MmioTransport impl MmioTransport 实现BusDevice device device的状态有 IrqTrigger VirtioDevice trait VirtIO设备框图 virtIO net Net实现了VirtioDevice Net还实现了MutEventSubscriber run_without_api流程 这里重点考察build/cargo_target/x86_64-unknown-linux-musl/release/firecracker --no-api --config-file myvmconfig.json方式运行的firecrackermyvmconfig.json内容如下: { \"boot-source\": { \"kernel_image_path\": \"build/kernel/linux-5.10/vmlinux-5.10-x86_64.bin\", \"boot_args\": \"console=ttyS0 reboot=k panic=1 pci=off\", \"initrd_path\": null }, \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"build/rootfs/bionic.rootfs.ext4\", \"is_root_device\": true, \"partuuid\": null, \"is_read_only\": false, \"cache_type\": \"Unsafe\", \"io_engine\": \"Sync\", \"rate_limiter\": null } ], \"machine-config\": { \"vcpu_count\": 2, \"mem_size_mib\": 1024, \"smt\": false, \"track_dirty_pages\": false }, \"balloon\": null, \"network-interfaces\": [], \"vsock\": null, \"logger\": null, \"metrics\": null, \"mmds-config\": null } 经过前面的命令行参数解析, 最后调用 run_without_api( &seccomp_filters, //这个是seccomp的bpf代码 vmm_config_json, //这个是配置文件的字符串 instance_info, boot_timer_enabled, mmds_size_limit, metadata_json.as_deref(), ) 这个函数先从json构建vmm, 然后在循环里run: fn run_without_api( seccomp_filters: &BpfThreadMap, config_json: Option, instance_info: InstanceInfo, bool_timer_enabled: bool, mmds_size_limit: usize, metadata_json: Option, ) -> FcExitCode { let mut event_manager = EventManager::new().expect(\"Unable to create EventManager\"); // Create the firecracker metrics object responsible for periodically printing metrics. let firecracker_metrics = Arc::new(Mutex::new(metrics::PeriodicMetrics::new())); event_manager.add_subscriber(firecracker_metrics.clone()); // Build the microVm. We can ignore VmResources since it's not used without api. let (_, vmm) = match build_microvm_from_json( seccomp_filters, &mut event_manager, // Safe to unwrap since '--no-api' requires this to be set. config_json.unwrap(), instance_info, bool_timer_enabled, mmds_size_limit, metadata_json, ) { Ok((res, vmm)) => (res, vmm), Err(exit_code) => return exit_code, }; // Start the metrics. firecracker_metrics .lock() .expect(\"Poisoned lock\") .start(metrics::WRITE_METRICS_PERIOD_MS); // Run the EventManager that drives everything in the microVM. loop { event_manager .run() .expect(\"Failed to start the event manager\"); if let Some(exit_code) = vmm.lock().unwrap().shutdown_exit_code() { return exit_code; } } } build_microvm_from_json就用到了核心模块vmm firecracker/src/vmm/src build_microvm_from_json build_microvm_from_json //根据json填充VmResources结构体并初始化 let mut vm_resources = VmResources::from_json() let vmm = vmm::builder::build_microvm_for_boot(&vm_resources) //建立guest内存, 思路是在host上mmap, 并记录内存region到变量 let guest_memory = create_guest_memory() //在x86上, 0-768M是内存, 768M到4G是MMIO, 4G以上还是内存 let arch_mem_regions = arch::arch_memory_regions(mem_size) vm_memory::create_guest_memory(&arch_mem_regions) 为每个region mmap一个region //先mmap一个大的size, size=原size+2个page, 属性是libc::PROT_NONE // Map the guarded range to PROT_NONE let guard_addr = unsafe { libc::mmap( std::ptr::null_mut(), guarded_size, libc::PROT_NONE, libc::MAP_ANONYMOUS | libc::MAP_PRIVATE | libc::MAP_NORESERVE, -1, 0, ) }; //再在刚刚map的region里面, 用原size map一个读写region // Inside the protected range, starting with guard_addr + PAGE_SIZE, // map the requested range with received protection and flags let region_addr = unsafe { libc::mmap( region_start_addr as *mut libc::c_void, //前面返回的addr加个page size, prot, flags | libc::MAP_FIXED, fd, offset as libc::off_t, ) }; //最后build MmapRegion并返回, 用的是https://github.com/rust-vmm/vm-memory //到这里好像只是生成GuestMemoryMmap数据结果, 并没有实际操作啥 //加载linux 内核, 代码在firecracker/src/vmm/src/builder.rs let entry_addr = load_kernel(boot_config, &guest_memory)?; let kernel_file = 先open kernel文件 //使用了https://github.com/rust-vmm/linux-loader //下面的Loader在x86上是ELF, 在ARM上是PE //把kernel_file加载到guest_memory let entry_addr = Loader::load::( &guest_memory, &kernel_file, arch::get_kernel_start()) //上面这个get_kernel_start()在x86上是1MB, aarch64上是2GB //先读elf header, 解析所有program header到 let mut phdrs: Vec = vec![]; for 每个phdr //写入guest内存, 似乎只是写入host上mmap的内存. 可能后面会用kvm的api把这些内存映射成guest内存 guest_mem.read_exact_from(mem_offset, kernel_image, phdr.p_filesz as usize) //从guest_memory find 一个region, 并写入initrd的内容; //在我们的配置里, initrd是null let initrd = load_initrd_from_config(boot_config, &guest_memory)?; //重写cmdline, 再原基础上增加virtio等配置 //创建VM let (mut vmm, mut vcpus) = create_vmm_and_vcpus( instance_info, event_manager, guest_memory, None, track_dirty_pages, vcpu_config.vcpu_count, )?; // Set up Kvm Vm and register memory regions. //调用kvm-ioctls let mut vm = setup_kvm_vm(&guest_memory, track_dirty_pages)?; //open /dev/kvm, 然后ioctl KVM_CREATE_VM let mut vm = Vm::new() vm.memory_init() //每个region调用 ioctl KVM_SET_USER_MEMORY_REGION //MMIO_MEM_START在x86上是(4G-768M), 在aarch64上是1G //IRQ_BASE到IRQ_MAX在x86上是5到23, 在aarch64上是32到128 //这里说的是virtio设备用的irq号范围 //mmio_device_manager包括mmio_base, irq, 和bus let mmio_device_manager = MMIODeviceManager::new(arch::MMIO_MEM_START, (arch::IRQ_BASE, arch::IRQ_MAX)); //IrqManager是管理(first..last)irq范围的简单结构体 IrqManager::new() //device的bus是个BtreeMap组织的按地址空间划分的设备的集合 devices::Bus::new() //创建中断控制器 setup_interrupt_controller(&mut vm)?; //x86上是ioctl KVM_CREATE_IRQCHIP //aarch64上是GICv2::create(vm, vcpu_count) vm.setup_irqchip() //新建个eventfd let vcpus_exit_evt = EventFd::new(libc::EFD_NONBLOCK) vcpus = create_vcpus(&vm, vcpu_count, &vcpus_exit_evt) //for里创建n个vCPU, ioctl KVM_CREATE_VCPU let vcpu = Vcpu::new() vcpu.kvm_vcpu.init(vm.fd()) set_stdout_nonblocking(); // servial device是pio设备, 在x86上有, aarch64上没有 let serial_device = setup_serial_device(event_manager, stdin, stdout) //由Serial device写1产生event let interrupt_evt = EventFdTrigger::new(EventFd::new(EFD_NONBLOCK)) //表示in buffer ready let kick_stdin_read_evt = EventFdTrigger::new(EventFd::new(EFD_NONBLOCK)) //SerialWrapper是event和Servial的桥梁 let serial = SerialWrapper { serial: Serial::with_events( interrupt_evt, SerialEventsWrapper { metrics: METRICS.uart.clone(), buffer_ready_event_fd: Some(kick_stdin_read_evt), }, out, ), input: Some(input), } //加入event manager, 最后的event loop里面会监听stdin和kick_stdin_read_evt fd event_manager.add_subscriber(serial.clone()); //只有x86有pio device, 把上面的serial_device加入到pio_device_manager let pio_device_manager = create_pio_dev_manager_with_legacy_devices(&vm, serial_device, reset_evt) let vmm = Vmm { events_observer: Some(Box::new(SerialStdin::get())), instance_info: instance_info.clone(), shutdown_exit_code: None, vm, guest_memory, uffd, vcpus_handles: Vec::new(), vcpus_exit_evt, mmio_device_manager, #[cfg(target_arch = \"x86_64\")] pio_device_manager, }; //最后返回vmm, vcpus Ok((vmm, vcpus)) //这个是给测试用的, kernel启动完成后, test版本的init会直接写/dev/mem某个地址魔术字(123) attach_boot_timer_device(&mu vmm, request_ts)?; let boot_timer = devices::pseudo::BootTimer::new(request_ts); //在mmio里面分配地址空间, 所谓的注册就是按地址空间assign设备, 设备有读写函数 vmm.mmio_device_manager.register_mmio_boot_timer(boot_timer) //目前balloon设备没使能 attach_balloon_device(&mut vmm, &mut boot_cmdline, balloon, event_manager)?; attach_virtio_device(event_manager, vmm, id, balloon.clone(), cmdline) event_manager.add_subscriber(device.clone()); let device = MmioTransport::new(vmm.guest_memory().clone(), device); //分配mmio地址范围, 注册到mmio manager; 并修改cmdline vmm.mmio_device_manager.register_mmio_virtio_for_boot(vmm.vm.fd(), id, device, cmdline) //可能有多个virtio块设备 attach_block_devices( &mut vmm, &mut boot_cmdline, vm_resources.block.list.iter(), event_manager, )?; for 每个 block //如果是root device, 就增加cmdline \"root=/dev/vda\"或\"root=PARTUUID=partuuid\" //见下面的函数分析 attach_virtio_device(event_manager, vmm, id, block.clone(), cmdline)?; //可能有多个virtio net设备 attach_net_devices( &mut vmm, &mut boot_cmdline, vm_resources.net_builder.iter(), event_manager, )?; //对应virtio socket device //guest可以通过AF_VSOCK通过vsock device和host的AF_UNIX socket通信 attach_unixsock_vsock_device(&mut vmm, &mut boot_cmdline, unix_vsock, event_manager)?; configure_system_for_boot( &vmm, vcpus.as_mut(), vcpu_config, entry_addr, &initrd, boot_cmdline, )?; //启动vcpu到pause状态 // Move vcpus to their own threads and start their state machine in the 'Paused' state. vmm.start_vcpus( vcpus, seccomp_filters .get(\"vcpu\") .ok_or_else(|| MissingSeccompFilters(\"vcpu\".to_string()))? .clone(), ) //给每个vcpu起个thread thread::Builder::new().spawn(move || { //Runs the vCPU in KVM context in a loop. Handles KVM_EXITs then goes back in. //run的逻辑是执行StateMachine循环 //state machine从paused开始 self.run(filter); //状态机循环 while let Some(state_fn) = state_machine.function { // Run the current state handler, and get the next one. state_machine = state_fn(machine); } }) //使能seccomp seccompiler::apply_filter() // The vcpus start off in the `Paused` state, let them run. vmm.resume_vm().map_err(Internal)?; self.mmio_device_manager.kick_devices(); //对每个vCPU send event let vmm = Arc::new(Mutex::new(vmm)); event_manager.add_subscriber(vmm.clone()); VmResources定义如下: 一个VMM就由block vsock balloon net等builder构成 #[derive(Default)] pub struct VmResources { /// The vCpu and memory configuration for this microVM. vm_config: VmConfig, /// The boot configuration for this microVM. boot_config: Option, /// The block devices. pub block: BlockBuilder, /// The vsock device. pub vsock: VsockBuilder, /// The balloon device. pub balloon: BalloonBuilder, /// The network devices builder. pub net_builder: NetBuilder, /// The optional Mmds data store. // This is initialised on demand (if ever used), so that we don't allocate it unless it's // actually used. pub mmds: Option>>, /// Data store limit for the mmds. pub mmds_size_limit: usize, /// Whether or not to load boot timer device. pub boot_timer: bool, } event manager https://github.com/rust-vmm/event-manager 使用了epoll机制的事件驱动库 基本上是个epoll的event loop, event subscriber注册的时候掉哟init, 在loop里有对应的event就调用process. aarch64 物理内存layout // ==== Address map in use in ARM development systems today ==== // // - 32-bit - - 36-bit - - 40-bit - //1024GB + + +-------------------+ devices::Bus 一个device都对应一段地址空间, 一个bus包括多个device, 按BtreeMap组织, key是device的地址范围, value是BusDevice /// A device container for routing reads and writes over some address space. /// /// This doesn't have any restrictions on what kind of device or address space this applies to. The /// only restriction is that no two devices can overlap in this address space. #[derive(Clone, Default)] pub struct Bus { //bus下面是BtreeMap管理的device devices: BTreeMap>>, } Bus有get_device, insert, read, write方法. read和write的基本逻辑是通过地址来判断是哪个device, 然后lock这个设备, 然后read/write 比如: /// Reads data from the device that owns the range containing `addr` and puts it into `data`. /// /// Returns true on success, otherwise `data` is untouched. pub fn read(&self, addr: u64, data: &mut [u8]) -> bool { //self.get_device(addr)返回(offset,dev), offset就是\"设备内\"偏移地址 if let Some((offset, dev)) = self.get_device(addr) { // OK to unwrap as lock() failing is a serious error condition and should panic. dev.lock() .expect(\"Failed to acquire device lock\") .read(offset, data); true } else { false } } 底层serial vm-superio-0.5.0/src/serial.rs serial是个泛型的结构体: The serial console emulation is done by emulating a serial COM port. Each serial COM port (COM1-4) has an associated Port I/O address base and 12 registers mapped into 8 consecutive Port I/O locations (with the first one being the base). This structure emulates the registers that make sense for UART 16550 (and below) and helps in the interaction between the driver and device by using a Trigger object for notifications. It also writes the guest's output to an out Write object. serial模拟了UART的16550的12个寄存器 用法 use std::io::{sink, Error, Result}; use std::ops::Deref; use vm_superio::Trigger; use vm_superio::Serial; use vmm_sys_util::eventfd::EventFd; struct EventFdTrigger(EventFd); impl Trigger for EventFdTrigger { type E = Error; fn trigger(&self) -> Result { self.write(1) } } impl Deref for EventFdTrigger { type Target = EventFd; fn deref(&self) -> &Self::Target { &self.0 } } impl EventFdTrigger { pub fn new(flag: i32) -> Self { EventFdTrigger(EventFd::new(flag).unwrap()) } pub fn try_clone(&self) -> Self { EventFdTrigger((**self).try_clone().unwrap()) } } let intr_evt = EventFdTrigger::new(libc::EFD_NONBLOCK); let mut serial = Serial::new(intr_evt.try_clone(), Vec::new()); // std::io::Sink can be used if user is not interested in guest's output. let serial_with_sink = Serial::new(intr_evt, sink()); // Write 0x01 to THR register. serial.write(0, 0x01).unwrap(); // Read from RBR register. let value = serial.read(0); // Send more bytes to the guest in one shot. let input = &[b'a', b'b', b'c']; // Before enqueuing bytes we first check if there is enough free space // in the FIFO. if serial.fifo_capacity() >= input.len() { serial.enqueue_raw_bytes(input).unwrap(); } Serial结构体 这是个泛型, 需要用三个trait: Trigger, SerialEvents, Write来实例化. pub struct Serial { // Some UART registers. baud_divisor_low: u8, baud_divisor_high: u8, interrupt_enable: u8, interrupt_identification: u8, line_control: u8, line_status: u8, modem_control: u8, modem_status: u8, scratch: u8, // This is the buffer that is used for achieving the Receiver register // functionality in FIFO mode. Reading from RBR will return the oldest // unread byte from the RX FIFO. in_buffer: VecDeque, // Used for notifying the driver about some in/out events. interrupt_evt: T, events: EV, out: W, } 三个Trait pub trait SerialEvents { /// The driver reads data from the input buffer. fn buffer_read(&self); /// The driver successfully wrote one byte to serial output. fn out_byte(&self); /// An error occurred while writing a byte to serial output resulting in a lost byte. fn tx_lost_byte(&self); /// This event can be used by the consumer to re-enable events coming from /// the serial input. fn in_buffer_empty(&self); } //一般都是EventFD, 用于trigger通知guest driver? pub trait Trigger { /// Underlying type for the potential error conditions returned by `Self::trigger`. type E; /// Trigger an event. fn trigger(&self) -> Result; } //Write就是io哪个Write 自定义Error /// Errors encountered while handling serial console operations. #[derive(Debug)] pub enum Error { /// Failed to trigger interrupt. Trigger(E), /// Couldn't write/flush to the given destination. IOError(io::Error), /// No space left in FIFO. FullFifo, } 用NoEvents的实例化的Serial NoEvents结构体就是实现了一个啥也不干的SerialEvents pub struct NoEvents; impl SerialEvents for NoEvents { fn buffer_read(&self) {} fn out_byte(&self) {} fn tx_lost_byte(&self) {} fn in_buffer_empty(&self) {} } 一个更具体的实例化: impl Serial { /// Creates a new `Serial` instance which writes the guest's output to /// `out` and uses `trigger` object to notify the driver about new /// events. /// /// # Arguments /// * `trigger` - The Trigger object that will be used to notify the driver /// about events. /// * `out` - An object for writing guest's output to. In case the output /// is not of interest, /// [std::io::Sink](https://doc.rust-lang.org/std/io/struct.Sink.html) /// can be used here. /// /// # Example /// /// You can see an example of how to use this function in the /// [`Example` section from `Serial`](struct.Serial.html#example). pub fn new(trigger: T, out: W) -> Serial { Self::with_events(trigger, NoEvents, out) } } 同样的Serial是范围更大的泛型: impl Serial { /// Creates a new `Serial` instance which writes the guest's output to /// `out`, uses `trigger` object to notify the driver about new /// events, and invokes the `serial_evts` implementation of `SerialEvents` /// during operation. /// /// # Arguments /// * `trigger` - The `Trigger` object that will be used to notify the driver /// about events. /// * `serial_evts` - The `SerialEvents` implementation used to track the occurrence /// of significant events in the serial operation logic. /// * `out` - An object for writing guest's output to. In case the output /// is not of interest, /// [std::io::Sink](https://doc.rust-lang.org/std/io/struct.Sink.html) /// can be used here. pub fn with_events(trigger: T, serial_evts: EV, out: W) -> Self { //用了很多const定义个u8的常量, 比如DEFAULT_BAUD_DIVISOR_LOW是0x0C Serial { baud_divisor_low: DEFAULT_BAUD_DIVISOR_LOW, baud_divisor_high: DEFAULT_BAUD_DIVISOR_HIGH, interrupt_enable: DEFAULT_INTERRUPT_ENABLE, interrupt_identification: DEFAULT_INTERRUPT_IDENTIFICATION, line_control: DEFAULT_LINE_CONTROL, line_status: DEFAULT_LINE_STATUS, modem_control: DEFAULT_MODEM_CONTROL, modem_status: DEFAULT_MODEM_STATUS, scratch: DEFAULT_SCRATCH, in_buffer: VecDeque::new(), interrupt_evt: trigger, events: serial_evts, out, } } /// Provides a reference to the interrupt event object. pub fn interrupt_evt(&self) -> &T { &self.interrupt_evt } /// Provides a reference to the serial events object. pub fn events(&self) -> &EV { &self.events } //具体操作, 私有方法, 基本上是对结构体的各field进行操作 fn is_dlab_set(&self) -> bool { (self.line_control & LCR_DLAB_BIT) != 0 } //还有很多, 省略 //读写函数, 谁来读写? driver //write不是io Write的格式, offset是预定义的常量表中的常量 /// Handles a write request from the driver at `offset` offset from the /// base Port I/O address. /// /// # Arguments /// * `offset` - The offset that will be added to the base PIO address /// for writing to a specific register. /// * `value` - The byte that should be written. /// /// # Example /// /// You can see an example of how to use this function in the /// [`Example` section from `Serial`](struct.Serial.html#example). pub fn write(&mut self, offset: u8, value: u8) -> Result> { match offset { DLAB_LOW_OFFSET if self.is_dlab_set() => self.baud_divisor_low = value, DLAB_HIGH_OFFSET if self.is_dlab_set() => self.baud_divisor_high = value, //关键路径, 每次写入一个字节; 写到stdout DATA_OFFSET => { let res = self .out //重点是这里, 这个out一般是stdout, guest driver的write, 通过这里的Serial Device(Self), 写到stdout .write_all(&[value]) .map_err(Error::IOError) .and_then(|_| self.out.flush().map_err(Error::IOError)) .map(|_| self.events.out_byte()) .map_err(|err| { self.events.tx_lost_byte(); err }); // Because we cannot block the driver, the THRE interrupt is sent // irrespective of whether we are able to write the byte or not self.thr_empty_interrupt().map_err(Error::Trigger)?; return res; } _ => {} } Ok(()) } //读的逻辑是从self.in_buffer pop出一个字节, 返回给调用者. /// Handles a read request from the driver at `offset` offset from the /// base Port I/O address. /// /// Returns the read value. /// /// # Arguments /// * `offset` - The offset that will be added to the base PIO address /// for reading from a specific register. /// /// # Example /// /// You can see an example of how to use this function in the /// [`Example` section from `Serial`](struct.Serial.html#example). pub fn read(&mut self, offset: u8) -> u8 { match offset { DLAB_LOW_OFFSET if self.is_dlab_set() => self.baud_divisor_low, DLAB_HIGH_OFFSET if self.is_dlab_set() => self.baud_divisor_high, DATA_OFFSET => { // Here we emulate the reset method for when RDA interrupt // was raised (i.e. read the receive buffer and clear the // interrupt identification register and RDA bit when no // more data is available). self.del_interrupt(IIR_RDA_BIT); let byte = self.in_buffer.pop_front().unwrap_or_default(); if self.in_buffer.is_empty() { self.clear_lsr_rda_bit(); self.events.in_buffer_empty(); } self.events.buffer_read(); byte } LCR_OFFSET => self.line_control, MCR_OFFSET => self.modem_control, LSR_OFFSET => self.line_status, _ => 0, } } /// Returns how much space is still available in the FIFO. /// /// # Example /// /// You can see an example of how to use this function in the /// [`Example` section from `Serial`](struct.Serial.html#example). #[inline] pub fn fifo_capacity(&self) -> usize { FIFO_SIZE - self.in_buffer.len() } /// Helps in sending more bytes to the guest in one shot, by storing /// `input` bytes in UART buffer and letting the driver know there is /// some pending data to be read by setting RDA bit and its corresponding /// interrupt when not already triggered. /// /// # Arguments /// * `input` - The data to be sent to the guest. /// /// # Returns /// /// The function returns the number of bytes it was able to write to the fifo, /// or `FullFifo` error when the fifo is full. Users can use /// [`fifo_capacity`](#method.fifo_capacity) before calling this function /// to check the available space. /// /// # Example /// /// You can see an example of how to use this function in the /// [`Example` section from `Serial`](struct.Serial.html#example). pub fn enqueue_raw_bytes(&mut self, input: &[u8]) -> Result> { let mut write_count = 0; if !self.is_in_loop_mode() { if self.fifo_capacity() == 0 { return Err(Error::FullFifo); } write_count = std::cmp::min(self.fifo_capacity(), input.len()); if write_count > 0 { self.in_buffer.extend(&input[0..write_count]); self.set_lsr_rda_bit(); //就是给Self.interrupt_evt这个eventfd写1 self.received_data_interrupt().map_err(Error::Trigger)?; } } Ok(write_count) } } driver在哪里读写? 待续 SerialWrapper SerialWrapper包括了底层Serial设备和input SerialWrapper: firecracker/src/devices/src/legacy/serial.rs 底层Serial: vm-superio-0.5.0/src/serial.rs pub struct SerialWrapper { pub serial: Serial, pub input: Option>, } 这个结构体是Serial device和event loop之间的桥梁. 之间用eventfd来通知 Host VMM Guest stdin/stdout Serial设备 read/write driver 具体来讲, guest driver通过BusDevice向Serial设备发出读写请求, VMM调用Serial设备的read/write函数来完成响应并在某些情况下触发中断通知(可能是给PioManager), 比如在给in_buffer读到data后产生received_data_interrupt. Serial结构体来维护UART16550的硬件的寄存器level的行为. 从stdin读输入发给guest流程 SerialWrapper的的实例实现了recv_bytes impl SerialWrapper { fn recv_bytes(&mut self) -> io::Result { let avail_cap = self.serial.fifo_capacity(); if let Some(input) = self.input.as_mut() { let mut out = vec![0u8; avail_cap]; //指定cap的vec //从stdin读 let count = input.read(&mut out)?; //看来&mut Vec能当作&mut [u8] if count > 0 { self.serial //这个有点讲究了, raw_input并不是底层Serial的方法, 而是本文件定义的trait //底层调用的是Servial设备的enqueue_raw_bytes方法, 往底层Servial的in_buffer填数据 .raw_input(&out[..count]) .map_err(|_| io::Error::from_raw_os_error(libc::ENOBUFS))?; } return Ok(count); } Err(io::Error::from_raw_os_error(libc::ENOTTY)) } } 这个recv_bytes被MutEventSubscriber trait调用, SerialWrapper也实现了MutEventSubscriber 里面的process就调用了recv_bytes 具体没怎么看懂 impl MutEventSubscriber for SerialWrapper { //process会在发生event的时候被调用, 传入event和ops用来表示event类型和维护event //可能有多个fd的源头, 但都共用这一个process函数. /// Handle events on the serial input fd. fn process(&mut self, event: Events, ops: &mut EventOps) { #[inline] fn unregister_source(ops: &mut EventOps, source: &T) { match ops.remove(Events::new(source, EventSet::IN)) { Ok(_) => (), Err(_) => error!(\"Could not unregister source fd: {}\", source.as_raw_fd()), } } let input_fd = self.serial_input_fd(); let buffer_ready_fd = self.buffer_ready_evt_fd(); if input_fd (), Err(err) => { error!(\"Detach serial device input source due to error in consuming the buffer ready event: {:?}\", err); unregister_source(ops, &input_fd); unregister_source(ops, &buffer_ready_fd); return; } } } // We expect to receive: `EventSet::IN`, `EventSet::HANG_UP` or // `EventSet::ERROR`. To process all these events we just have to // read from the serial input. match self.recv_bytes() { Ok(count) => { // Handle EOF if the event came from the input source. if input_fd == event.fd() && count == 0 { unregister_source(ops, &input_fd); unregister_source(ops, &buffer_ready_fd); warn!(\"Detached the serial input due to peer close/error.\"); } } Err(e) => { match e.raw_os_error() { Some(errno) if errno == libc::ENOBUFS => { unregister_source(ops, &input_fd); } //这里是none-block read没东西的时候会返回EAGAIN或者EWOULDBLOCK, 都差不多 Some(errno) if errno == libc::EWOULDBLOCK => { self.handle_ewouldblock(ops); } Some(errno) if errno == libc::ENOTTY => { error!(\"The serial device does not have the input source attached.\"); unregister_source(ops, &input_fd); unregister_source(ops, &buffer_ready_fd); } Some(_) | None => { // Unknown error, detach the serial input source. unregister_source(ops, &input_fd); unregister_source(ops, &buffer_ready_fd); warn!(\"Detached the serial input due to peer close/error.\"); } } } } } /// Initial registration of pollable objects. /// If serial input is present, register the serial input FD as readable. fn init(&mut self, ops: &mut EventOps) { //input就是stdin, buffer_ready_event_fd就是前面的kick_stdin_read_evt这个eventfd if self.input.is_some() && self.serial.events().buffer_ready_event_fd.is_some() { let serial_fd = self.serial_input_fd(); let buf_ready_evt = self.buffer_ready_evt_fd(); if serial_fd != -1 { //实际上是把stdin加到epoll if let Err(e) = ops.add(Events::new(&serial_fd, EventSet::IN)) { warn!(\"Failed to register serial input fd: {}\", e); } } //这个实际上是kick_stdin_read_evt这个eventfd if let Err(e) = ops.add(Events::new(&buf_ready_evt, EventSet::IN)) { warn!(\"Failed to register serial buffer ready event: {}\", e); } } } } 实现了BusDevice的按地址读写的trait 按总线地址读写, 最终转化为设备内偏移地址读写 impl BusDevice for SerialWrapper { //读是从内部in_buffer读 fn read(&mut self, offset: u64, data: &mut [u8]) { if data.len() != 1 { self.serial.events().metrics.missed_read_count.inc(); return; } data[0] = self.serial.read(offset as u8); } //写是写到stdout fn write(&mut self, offset: u64, data: &[u8]) { if data.len() != 1 { self.serial.events().metrics.missed_write_count.inc(); return; } if let Err(e) = self.serial.write(offset as u8, data[0]) { // Counter incremented for any handle_write() error. error!(\"Failed the write to serial: {:?}\", e); self.serial.events().metrics.error_count.inc(); } } } attach_virtio_device /// Attaches a VirtioDevice device to the device manager and event manager. fn attach_virtio_device( event_manager: &mut EventManager, vmm: &mut Vmm, id: String, device: Arc>, cmdline: &mut LoaderKernelCmdline, ) -> std::result::Result { use self::StartMicrovmError::*; //注册事件订阅 event_manager.add_subscriber(device.clone()); let device = MmioTransport::new(vmm.guest_memory().clone(), device); vmm.mmio_device_manager .register_mmio_virtio_for_boot(vmm.vm.fd(), id, device, cmdline) //分配mmio的addr len和irq资源, 策略是依次顺序分配 let mmio_slot = self.allocate_new_slot(1)?; self.register_mmio_virtio(vm, device_id, mmio_device, &mmio_slot)?; let locked_device = mmio_device.locked_device(); identifier = (DeviceType::Virtio(locked_device.device_type()), device_id); //对每个queue for (i, queue_evt) in locked_device.queue_events().iter().enumerate() //NOTIFY_REG_OFFSET是0x50, 加上slot.addr这个mmio device的base地址 //注意, 并不是所有的mmio设备的地址空间访问都会触发event, 这个io_addr只是特定地址, 用来notify device的. let io_addr = IoEventAddress::Mmio(slot.addr + u64::from(devices::virtio::NOTIFY_REG_OFFSET)); //这个queue_evt是每个queue的eventfd, 写io_addr就会触发event, 说明guest driver要通知device来干活了 //调用了kvm的ioctl vm.register_ioevent(queue_evt, &io_addr, i as u32) //写这指定地址的时候发event到queue_evt vm.register_irqfd() //注册中断注入guest的eventfd和irq号, 调用kvm ioctl KVM_IRQFD; 意思是只要这个eventfd被写入, 内核的kvm模块就会给guest发指定的irq号中断. register_mmio_device() } vm.register_ioevent(queue_evt, &io_addr, i as u32) 三个参数如下: fd - EventFd which will be signaled. When signaling, the usual vmexit to userspace is prevented.addr - Address being written to.datamatch - Limits signaling fd to only the cases where the value being written is equal to this parameter. The size of datamatch is important and it must match the expected size of the guest's write. guest驱动需要某种方法来通知device, kvm的ioeventfd就是干这个用的. 用eventfd的好处是这个guest driver到device的通知不需要vmexit. Registers an event to be signaled whenever a certain address is written to. When signaling, the usual vmexit to userspace is prevented. 对应KVM的KVM_IOEVENTFD This ioctl attaches or detaches an ioeventfd to a legal pio/mmio address within the guest. A guest write in the registered address will signal the provided event instead of triggering an exit. If datamatch flag is set, the event will be signaled only if the written value to the registered address is equal to datamatch in struct kvm_ioeventfd. 注意: 这个对每个queue都调用了vm.register_ioevent(queue_evt, &io_addr, i as u32), 作用是给io_addr地址绑定一个queue_evt, 当driver写i到这个io_aadr地址的时候, signal给queue_evt. 但问题是, 如果是多个queue, 多个queue_evt都\"绑定\"到同一个io_addr.我猜测这个API是支持多个一个地址对应多个eventfd的, 可能由datamatch的值来区分这个signal发送到哪个eventfd vm.register_irqfd vm.register_irqfd(locked_device.interrupt_evt(), slot.irqs[0]) 调用kvm的ioctl的KVM_IRQFD(见下面) IRQFD是device写eventfd, 通过kvm触发guest中断的机制. kvm的irq相关API https://www.kernel.org/doc/html/latest/virt/kvm/api.html KVM_CREATE_IRQCHIP Creates an interrupt controller model in the kernel. On x86, creates a virtual ioapic, a virtual PIC (two PICs, nested), and sets up future vcpus to have a local APIC. IRQ routing for GSIs 0-15 is set to both PIC and IOAPIC; GSI 16-23 only go to the IOAPIC. On arm64, a GICv2 is created. Any other GIC versions require the usage of KVM_CREATE_DEVICE, which also supports creating a GICv2. Using KVM_CREATE_DEVICE is preferred over KVM_CREATE_IRQCHIP for GICv2. On s390, a dummy irq routing table is created. KVM_SET_GSI_ROUTING Sets the GSI routing table entries, overwriting any previously set entries. KVM_IRQFD Allows setting an eventfd to directly trigger a guest interrupt. kvm_irqfd.fd specifies the file descriptor to use as the eventfd and kvm_irqfd.gsi specifies the irqchip pin toggled by this event. When an event is triggered on the eventfd, an interrupt is injected into the guest using the specified gsi pin. The irqfd is removed using the KVM_IRQFD_FLAG_DEASSIGN flag, specifying both kvm_irqfd.fd and kvm_irqfd.gsi. With KVM_CAP_IRQFD_RESAMPLE, KVM_IRQFD supports a de-assert and notify mechanism allowing emulation of level-triggered, irqfd-based interrupts. When KVM_IRQFD_FLAG_RESAMPLE is set the user must pass an additional eventfd in the kvm_irqfd.resamplefd field. When operating in resample mode, posting of an interrupt through kvm_irq.fd asserts the specified gsi in the irqchip. When the irqchip is resampled, such as from an EOI, the gsi is de-asserted and the user is notified via kvm_irqfd.resamplefd. It is the user’s responsibility to re-queue the interrupt if the device making use of it still requires service. Note that closing the resamplefd is not sufficient to disable the irqfd. The KVM_IRQFD_FLAG_RESAMPLE is only necessary on assignment and need not be specified with KVM_IRQFD_FLAG_DEASSIGN. On arm64, gsi routing being supported, the following can happen: in case no routing entry is associated to this gsi, injection fails in case the gsi is associated to an irqchip routing entry, irqchip.pin + 32 corresponds to the injected SPI ID. in case the gsi is associated to an MSI routing entry, the MSI message and device ID are translated into an LPI (support restricted to GICv3 ITS in-kernel emulation). KVM_CREATE_DEVICE Creates an emulated device in the kernel. The file descriptor returned in fd can be used with KVM_SET/GET/HAS_DEVICE_ATTR. If the KVM_CREATE_DEVICE_TEST flag is set, only test whether the device type is supported (not necessarily whether it can be created in the current vm). Individual devices should not define flags. Attributes should be used for specifying any behavior that is not implied by the device type number. 都有哪些可以被create Devices ARM Virtual Interrupt Translation Service (ITS) ARM Virtual Generic Interrupt Controller v2 (VGIC) ARM Virtual Generic Interrupt Controller v3 and later (VGICv3) MPIC interrupt controller FLIC (floating interrupt controller) Generic vcpu interface VFIO virtual device Generic vm interface XICS interrupt controller POWER9 eXternal Interrupt Virtualization Engine (XIVE Gen1) ARM gic v3 Only one VGIC instance may be instantiated through this API. The created VGIC will act as the VM interrupt controller, requiring emulated user-space devices to inject interrupts to the VGIC instead of directly to CPUs. It is not possible to create both a GICv3 and GICv2 on the same VM. Creating a guest GICv3 device requires a host GICv3 as well. KVM_DEV_ARM_VGIC_GRP_ADDR 定义了vgic寄存器在guest物理地址空间的基地址 KVM_VGIC_V3_ADDR_TYPE_DIST (rw, 64-bit) Base address in the guest physical address space of the GICv3 distributor register mappings. Only valid for KVM_DEV_TYPE_ARM_VGIC_V3. This address needs to be 64K aligned and the region covers 64 KByte. KVM_VGIC_V3_ADDR_TYPE_REDIST (rw, 64-bit) Base address in the guest physical address space of the GICv3 redistributor register mappings. There are two 64K pages for each VCPU and all of the redistributor pages are contiguous. Only valid for KVM_DEV_TYPE_ARM_VGIC_V3. This address needs to be 64K aligned. MmioTransport mplements the MMIO transport for virtio devices. This requires 3 points of installation to work with a VM: Mmio reads and writes must be sent to this device at what is referred to here as MMIO base. Mmio::queue_evts must be installed at virtio::NOTIFY_REG_OFFSET offset from the MMIO base. Each event in the array must be signaled if the index is written at that offset. Mmio::interrupt_evt must signal an interrupt that the guest driver is listening to when it is written to. Typically one page (4096 bytes) of MMIO address space is sufficient to handle this transport and inner virtio device. 对应的结构体: pub struct MmioTransport { device: Arc>, // The register where feature bits are stored. pub(crate) features_select: u32, // The register where features page is selected. pub(crate) acked_features_select: u32, pub(crate) queue_select: u32, pub(crate) device_status: u32, pub(crate) config_generation: u32, mem: GuestMemoryMmap, pub(crate) interrupt_status: Arc, } impl MmioTransport impl MmioTransport { /// Constructs a new MMIO transport for the given virtio device. pub fn new(mem: GuestMemoryMmap, device: Arc>) -> MmioTransport { //这里小知识点: device.lock()返回的是mutextGuard, 它会在生命周期结束后自动调用unlock. //不用担心一直会lock, 因为device.lock()的声明周期只有下面一行 //这行结束了其实就已经unlock了. let interrupt_status = device.lock().expect(\"Poisoned lock\").interrupt_status(); //new这个结构体 MmioTransport { device, features_select: 0, acked_features_select: 0, queue_select: 0, device_status: device_status::INIT, config_generation: 0, mem, interrupt_status, } } pub fn locked_device(&self) -> MutexGuard { self.device.lock().expect(\"Poisoned lock\") } // Gets the encapsulated VirtioDevice. pub fn device(&self) -> Arc> { self.device.clone() } fn check_device_status(&self, set: u32, clr: u32) -> bool { self.device_status & (set | clr) == set } fn are_queues_valid(&self) -> bool { self.locked_device() .queues() .iter() .all(|q| q.is_valid(&self.mem)) } //注意到泛型U, 并没有约束; //这里的意思是入参d的类型是U, 表示默认值. fn with_queue(&self, d: U, f: F) -> U where F: FnOnce(&Queue) -> U, { match self .locked_device() .queues() .get(self.queue_select as usize) { Some(queue) => f(queue), None => d, } } fn with_queue_mut(&mut self, f: F) -> bool { if let Some(queue) = self .locked_device() .queues_mut() .get_mut(self.queue_select as usize) { f(queue); true } else { false } } fn update_queue_field(&mut self, f: F) { if self.check_device_status( device_status::FEATURES_OK, device_status::DRIVER_OK | device_status::FAILED, ) { self.with_queue_mut(f); } else { warn!( \"update virtio queue in invalid state 0x{:x}\", self.device_status ); } } fn reset(&mut self) { 重置结构体\"寄存器\" } //根据VirtIO Spec 1.0, section 2.1.1 and 3.1.1 //在device的write里面调用, 实际上是给guest的driver用的 fn set_device_status(&mut self, status: u32) { } } 实现BusDevice 根据virtIO规范MMIO transport方式:https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html#x1-1650002 impl BusDevice for MmioTransport { fn read(&mut self, offset: u64, data: &mut [u8]) { match offset { 0x00..=0xff if data.len() == 4 => { let v = match offset { 0x0 => MMIO_MAGIC_VALUE, 0x04 => MMIO_VERSION, 0x08 => self.locked_device().device_type(), 0x0c => VENDOR_ID, // vendor id 0x10 => { //32bit的feature flag let mut features = self .locked_device() .avail_features_by_page(self.features_select); if self.features_select == 1 { features |= 0x1; // enable support of VirtIO Version 1 } features } //对已经选中的queue(QueueSel), 读出queue内元素个数; Reading from the register returns the maximum size (number of elements) of the queue the device is ready to process or zero (0x0) if the queue is not available. 0x34 => self.with_queue(0, |q| u32::from(q.get_max_size())), //对已经选中的queue, 写1表示ready. 读是读上一次的值 0x44 => self.with_queue(0, |q| q.ready as u32), //中断状态寄存器, 要么是Used Buffer Notification(bit 0), 要么是Configuration Change Notification(bit 1) 0x60 => self.interrupt_status.load(Ordering::SeqCst) as u32, //设备状态寄存器. 0x70 => self.device_status, //配置空间原子性寄存器, 两次读的一样就是原子的? 0xfc => self.config_generation, _ => { warn!(\"unknown virtio mmio register read: 0x{:x}\", offset); return; } }; byte_order::write_le_u32(data, v); //注意这里, 使用byte_order的小端写 } 0x100..=0xfff => self.locked_device().read_config(offset - 0x100, data), _ => { warn!( \"invalid virtio mmio read: 0x{:x}:0x{:x}\", offset, data.len() ); } }; } fn write(&mut self, offset: u64, data: &[u8]) { fn hi(v: &mut GuestAddress, x: u32) { *v = (*v & 0xffff_ffff) | (u64::from(x) { let v = byte_order::read_le_u32(data); //按小端方式理解data match offset { //Device (host) features word selection. 写这个寄存器选择feature flag 0x14 => self.features_select = v, //Flags representing device features understood and activated by the driver 0x20 => { if self.check_device_status( device_status::DRIVER, device_status::FEATURES_OK | device_status::FAILED, ) { self.locked_device() .ack_features_by_page(self.acked_features_select, v); } else { warn!( \"ack virtio features in invalid state 0x{:x}\", self.device_status ); } } //Activated (guest) features word selection 0x24 => self.acked_features_select = v, //这个就是QueueSel, 也叫Virtual queue index, 从0开始 0x30 => self.queue_select = v, //Virtual queue size 0x38 => self.update_queue_field(|q| q.size = v as u16), //queue read //Writing one (0x1) to this register notifies the device that it can execute requests from this virtual queue. 0x44 => self.update_queue_field(|q| q.ready = v == 1), //中断应答 0x64 => { if self.check_device_status(device_status::DRIVER_OK, 0) { self.interrupt_status .fetch_and(!(v as usize), Ordering::SeqCst); } } //Device status: Writing non-zero values to this register sets the status flags, indicating the driver progress 0x70 => self.set_device_status(v), //Virtual queue’s Descriptor Area 64 bit long physical address 0x80 => self.update_queue_field(|q| lo(&mut q.desc_table, v)), 0x84 => self.update_queue_field(|q| hi(&mut q.desc_table, v)), //Virtual queue’s Driver Area 64 bit long physical address 0x90 => self.update_queue_field(|q| lo(&mut q.avail_ring, v)), 0x94 => self.update_queue_field(|q| hi(&mut q.avail_ring, v)), //Virtual queue’s Device Area 64 bit long physical address 0xa0 => self.update_queue_field(|q| lo(&mut q.used_ring, v)), 0xa4 => self.update_queue_field(|q| hi(&mut q.used_ring, v)), _ => { warn!(\"unknown virtio mmio register write: 0x{:x}\", offset); } } } 0x100..=0xfff => { if self.check_device_status(device_status::DRIVER, device_status::FAILED) { self.locked_device().write_config(offset - 0x100, data) } else { warn!(\"can not write to device config data area before driver is ready\"); } } _ => { warn!( \"invalid virtio mmio write: 0x{:x}:0x{:x}\", offset, data.len() ); } } } } device device的状态有 /// Enum that indicates if a VirtioDevice is inactive or has been activated /// and memory attached to it. pub enum DeviceState { Inactive, Activated(GuestMemoryMmap), } 这个enum也有方法, 其中mem()方法返回GuestmemoryMmap impl DeviceState { /// Checks if the device is activated. pub fn is_activated(&self) -> bool { match self { DeviceState::Inactive => false, DeviceState::Activated(_) => true, } } /// Gets the memory attached to the device if it is activated. pub fn mem(&self) -> Option { match self { DeviceState::Activated(ref mem) => Some(mem), DeviceState::Inactive => None, } } } IrqTrigger IrqTrigger包含一个eventFd叫irq_evt, trigger_irq方法就是写这个eventFd. /// Helper struct that is responsible for triggering guest IRQs pub struct IrqTrigger { pub(crate) irq_status: Arc, pub(crate) irq_evt: EventFd, } impl IrqTrigger { pub fn new() -> std::io::Result { Ok(Self { irq_status: Arc::new(AtomicUsize::new(0)), irq_evt: EventFd::new(libc::EFD_NONBLOCK)?, }) } pub fn trigger_irq(&self, irq_type: IrqType) -> std::result::Result { let irq = match irq_type { IrqType::Config => VIRTIO_MMIO_INT_CONFIG, IrqType::Vring => VIRTIO_MMIO_INT_VRING, }; //irq状态 self.irq_status.fetch_or(irq as usize, Ordering::SeqCst); //eventfd写1 self.irq_evt.write(1).map_err(|e| { error!(\"Failed to send irq to the guest: {:?}\", e); e })?; Ok(()) } } VirtioDevice trait /// Trait for virtio devices to be driven by a virtio transport. /// /// The lifecycle of a virtio device is to be moved to a virtio transport, which will then query the /// device. The virtio devices needs to create queues, events and event fds for interrupts and expose /// them to the transport via get_queues/get_queue_events/get_interrupt/get_interrupt_status fns. pub trait VirtioDevice: AsAny + Send { /// Get the available features offered by device. fn avail_features(&self) -> u64; /// Get acknowledged features of the driver. fn acked_features(&self) -> u64; /// Set acknowledged features of the driver. /// This function must maintain the following invariant: /// - self.avail_features() & self.acked_features() = self.get_acked_features() fn set_acked_features(&mut self, acked_features: u64); fn has_feature(&self, feature: u64) -> bool { (self.acked_features() & 1 u32; /// Returns the device queues. fn queues(&self) -> &[Queue]; /// Returns a mutable reference to the device queues. fn queues_mut(&mut self) -> &mut [Queue]; /// Returns the device queues event fds. fn queue_events(&self) -> &[EventFd]; /// Returns the device interrupt eventfd. fn interrupt_evt(&self) -> &EventFd; /// Returns the current device interrupt status. fn interrupt_status(&self) -> Arc; /// The set of feature bits shifted by `page * 32`. fn avail_features_by_page(&self, page: u32) -> u32 { let avail_features = self.avail_features(); match page { // Get the lower 32-bits of the features bitfield. 0 => avail_features as u32, // Get the upper 32-bits of the features bitfield. 1 => (avail_features >> 32) as u32, _ => { warn!(\"Received request for unknown features page.\"); 0u32 } } } /// Acknowledges that this set of features should be enabled. fn ack_features_by_page(&mut self, page: u32, value: u32) { let mut v = match page { 0 => u64::from(value), 1 => u64::from(value) { warn!(\"Cannot acknowledge unknown features page: {}\", page); 0u64 } }; // Check if the guest is ACK'ing a feature that we didn't claim to have. let avail_features = self.avail_features(); let unrequested_features = v & !avail_features; if unrequested_features != 0 { warn!(\"Received acknowledge request for unknown feature: {:x}\", v); // Don't count these features as acked. v &= !unrequested_features; } self.set_acked_features(self.acked_features() | v); } /// Reads this device configuration space at `offset`. fn read_config(&self, offset: u64, data: &mut [u8]); /// Writes to this device configuration space at `offset`. fn write_config(&mut self, offset: u64, data: &[u8]); /// Performs the formal activation for a device, which can be verified also with `is_activated`. fn activate(&mut self, mem: GuestMemoryMmap) -> ActivateResult; /// Checks if the resources of this device are activated. fn is_activated(&self) -> bool; /// Optionally deactivates this device and returns ownership of the guest memory map, interrupt /// event, and queue events. fn reset(&mut self) -> Option)> { None } } VirtIO设备框图 virtIO net virtIO net的定义很复杂 pub struct Net { pub(crate) id: String, pub tap: Tap, //对接的tap设备 pub(crate) avail_features: u64, pub(crate) acked_features: u64, pub(crate) queues: Vec, pub(crate) queue_evts: Vec, pub(crate) rx_rate_limiter: RateLimiter, pub(crate) tx_rate_limiter: RateLimiter, pub(crate) rx_deferred_frame: bool, rx_deferred_irqs: bool, rx_bytes_read: usize, rx_frame_buf: [u8; MAX_BUFFER_SIZE], tx_iovec: Vec, tx_frame_buf: [u8; MAX_BUFFER_SIZE], pub(crate) irq_trigger: IrqTrigger, //中断触发, 里面是eventfd pub(crate) config_space: ConfigSpace, pub(crate) guest_mac: Option, pub(crate) device_state: DeviceState, pub(crate) activate_evt: EventFd, pub mmds_ns: Option, #[cfg(test)] pub(crate) mocks: Mocks, } Net实现了很多方法, 比如交换tap设备和queue的数据: impl Net { new_with_tap() id() guest_mac() iface_name() mmds_ns() signal_used_queue() {self.irq_trigger.trigger_irq()} signal_rx_used_queue() do_write_frame_to_guest( write_frame_to_guest() read_from_mmds_or_tap() //处理从tap设备来的数据, 然后给guest发irq: //self.signal_used_queue() -> self.irq_trigger.trigger_irq(IrqType::Vring) 还是写eventfd process_rx() handle_deferred_frame() resume_rx() process_tx() read_tap() process_rx_queue_event() process_tap_rx_event() process_tx_queue_event() process_virtio_queues() } Net实现了VirtioDevice 相对比较薄的一层, 实现了下面的方法: Net还实现了MutEventSubscriber impl MutEventSubscriber for Net { fn process(&mut self, event: Events, ops: &mut EventOps) { let source = event.fd(); let event_set = event.event_set(); // TODO: also check for errors. Pending high level discussions on how we want // to handle errors in devices. let supported_events = EventSet::IN; if !supported_events.contains(event_set) { warn!( \"Received unknown event: {:?} from source: {:?}\", event_set, source ); return; } if self.is_activated() { let virtq_rx_ev_fd = self.queue_evts[RX_INDEX].as_raw_fd(); let virtq_tx_ev_fd = self.queue_evts[TX_INDEX].as_raw_fd(); let rx_rate_limiter_fd = self.rx_rate_limiter.as_raw_fd(); let tx_rate_limiter_fd = self.tx_rate_limiter.as_raw_fd(); let tap_fd = self.tap.as_raw_fd(); let activate_fd = self.activate_evt.as_raw_fd(); // Looks better than C style if/else if/else. match source { _ if source == virtq_rx_ev_fd => self.process_rx_queue_event(), _ if source == tap_fd => self.process_tap_rx_event(), //tap设备来数据了 _ if source == virtq_tx_ev_fd => self.process_tx_queue_event(), _ if source == rx_rate_limiter_fd => self.process_rx_rate_limiter_event(), _ if source == tx_rate_limiter_fd => self.process_tx_rate_limiter_event(), _ if activate_fd == source => self.process_activate_event(ops), _ => { warn!(\"Net: Spurious event received: {:?}\", source); METRICS.net.event_fails.inc(); } } } else { warn!( \"Net: The device is not yet activated. Spurious event received: {:?}\", source ); } } fn init(&mut self, ops: &mut EventOps) { // This function can be called during different points in the device lifetime: // - shortly after device creation, // - on device activation (is-activated already true at this point), // - on device restore from snapshot. if self.is_activated() { self.register_runtime_events(ops); } else { self.register_activate_event(ops); } } } "},"notes/rust_firecracker_使用.html":{"url":"notes/rust_firecracker_使用.html","title":"firecracker使用","keywords":"","body":" 编译 编译firecracker 编译kernel 编译rootfs 运行 配置文件方式运行 rest API方式运行 devctr镜像 顶层cargo firecracker/tools/devtool脚本 cmd_build 先build seccompiler 再build rebase-snap build firecracker build jailer build_kernel build_rootfs firecracker/resources/tests/init.c firecracker/resources/tests/fillmem.c firecracker/resources/tests/readmem.c 做镜像 编译 先clone代码: git clone https://github.com/firecracker-microvm/firecracker 编译firecracker firecracker是rust写的, 但编译不需要本地依赖rust环境, 而是在docker内完成的. 使用了docker imagepublic.ecr.aws/firecracker/fcuvm:v35, 大小3.25G 因为使用了x86_64-unknown-linux-musl做为target, 所以最后的可执行文件是静态链接的 默认debug版本:tools/devtool build生成:build/cargo_target/x86_64-unknown-linux-musl/debug/firecracker 38M, 静态链接, 带符号 指定release版本tools/devtool build --release生成:build/cargo_target/x86_64-unknown-linux-musl/release/firecracker 4.1M, 静态链接, 带符号 编译kernel tools/devtool build_kernel -c resources/guest_configs/microvm-kernel-x86_64-5.10.config -n 8生成:build/kernel/linux-5.10/vmlinux-5.10-x86_64.bin 42M, 带符号的linux elf, 模块全部编入kernel. 编译rootfs tools/devtool build_rootfs -s 300MB生成:build/rootfs/bionic.rootfs.ext4 300M 运行 配置文件方式运行 build/cargo_target/x86_64-unknown-linux-musl/release/firecracker --api-sock /tmp/firecracker.socket --config-file myvmconfig.json会打印kernel启动过程, 并自动以root登陆 myvmconfig.json内容如下: { \"boot-source\": { \"kernel_image_path\": \"build/kernel/linux-5.10/vmlinux-5.10-x86_64.bin\", \"boot_args\": \"console=ttyS0 reboot=k panic=1 pci=off\", \"initrd_path\": null }, \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"build/rootfs/bionic.rootfs.ext4\", \"is_root_device\": true, \"partuuid\": null, \"is_read_only\": false, \"cache_type\": \"Unsafe\", \"io_engine\": \"Sync\", \"rate_limiter\": null } ], \"machine-config\": { \"vcpu_count\": 2, \"mem_size_mib\": 1024, \"smt\": false, \"track_dirty_pages\": false }, \"balloon\": null, \"network-interfaces\": [], \"vsock\": null, \"logger\": null, \"metrics\": null, \"mmds-config\": null } 跑的是ubuntu, 带systemd的 启动迅速 reboot会触发kernel退出, 但并不重启 没有网络接口 根文件系统挂在/dev/vda上 VM配置了1024M内存, 但运行时firecracker进程占用95M, 虚拟内存1032M. rest API方式运行 firecracker启动的时候要指定一个API socket, 每个VM一个. 使用这个socket, 可以用rest API方式来运行和管理VM. devctr镜像 devctr是开发中使用的镜像, 所有的操作都通过这个镜像完成. 基于ubuntu18 安装了常用的开发工具binutils-dev clang cmake gcc 等等 安装了rustcurl https://sh.rustup.rs -sSf | sh -s -- -y rustup target add x86_64-unknown-linux-musl rustup component add rustfmt rustup component add clippy-preview rustup install \"stable\" 使用了开源的init程序, 静态编译版本# Add the tini init binary. ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION_TAG}/tini-static-amd64 /sbin/tini RUN chmod +x /sbin/tini WORKDIR \"$FIRECRACKER_SRC_DIR\" ENTRYPOINT [\"/sbin/tini\", \"--\"] 顶层cargo cargo.toml [workspace] members = [\"src/firecracker\", \"src/jailer\", \"src/seccompiler\", \"src/rebase-snap\"] default-members = [\"src/firecracker\"] [profile.dev] panic = \"abort\" [profile.release] panic = \"abort\" lto = true [patch.crates-io] kvm-bindings = { git = \"https://github.com/firecracker-microvm/kvm-bindings\", tag = \"v0.5.0-1\", features = [\"fam-wrappers\"] } cargo的build系统会自动维护cargo.lock来描述版本信息. 下面的命令可以更新依赖的版本信息: $ cargo update # updates all dependencies $ cargo update -p regex # updates just “regex” firecracker/tools/devtool脚本 # By default, all devtool commands run the container transparently, removing # it after the command completes. Any persisting files will be stored under # build/. # If, for any reason, you want to access the container directly, please use # `devtool shell`. This will perform the initial setup (bind-mounting the # sources dir, setting privileges) and will then drop into a BASH shell inside # the container. # # Building: # Run `./devtool build`. # By default, the debug binaries are built and placed under build/debug/. # To build the release version, run `./devtool build --release` instead. # You can then find the binaries under build/release/. # # Testing: # Run `./devtool test`. # This will run the entire integration test battery. The testing system is # based on pytest (http://pytest.org). # # Opening a shell prompt inside the development container: # Run `./devtool shell`. # # Additional information: # Run `./devtool help`. run_devctr函数写的很好. docker -v的z参数表示可以共享, 参考https://docs.docker.com/storage/bind-mounts/#configure-the-selinux-label # Helper function to run the dev container. # Usage: run_devctr -- # Example: run_devctr --privileged -- bash -c \"echo 'hello world'\" run_devctr() { docker_args=() ctr_args=() docker_args_done=false while [[ $# -gt 0 ]]; do [[ \"$1\" = \"--\" ]] && { docker_args_done=true shift continue } [[ $docker_args_done = true ]] && ctr_args+=(\"$1\") || docker_args+=(\"$1\") shift done # If we're running in a terminal, pass the terminal to Docker and run # the container interactively [[ -t 0 ]] && docker_args+=(\"-i\") [[ -t 1 ]] && docker_args+=(\"-t\") # Try to pass these environments from host into container for network proxies proxies=(http_proxy HTTP_PROXY https_proxy HTTPS_PROXY no_proxy NO_PROXY) for i in \"${proxies[@]}\"; do if [[ ! -z ${!i} ]]; then docker_args+=(\"--env\") && docker_args+=(\"$i=${!i}\") fi done # Finally, run the dev container # Use 'z' on the --volume parameter for docker to automatically relabel the # content and allow sharing between containers. docker run \"${docker_args[@]}\" \\ --rm \\ --volume /dev:/dev \\ --volume \"$FC_ROOT_DIR:$CTR_FC_ROOT_DIR:z\" \\ --env OPT_LOCAL_IMAGES_PATH=\"$(dirname \"$CTR_MICROVM_IMAGES_DIR\")\" \\ --env PYTHONDONTWRITEBYTECODE=1 \\ \"$DEVCTR_IMAGE\" \"${ctr_args[@]}\" } cmd_build 默认debug版本, 默认libc是musl target是x86_64-unknown-linux-musl 先build seccompiler seccompiler是个单独的binary, 把json转成BPF程序保存到文件中. # Build seccompiler-bin. run_devctr \\ --user \"$(id -u):$(id -g)\" \\ --workdir \"$CTR_FC_ROOT_DIR\" \\ ${extra_args} \\ -- \\ cargo build -p seccompiler --bin seccompiler-bin \\ --target-dir \"$CTR_CARGO_SECCOMPILER_TARGET_DIR\" \\ \"${cargo_args[@]}\" ret=$? 注: -p seccompiler: 只build seccompiler 再build rebase-snap Tool that copies all the non-sparse sections from a diff file onto a base file # Build rebase-snap. run_devctr \\ --user \"$(id -u):$(id -g)\" \\ --workdir \"$CTR_FC_ROOT_DIR\" \\ ${extra_args} \\ -- \\ cargo build -p rebase-snap \\ --target-dir \"$CTR_CARGO_REBASE_SNAP_TARGET_DIR\" \\ \"${cargo_args[@]}\" ret=$? build firecracker # Build Firecracker. run_devctr \\ --user \"$(id -u):$(id -g)\" \\ --workdir \"$CTR_FC_ROOT_DIR\" \\ ${extra_args} \\ -- \\ cargo build \\ --target-dir \"$CTR_CARGO_TARGET_DIR\" \\ \"${cargo_args[@]}\" ret=$? build jailer # Build jailer only in case of musl for compatibility reasons. if [ \"$libc\" == \"musl\" ];then run_devctr \\ --user \"$(id -u):$(id -g)\" \\ --workdir \"$CTR_FC_ROOT_DIR\" \\ ${extra_args} \\ -- \\ cargo build -p jailer \\ --target-dir \"$CTR_CARGO_TARGET_DIR\" \\ \"${cargo_args[@]}\" fi build_kernel 比如:./tools/devtool build_kernel -c resources/guest_configs/microvm-kernel-arm64-4.14.config # arch不同, vmlinux的format也不同 arch=$(uname -m) if [ \"$arch\" = \"x86_64\" ]; then target=\"vmlinux\" cfg_pattern=\"x86\" format=\"elf\" elif [ \"$arch\" = \"aarch64\" ]; then target=\"Image\" cfg_pattern=\"arm64\" format=\"pe\" recipe_url=\"https://raw.githubusercontent.com/rust-vmm/vmm-reference/$recipe_commit/resources/kernel/make_kernel.sh\" # 从自己的github的另一个库rust-vmm/vmm-reference下载 make_kernel.sh run_devctr \\ --user \"$(id -u):$(id -g)\" \\ --workdir \"$kernel_dir_ctr\" \\ -- /bin/bash -c \"curl -LO \"$recipe_url\" && source make_kernel.sh && extract_kernel_srcs \"$KERNEL_VERSION\"\" cp \"$KERNEL_CFG\" \"$kernel_dir_host/linux-$KERNEL_VERSION/.config\" KERNEL_BINARY_NAME=\"vmlinux-$KERNEL_VERSION-$arch.bin\" #真正的make kernel run_devctr \\ --user \"$(id -u):$(id -g)\" \\ --workdir \"$kernel_dir_ctr\" \\ -- /bin/bash -c \"source make_kernel.sh && make_kernel \"$kernel_dir_ctr/linux-$KERNEL_VERSION\" $format $target \"$nprocs\" \"$KERNEL_BINARY_NAME\"\" build_rootfs default rootfs size是300M, 用ubuntu18.04, 目标是$flavour.rootfs.ext4 先编译几个c文件, 用作测试? run_devctr \\ --workdir \"$CTR_FC_ROOT_DIR\" \\ -- /bin/bash -c \"gcc -o $rootfs_dir_ctr/init $resources_dir_ctr/init.c && \\ gcc -o $rootfs_dir_ctr/fillmem $resources_dir_ctr/fillmem.c && \\ gcc -o $rootfs_dir_ctr/readmem $resources_dir_ctr/readmem.c\" firecracker/resources/tests/init.c 在调用/sbin/openrc-init之前, 向/dev/mem的特定地址(比如aarch64的0x40000000 1G)写入数字123 用于通知VMM kernel已经启动完毕 // Base address values are defined in arch/src/lib.rs as arch::MMIO_MEM_START. // Values are computed in arch/src//mod.rs from the architecture layouts. // Position on the bus is defined by MMIO_LEN increments, where MMIO_LEN is // defined as 0x1000 in vmm/src/device_manager/mmio.rs. #ifdef __x86_64__ #define MAGIC_MMIO_SIGNAL_GUEST_BOOT_COMPLETE 0xd0000000 #endif #ifdef __aarch64__ #define MAGIC_MMIO_SIGNAL_GUEST_BOOT_COMPLETE 0x40000000 #endif #define MAGIC_VALUE_SIGNAL_GUEST_BOOT_COMPLETE 123 int main () { int fd = open(\"/dev/mem\", (O_RDWR | O_SYNC | O_CLOEXEC)); int mapped_size = getpagesize(); char *map_base = mmap(NULL, mapped_size, PROT_WRITE, MAP_SHARED, fd, MAGIC_MMIO_SIGNAL_GUEST_BOOT_COMPLETE); *map_base = MAGIC_VALUE_SIGNAL_GUEST_BOOT_COMPLETE; msync(map_base, mapped_size, MS_ASYNC); const char *init = \"/sbin/openrc-init\"; char *const argv[] = { \"/sbin/init\", NULL }; char *const envp[] = { }; execve(init, argv, envp); } firecracker/resources/tests/fillmem.c Usage: ./fillmem mb_count先mmap再memset firecracker/resources/tests/readmem.c Usage: ./readmem mb_count value 做镜像 用ubuntu18.04 container的 truncate -s \"$SIZE\" \"$img_file\" mkfs.ext4 -F \"$img_file\" docker run -v \"$FC_ROOT_DIR:/firecracker\" ubuntu:18.04 bash -s 注: 使用'EOF'格式的heredoc, 其内部的变量不会展开 "},"notes/rust_cloud-hypervisor_代码.html":{"url":"notes/rust_cloud-hypervisor_代码.html","title":"cloud hypervisor代码","keywords":"","body":"cloud hypervisor是一种基于rust-vmm的VMM实现. 它和其他VMM的对比在这里 code walk in single picture 编译 build时可选的feature列表 REST API和CLI 先用cli创建一个empty的实例, 默认1vCPU, 512M内存. 随后用REST API来创建vm 然后boot这个实例 其他命令 cli和REST的关系 http路由 内部channel 代码梳理 main hypervisor的抽象 vm抽象基本上是基于kvm api的 vmops cli create vm代码流程 REST API流程实例 用户发REST API 这个VMM对应的http server响应请求 从http的raw data里(json格式)解析VmConfig vm_create使用内部channel向VMM的API发送请求 内部channel处理请求 处理这次的VmCreate 返回response impl Vmm x86_64和aarch64的mem layout vm_boot DeviceManager DeviceManager结构体 DeviceManager方法 DeviceNode包括id, 资源, 层级, pci信息 PCI segment PciBus PciDevice virtio设备 virtionet Net结构体 impl VirtioDevice for Net activate NetQueuePair 其他trait virtio block IO Bus和MMIO Bus Bus方法 中断 interrupt group 中断控制器 msi中断控制器 MsiInterruptGroup KVM_IRQFD code walk in single picture 编译 git clone https://github.com/cloud-hypervisor/cloud-hypervisor.git cd cloud-hypervisor/ # docker方式编译 -- 推荐 # 如果需要proxy, 在cmd_build函数docker run命令行加--env http_proxy=\"http://10.158.100.6:8080/\" scripts/dev_cli.sh build --release --libc musl # 产生的bin: build/cargo_target/x86_64-unknown-linux-musl/release/cloud-hypervisor # 本地方式编译, 完全静态链接版本要使用x86_64-unknown-linux-musl rustup target add x86_64-unknown-linux-musl # 完全静态版本一定要加--all, 还要安装musl-tools sudo apt install musl-tools cargo build --release --target=x86_64-unknown-linux-musl --all # 产生的bin: target/x86_64-unknown-linux-musl/release/cloud-hypervisor build时可选的feature列表 #[cfg(target_arch = \"x86_64\")] #[cfg(target_arch = \"aarch64\")] #[cfg(feature = \"guest_debug\")] #[cfg(feature = \"fwdebug\")] #[cfg(feature = \"tdx\")] #[cfg(feature = \"kvm\")] #[cfg(all(feature = \"mshv\", target_arch = \"x86_64\"))] #[cfg(feature = \"gdb\")] REST API和CLI cloud hypervisor以cli方式启动, 并启动http服务, 提供REST接口. 如果cli传入vmm的参数, 则http服务会根据后面的REST api来创建VM 先用cli创建一个empty的实例, 默认1vCPU, 512M内存. $ ./target/debug/cloud-hypervisor --api-socket /tmp/cloud-hypervisor.sock Cloud Hypervisor Guest API server: /tmp/cloud-hypervisor.sock vCPUs: 1 Memory: 512 MB Kernel: None Kernel cmdline: Disk(s): None 随后用REST API来创建vm curl --unix-socket /tmp/cloud-hypervisor.sock -i \\ -X PUT 'http://localhost/api/v1/vm.create' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"cpus\":{\"boot_vcpus\": 4, \"max_vcpus\": 4}, \"kernel\":{\"path\":\"/opt/clh/kernel/vmlinux-virtio-fs-virtio-iommu\"}, \"cmdline\":{\"args\":\"console=ttyS0 console=hvc0 root=/dev/vda1 rw\"}, \"disks\":[{\"path\":\"/opt/clh/images/focal-server-cloudimg-amd64.raw\"}], \"rng\":{\"src\":\"/dev/urandom\"}, \"net\":[{\"ip\":\"192.168.10.10\", \"mask\":\"255.255.255.0\", \"mac\":\"12:34:56:78:90:01\"}] }' 所有的json选项可在vmm/src/config.rs的struct VmConfig里面查看. struct VmConfig用了rust的序列化框架serde, 把结构体直接映射成 然后boot这个实例 curl --unix-socket /tmp/cloud-hypervisor.sock -i -X PUT 'http://localhost/api/v1/vm.boot' 其他命令 # dump vm的config curl --unix-socket /tmp/cloud-hypervisor.sock -i \\ -X GET 'http://localhost/api/v1/vm.info' \\ -H 'Accept: application/json' # reboot vm curl --unix-socket /tmp/cloud-hypervisor.sock -i -X PUT 'http://localhost/api/v1/vm.reboot' # shut down curl --unix-socket /tmp/cloud-hypervisor.sock -i -X PUT 'http://localhost/api/v1/vm.shutdown' cli和REST的关系 http路由 HTTP_ROUTES是个全局变量 lazy_static! { /// HTTP_ROUTES contain all the cloud-hypervisor HTTP routes. pub static ref HTTP_ROUTES: HttpRoutes = { let mut r = HttpRoutes { routes: HashMap::new(), }; r.routes.insert(endpoint!(\"/vm.add-device\"), Box::new(VmActionHandler::new(VmAction::AddDevice(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-user-device\"), Box::new(VmActionHandler::new(VmAction::AddUserDevice(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-disk\"), Box::new(VmActionHandler::new(VmAction::AddDisk(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-fs\"), Box::new(VmActionHandler::new(VmAction::AddFs(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-net\"), Box::new(VmActionHandler::new(VmAction::AddNet(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-pmem\"), Box::new(VmActionHandler::new(VmAction::AddPmem(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-vdpa\"), Box::new(VmActionHandler::new(VmAction::AddVdpa(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-vsock\"), Box::new(VmActionHandler::new(VmAction::AddVsock(Arc::default())))); r.routes.insert(endpoint!(\"/vm.boot\"), Box::new(VmActionHandler::new(VmAction::Boot))); r.routes.insert(endpoint!(\"/vm.counters\"), Box::new(VmActionHandler::new(VmAction::Counters))); r.routes.insert(endpoint!(\"/vm.create\"), Box::new(VmCreate {})); r.routes.insert(endpoint!(\"/vm.delete\"), Box::new(VmActionHandler::new(VmAction::Delete))); r.routes.insert(endpoint!(\"/vm.info\"), Box::new(VmInfo {})); r.routes.insert(endpoint!(\"/vm.pause\"), Box::new(VmActionHandler::new(VmAction::Pause))); r.routes.insert(endpoint!(\"/vm.power-button\"), Box::new(VmActionHandler::new(VmAction::PowerButton))); r.routes.insert(endpoint!(\"/vm.reboot\"), Box::new(VmActionHandler::new(VmAction::Reboot))); r.routes.insert(endpoint!(\"/vm.receive-migration\"), Box::new(VmActionHandler::new(VmAction::ReceiveMigration(Arc::default())))); r.routes.insert(endpoint!(\"/vm.remove-device\"), Box::new(VmActionHandler::new(VmAction::RemoveDevice(Arc::default())))); r.routes.insert(endpoint!(\"/vm.resize\"), Box::new(VmActionHandler::new(VmAction::Resize(Arc::default())))); r.routes.insert(endpoint!(\"/vm.resize-zone\"), Box::new(VmActionHandler::new(VmAction::ResizeZone(Arc::default())))); r.routes.insert(endpoint!(\"/vm.restore\"), Box::new(VmActionHandler::new(VmAction::Restore(Arc::default())))); r.routes.insert(endpoint!(\"/vm.resume\"), Box::new(VmActionHandler::new(VmAction::Resume))); r.routes.insert(endpoint!(\"/vm.send-migration\"), Box::new(VmActionHandler::new(VmAction::SendMigration(Arc::default())))); r.routes.insert(endpoint!(\"/vm.shutdown\"), Box::new(VmActionHandler::new(VmAction::Shutdown))); r.routes.insert(endpoint!(\"/vm.snapshot\"), Box::new(VmActionHandler::new(VmAction::Snapshot(Arc::default())))); r.routes.insert(endpoint!(\"/vmm.ping\"), Box::new(VmmPing {})); r.routes.insert(endpoint!(\"/vmm.shutdown\"), Box::new(VmmShutdown {})); r }; } 内部channel rust标准库的channel是mpsc, 创建channel: // 编译器会从下文推断出这个channel传输的是ApiRequest let (api_request_sender, api_request_receiver) = std::sync::mpsc::channel(); 这个ApiRequest是个enum, 所有的http请求都定义在这; 注意这里回复还是一个Sender, 里面是ApiResponse. 这个就是rust版本的channel in channel: 发出http请求的一方, 构造请求和一个专有的Sender句柄给服务方, 并等待在对应的Receiver; 服务方把ApiResponse响应写回到这个Sender里面; #[allow(clippy::large_enum_variant)] #[derive(Debug)] pub enum ApiRequest { /// Create the virtual machine. This request payload is a VM configuration /// (VmConfig). /// If the VMM API server could not create the VM, it will send a VmCreate /// error back. VmCreate(Arc>, Sender), /// Boot the previously created virtual machine. /// If the VM was not previously created, the VMM API server will send a /// VmBoot error back. VmBoot(Sender), /// Delete the previously created virtual machine. /// If the VM was not previously created, the VMM API server will send a /// VmDelete error back. /// If the VM is booted, we shut it down first. VmDelete(Sender), /// Request the VM information. VmInfo(Sender), /// Request the VMM API server status VmmPing(Sender), /// Pause a VM. VmPause(Sender), /// Resume a VM. VmResume(Sender), /// Get counters for a VM. VmCounters(Sender), /// Shut the previously booted virtual machine down. /// If the VM was not previously booted or created, the VMM API server /// will send a VmShutdown error back. VmShutdown(Sender), /// Reboot the previously booted virtual machine. /// If the VM was not previously booted or created, the VMM API server /// will send a VmReboot error back. VmReboot(Sender), /// Shut the VMM down. /// This will shutdown and delete the current VM, if any, and then exit the /// VMM process. VmmShutdown(Sender), /// Resize the VM. VmResize(Arc, Sender), /// Resize the memory zone. VmResizeZone(Arc, Sender), /// Add a device to the VM. VmAddDevice(Arc, Sender), /// Add a user device to the VM. VmAddUserDevice(Arc, Sender), /// Remove a device from the VM. VmRemoveDevice(Arc, Sender), /// Add a disk to the VM. VmAddDisk(Arc, Sender), /// Add a fs to the VM. VmAddFs(Arc, Sender), /// Add a pmem device to the VM. VmAddPmem(Arc, Sender), /// Add a network device to the VM. VmAddNet(Arc, Sender), /// Add a vDPA device to the VM. VmAddVdpa(Arc, Sender), /// Add a vsock device to the VM. VmAddVsock(Arc, Sender), /// Take a VM snapshot VmSnapshot(Arc, Sender), /// Restore from a VM snapshot VmRestore(Arc, Sender), /// Incoming migration VmReceiveMigration(Arc, Sender), /// Outgoing migration VmSendMigration(Arc, Sender), // Trigger power button VmPowerButton(Sender), } 代码梳理 main main函数在cloud-hypervisor/src/main.rs fn main() { // Ensure all created files (.e.g sockets) are only accessible by this user let _ = unsafe { libc::umask(0o077) }; //默认vCPU=1, 物理地址46bit; mem=512M; 使用/dev/urandom let (default_vcpus, default_memory, default_rng) = prepare_default_values(); //使用了流行的cli库clap, 瀑布式的定义args //get_matches就是parse()命令行 let cmd_arguments = create_app(&default_vcpus, &default_memory, &default_rng).get_matches(); let exit_code = match start_vmm(cmd_arguments); //支持kvm或mshv, 编译时选择 let hypervisor = hypervisor::new() let kvm_obj = Kvm::new() Ok(KvmHypervisor { kvm: kvm_obj }) let vmm_thread = vmm::start_vmm_thread( env!(\"CARGO_PKG_VERSION\").to_string(), &api_socket_path, api_socket_fd, api_evt.try_clone().unwrap(), http_sender, api_request_receiver, #[cfg(feature = \"gdb\")] gdb_socket_path, #[cfg(feature = \"gdb\")] debug_evt.try_clone().unwrap(), #[cfg(feature = \"gdb\")] vm_debug_evt.try_clone().unwrap(), &seccomp_action, hypervisor, //这个是上面的kvm实例化的hypervisor ) let thread = { //新建thread做主event处理循环 thread::Builder::new() .name(\"vmm\".to_string()) .spawn(move || { //新建vmm, 主要是注册event, 并没有开始真正干活 let mut vmm = Vmm::new( vmm_version.to_string(), api_event, vmm_seccomp_action, hypervisor, exit_evt, )?; //event循环 vmm.control_loop(Arc::new(api_receiver)) let epoll_fd = self.epoll.as_raw_fd(); //在loop里epoll wait, 并根据注册epoll add的token来分发 for event in events.iter().take(num_events) { let dispatch_event: EpollDispatch = event.data.into(); match dispatch_event { EpollDispatch::Unknown => {} EpollDispatch::Exit => {} EpollDispatch::Reset => {} EpollDispatch::ActivateVirtioDevices => {} EpollDispatch::Api => { //consume 触发内部channel的eventfd self.api_evt.read().map_err(Error::EventFdRead)?; //处理内部channel过来的请求并返回结果 let api_request = api_receiver.recv() match api_request { ApiRequest::VmCreate(config, sender) => {} ApiRequest::VmBoot(sender) => {} ... } } } } } }; // 起http线程, 用的是micro_http的库 api::start_http_path_thread() let server = HttpServer::new_from_fd() start_http_thread(server) hread::Builder::new() //新线程 loop { match server.requests() { Ok(request_vec) => { for server_request in request_vec { server.respond(server_request.process( |request| { handle_http_request(request, &api_notifier, &api_sender) } )) } } } } //带api前缀的都是发http请求到vmm.control_loop的. vmm::api::vm_create() vmm::api::vm_boot() //或者vmm::api::vm_restore() vmm_thread.join() std::process::exit(exit_code); } hypervisor的抽象 能在最顶层抽象一个hypervisor, 同时支持多种虚拟化技术. 用了抽象函数返回另一个抽象的模式, 即create_vm返回一个Vm trait object /// /// Trait to represent a Hypervisor /// /// This crate provides a hypervisor-agnostic interfaces /// pub trait Hypervisor: Send + Sync { /// /// Create a Vm using the underlying hypervisor /// Return a hypervisor-agnostic Vm trait object /// fn create_vm(&self) -> Result>; /// /// Create a Vm of a specific type using the underlying hypervisor /// Return a hypervisor-agnostic Vm trait object /// fn create_vm_with_type(&self, _vm_type: u64) -> Result> { unreachable!() } #[cfg(target_arch = \"x86_64\")] /// /// Get the supported CpuID /// fn get_cpuid(&self) -> Result; /// /// Check particular extensions if any /// fn check_required_extensions(&self) -> Result { Ok(()) } #[cfg(target_arch = \"x86_64\")] /// /// Retrieve the list of MSRs supported by the hypervisor. /// fn get_msr_list(&self) -> Result; #[cfg(target_arch = \"aarch64\")] /// /// Retrieve AArch64 host maximum IPA size supported by KVM. /// fn get_host_ipa_limit(&self) -> i32; /// /// Retrieve TDX capabilities /// #[cfg(feature = \"tdx\")] fn tdx_capabilities(&self) -> Result; } vm抽象基本上是基于kvm api的 /// /// Trait to represent a Vm /// /// This crate provides a hypervisor-agnostic interfaces for Vm /// pub trait Vm: Send + Sync { #[cfg(target_arch = \"x86_64\")] /// Sets the address of the one-page region in the VM's address space. fn set_identity_map_address(&self, address: u64) -> Result; #[cfg(target_arch = \"x86_64\")] /// Sets the address of the three-page region in the VM's address space. fn set_tss_address(&self, offset: usize) -> Result; /// Creates an in-kernel interrupt controller. fn create_irq_chip(&self) -> Result; /// Registers an event that will, when signaled, trigger the `gsi` IRQ. fn register_irqfd(&self, fd: &EventFd, gsi: u32) -> Result; /// Unregister an event that will, when signaled, trigger the `gsi` IRQ. fn unregister_irqfd(&self, fd: &EventFd, gsi: u32) -> Result; /// Creates a new KVM vCPU file descriptor and maps the memory corresponding fn create_vcpu(&self, id: u8, vm_ops: Option>) -> Result>; /// Registers an event to be signaled whenever a certain address is written to. fn register_ioevent( &self, fd: &EventFd, addr: &IoEventAddress, datamatch: Option, ) -> Result; /// Unregister an event from a certain address it has been previously registered to. fn unregister_ioevent(&self, fd: &EventFd, addr: &IoEventAddress) -> Result; // Construct a routing entry fn make_routing_entry(&self, gsi: u32, config: &InterruptSourceConfig) -> IrqRoutingEntry; /// Sets the GSI routing table entries, overwriting any previously set fn set_gsi_routing(&self, entries: &[IrqRoutingEntry]) -> Result; /// Creates a memory region structure that can be used with {create/remove}_user_memory_region fn make_user_memory_region( &self, slot: u32, guest_phys_addr: u64, memory_size: u64, userspace_addr: u64, readonly: bool, log_dirty_pages: bool, ) -> MemoryRegion; /// Creates a guest physical memory slot. fn create_user_memory_region(&self, user_memory_region: MemoryRegion) -> Result; /// Removes a guest physical memory slot. fn remove_user_memory_region(&self, user_memory_region: MemoryRegion) -> Result; /// Creates an emulated device in the kernel. fn create_device(&self, device: &mut CreateDevice) -> Result>; /// Returns the preferred CPU target type which can be emulated by KVM on underlying host. #[cfg(any(target_arch = \"arm\", target_arch = \"aarch64\"))] fn get_preferred_target(&self, kvi: &mut VcpuInit) -> Result; /// Enable split Irq capability #[cfg(target_arch = \"x86_64\")] fn enable_split_irq(&self) -> Result; #[cfg(target_arch = \"x86_64\")] fn enable_sgx_attribute(&self, file: File) -> Result; /// Retrieve guest clock. #[cfg(all(feature = \"kvm\", target_arch = \"x86_64\"))] fn get_clock(&self) -> Result; /// Set guest clock. #[cfg(all(feature = \"kvm\", target_arch = \"x86_64\"))] fn set_clock(&self, data: &ClockData) -> Result; #[cfg(feature = \"kvm\")] /// Checks if a particular `Cap` is available. fn check_extension(&self, c: Cap) -> bool; /// Create a device that is used for passthrough fn create_passthrough_device(&self) -> Result>; /// Get the Vm state. Return VM specific data fn state(&self) -> Result; /// Set the VM state fn set_state(&self, state: VmState) -> Result; /// Start logging dirty pages fn start_dirty_log(&self) -> Result; /// Stop logging dirty pages fn stop_dirty_log(&self) -> Result; /// Get dirty pages bitmap fn get_dirty_log(&self, slot: u32, base_gpa: u64, memory_size: u64) -> Result>; #[cfg(feature = \"tdx\")] /// Initalize TDX on this VM fn tdx_init(&self, cpuid: &CpuId, max_vcpus: u32) -> Result; #[cfg(feature = \"tdx\")] /// Finalize the configuration of TDX on this VM fn tdx_finalize(&self) -> Result; #[cfg(feature = \"tdx\")] /// Initalize a TDX memory region for this VM fn tdx_init_memory_region( &self, host_address: u64, guest_address: u64, size: u64, measure: bool, ) -> Result; } vmops vm的op主要针对gpa, guest physical address pub trait VmOps: Send + Sync { // 对guest dram来说的 fn guest_mem_write(&self, gpa: u64, buf: &[u8]) -> Result; fn guest_mem_read(&self, gpa: u64, buf: &mut [u8]) -> Result; // 对guest mmio来说的 fn mmio_read(&self, gpa: u64, data: &mut [u8]) -> Result; fn mmio_write(&self, gpa: u64, data: &[u8]) -> Result; // 对guest pio来说的 #[cfg(target_arch = \"x86_64\")] fn pio_read(&self, port: u64, data: &mut [u8]) -> Result; #[cfg(target_arch = \"x86_64\")] fn pio_write(&self, port: u64, data: &[u8]) -> Result; } cli create vm代码流程 命令行传入的kernel选项, 会被parse成vm config, 再创建vm let vm_params = config::VmParams::from_arg_matches(&cmd_arguments); let vm_config = config::VmConfig::parse(vm_params) // Create and boot the VM based off the VM config we just built. let sender = api_request_sender.clone(); vmm::api::vm_create( api_evt.try_clone().unwrap(), api_request_sender, Arc::new(Mutex::new(vm_config)), ) 这个vm_create就是给内部http服务发请求: pub fn vm_create( api_evt: EventFd, api_sender: Sender, config: Arc>, ) -> ApiResult { let (response_sender, response_receiver) = channel(); // Send the VM creation request. api_sender .send(ApiRequest::VmCreate(config, response_sender)) .map_err(ApiError::RequestSend)?; api_evt.write(1).map_err(ApiError::EventFdWrite)?; response_receiver.recv().map_err(ApiError::ResponseRecv)??; Ok(()) } REST API流程实例 用户发REST API A user or operator sends an HTTP request to the Cloud Hypervisor REST API in order to creates a virtual machine: #!/bin/bash curl --unix-socket /tmp/cloud-hypervisor.sock -i \\ -X PUT 'http://localhost/api/v1/vm.create' \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"cpus\":{\"boot_vcpus\": 4, \"max_vcpus\": 4}, \"kernel\":{\"path\":\"/opt/clh/kernel/vmlinux-virtio-fs-virtio-iommu\"}, \"cmdline\":{\"args\":\"console=ttyS0 console=hvc0 root=/dev/vda1 rw\"}, \"disks\":[{\"path\":\"/opt/clh/images/focal-server-cloudimg-amd64.raw\"}], \"rng\":{\"src\":\"/dev/urandom\"}, \"net\":[{\"ip\":\"192.168.10.10\", \"mask\":\"255.255.255.0\", \"mac\":\"12:34:56:78:90:01\"}] }' 这个VMM对应的http server响应请求 The Cloud Hypervisor HTTP thread processes the request and de-serializes the HTTP request JSON body into an internal VmConfig structure. micro_http响应这个请求, 调用提前注册好的EndpointHandler: // /api/v1/vm.create handler pub struct VmCreate {} impl EndpointHandler for VmCreate { fn handle_request( &self, req: &Request, api_notifier: EventFd, api_sender: Sender, ) -> Response { match req.method() { Method::Put => { match &req.body { Some(body) => { // Deserialize into a VmConfig let vm_config: VmConfig = match serde_json::from_slice(body.raw()) .map_err(HttpError::SerdeJsonDeserialize) { Ok(config) => config, Err(e) => return error_response(e, StatusCode::BadRequest), }; // Call vm_create() match vm_create(api_notifier, api_sender, Arc::new(Mutex::new(vm_config))) .map_err(HttpError::ApiError) { Ok(_) => Response::new(Version::Http11, StatusCode::NoContent), Err(e) => error_response(e, StatusCode::InternalServerError), } } None => Response::new(Version::Http11, StatusCode::BadRequest), } } _ => error_response(HttpError::BadRequest, StatusCode::BadRequest), } } } 从http的raw data里(json格式)解析VmConfig let vm_config: VmConfig = match serde_json::from_slice(body.raw()) vm_create使用内部channel向VMM的API发送请求 vm_create使用内部channel向VMM的API发送请求, 请求的内容是VmConfig, 并使用channel in channel来等待回复 VmCreate(Arc>, Sender) // 构造内部channel let (response_sender, response_receiver) = std::sync::mpsc::channel(); // Send the VM creation request. api_sender .send(ApiRequest::VmCreate(config, response_sender)) .map_err(ApiError::RequestSend)?; api_evt.write(1).map_err(ApiError::EventFdWrite)?; response_receiver.recv().map_err(ApiError::ResponseRecv)??; 内部channel处理请求 // Read from the API receiver channel let api_request = api_receiver.recv().map_err(Error::ApiRequestRecv)?; 处理这次的VmCreate The Cloud Hypervisor control loop matches the received internal API against the VmCreate payload, and extracts both the VmConfig structure and the Sender from the command payload. It stores the VmConfig structure and replies back to the sender ((The HTTP thread): match api_request { ApiRequest::VmCreate(config, sender) => { // We only store the passed VM config. // The VM will be created when being asked to boot it. let response = if self.vm_config.is_none() { self.vm_config = Some(config); Ok(ApiResponsePayload::Empty) } else { Err(ApiError::VmAlreadyCreated) }; sender.send(response).map_err(Error::ApiResponseSend)?; } 这里create vm并没有真正的create, 而只是保存vmconfig, 待到boot的时候再创建 返回response 可以看到, 用户的curl请求等到动作执行完毕后, 就会收到response impl Vmm 代码在cloud-hypervisor/vmm/src/lib.rsvm_create和vm_boot等真正执行在Vmm的方法里: impl Vmm { new() vm_create() vm_boot() } x86_64和aarch64的mem layout 和firecracker相比, cloudhypervisor重点在pci mmio. vm_boot 前面说过, vm_create只是保存vmconfig, 而vm_boot是真正创建并运行vm的地方 let vm = Vm::new( Arc::clone(vm_config), exit_evt, reset_evt, &self.seccomp_action, self.hypervisor.clone(), activate_evt, None, None, None, )?; //vm代码在cloud-hypervisor/vmm/src/vm.rs let vm = hypervisor.create_vm().unwrap(); //kvm ioctl KVM_CREATE_VM //return Arc::new(KvmVm {fd: vm_fd, state: VmState {}, ...} //下面3个事x86独有 vm.set_identity_map_address(KVM_IDENTITY_MAP_START.0) vm.set_tss_address(KVM_TSS_START.0 as usize) vm.enable_split_irq() //KVM_ENABLE_CAP let memory_manager = MemoryManager::new(vm.clone(), ...) //建立内存region Vec, 这个Vec的元素是(start, size, type) //比如一般内存分2G, [(0,2G,ram), (3G, 640M, SubRegion), (3G+640M, 大概200M, Reserved)] let arch_mem_regions = arch::arch_memory_regions(ram_size); //很复杂 ... let allocator = SystemAllocator::new( io_base:0 io_size: 64K platform_mmio_base: max_mem - 1M platform_mmio_size: 1M mmio_hole_base: 0xc000_0000 mmio_hole_size: 640M X86_64_IRQ_BASE: 5 irq_num: 24-5 ) //acpi在地址空间最后1M的platform mmio区域 acpi_address = allocator.lock().unwrap().allocate_platform_mmio_addresses(MEMORY_MANAGER_ACPI_SIZE) //从0开始到start_of_device_area的区域是ram let ram_allocator = AddressAllocator::new(GuestAddress(0), start_of_device_area.0) let mut memory_manager = MemoryManager { boot_guest_memory, guest_memory, next_memory_slot, start_of_device_area, end_of_device_area, end_of_ram_area, vm, hotplug_slots, selected_slot, mergeable: config.mergeable, allocator, hotplug_method: config.hotplug_method, boot_ram, current_ram, next_hotplug_slot, shared: config.shared, hugepages: config.hugepages, hugepage_size: config.hugepage_size, prefault: config.prefault, user_provided_zones, snapshot_memory_ranges: MemoryRangeTable::default(), memory_zones, guest_ram_mappings: Vec::new(), acpi_address, log_dirty: dynamic, // Cannot log dirty pages on a TD arch_mem_regions, ram_allocator, dynamic, }; memory_manager.allocate_address_space()?; for (zone_id, regions) in list { for (region, virtio_mem) in regions { //ioctl KVM_SET_USER_MEMORY_REGION //记录user memory region和slot的关系到Vec self.guest_ram_mappings.push(GuestRamMapping { gpa: region.start_addr().raw_value(), size: region.len(), slot, zone_id: zone_id.clone(), virtio_mem, file_offset, }); } } for 每个非ram的region self.ram_allocator .allocate(Some(region.start_addr()), region.len(), None) //内部使用BtreeMap来管理内存ranges, 把每个range insert到BtreeMap //应该都是针对guest 物理地址 self.ranges.insert(new_addr, size); let new_vm = Vm::new_from_memory_manager(memory_manager, vm, ...) //1. 起一个后台线程load kernel //支持load ELF 或 firmware, firmware load到4G地址 Self::load_kernel_async(&kernel, &memory_manager, &config)? linux_loader::loader::elf::Elf::load() linux_loader::loader::load_cmdline() //到CMDLINE_START地址 //或者强制load firmware到4G-size地址, 并手动添加映射 //2. create numa node //主要是从vm config里面提取config.memory_zones和config.distances信息填充到BTreeMap let numa_nodes = Self::create_numa_nodes() //3. 创建device manager, 见下面的详解 //cloud-hypervisor/vmm/src/device_manager.rs let device_manager = DeviceManager::new( vm.clone(), config.clone(), memory_manager.clone(), &exit_evt, &reset_evt, seccomp_action.clone(), numa_nodes.clone(), &activate_evt, force_iommu, restoring, boot_id_list, timestamp, ) /* 1. 新建device tree: HashMap 2. num_pci_segments默认为1, 可以配 3. 确定device区域(其实就是PCI设备区域), 见上面layout图 4. 新建address_manager { allocator: memory_manager.lock().unwrap().allocator(), io_bus: Arc::new(Bus::new()), mmio_bus: Arc::new(Bus::new()), vm: vm.clone(), device_tree: Arc::clone(&device_tree), pci_mmio_allocators, } 5. 新建msi_interrupt_manager, IOAPIC需要它, legacy_interrupt_manager需要IOAPIC gsi_msi_routes是个HashMap, 所有pci的device都共享这个gsi RoutingEntry对应kvm_irq_routing_entry 6. 分配acpi_address 7. 为pci预留legacy的中断slot: pci_irq_slots = [0; 32]; 8. 新建默认的pci_segment, id为0, 一个pci segment包括一个pci root桥, 一个PCI bus 9. 构建pci_segments Vec, 目前看包括id 0和id1的pci_segment 10. 用上面的材料构建device_manager结构体 11. 把device_manager insert到address_manager 12. 返回device_manager */ let memory = memory_manager.lock().unwrap().guest_memory(); //只有x86有io bus let io_bus = Arc::clone(device_manager.lock().unwrap().io_bus()); let mmio_bus = Arc::clone(device_manager.lock().unwrap().mmio_bus()); //只有x86有pci io let pci_config_io = device_manager.lock().unwrap().pci_config_io() as Arc>; //4. vm_ops let vm_ops: Arc = Arc::new(VmOpsHandler { memory, #[cfg(target_arch = \"x86_64\")] io_bus, mmio_bus, #[cfg(target_arch = \"x86_64\")] pci_config_io, }); //5. 创建cpu_manager 通过上面的device_manager memory_manager vm_ops来\"操作\"VM cpu::CpuManager::new() //6. 从文件准备initramfs //7. 构建vm结构体并返回 Ok(Vm { #[cfg(any(target_arch = \"aarch64\", feature = \"tdx\"))] kernel, initramfs, device_manager, config, on_tty, threads: Vec::with_capacity(1), signals: None, state: RwLock::new(VmState::Created), cpu_manager, memory_manager, vm, #[cfg(all(feature = \"kvm\", target_arch = \"x86_64\"))] saved_clock: None, numa_nodes, seccomp_action: seccomp_action.clone(), exit_evt, #[cfg(all(feature = \"kvm\", target_arch = \"x86_64\"))] hypervisor, stop_on_boot, #[cfg(target_arch = \"x86_64\")] load_kernel_handle, }) //根据vm config创建设备, 主要是pci的virtio设备, vfio设备等 new_vm.device_manager.create_devices(serial_pty, console_pty, console_resize_pipe) /* 1. 创建interrupt_controller 对x86来说是IOAPIC, 它的上游是self.msi_interrupt_manager 并把这个中断控制器添加到address_manager bus_devices 和device_tree 对aarch64来说是GIC, gic::Gic::new(), 上游也是self.msi_interrupt_manager 2. 创建legacy的interrupt_controller, 基于上面的interrupt_controller 3. add_legacy_devices() 4. add_acpi_devices() 5. add_console_device() 6. make_virtio_devices() let mut virtio_devices: Vec = Vec::new(); make_virtio_block_devices() make_virtio_net_devices() make_virtio_rng_devices() make_virtio_fs_devices() make_virtio_pmem_devices() make_virtio_vsock_devices() make_virtio_mem_devices() make_virtio_balloon_devices() make_virtio_watchdog_devices() make_vdpa_devices() 7. add_pci_devices(上面的virtio设备) 创建iommu_device: virtio_devices::Iommu::new() //对每个virtio设备 for handle in virtio_devices { add_virtio_pci_device() // 默认add到pci segment 0, 没有iommu, 没有dma 1. id为\"xxx_virtio-pci\" 2. 给这个pci device分配pci资源, bdf号等 (pci_segment_id, pci_device_bdf, resources) = self.pci_resources() 3. msi中断个数为queue个数+1 4. 创建virtio_pci_device //Constructs a new PCI transport for the given virtio device. virtio_pci_device = VirtioPciDevice::new( id, memory, virtio_device, msix_num, access_platform, &self.msi_interrupt_manager, pci_device_bdf.into(), self.activate_evt, use_64bit_bar, //除了virtio block都是64位. dma_handler ) 每个queue都配一个eventfd 创建queues, queue有两个泛型参数 根据virtio规范, pci_device_id是0x1040+device_type. 创建interrupt group, 即把下面的每个irq都绑定一个eventfd到irq 配置msix_config 配置class, 比如PciClassCode::NetworkController等 组成configuration space需要的信息: let configuration = PciConfiguration::new( VIRTIO_PCI_VENDOR_ID, pci_device_id, 0x1, // For modern virtio-PCI devices class, subclass, None, PciHeaderType::Device, VIRTIO_PCI_VENDOR_ID, pci_device_id, msix_config_clone, ); 组成virtio pci device let mut virtio_pci_device = VirtioPciDevice { id, configuration, common_config: VirtioPciCommonConfig { access_platform, driver_status: 0, config_generation: 0, device_feature_select: 0, driver_feature_select: 0, queue_select: 0, msix_config: Arc::new(AtomicU16::new(VIRTQ_MSI_NO_VECTOR)), msix_queues: Arc::new(Mutex::new(vec![VIRTQ_MSI_NO_VECTOR; num_queues])), }, msix_config, msix_num, device, device_activated: Arc::new(AtomicBool::new(false)), interrupt_status: Arc::new(AtomicUsize::new(0)), virtio_interrupt: None, queues, queue_evts, memory: Some(memory), settings_bar: 0, use_64bit_bar, interrupt_source_group, cap_pci_cfg_info: VirtioPciCfgCapInfo::default(), bar_regions: vec![], activate_evt, activate_barrier: Arc::new(Barrier::new(2)), dma_handler, }; //设置msix中断控制器 virtio_pci_device.virtio_interrupt = VirtioInterruptMsix::new() return 这个virtio_pci_device 5. add到pci, 返回bar地址信息 new_resources = self.add_pci_device(virtio_pci_device) 分配bar空间, 返回Vec.这是PciDevice本身的方法 pci_bus.add_device() //添加到hashmap 记录这个Bus_device到bus_devices pci_bus.register_mapping() //注册bar地址到device manager的MMIO Bus, 关联本BusDevice到这个bar地址 6. virtio设备的每个queue, 都有个\"通知\"地址 = bar地址base+NOTIFICATION_BAR_OFFSET+queue index*4, 每个通知地址都有一个eventfd self.address_manager.register_ioevent() 调用KVM的KVM_IOEVENTFD ioctl 7. 把本设备加入到device_tree } add_vfio_devices() add_user_devices() self.bus_devices .push(Arc::clone(&segment.pci_config_mmio) as Arc>); 8. done self.virtio_devices = virtio_devices */ vm.boot() //新建thread处理signal: SIGWINCH, SIGTERM, SIGINT self.setup_signal_handler() //stdin设为raw mode self.setup_tty() //load kernel let entry_point = self.entry_point() self.cpu_manager.create_boot_vcpus() for each vcpu kvm create vcpu self.cpu_manager.start_boot_vcpus() for each vcpu 创建新线程, 在里面loop: vcpu.run() VcpuExit::IoIn VcpuExit::IoOut VcpuExit::Shutdown VcpuExit::SystemEvent VcpuExit::MmioRead VcpuExit::MmioWrite VcpuExit::Hyperv DeviceManager DeviceManager结构体 pub struct DeviceManager { // Manage address space related to devices address_manager: Arc, // Console abstraction console: Arc, // console PTY console_pty: Option>>, // serial PTY serial_pty: Option>>, // Serial Manager serial_manager: Option>, // pty foreground status, console_resize_pipe: Option>, // Interrupt controller #[cfg(target_arch = \"x86_64\")] interrupt_controller: Option>>, #[cfg(target_arch = \"aarch64\")] interrupt_controller: Option>>, // Things to be added to the commandline (e.g. aarch64 early console) #[cfg(target_arch = \"aarch64\")] cmdline_additions: Vec, // ACPI GED notification device ged_notification_device: Option>>, // VM configuration config: Arc>, // Memory Manager memory_manager: Arc>, // The virtio devices on the system virtio_devices: Vec, // List of bus devices // Let the DeviceManager keep strong references to the BusDevice devices. // This allows the IO and MMIO buses to be provided with Weak references, // which prevents cyclic dependencies. bus_devices: Vec>>, // Counter to keep track of the consumed device IDs. device_id_cnt: Wrapping, pci_segments: Vec, #[cfg_attr(target_arch = \"aarch64\", allow(dead_code))] // MSI Interrupt Manager msi_interrupt_manager: Arc>, #[cfg_attr(feature = \"mshv\", allow(dead_code))] // Legacy Interrupt Manager legacy_interrupt_manager: Option>>, // Passthrough device handle passthrough_device: Option>, // VFIO container // Only one container can be created, therefore it is stored as part of the // DeviceManager to be reused. vfio_container: Option>, // Paravirtualized IOMMU iommu_device: Option>>, iommu_mapping: Option>, // PCI information about devices attached to the paravirtualized IOMMU // It contains the virtual IOMMU PCI BDF along with the list of PCI BDF // representing the devices attached to the virtual IOMMU. This is useful // information for filling the ACPI VIOT table. iommu_attached_devices: Option)>, // Tree of devices, representing the dependencies between devices. // Useful for introspection, snapshot and restore. device_tree: Arc>, // Exit event exit_evt: EventFd, reset_evt: EventFd, #[cfg(target_arch = \"aarch64\")] id_to_dev_info: HashMap, // seccomp action seccomp_action: SeccompAction, // List of guest NUMA nodes. numa_nodes: NumaNodes, // Possible handle to the virtio-balloon device balloon: Option>>, // Virtio Device activation EventFd to allow the VMM thread to trigger device // activation and thus start the threads from the VMM thread activate_evt: EventFd, acpi_address: GuestAddress, selected_segment: usize, // Possible handle to the virtio-mem device virtio_mem_devices: Vec>>, #[cfg(target_arch = \"aarch64\")] // GPIO device for AArch64 gpio_device: Option>>, #[cfg(target_arch = \"aarch64\")] // Flash device for UEFI on AArch64 uefi_flash: Option>, // Flag to force setting the iommu on virtio devices force_iommu: bool, // Helps identify if the VM is currently being restored restoring: bool, // io_uring availability if detected io_uring_supported: Option, // List of unique identifiers provided at boot through the configuration. boot_id_list: BTreeSet, // Start time of the VM timestamp: Instant, } DeviceManager方法 impl DeviceManager { new() serial_pty() console_pty() console_resize_pipe() create_devices() state() set_state() get_msi_iova_space() get_device_info() add_pci_devices() add_interrupt_controller() get_interrupt_controller() add_acpi_devices() add_legacy_devices() add_serial_device() modify_mode() set_raw_mode() add_virtio_console_device() add_console_device() make_virtio_devices() make_virtio_block_devices() make_virtio_net_devices() make_virtio_rng_devices() make_virtio_fs_devices() make_virtio_pmem_devices() make_virtio_vsock_devices() make_virtio_mem_devices() make_virtio_balloon_devices() make_virtio_watchdog_devices() make_vdpa_devices() next_device_name() add_passthrough_device() create_vfio_container() add_vfio_devices() add_vfio_user_device() add_pci_device() add_virtio_pci_device() pci_resources() io_bus() mmio_bus() allocator() //address_manager.allocator interrupt_controller() pci_config_io() pci_segments() console() cmdline_additions() update_memory() activate_virtio_devices() notify_hotplug() add_device() add_user_device() remove_device() eject_device() hotplug_virtio_pci_device() is_iommu_segment() add_disk() add_fs() add_pmem() add_net() add_vdpa() add_vsock() counters() resize_balloon() device_tree() restore_devices() notify_power_button() iommu_attached_devices() uefi_flash() validate_identifier() } DeviceNode包括id, 资源, 层级, pci信息 尤其是每个Device都是PCI的device #[derive(Clone, Serialize, Deserialize)] pub struct DeviceNode { pub id: String, pub resources: Vec, pub parent: Option, pub children: Vec, #[serde(skip)] pub migratable: Option>>, pub pci_bdf: Option, #[serde(skip)] pub pci_device_handle: Option, } PCI segment pub(crate) struct PciSegment { pub(crate) id: u16, pub(crate) pci_bus: Arc>, pub(crate) pci_config_mmio: Arc>, pub(crate) mmio_config_address: u64, #[cfg(target_arch = \"x86_64\")] pub(crate) pci_config_io: Option>>, // Bitmap of PCI devices to hotplug. pub(crate) pci_devices_up: u32, // Bitmap of PCI devices to hotunplug. pub(crate) pci_devices_down: u32, // List of allocated IRQs for each PCI slot. pub(crate) pci_irq_slots: [u8; 32], // Device memory covered by this segment pub(crate) start_of_device_area: u64, pub(crate) end_of_device_area: u64, pub(crate) allocator: Arc>, } pci的config空间是vmm的一个结构体: /// Contains the configuration space of a PCI node. /// See the [specification](https://en.wikipedia.org/wiki/PCI_configuration_space). /// The configuration space is accessed with DWORD reads and writes from the guest. pub struct PciConfiguration { registers: [u32; NUM_CONFIGURATION_REGISTERS], writable_bits: [u32; NUM_CONFIGURATION_REGISTERS], // writable bits for each register. bars: [PciBar; NUM_BAR_REGS], rom_bar_addr: u32, rom_bar_size: u32, rom_bar_used: bool, // Contains the byte offset and size of the last capability. last_capability: Option, msix_cap_reg_idx: Option, msix_config: Option>>, } new一个segment impl PciSegment { pub(crate) fn new() { 1. 新建一个pci root桥 默认是VENDOR_ID_INTEL(0x8086)和DEVICE_ID_INTEL_VIRT_PCIE_HOST(0xD57) PciClassCode::BridgeDevice &PciBridgeSubclass::HostBridge PciHeaderType::Device 2. 新建PCI bus, PCI bus用hashmap来管理pci 设备: HashMap>> 3. 新建pci_config_mmio, 一个就是一个PciBus实例 4. 把pci_config_mmio insert到address_manager的mmio_bus 5. 组装一个segment并返回 } } PciBus impl PciBus { new() register_mapping() add_device() remove_by_device() next_device_id() get_device_id() put_device_id() } PciDevice pub trait PciDevice: BusDevice { allocate_bars() free_bars() write_config_register() read_config_register() detect_bar_reprogramming() read_bar() write_bar() move_bar() as_any() id() } virtio设备 virtionet 代码cloud-hypervisor/virtio-devices/src/net.rs //主要是创建tap设备, 配置virtio net的属性 make_virtio_net_devices() 1. 生成id, 即xxx_net 2. 支持vhost user, 但一般还是virtio-net对接tap设备 virtio_devices::Net::new() tap设备有名就open有名的, 没名就默认叫vmtap%d tap设备可以配IP 这里为了模拟多queue, 使用了多个tap Self::new_with_tap() 设置feature标记各种offload, tso, 多队列 Ok(Net { common: VirtioCommon { device_type: VirtioDeviceType::Net as u32, avail_features, queue_sizes: vec![queue_size; queue_num], paused_sync: Some(Arc::new(Barrier::new((num_queues / 2) + 1))), min_queues: 2, ..Default::default() }, id, taps, config, ctrl_queue_epoll_thread: None, counters: NetCounters::default(), seccomp_action, rate_limiter_config, exit_evt, }) 3. 插入到设备树 self.device_tree.insert() 4. 返回MetaVirtioDevice Ok(MetaVirtioDevice { virtio_device, iommu: net_cfg.iommu, id, pci_segment: net_cfg.pci_segment, dma_handler: None, }) Net结构体 pub struct Net { common: VirtioCommon, id: String, taps: Vec, config: VirtioNetConfig, ctrl_queue_epoll_thread: Option>, counters: NetCounters, seccomp_action: SeccompAction, rate_limiter_config: Option, exit_evt: EventFd, } impl VirtioDevice for Net impl VirtioDevice for Net { device_type() queue_max_sizes() features() ack_features() read_config() activate() //这里应该是入口 reset() counters() set_access_platform() } activate 在vmm.control_loop的事件循环里, 会有EpollDispatch::ActivateVirtioDevices事件来触发virtio设备的使能: fn activate { 1. self.common.activate() 2. 创建NetCtrlEpollHandler, 起个线程\"_net1_ctrl\", 在里面epoll循环处理ctrl queue的事件 3. 每个q都创建NetEpollHandler, 起个线程\"_net1_qp%i\", 在epoll里循环处理data queue的事件 } data queue的事件: match ev_type { RX_QUEUE_EVENT: self.handle_rx_event() TX_QUEUE_EVENT: self.handle_tx_event() TX_TAP_EVENT: self.handle_tx_event() RX_TAP_EVENT: self.handle_rx_tap_event() 还有rate limit事件... } NetQueuePair cloud-hypervisor/net_util/src/queue_pair.rs 前面的事件, 最后都会落脚到 impl NetQueuePair { process_tx() self.tx.process_desc_chain() 基本上就是在循环里直接操作desc指向的buffer, 最后调用libc::writev把这些buffer写到tap的fd里面. 最后更新used index process_rx() self.rx.process_desc_chain 基本上是从desc链的信息中, 找到guest driver提前准备好的buffer, 组成iovecs, 用libc::readv函数读到这个iovecs里 } 其他trait Net还满足: Drop trait Pausable trait Snapshottable trait Transportable trait Migratable trait virtio block make_virtio_block_device() 1. 生成id, 一般是xxx_disk 2. 支持vhost user对接spdk 3. 打开块设备文件, 支持qcow2, vhd和raw, 返回一个trait object: Box 4. 传入上面的材料, 创建virtio block设备 virtio_devices::Block::new() 设置feature标记, FLUSH, CONFIG_WCE, BLK_SIZE, TOPOLOGY 计算disk参数, 比如block size, sector个数, writeback, 多队列等 返回Block设备 Ok(Block { common: VirtioCommon { device_type: VirtioDeviceType::Block as u32, avail_features, paused_sync: Some(Arc::new(Barrier::new(num_queues + 1))), queue_sizes: vec![queue_size; num_queues], min_queues: 1, ..Default::default() }, id, disk_image, disk_path, disk_nsectors, config, writeback: Arc::new(AtomicBool::new(true)), counters: BlockCounters::default(), seccomp_action, rate_limiter_config, exit_evt, }) 5. 插入到设备树 self.device_tree.insert() 6. 返回MetaVirtioDevice Ok(MetaVirtioDevice { virtio_device, iommu: net_cfg.iommu, id, pci_segment: net_cfg.pci_segment, dma_handler: None, }) IO Bus和MMIO Bus 顶层的device manager包括两个bus: io bus只有x86有io bus mmio busmmio bus是最常用的. io或者mmio的Bus抽象都是一个BtreeMap, 靠地址range来路由读写操作到底指向哪个具体的设备 pub struct Bus { devices: RwLock>>>, } Bus设备必须有offset读写的方法 pub trait BusDevice: Send { /// Reads at `offset` from this device fn read(&mut self, base: u64, offset: u64, data: &mut [u8]) {} /// Writes at `offset` into this device fn write(&mut self, base: u64, offset: u64, data: &[u8]) -> Option> { None } } 注: 对比firecracker的Bus定义, 和这里差不多: pub struct Bus { //bus下面是BtreeMap管理的device devices: BTreeMap>>, } Bus方法 一个BusDevice对应一个地址范围 impl Bus { new() //根据地址找上一个设备 resolve(&self, addr: u64) -> Option>)> //在指定地址范围插入设备 insert(&self, device: Arc>, base: u64, len: u64) //删除指定地址范围的设备 remove(&self, base: u64, len: u64) //读写 read(&self, addr: u64, data: &mut [u8]) write(&self, addr: u64, data: &[u8]) } 中断 和硬件的irq稍有区别, virt irq在这里被叫做Interrupt Source A device may support multiple types of interrupts, and each type of interrupt may support one or multiple interrupt sources. For example, a PCI device may support: Legacy Irq: exactly one interrupt source. PCI MSI Irq: 1,2,4,8,16,32 interrupt sources. PCI MSIx Irq: 2^n(n=0-11) interrupt sources. 一个设备可以有多个中断源, 比如MSI可以有32个源, MSIx可以有4096个源. A distinct Interrupt Source Identifier (ISID) will be assigned to each interrupt source. An ID allocator will be used to allocate and free Interrupt Source Identifiers for devices. To decouple the vm-device crate from the ID allocator, the vm-device crate doesn't take the responsibility to allocate/free Interrupt Source IDs but only makes use of assigned IDs. 每个中断源都被分配一个ID, 即ISID. vm-device只负责使用这些ID The overall flow to deal with interrupts is: The VMM creates an interrupt manager The VMM creates a device manager, passing on an reference to the interrupt manager The device manager passes on an reference to the interrupt manager to all registered devices The guest kernel loads drivers for virtual devices The guest device driver determines the type and number of interrupts needed, and update the device configuration The virtual device backend requests the interrupt manager to create an interrupt group according to guest configuration information 中断并不是在一开始就固定分配好的, 而是需要guest kernel驱动来触发. VMM创建好中断管理器后, 由设备管理器把中断控制器的引用传递给每个具体的设备; guest kernel驱动在初始化虚拟设备的时候, 决定自己的中断类型, 比如MSI, 和中断个数; 对应虚拟设备的backend根据guest驱动提供的信息, 向中断管理器请求创建一个中断group. interrupt group InterruptManager只有两个API, create_group()和destroy_group(), 用于新建/销毁 InterruptSourceGroup /// Trait to manage interrupt sources for virtual device backends. /// /// The InterruptManager implementations should protect itself from concurrent accesses internally, /// so it could be invoked from multi-threaded context. pub trait InterruptManager: Send + Sync { type GroupConfig; /// Create an [InterruptSourceGroup](trait.InterruptSourceGroup.html) object to manage /// interrupt sources for a virtual device /// /// An [InterruptSourceGroup](trait.InterruptSourceGroup.html) object manages all interrupt /// sources of the same type for a virtual device. /// /// # Arguments /// * interrupt_type: type of interrupt source. /// * base: base Interrupt Source ID to be managed by the group object. /// * count: number of Interrupt Sources to be managed by the group object. fn create_group(&self, config: Self::GroupConfig) -> Result>; /// Destroy an [InterruptSourceGroup](trait.InterruptSourceGroup.html) object created by /// [create_group()](trait.InterruptManager.html#tymethod.create_group). /// /// Assume the caller takes the responsibility to disable all interrupt sources of the group /// before calling destroy_group(). This assumption helps to simplify InterruptSourceGroup /// implementations. fn destroy_group(&self, group: Arc) -> Result; } InterruptSourceGroup负责具体中断的维护: 使能/禁止/向guest注入中断/更新中断配置 pub trait InterruptSourceGroup: Send + Sync { /// Enable the interrupt sources in the group to generate interrupts. fn enable(&self) -> Result { // Not all interrupt sources can be enabled. // To accommodate this, we can have a no-op here. Ok(()) } /// Disable the interrupt sources in the group to generate interrupts. fn disable(&self) -> Result { // Not all interrupt sources can be disabled. // To accommodate this, we can have a no-op here. Ok(()) } /// Inject an interrupt from this interrupt source into the guest. fn trigger(&self, index: InterruptIndex) -> Result; /// Returns an interrupt notifier from this interrupt. /// /// An interrupt notifier allows for external components and processes /// to inject interrupts into a guest, by writing to the file returned /// by this method. #[allow(unused_variables)] fn notifier(&self, index: InterruptIndex) -> Option; /// Update the interrupt source group configuration. /// /// # Arguments /// * index: sub-index into the group. /// * config: configuration data for the interrupt source. /// * masked: if the interrupt is masked fn update( &self, index: InterruptIndex, config: InterruptSourceConfig, masked: bool, ) -> Result; } 中断控制器 分为两大类, msi和legacy msi中断控制器 pub struct MsiInterruptManager { allocator: Arc>, vm: Arc, gsi_msi_routes: Arc>>, } MsiInterruptManager实现了InterruptManager trait: impl InterruptManager for MsiInterruptManager { type GroupConfig = MsiIrqGroupConfig; fn create_group(&self, config: Self::GroupConfig) -> Result> { let mut allocator = self.allocator.lock().unwrap(); //把这次group里的所有中断插入到irq_routes, 以hashmap形式索引, key是u32 //每个中断源对应一个eventfd和一个gsi号组成的InterruptRoute let mut irq_routes: HashMap = HashMap::with_capacity(config.count as usize); for i in config.base..config.base + config.count { irq_routes.insert(i, InterruptRoute::new(&mut allocator)?); } Ok(Arc::new(MsiInterruptGroup::new( self.vm.clone(), self.gsi_msi_routes.clone(), irq_routes, ))) } fn destroy_group(&self, _group: Arc) -> Result { Ok(()) } } MsiInterruptGroup impl InterruptSourceGroup for MsiInterruptGroup { //对这个group下面的每个中断源 fn enable(&self) -> Result { for (_, route) in self.irq_routes.iter() { route.enable(&self.vm)?; //调用kvm的ioctl KVM_IRQFD注册eventfd, 绑定到对应的gsi. 写这个eventfd会触发kvm注入中断到irqchip的gsi pin vm.register_irqfd(&self.irq_fd, self.gsi) } } disable() //取消注册 trigger() //就是写对应的eventfd notifier() //返回eventfd的clone } KVM_IRQFD Allows setting an eventfd to directly trigger a guest interrupt. kvm_irqfd.fd specifies the file descriptor to use as the eventfd and kvm_irqfd.gsi specifies the irqchip pin toggled by this event. When an event is triggered on the eventfd, an interrupt is injected into the guest using the specified gsi pin kvm的ioctl KVM_IRQFD注册eventfd, 绑定到对应的gsi. 写这个eventfd会触发kvm注入中断到irqchip的gsi pin "},"notes/rust_cloud-hypervisor_使用.html":{"url":"notes/rust_cloud-hypervisor_使用.html","title":"cloud hypervisor使用","keywords":"","body":" 命令记录 命令行启动 ch-remote启动 启动 可选参数 virtiofsd rest API ping dump vm info reboot shutdown 其他 guest kernel启动打印 gdb调试 rust-gdb 测试场景: vm内virtio-net网口ping对应的tap口 gdb观察 ping是否会触发VM exit -- 否 vmm后端怎么工作 写文件是否会触发VM exit -- 否 lspci是否会触发VM exit -- 是 最小化启动 参考集成 release过程 命令记录 命令行启动 bin/cloud-hypervisor --seccomp false --api-socket clh.sock --cpus boot=2 --memory size=2048M,shared=on --kernel kernel/vmlinux.bin --initramfs rootfs/boot/rootfs.cpio.gz --cmdline \"console=hvc0 config_overlay=linux_shell_only=1 init=/init\" ch-remote启动 # 先启动主程序, 主程序等待命令 bin/cloud-hypervisor --seccomp false --api-socket clh.sock # create bin/ch-remote --api-socket clh.sock create 启动 # virt-customize需要这个 apt install libguestfs-tools # 默认ubuntu的cloud image是没有默认用户的, 也没有root密码 # 用下面的命令配置一个 sudo virt-customize -a focal-server-cloudimg-amd64.img --root-password password:ubuntu wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw wget https://github.com/cloud-hypervisor/rust-hypervisor-firmware/releases/download/0.4.0/hypervisor-fw 启动示例: $ sudo setcap cap_net_admin+ep ./cloud-hypervisor/target/release/cloud-hypervisor # 如果有权限问题, 把/dev/kvm的other设为读写 # 或者加入kvm组 sudo chmod o+rw /dev/kvm $ ./cloud-hypervisor/target/release/cloud-hypervisor \\ --kernel ./hypervisor-fw \\ --disk path=focal-server-cloudimg-amd64.raw \\ --cpus boot=4 \\ --memory size=1024M \\ --net \"tap=,mac=,ip=,mask=\" 可选参数 cloud-hypervisor -h --api-socket /path/to/uds --kernel --cmdline --console --cpus --device --disk --event-monitor --fs --initramfs --log-file --memory --memory-zone --net --numa --platform --pmem --restore --rng --seccomp --serial --vsock --watchdog virtiofsd virtiofsd是用virtiofs协议来共享host文件系统到guest的一个工具.host上需要运行virtiofsd, 指定socket路径和要共享的目录; 同时给cloud hypervisor指定一个tag virtiofsd --log-level error --seccomp none --cache=never --socket-path=$WORK_DIR/run/rootextra.sock --shared-dir=$WORK_DIR/rootfs & ch-remote --api-socket $WORK_DIR/run/clh.sock add-fs tag=rootextra,socket=$WORK_DIR/run/rootextra.sock 在guest里面, 用指定的tag来mount: mount rootextra /rootextra -t virtiofs -o noatime rest API ping curl --unix-socket /tmp/clh.sock -i -X GET 'http://localhost/api/v1/vmm.ping' dump vm info 假设使用--api-socket /tmp/clh.sock启动clh curl --unix-socket /tmp/clh.sock -i -X GET 'http://localhost/api/v1/vm.info' -H 'Accept: application/json' | tail -1 | jq . 列表如下: { \"config\": { \"cpus\": { \"boot_vcpus\": 1, \"max_vcpus\": 1, \"topology\": null, \"kvm_hyperv\": false, \"max_phys_bits\": 46, \"affinity\": null, \"features\": {} }, \"memory\": { \"size\": 1073741824, \"mergeable\": false, \"hotplug_method\": \"Acpi\", \"hotplug_size\": null, \"hotplugged_size\": null, \"shared\": false, \"hugepages\": false, \"hugepage_size\": null, \"prefault\": false, \"zones\": null }, \"kernel\": { \"path\": \"./hypervisor-fw\" }, \"initramfs\": null, \"cmdline\": { \"args\": \"\" }, \"disks\": [ { \"path\": \"focal-server-cloudimg-amd64.raw\", \"readonly\": false, \"direct\": false, \"iommu\": false, \"num_queues\": 1, \"queue_size\": 128, \"vhost_user\": false, \"vhost_socket\": null, \"poll_queue\": true, \"rate_limiter_config\": null, \"id\": \"_disk0\", \"disable_io_uring\": false, \"pci_segment\": 0 } \"net\": [ { \"tap\": null, \"ip\": \"192.168.249.1\", \"mask\": \"255.255.255.0\", \"mac\": \"2e:cc:5f:b8:cd:dc\", \"host_mac\": \"82:22:4f:c3:21:da\", \"iommu\": false, \"num_queues\": 2, \"queue_size\": 256, \"vhost_user\": false, \"vhost_socket\": null, \"vhost_mode\": \"Client\", \"id\": \"_net1\", \"fds\": null, \"rate_limiter_config\": null, \"pci_segment\": 0 } ], \"rng\": { \"src\": \"/dev/urandom\", \"iommu\": false }, \"balloon\": null, \"fs\": null, \"pmem\": null, \"serial\": { \"file\": null, \"mode\": \"Null\", \"iommu\": false }, \"console\": { \"file\": null, \"mode\": \"Tty\", \"iommu\": false }, \"devices\": null, \"user_devices\": null, \"vdpa\": null, \"vsock\": null, \"iommu\": false, \"sgx_epc\": null, \"numa\": null, \"watchdog\": false, \"platform\": null }, \"state\": \"Running\", \"memory_actual_size\": 1073741824, \"device_tree\": { \"__rng\": { \"id\": \"__rng\", \"resources\": [], \"parent\": \"_virtio-pci-__rng\", \"children\": [], \"pci_bdf\": null }, \"_disk0\": { \"id\": \"_disk0\", \"resources\": [], \"parent\": \"_virtio-pci-_disk0\", \"children\": [], \"pci_bdf\": null }, \"_net1\": { \"id\": \"_net1\", \"resources\": [], \"parent\": \"_virtio-pci-_net1\", \"children\": [], \"pci_bdf\": null }, \"_virtio-pci-__console\": { \"id\": \"_virtio-pci-__console\", \"resources\": [ { \"PciBar\": { \"index\": 0, \"base\": 70364448686080, \"size\": 524288, \"type_\": \"Mmio64\", \"prefetchable\": false } } ], \"parent\": null, \"children\": [ \"__console\" ], \"pci_bdf\": \"0000:00:01.0\" }, \"__serial\": { \"id\": \"__serial\", \"resources\": [], \"parent\": null, \"children\": [], \"pci_bdf\": null }, \"_virtio-pci-_disk0\": { \"id\": \"_virtio-pci-_disk0\", \"resources\": [ { \"PciBar\": { \"index\": 0, \"base\": 3891789824, \"size\": 524288, \"type_\": \"Mmio32\", \"prefetchable\": false } } ], \"parent\": null, \"children\": [ \"_disk0\" ], \"pci_bdf\": \"0000:00:02.0\" }, \"_virtio-pci-_net1\": { \"id\": \"_virtio-pci-_net1\", \"resources\": [ { \"PciBar\": { \"index\": 0, \"base\": 70364448161792, \"size\": 524288, \"type_\": \"Mmio64\", \"prefetchable\": false } } ], \"parent\": null, \"children\": [ \"_net1\" ], \"pci_bdf\": \"0000:00:03.0\" }, \"__console\": { \"id\": \"__console\", \"resources\": [], \"parent\": \"_virtio-pci-__console\", \"children\": [], \"pci_bdf\": null }, \"__ioapic\": { \"id\": \"__ioapic\", \"resources\": [], \"parent\": null, \"children\": [], \"pci_bdf\": null }, \"_virtio-pci-__rng\": { \"id\": \"_virtio-pci-__rng\", \"resources\": [ { \"PciBar\": { \"index\": 0, \"base\": 70364447637504, \"size\": 524288, \"type_\": \"Mmio64\", \"prefetchable\": false } } ], \"parent\": null, \"children\": [ \"__rng\" ], \"pci_bdf\": \"0000:00:04.0\" } } } reboot shutdown curl --unix-socket /tmp/cloud-hypervisor.sock -i -X PUT 'http://localhost/api/v1/vm.reboot' curl --unix-socket /tmp/cloud-hypervisor.sock -i -X PUT 'http://localhost/api/v1/vm.shutdown' 其他 比如暂停, 恢复, add net, add disk, remove device, dump counters等等都支持. 例如: curl --unix-socket /tmp/clh.sock -i -X GET 'http://localhost/api/v1/vm.counters' guest kernel启动打印 以ubuntu的云镜像为例: Command line: BOOT_IMAGE=/boot/vmlinuz-5.4.0-113-generic root=LABEL=cloudimg-rootfs ro console=tty1 console=ttyS0 BIOS-provided physical RAM map efi: EFI v2.80 by Hypervisor detected: KVM clocksource: kvm-clock tsc: Detected 2394.454 MHz processor Zone ranges: DMA [mem 0x0000000000001000-0x0000000000ffffff] DMA32 [mem 0x0000000001000000-0x000000003fffffff] Normal empty Device empty Early memory node ranges node 0: [mem 0x0000000000001000-0x000000000009ffff] node 0: [mem 0x000000000013f000-0x000000003fffffff] Initmem setup node 0 [mem 0x0000000000001000-0x000000003fffffff] On node 0 totalpages: 261984 //1G Booting paravirtualized kernel on KVM //guest kernel知道自己是在KVM上启动的 NR_IRQS: 524544, nr_irqs: 256, preallocated irqs: 0 printk: console [tty1] enabled printk: console [ttyS0] enabled LSM: Security Framework initializing Yama: becoming mindful. AppArmor: AppArmor initialized PCI host bridge to bus 0000:00 pci_bus 0000:00: root bus resource [mem 0xe8000000-0xe80fffff] pci_bus 0000:00: root bus resource [mem 0xc0000000-0xe7ffffff window] pci_bus 0000:00: root bus resource [mem 0x100000000-0x3ffeffffffff window] pci_bus 0000:00: root bus resource [io 0x0000-0x0cf7 window] pci_bus 0000:00: root bus resource [io 0x0d00-0xffff window] pci_bus 0000:00: root bus resource [bus 00] pci 0000:00:00.0: [8086:0d57] type 00 class 0x060000 //INTEL pci 0000:00:01.0: [1af4:1043] type 00 class 0xffff00 //virtio pci 0000:00:01.0: reg 0x10: [mem 0x3ffefff80000-0x3ffeffffffff 64bit] //资源已经分配好 pci 0000:00:02.0: [1af4:1042] type 00 class 0x018000 pci 0000:00:02.0: reg 0x10: [mem 0xe7f80000-0xe7ffffff] pci 0000:00:03.0: [1af4:1041] type 00 class 0x020000 pci 0000:00:03.0: reg 0x10: [mem 0x3ffefff00000-0x3ffefff7ffff 64bit] pci 0000:00:04.0: [1af4:1044] type 00 class 0xffff00 pci 0000:00:04.0: reg 0x10: [mem 0x3ffeffe80000-0x3ffeffefffff 64bit] pci_bus 0000:00: on NUMA node 0 iommu: Default domain type: Translated PCI: Using ACPI for IRQ routing PCI: pci_cache_line_size set to 64 bytes tcpip协议栈初始化 virtio-pci 0000:00:01.0: enabling device (0000 -> 0002) virtio-pci 0000:00:02.0: enabling device (0000 -> 0002) virtio-pci 0000:00:03.0: enabling device (0000 -> 0002) virtio-pci 0000:00:04.0: enabling device (0000 -> 0002) Serial: 8250/16550 driver, 32 ports, IRQ sharing enabled loop, tun, vfio, usb, i2c驱动初始化 systemd开始工作 一堆的audit打印... gdb调试 用上面的命令启动hypervisor后, 用gdb调试: gdb cloud-hypervisor -p 18294 Reading symbols from cloud-hypervisor...done (gdb) b mmio_read Breakpoint 1 at 0x7f766eb1d1ee: file vmm/src/vm.rs, line 384. (gdb) b mmio_write Breakpoint 2 at 0x7f766eb1d3a1: file vmm/src/vm.rs, line 391. (gdb) info b Num Type Disp Enb Address What 1 breakpoint keep y 0x00007f766eb1d1ee in ::mmio_read at vmm/src/vm.rs:384 2 breakpoint keep y 0x00007f766eb1d3a1 in ::mmio_write at vmm/src/vm.rs:391 (gdb) b src/kvm/mod.rs:1145 Breakpoint 4 at 0x7f766f2759d7: file hypervisor/src/kvm/mod.rs, line 1145. //只对thread 4打断点 (gdb) b kvm_ioctls::ioctls::vcpu::VcpuFd::run thread 4 rust-gdb 参考https://bitshifter.github.io/rr+rust/index.html#1 需要在root用户下安装rust. 我从普通用户拷贝~/.cargo和~/.rustp好像也能用. # su root ~/.cargo/bin/rust-gdb -p 19507 b kvm_ioctls::ioctls::vcpu::VcpuFd::run 测试场景: vm内virtio-net网口ping对应的tap口 启动hypervisor后, VM内有virtio-net网口: root@ubuntu:~# ethtool -i ens3 driver: virtio_net version: 1.0.0 bus-info: 0000:00:03.0 查看pci拓扑: 注: lspci执行过程中, 会频繁的触发vm exit, 断点表面是VM在做VcpuExit::IoOut root@ubuntu:~# lspci 00:00.0 Host bridge: Intel Corporation Device 0d57 00:01.0 Unassigned class [ffff]: Red Hat, Inc. Virtio console (rev 01) 00:02.0 Mass storage controller: Red Hat, Inc. Virtio block device (rev 01) 00:03.0 Ethernet controller: Red Hat, Inc. Virtio network device (rev 01) 00:04.0 Unassigned class [ffff]: Red Hat, Inc. Virtio RNG (rev 01) 同时, hypervisor会在host上创建一个网口vmtap0, 并配置IP192.168.249.1/24 默认vm的ens3是down的, 下面配置其为up, ip为192.168.249.2 ip link set up dev ens3 ip addr add 192.168.249.2/24 dev ens3 此时可以ping通vmtap0: ping 192.168.249.1 gdb观察 设断点: # su root ~/.cargo/bin/rust-gdb -p 19507 b kvm_ioctls::ioctls::vcpu::VcpuFd::run 这里的kvm_ioctls::ioctls::vcpu::VcpuFd::run是下面代码:是KVM_RUN的循环主体. ping是否会触发VM exit -- 否 一直ping, 同时做gdb观察 gdb设置上面的断点后, continue执行, 除了第一次continue后会触发断点, 后面不管怎么ping, 都没有触发断点. 说明: virtio-net在ping的过程中, VM并没有exit, VM一直\"全速\"运行 因为VM没有exit, 也就没有mmio_read, mmio_write等触发VMM后端的动作; 就是说guest driver在收发报文的时候, 对vring的操作, 对bar寄存器的操作, 统统没有触发mmio exit. vmm后端怎么工作 前面看到, ping的过程中没有观察到VM exit, 那VMM后端如何响应guest driver的读写寄存器请求的呢? 会不会在其他代码路径下面调用了寄存器访问的函数呢? 先看看有没有人调用mmio_write和mmio_read (gdb) b mmio_read (gdb) b mmio_write 注: 用b vmm::vm::VmOpsHandler::mmio_read是不认的. 用这样的语法gdb可以认: (gdb) b ::mmio_read 继续在VM里ping, 没有触发断点. 再增加 (gdb) b vm_device::bus::Bus::read (gdb) b vm_device::bus::Bus::write 依旧没触发. 看看有没有认调用VirtioPciDevice的read: b ::read b ::write 还是没有触发 继续看pci的读写bar操作: b ::read_bar b ::write_bar 还是没有触发 至此, VM exit路径没有触发virtio-net后端的动作. 所以, 到这里很清楚了, virtio-net设备在工作的时候, 完全不需要在VM exit和VM enter之间进行切换. guest driver和vmm device在virtio ring的协议下, 通过ioeventfd和irqfd来互相\"通知\", 在VM不exit的情况下, 完成报文的交互. 见cloud-hypervisor/net_util/src/queue_pair.rs的process_tx()和process_rx()函数. 再次确认, pci::device::PciDevice::read_bar/write_bar并没有在virtio-net报文交换的过程中被调用. 写文件是否会触发VM exit -- 否 echo abc > abc.txt sync //执行的很快 dd if=/dev/zero of=out.dd bs=1M count=4 4194304 bytes (4.2 MB, 4.0 MiB) copied, 0.0129554 s, 324 MB/s lspci是否会触发VM exit -- 是 在vm里面执行lspci, 可以看到断点被触发: (gdb) bt #0 kvm_ioctls::ioctls::vcpu::VcpuFd::run #1 0x00007fee2b19b9c5 in ::run #2 0x00007fee2aca1b45 in vmm::cpu::Vcpu::run #3 0x00007fee2ac7f5cb in vmm::cpu::CpuManager::start_vcpu::{{closure}}::{{closure}} () #4 0x00007fee2acd51bb in std::panicking::try::do_call #5 0x00007fee2acd5c5b in __rust_try #6 0x00007fee2acd46e1 in std::panicking::try #7 0x00007fee2ab64211 in std::panic::catch_unwind #8 0x00007fee2ac7eaf4 in vmm::cpu::CpuManager::start_vcpu::{{closure}} #9 0x00007fee2aa607c3 in std::sys_common::backtrace::__rust_begin_short_backtrace #10 0x00007fee2a9cbde0 in std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}} #11 0x00007fee2acdc7c4 in #12 0x00007fee2acd50a2 in std::panicking::try::do_call #13 0x00007fee2acd5c5b in __rust_try #14 0x00007fee2acd44e7 in std::panicking::try #15 0x00007fee2ab64254 in std::panic::catch_unwind #16 0x00007fee2a9cafb3 in std::thread::Builder::spawn_unchecked_::{{closure}} #17 0x00007fee2a82f74f in core::ops::function::FnOnce::call_once{{vtable-shim}} #18 0x00007fee2b423ff3 in as core::ops::function::FnOnce>::call_once #19 as core::ops::function::FnOnce>::call_once #20 std::sys::unix::thread::Thread::new::thread_start #21 0x00007fee2b44c2e5 in start #22 0x00007fee2b44d3d9 in __clone (gdb) n (gdb) n (gdb) finish (gdb) n 1147 VcpuExit::IoIn(addr, data) => { 最后跟下来vm在做VcpuExit::IoIn和VcpuExit::IoOut, vmm调用对应的pio_read和pio_write来响应. 注: lspci没有触发mmio相关的调用, 只有pio. 最小化启动 https://bl.ocks.org/gdamjan/1f260b58eb9fb1ba62d2234958582405 https://alpinelinux.org/downloads/ 参考集成 见: cloud-hypervisor/scripts/run_integration_tests_x86_64.sh release过程 jobs: steps: - name: Code checkout uses: actions/checkout@v2 - name: Install musl-gcc run: sudo apt install -y musl-tools //需要musl-tools - name: Install Rust toolchain (x86_64-unknown-linux-gnu) //gnu target是动态链接 uses: actions-rs/toolchain@v1 with: toolchain: \"1.60\" target: x86_64-unknown-linux-gnu - name: Install Rust toolchain (x86_64-unknown-linux-musl) //musl target是静态链接 uses: actions-rs/toolchain@v1 with: toolchain: \"1.60\" target: x86_64-unknown-linux-musl - name: Build uses: actions-rs/cargo@v1 with: toolchain: \"1.60\" command: build args: --all --release --target=x86_64-unknown-linux-gnu - name: Static Build uses: actions-rs/cargo@v1 with: toolchain: \"1.60\" command: build args: --all --release --target=x86_64-unknown-linux-musl - name: Strip cloud-hypervisor binaries run: strip target/*/release/cloud-hypervisor //strip - name: Install Rust toolchain (aarch64-unknown-linux-musl) //aarch64 musl uses: actions-rs/toolchain@v1 with: toolchain: \"1.60\" target: aarch64-unknown-linux-musl override: true - name: Static Build (AArch64) uses: actions-rs/cargo@v1 with: use-cross: true command: build args: --all --release --target=aarch64-unknown-linux-musl - name: Upload static cloud-hypervisor //上传asset id: upload-release-static-cloud-hypervisor uses: actions/upload-release-asset@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: upload_url: ${{ steps.create_release.outputs.upload_url }} asset_path: target/x86_64-unknown-linux-musl/release/cloud-hypervisor asset_name: cloud-hypervisor-static asset_content_type: application/octet-stream "},"notes/rust_cloud-hypervisor_问题与解决.html":{"url":"notes/rust_cloud-hypervisor_问题与解决.html","title":"cloud hypervisor问题与解决","keywords":"","body":" cloud hypervisor无法启动 会是什么原因 检查kvm是否nested enable nested KVM 不是nested没开 升级kernel 编译virtiofsd target是musl也不总是完全的静态链接 如何静态链接virtiofsd cloud hypervisor无法启动 直接启动会报错:看提示已经能够显示出错的具体代码位置了. 加上RUST_BACKTRACE=1会更具体: 又提示RUST_BACKTRACE=full会更详细: 可以看到: rust的调用层级很多, 和go有的一拼 RUST_BACKTRACE=1会过滤掉最近的0到12层调用栈, 这些都是rust_begin_unwind的内部流程, 一般用户不需要关心 真正出问题的是vmm::vm::Vm::new, 即RUST_BACKTRACE=full时的第16层调用栈, 很奇怪的是在调用栈里没提示是哪一行. 但调用栈打印之前就有打印提示出错文件和行号:vmm/src/vm.rs:729:48 unwrap出错会直接panic 用环境变量RUST_BACKTRACE=这招和go很像GOTRACEBACK= 会是什么原因 突然想到这个机器本来就是kvm的虚拟机, 是否是kvm嵌套没打开呢? 参考:https://docs.fedoraproject.org/en-US/quick-docs/using-nested-virtualization-in-kvm/ 检查kvm是否nested cat /sys/module/kvm_intel/parameters/nested 果然这个机器显示N enable nested KVM To enable nested virtualization for Intel processors: Shut down all running VMs and unload the kvm_probe module:sudo modprobe -r kvm_intel Activate the nesting feature:sudo modprobe kvm_intel nested=1 Nested virtualization is enabled until the host is rebooted. To enable it permanently, add the following line to the /etc/modprobe.d/kvm.conf file:options kvm_intel nested=1 AMD的CPU把上面的kvm_intel改成kvm_amd 不是nested没开 按照上面的方法, 重新加载kvm_intel并使能nested=1, 也成功了. 但问题依旧.那估计就是kernel版本太低了:Linux spine.novalocal 3.10.0-1160.2.2.el7.x86_64 不支持KVM_CAP_IMMEDIATE_EXIT功能 https://github.com/rust-vmm/kvm-ioctls/src/cap.rs ImmediateExit = KVM_CAP_IMMEDIATE_EXIT, 升级kernel centos 7的kernel比较老, 直接用标准方式升级版本还是3.10. 下面用epel升级kernel yum --enablerepo=elrepo-kernel install kernel-lt 改grub默认从新kernel启动, 启动后版本是: $ uname -a Linux spine.novalocal 5.4.207-1.el7.elrepo.x86_64 #1 SMP Tue Jul 19 10:40:55 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux 使用新kernel问题解决! 编译virtiofsd virtiofsd是rust版本的daemon进程, 用来通过viriofs协议和VM共享host目录. git clone https://gitlab.com/virtio-fs/virtiofsd cargo build --release 错误: /usr/bin/ld: cannot find -lseccomp /usr/bin/ld: cannot find -lcap-ng collect2: error: ld returned 1 exit status 解决: sudo apt install libseccomp-dev libcap-ng-dev target是musl也不总是完全的静态链接 比如这个virtiofsd, 用了musl libc之后, libc的部分是静态链接的. 但还是引用了libseccomp和libcap 如何静态链接virtiofsd virtiofsd的官方repo就可以编译出完全静态的二进制, 它是如何做到的? 见https://gitlab.com/virtio-fs/virtiofsd/-/blob/main/.gitlab-ci.yml apk add libcap-ng-static libseccomp-static musl-dev RUSTFLAGS='-C target-feature=+crt-static -C link-self-contained=yes' LIBSECCOMP_LINK_TYPE=static LIBSECCOMP_LIB_PATH=/usr/lib LIBCAPNG_LINK_TYPE=static LIBCAPNG_LIB_PATH=/usr/lib cargo build --release --target x86_64-unknown-linux-musl "},"notes/virtualization_virtio规范阅读笔记.html":{"url":"notes/virtualization_virtio规范阅读笔记.html","title":"virtio规范阅读笔记.md","keywords":"","body":" virtio规范 virtio设备组成 Device status field Feature bits Device Configuration space One or more virtqueues 描述符格式 Virtqueue Available Ring格式 中断抑制 Virtqueue Used Ring 通知抑制 初始化 驱动和设备交互 驱动提供buffer给设备 从设备接受buffer virtio over pci device的配置空间 Common configuration, cfg_type 1 Notification structure, cfg_type 2 ISR status寄存器, cfg_type 3 PCI configuration access capability, cfg_type 5 virtio pci设备初始化过程 通知和中断 驱动通知设备 从设备的Virtqueue中断到驱动 驱动处理中断 Virtio Over MMIO MMIO Device Discovery MMIO Device Register Layout Virtqueue Configuration device类型 Network Device Block Device 其他类型的virtio device 以前整理 分离式 split virtqueue queue的操作 给device提供buffer 处理device用过的buffer 紧凑式 packed queue Scatter-Gather支持 virtio规范 virtio规范v1.0virtio规范v1.12 virtio设备组成 virtio设备可以被pci, mmio, channel io等bus发现, 每个virtio设备包括: Device status field 包括几个状态flag, 用于指示初始化过程的每个阶段的状态. ACKNOWLEDGE DRIVER FAILED FEATURES_OK DRIVER_OK DEVICE_NEEDS_RESET Feature bits device提供feature list, driver来选择. 在初始化时协商 Device Configuration space 配置空间用于初始化, 一般不经常改动 小端字节序 driver必须按32bit读写 有些configuration是可选的, 先查对应的feature bit是否生效 One or more virtqueues virtqueue负责数据传输, 设备可以有0个或多个virtqueue. 每个queue有16bit的size参数, 用于设置entry个数(最大32768) 每个queue都包括: Descriptor Table Available Ring Used Ring 这三部分都在guest内存里, 物理地址连续. 这三部分的对齐要求和size如下: Virtqueue Part Alignment Size(in bytes) Descriptor Table 16 16∗(Queue Size) Available Ring 2 6 + 2∗(Queue Size) Used Ring 4 6 + 8∗(Queue Size) guest驱动想发送一个buffer到virtio设备时: 驱动先在Descriptor Table填好一个slot, 或者几个slot链 驱动写这个Descriptor index到Available Ring 驱动写notification通知device device使用完这个buffer后, 写Descriptor index到Used Ring device发中断给驱动 驱动来安排descriptors的frame, 比如network使用12字节头后面加1514字节的packet, 或者直接安排一个1526字节的output descriptor 设备不该对这个frame有假设. 描述符格式 buffer分为可读的和可写的, 一个描述符只能是二者之一. addr是在guest内存的物理地址. struct virtq_desc { /* Address (guest-physical). */ le64 addr; /* Length. */ le32 len; /* This marks a buffer as continuing via the next field. */ #define VIRTQ_DESC_F_NEXT 1 /* This marks a buffer as device write-only (otherwise device read-only). */ #define VIRTQ_DESC_F_WRITE 2 /* This means the buffer contains a list of buffer descriptors. */ #define VIRTQ_DESC_F_INDIRECT 4 /* The flags as indicated above. */ le16 flags; /* Next field if flags & NEXT */ le16 next; }; Virtqueue Available Ring格式 struct virtq_avail { #define VIRTQ_AVAIL_F_NO_INTERRUPT 1 le16 flags; le16 idx; le16 ring[ /* Queue Size */ ]; le16 used_event; /* Only if VIRTIO_F_EVENT_IDX */ }; 这个avail ring是驱动写, device读的, 用于驱动向device提供buffer. 一个entry对应一个descriptor table里的entry(如果是链的话, 对应链表头的那个entry) idx表示驱动会把下一个descriptor放在ring的哪里. 中断抑制 搜索VIRTIO_F_EVENT_IDX feature bit Virtqueue Used Ring struct virtq_used { #define VIRTQ_USED_F_NO_NOTIFY 1 le16 flags; le16 idx; struct virtq_used_elem ring[ /* Queue Size */]; le16 avail_event; /* Only if VIRTIO_F_EVENT_IDX */ }; /* le32 is used here for ids for padding reasons. */ struct virtq_used_elem { /* Index of start of used descriptor chain. */ le32 id; /* Total length of the descriptor chain which was used (written to) */ le32 len; }; 设备写, 驱动读. 表示设备把用完的buffer还给驱动. 通知抑制 类似中断抑制 初始化 driver对device初始化从reset device开始, 然后设置状态标记bit, 比如set ACKNOWLEDGE status bit. driver和device都可以对device的configuration spce的每个域做修改, 为了避免昂贵的配置空间读, 如果配置空间发生改变, 修改方通知另一方. The driver MUST follow this sequence to initialize a device: Reset the device. Set the ACKNOWLEDGE status bit: the guest OS has notice the device. Set the DRIVER status bit: the guest OS knows how to drive the device. Read device feature bits, and write the subset of feature bits understood by the OS and driver to the device. During this step the driver MAY read (but MUST NOT write) the device-specific configuration fields to check that it can support the device before accepting it. Set the FEATURES_OK status bit. The driver MUST NOT accept new feature bits after this step. Re-read device status to ensure the FEATURES_OK bit is still set: otherwise, the device does not support our subset of features and the device is unusable. Perform device-specific setup, including discovery of virtqueues for the device, optional per-bus setup, reading and possibly writing the device’s virtio configuration space, and population of virtqueues. Set the DRIVER_OK status bit. At this point the device is “live”. 驱动和设备交互 比如, 最简单的virtio net设备, 有一个transmit virtqueue和一个receive virtqueue 驱动把要发送的packet放到transmit virtqueue, 并在device use掉这个packet后释放对应的buffer 设备把收到的packet放到receive virtqueue, 驱动处理这个\"used\" buffer. 所以, 对驱动来说, 有两个类型的操作 Supplying Buffers to The Device Receiving Used Buffers From The Device 驱动提供buffer给设备 A buffer consists of zero or more device-readable physically-contiguous elements followed by zero or more physically-contiguous device-writable elements (each has at least one element). This algorithm maps it into the descriptor table to form a descriptor chain: for each buffer element, b: Get the next free descriptor table entry, d Set d.addr to the physical address of the start of b Set d.len to the length of b. If b is device-writable, set d.flags to VIRTQ_DESC_F_WRITE, otherwise 0. If there is a buffer element after this: Set d.next to the index of the next free descriptor element. Set the VIRTQ_DESC_F_NEXT bit in d.flags. 然后驱动更新aviable ring, 然后更新idx. 驱动在更新inx前要做barrier操作, 让前面的操作生效. 然后驱动通知device, 这个通知的方法是按bus来的. 通知一般比较昂贵, 可以设置通知抑制来减小overhead. 从设备接受buffer 驱动在收到中断后, 处理used ring. 这个中断需要VMM来实施中断注入guest, 也非常昂贵. 同样可以被抑制. virtio over pci virtio device首先要满足PCI规范. 注: PCI都是小端的. PCI Vendor ID: 0x1AF4 device ID 分类: 过渡阶段的的PCI device ID如下正式的device id从0x1040开始. 比如network card的正式ID是0x1041, 而过渡ID是0x1000 device的配置空间 小端格式, 64bit寄存器也要分两个32bit寄存器来访问, 低32bit在前, 高32bit在后. device的配置空间包括以下几部分: 1 Common configuration 2 Notifications 3 ISR Status 4 Device-specific configuration (optional) 5 PCI configuration access 每部分都可以分别用bar(Base Address register)来map到物理地址空间; 或者用PCI配置空间的特殊寄存器VIRTIO_PCI_CAP_PCI_CFG来访问. 使用pci配置空间的capability list来配置以上5个部分. struct virtio_pci_cap { u8 cap_vndr; /* Generic PCI field: PCI_CAP_ID_VNDR */ u8 cap_next; /* Generic PCI field: next ptr. */ u8 cap_len; /* Generic PCI field: capability length */ u8 cfg_type; /* Identifies the structure. 序号如上 */ u8 bar; /* Where to find it. 0x0到0x5, 表示用哪个bar来map上面的cfg_type*/ u8 padding[3]; /* Pad to full dword. */ le32 offset; /* Offset within bar. */ le32 length; /* Length of the structure, in bytes. */ }; Common configuration, cfg_type 1 用于和driver协商feature, queue个数, queue size, desc table, avail ring和used ring的地址. struct virtio_pci_common_cfg { /* About the whole device. */ le32 device_feature_select; /* read-write */ le32 device_feature; /* read-only for driver */ le32 driver_feature_select; /* read-write */ le32 driver_feature; /* read-write */ le16 msix_config; /* read-write */ le16 num_queues; /* read-only for driver */ u8 device_status; /* read-write */ u8 config_generation; /* read-only for driver */ /* About a specific virtqueue. */ le16 queue_select; /* read-write */ le16 queue_size; /* read-write */ le16 queue_msix_vector; /* read-write */ le16 queue_enable; /* read-write */ le16 queue_notify_off; /* read-only for driver */ //以下就是描述符区, driver区, device区的物理地址 //guest driver把物理地址写入下面三个寄存器, 分别对应desc区, avail区和used区. le64 queue_desc; /* read-write */ le64 queue_avail; /* read-write */ le64 queue_used; /* read-write */ }; Notification structure, cfg_type 2 This capability is immediately followed by an additional field, like so: struct virtio_pci_notify_cap { struct virtio_pci_cap cap; le32 notify_off_multiplier; /* Multiplier for queue_notify_off. */ }; 这个notify_off_multiplier和其他两个参数一起, 可以计算出queue notification在bar内的偏移地址:cap.offset + queue_notify_off * notify_off_multiplierThe cap.offset and notify_off_multiplier are taken from the notification capability structure above, and the queue_notify_off is taken from the common configuration structure. Note: For example, if notifier_off_multiplier is 0, the device uses the same Queue Notify address for all queues. ISR status寄存器, cfg_type 3 只有一个字节, 其中只有2位有用, 用于区分是queue中断还是配置变更中断. 驱动读清, 即驱动读这个isr状态寄存器的时候, 寄存器清零, 中断de-assert. PCI configuration access capability, cfg_type 5 提供另外一种访问device配置空间(cfg_type 1, 2,, 3, 4)的方法. The capability is immediately followed by an additional field like so: struct virtio_pci_cfg_cap { struct virtio_pci_cap cap; u8 pci_cfg_data[4]; /* Data for BAR access. */ }; The fields cap.bar, cap.length, cap.offset and pci_cfg_data are read-write (RW) for the driver. To access a device region, the driver writes into the capability structure (ie. within the PCI configuration space) as follows: The driver sets the BAR to access by writing to cap.bar. The driver sets the size of the access by writing 1, 2 or 4 to cap.length. The driver sets the offset within the BAR by writing to cap.offset. At that point, pci_cfg_data will provide a window of size cap.length into the given cap.bar at offset cap.offset. virtio pci设备初始化过程 驱动scan PCI capability list, 检测出上面五种配置layout MSI-x配置 Virtqueue配置 1 Write the virtqueue index (first queue is 0) to queue_select. 2 Read the virtqueue size from queue_size. This controls how big the virtqueue is (see 2.4 Virtqueues). If this field is 0, the virtqueue does not exist. 3 Optionally, select a smaller virtqueue size and write it to queue_size. 4 Allocate and zero Descriptor Table, Available and Used rings for the virtqueue in contiguous physical memory. 5 Optionally, if MSI-X capability is present and enabled on the device, select a vector to use to request interrupts triggered by virtqueue events. Write the MSI-X Table entry number corresponding to this vector into queue_msix_vector. Read queue_msix_vector: on success, previously written value is returned; on failure, NO_VECTOR value is returned. 通知和中断 驱动通知设备 The driver notifies the device by writing the 16-bit virtqueue index of this virtqueue to the Queue Notify address. 从设备的Virtqueue中断到驱动 If an interrupt is necessary for a virtqueue, the device would typically act as follows: If MSI-X capability is disabled: Set the lower bit of the ISR Status field for the device. Send the appropriate PCI interrupt for the device. If MSI-X capability is enabled: If queue_msix_vector is not NO_VECTOR, request the appropriate MSI-X interrupt message for the device, queue_msix_vector sets the MSI-X Table entry number. 驱动处理中断 The driver interrupt handler would typically: If MSI-X capability is disabled: Read the ISR Status field, which will reset it to zero. If the lower bit is set: look through the used rings of all virtqueues for the device, to see if any progress has been made by the device which requires servicing. If the second lower bit is set: re-examine the configuration space to see what changed. If MSI-X capability is enabled: Look through the used rings of all virtqueues mapped to that MSI-X vector for the device, to see if any progress has been made by the device which requires servicing. If the MSI-X vector is equal to config_msix_vector, re-examine the configuration space to see what changed. Virtio Over MMIO Virtual environments without PCI support (a common situation in embedded devices models) might use simple memory mapped device (“virtio-mmio”) instead of the PCI device. The memory mapped virtio device behaviour is based on the PCI device specification. Therefore most operations including device initialization, queues configuration and buffer transfers are nearly identical. Existing differences are described in the following sections. MMIO Device Discovery mmio的virtio设备一般需要配dts, 因为mmio没有一个发现机制 Unlike PCI, MMIO provides no generic device discovery mechanism. For each device, the guest OS will need to know the location of the registers and interrupt(s) used. The suggested binding for systems using flattened device trees is shown in this example: // EXAMPLE: virtio_block device taking 512 bytes at 0x1e000, interrupt 42. virtio_block@1e000 { compatible = \"virtio,mmio\"; reg = ; interrupts = ; } MMIO Device Register Layout virtio的所有寄存器都以上面的base地址为开始, 依次排列 MMIO virtio devices provide a set of memory mapped control registers followed by a device-specific configuration space, described in the table 4.1. All register values are organized as Little Endian. 下面的table定义了mmio的寄存器layout 比如 Name Offset RW Description MagicValue 0x000 R 0x74726976 (a Little Endian equivalent of the “virt” string). QueueNumMax 0x034 R Maximum virtual queue size Reading from the register returns the maximum size (number of elements) of the queue the device is ready to process or zero (0x0) if the queue is not available. This applies to the queue selected by writing to QueueSel. QueueReady 0x044 RW Virtual queue ready bit Writing one (0x1) to this register notifies the device that it can execute requests from this virtual queue. Reading from this register returns the last value written to it. Both read and write accesses apply to the queue selected by writing to QueueSel. InterruptStatus 0x60 R 中断状态寄存器 InterruptACK 0x064 W 中断应答寄存器 0x100..=0xfff 控制空间寄存器 Configuration space Device-specific configuration space starts at the offset 0x100 and is accessed with byte alignment. Its meaning and size depend on the device and the driver. 补充, 下面的几组寄存器是和driver交换virt queue地址信息的: 另外0x50这个offset也很重要, 这个是guest driver通知device的寄存器, driver写这个地址, 就会触发关联的eventfd给device发信号. Virtqueue Configuration The driver will typically initialize the virtual queue in the following way: Select the queue writing its index (first queue is 0) to QueueSel. Check if the queue is not already in use: read QueueReady, and expect a returned value of zero (0x0). Read maximum queue size (number of elements) from QueueNumMax. If the returned value is zero (0x0) the queue is not available. Allocate and zero the queue memory, making sure the memory is physically contiguous. Notify the device about the queue size by writing the size to QueueNum. Write physical addresses of the queue’s Descriptor Area, Driver Area and Device Area to (respectively) the QueueDescLow/QueueDescHigh, QueueDriverLow/QueueDriverHigh and QueueDeviceLow/QueueDeviceHigh register pairs. Write 0x1 to QueueReady. device类型 Network Device virtqueuenetwork设备的virtqueue是一对或多对receiveq和transmitq, 最后一个是controlq0 receiveq11 transmitq1…2(N-1) receiveqN2(N-1)+1 transmitqN2N controlq 要发送的报文头+报文被放到transmitq, 接收到的报文头+报文被放到receiveq这个报文头的格式如下: struct virtio_net_hdr { #define VIRTIO_NET_HDR_F_NEEDS_CSUM 1 #define VIRTIO_NET_HDR_F_DATA_VALID 2 #define VIRTIO_NET_HDR_F_RSC_INFO 4 u8 flags; #define VIRTIO_NET_HDR_GSO_NONE 0 #define VIRTIO_NET_HDR_GSO_TCPV4 1 #define VIRTIO_NET_HDR_GSO_UDP 3 #define VIRTIO_NET_HDR_GSO_TCPV6 4 #define VIRTIO_NET_HDR_GSO_UDP_L4 5 #define VIRTIO_NET_HDR_GSO_ECN 0x80 u8 gso_type; le16 hdr_len; le16 gso_size; le16 csum_start; le16 csum_offset; le16 num_buffers; le32 hash_value; (Only if VIRTIO_NET_F_HASH_REPORT negotiated) le16 hash_report; (Only if VIRTIO_NET_F_HASH_REPORT negotiated) le16 padding_reserved; (Only if VIRTIO_NET_F_HASH_REPORT negotiated) }; Block Device The virtio block device is a simple virtual block device (ie. disk). Read and write requests (and other exotic requests) are placed in one of its queues, and serviced (probably out of order) by the device except where noted. virtqueue0 requestq1…N-1 requestqNN=1 if VIRTIO_BLK_F_MQ is not negotiated, otherwise N is set by num_queues. 驱动把virtio_blk_req放到virtqueues, layout如下 struct virtio_blk_req { le32 type; //io有很多种操作, 读/写/flush/erase等等 le32 reserved; le64 sector; u8 data[]; u8 status; }; configuration layout struct virtio_blk_config { le64 capacity; le32 size_max; le32 seg_max; struct virtio_blk_geometry { le16 cylinders; u8 heads; u8 sectors; } geometry; le32 blk_size; struct virtio_blk_topology { // # of logical blocks per physical block (log2) u8 physical_block_exp; // offset of first aligned logical block u8 alignment_offset; // suggested minimum I/O size in blocks le16 min_io_size; // optimal (suggested maximum) I/O size in blocks le32 opt_io_size; } topology; u8 writeback; u8 unused0; u16 num_queues; le32 max_discard_sectors; le32 max_discard_seg; le32 discard_sector_alignment; le32 max_write_zeroes_sectors; le32 max_write_zeroes_seg; u8 write_zeroes_may_unmap; u8 unused1[3]; le32 max_secure_erase_sectors; le32 max_secure_erase_seg; le32 secure_erase_sector_alignment; }; 其他类型的virtio device Console Device Entropy Device Traditional Memory Balloon Device SCSI Host Device GPU Device Input Device Crypto Device Socket Device File System Device RPMB Device IOMMU device Sound Device Memory Device I2C Adapter Device SCMI Device GPIO Device PMEM Device 以前整理 分离式 split virtqueue 老的spec用的格式, 在内存上是一段连续的地址空间, 包括三部分: Descriptor Table - occupies the Descriptor Area Available Ring - occupies the Driver Area, 只可driver写 Used Ring - occupies the Device Area, 只可device写 //代码: linux/include/uapi/linux/virtio_ring.h struct vring { unsigned int num; struct vring_desc *desc; struct vring_avail *avail; struct vring_used *used; }; /* The standard layout for the ring is a continuous chunk of memory which looks * like this. We assume num is a power of 2. * * struct vring * { * // The actual descriptors (16 bytes each) * struct vring_desc desc[num]; * * // A ring of available descriptor heads with free-running index. * __virtio16 avail_flags; * __virtio16 avail_idx; * __virtio16 available[num]; * __virtio16 used_event_idx; * * // Padding to the next align boundary. * char pad[]; * * // A ring of used descriptor heads with free-running index. * __virtio16 used_flags; * __virtio16 used_idx; * struct vring_used_elem used[num]; * __virtio16 avail_event_idx; * }; */ queue的操作 比如NIC驱动, 有两个队列: 发送队列(transmit virtqueue)和接收队列(receive virtqueue)driver把出报文加到发送队列, 在报文buffer被device使用完后再free报文bufferdevice把入报文加到接收队列; 更新used index, 后面driver再处理. 给device提供buffer driver把buffer放到Descriptor Table, descriptor有next域, 可以做链 dirver把buffer链表头放到Available Ring的available[next entry] batching模式允许一次放多个报文 driver执行memory barrier指令, 确保device能看到描述符表和available ring的更新 driver更新available idx, 加上本次的描述符链表头的个数. driver还要执行memory barrier指令, 确保在通知device之前, idx已经更新成功 如果通知没有被抑制, dirver发available buffer通知给device available ring和descriptor table的size是一样的(最大32768), 这样的话, 只要descriptor不溢出, ring就不会溢出 因为通知通常比较昂贵, 可以抑制通知; 通过used_event_idx和avil_event_idx配合来实现. 通知昂贵到什么程度呢? 比如OVS的vhost-user backend(这里的device)通知VM的kernel virtio-net驱动(这里的driver), 过程如下: 写eventfd, 产生系统调用到内核态 在内核态里, write()系统调用KVM注册的回调函数irqfd_wakeup() 唤醒kworker线程, 后者调用irqfd_inject()完成中断注入到VM, 唤醒VM的VCPU进程. VM的VCPU被唤醒, 开始处理中断 根据实验测算, 以上过程在几十us量级. 处理device用过的buffer device使用完buffer, 或是读或是写 device发used buffer通知给driver driver处理这些用过的buffer, 处理期间, 可能禁止通知, 再打开通知的时候, 检查并处理新的used buffer. 紧凑式 packed queue 分离式的queue对driver和device来说, 是读写分离的; 而紧凑式的queue是可读可写的, driver和device都可以读写. 通过协商VIRTIO_F_RING_PACKED标志来判断是否支持packed queue. packed queue最大支持32768个entry 也有三部分: descriptor ring - 描述符区struct pvirtq_desc { /* Buffer Address. */ le64 addr; /* Buffer Length. */ le32 len; /* Buffer ID. */ le16 id; /* The flags depending on descriptor type. */ le16 flags; }; driver event suppression - driver写, device读; 控制device发used buffer通知的频率? 减小通知发送次数 device event suppression - device写, driver读; 控制driver发available buffer通知的频率? 减小通知发送次数struct pvirtq_event_suppress { le16 { desc_event_off : 15; /* Descriptor Ring Change Event Offset */ desc_event_wrap : 1; /* Descriptor Ring Change Event Wrap Counter */ } desc; /* If desc_event_flags set to RING_EVENT_FLAGS_DESC */ le16 { desc_event_flags : 2, /* Descriptor Ring Change Event Flags */ reserved : 14; /* Reserved, set to 0 */ } flags; }; driver把buffer ID写入描述符ring, 通知device; device处理完成后, 把描述符写回描述符ring(覆盖之前driver写的那个) Scatter-Gather支持 有些驱动需要有能力提供多个buffer的列表(Scatter-Gather list), 或者是描述符链的结构, 或者是用间接描述符(indirect descriptor) 链式的方式, 除了list的最后一个描述符, 其他描述符的flags都有VIRTQ_DESC_F_NEXT标记; 这其实和上面的链方式差不多, 我理解看到flag里有VIRTQ_DESC_F_NEXT, 那描述符ring里的下一个描述符, 也属于同一个描述符链, 这样依次找下去, 没有标记的就是最后一个描述符, 它带buffer ID. 间接描述符方式 描述符带VIRTQ_DESC_F_INDIRECT标记, 指向一个间接描述符表 "},"notes/qemu_ovs_虚拟化环境.html":{"url":"notes/qemu_ovs_虚拟化环境.html","title":"qemu OVS 虚拟化环境准备","keywords":"","body":" virsh常用命令 虚拟化环境准备 安装qemu libvirt virt-manager等 qemu编译安装(可选) libvirt编译安装高版本(可选) libvirt告警信息 libvirt介绍 virt-manager安装高版本(可选) 安装虚拟机 UEFI支持 qemu的user 安装linux网桥工具 创建虚拟机 访问qemu的monitor 虚拟机共享OS image 问题 解决 虚拟网络 NAT网络 给VM添加OVS bridge 使用virt-manager直接添加 使用virsh添加 启动VM 给VM添加vhost接口 troubleshooting 参考 virsh常用命令 # 查看VM接口信息, 包括MAC地址 $ sudo virsh domiflist hxt-centos7.5-01 虚拟化环境准备 安装qemu libvirt virt-manager等 sudo yum install -y qemu-kvm-ma qemu-img virt-manager libvirt libvirt-python libvirt-client virt-install virt-viewer qemu编译安装(可选) 可以使用内部git repo #clone qemu git clone ssh://1680532@bjsss013.hxtcorp.net:29418/virtualization/qemu #使用2.10版本 git checkout stable-2.10 #或使用更新的2.12版本 git checkout stable-2.12 #我们还是习惯性的切换到新的编译器 scl enable devtoolset-6 bash #编译安装 sudo yum install glib2-devel pixman-devel libfdt-devel libaio-devel libseccomp-devel ./configure --target-list=aarch64-softmmu --enable-kvm --enable-linux-aio --enable-seccomp make -j sudo make install -j 默认路径/usr/local 编译成功后, 有qemu-system-aarch64 不指定aarch64-softmmu的话, 默认编译的target list会很多, 其他架构如mips x86_64等也会编译 注意: 后面virt-manager实际使用的是/usr/libexec/qemu-kvm, 后者是yum install qemu-kvm-ma而来, 和这里的qemu-system-aarch64没有关系. libvirt编译安装高版本(可选) #centos7.5默认版本为3.9, 这里用最新的4.6 wget https://libvirt.org/sources/libvirt-4.6.0.tar.xz #依赖包 sudo yum install gnutls-devel gnutls-utils libnl-devel libxml2-devel device-mapper-devel libpciaccess-devel yajl-devel jansson-devel sudo yum install dnsmasq dnsmasq-utils #编译 ./configure --with-qemu make -j #调试时建议不要install, 默认装到/usr/local; .service文件在/usr/local/lib/systemd/system/ #sudo make install -j #运行 sudo systemctl start libvirtd #或 sudo src/virtlogd & sudo src/libvirtd #virtsh是libvirt自带的命令行配置工具 sudo tools/virsh #查看已经安装的虚拟机 sudo tools/virsh list --all 默认安装到/usr/local/bin 默认的socket是/usr/local/var/run/libvirt/libvirt-sock 注意configure时加--with-qemu, 我之前没加也能编译过, 但却缺少qemu/kvm的支持 : ( 检查config.log, 寻找QEMU:应该为yes libvirt告警信息 libvirtd运行时有几个告警, 应该改掉 warning : virQEMUCapsInit:958 : Failed to get host CPU cache info error : virCPUGetHost:457 : this function is not supported by the connection driver: cannot detect host CPU model for aarch64 architecture warning : virLXCDriverCapsInit:82 : Failed to get host CPU cache info warning : umlCapsInit:73 : Failed to get host CPU cache info libvirt介绍 virt-manager安装高版本(可选) #安装必须的包: sudo yum install libvirt-glib libvirt-python libosinfo python-ipaddr python-requests gtk3 vte3 gtk-vnc2 #virt-manager是python脚本, 可以直接运行 wget https://virt-manager.org/download/sources/virt-manager/virt-manager-1.5.1.tar.gz #解压后直接运行 cd virt-manager-1.5.1 #也可以安装, 默认安装到/usr/bin/ sudo ./setup.py install #指定socket连接libvirtd sudo ./virt-manager -c 'qemu+unix:///system?socket=/usr/local/var/run/libvirt/libvirt-sock' 本文使用MobaXterm来显示virt-manager的图形界面, 其原理是打开ssh的X11 forward功能, 用MobaXterm内置的X server来显示图形 用普通用户可能有authentication unavailable: no polkit agent available to authenticate action 'org.libvirt.unix.manage'错误, 解决办法: sudo usermod --append --groups libvirt $(whoami) X11 forward只对ssh的owner用户有效, 用sudo会导致GUI无法显示, 原因是x11里root用户也没有对当前普通用户session的权限, 解决方法: 把当前用户的授权记录加到root用户 sudo xauth add $(xauth -f /home/bai/.Xauthority list | tail -1) virsh或virt-manager支持remote方式连接hypervisor, 具体格式参考remote URI 在本例中, 因为编译的libvirt默认路径与通常发行版的路径不同, 这里使用qemu+unix连接本地的socket. 安装虚拟机 UEFI支持 安装UEFI简单来说就是先add一个repoqemu-firmware-jenkins, 再yum install edk2.git-aarch64.noarch 参考KVM:UEFI rpm -ql可以看到这个package里面的文件:$ rpm -ql edk2.git-aarch64.noarch /usr/share/edk2.git /usr/share/edk2.git/aarch64 /usr/share/edk2.git/aarch64/QEMU_EFI-pflash.raw /usr/share/edk2.git/aarch64/QEMU_EFI.fd /usr/share/edk2.git/aarch64/QEMU_VARS.fd /usr/share/edk2.git/aarch64/vars-template-pflash.raw 在/etc/libvirt/qemu.conf加入:nvram = [ \"/usr/share/edk2.git/aarch64/QEMU_EFI-pflash.raw:/usr/share/edk2.git/vars-template-pflash.raw\", ] 如果使用本文前面提到的手动编译的libvirt, 则要在/usr/local/etc/libvirt/qemu.conf里加入. qemu的user qemu是用libvirtd起的, 在配置文件/usr/local/etc/libvirt/qemu.conf里, 改成如下, 使qemu的默认用户是root. user = \"root\" group = \"root\" 安装linux网桥工具 sudo yum install bridge-utils 创建虚拟机 新建一个虚拟机, 这里我们导入一个创建好的disk image(从ISO安装而来), 配置8 core/16384 mem; 其他基本上都是默认配置这里创建了三个VM, hxt-centos7.5-00, hxt-centos7.5-01, hxt-centos7.5-02 #导出xml配置文件 sudo tools/virsh dumpxml hxt-centos7.5-00 > ~/hxt-centos7.5-00.xml #使用已有的配置文件创建虚拟机 sudo tools/virsh create ~/repo/save/vm/hxt-centos7.5-base.xml 访问qemu的monitor #virsh访问qemu的monitor: --hmp是human monitor protocol的意思 #sudo virsh qemu-monitor-command --hmp sudo virsh qemu-monitor-command test --hmp help #很有用的info命令族 sudo virsh qemu-monitor-command test --hmp info #info qtree可以查看详细的设备信息 sudo virsh qemu-monitor-command test --hmp info qtree 虚拟机共享OS image 多个虚拟机的OS image通常都是基本相同的, 于是自然想创建一个base image, 几个VM共享它; 而且某个VM的改动, 只保存改变的部分, 能做到吗? 可以, qemu的qcow2, 就是qemu copy-on-write的缩写, 天然支持, 下面这个文章讲的非常清楚 qemu qcow2 backing files & overlays 前面我们用的image固定大小为32G, 为了节省空间, 先shrink sudo cp hxt-centos7.5-base.qcow2 hxt-centos7.5-base.qcow2.back #压缩后33G的image变成850M, 效果非常好 sudo qemu-img convert -O qcow2 -c hxt-centos7.5-base.qcow2.back hxt-centos7.5-base-shrink.qcow2 #比如我想用hxt-centos7.5-base.qcow2做基础image, 分别创建两个thin的image sudo qemu-img create -f qcow2 -b hxt-centos7.5-base.qcow2 hxt-centos7.5-01.qcow2 sudo qemu-img create -f qcow2 -b hxt-centos7.5-base.qcow2 hxt-centos7.5-02.qcow2 #还可以commit在thin image上的改变, 因为用qemu-imag info能看出来它的backing image是谁 sudo qemu-img commit /guests/F21server.qcow2.snap 参考: https://dustymabe.com/2015/01/11/qemu-img-backing-files-a-poor-mans-snapshotrollback/ 问题 导入disk image失败, 现象是UEFI不能启动os image, 经验证和shrink无关, 直接cp的image也不行, 似乎virt-manager新建VM时导入disk image功能不正常? UEFI打印的错误信息: !!!!!!!! ProtectUefiImageCommon - Section Alignment(0x20) is incorrect !!!!!!!! FSOpen: Open '\\EFI\\BOOT\\fbaa64.efi' Success FSOpen: Open '\\EFI\\BOOT\\fbaa64.efi' Success Section 0 has negative size Failed to load image: Unsupported start_image() returned Unsupported Error: Image at 0007BE8D000 start failed: Unsupported Unloading driver at 0x0007BE8D000 Image Return Status = Unsupported 网上有人报这个错误, 是UEFI的问题: https://bugzilla.redhat.com/show_bug.cgi?id=1527283 解决 除了要拷贝disk image, 还要拷贝uefi的VARS.fd文件, 比如 cd /usr/local/var/lib/libvirt/qemu/nvram #拷贝之前可用的vars.fd sudo cp hxt-centos7.5-01_VARS.fd hxt-centos7.5-base_VARS.fd 虚拟网络 NAT网络 NAT网络用于Internet访问. 创建虚拟网络network1, 模式为NAT to eth0 NAT网络network1 libvirtd创建了虚拟网桥virbr1, 以及对应三个VM的三个接口, 如下: $ brctl show virbr1 bridge name bridge id STP enabled interfaces virbr1 8000.525400e10733 yes virbr1-nic vnet0 vnet1 vnet2 上面的vnetN是tap设备, 工作在kernel协议栈二层. 其原理和tun一样, 区别在于tun工作在三层. tun/tap设备原理图 进入VM, 配置网络, 测试外网连通性 dhclient eth0 ip addr curl baidu.com 给VM添加OVS bridge 这里ovs已经运行并创建了一个bridge: ovsbr0 export PATH=$PATH:/usr/local/share/openvswitch/scripts sudo ovs-ctl start #打开dpdk初始化 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true #新增一个bridge sudo ovs-vsctl add-br ovsbr0 -- set bridge ovsbr0 datapath_type=netdev 使用virt-manager直接添加 点Add Hardware, 按下图添加ovsbr0 注意: 还要在xml里面加上, 详见下一小节: 否则会出现错误: Unable to add bridge ovsbr0 port vnet1: Operation not supported 使用virsh添加 参考Open vSwitch with Libvirt和OpenVSwitch KVM libvirt #这里使用libvirt-4.6.0源码目录 cd libvirt-4.6.0 #编辑xml, 加上下面的配置: sudo tools/virsh edit hxt-centos7.5-01 启动VM 添加了OVS bridge以后启动VM, 这里我们启动2个VM, 可以看到libvirtd会自动创建相应的port: $ sudo ovs-vsctl show f81a1c52-91c1-40a2-b993-6dfeea09317e Bridge \"ovsbr0\" Port \"vnet3\" Interface \"vnet3\" Port \"vnet1\" Interface \"vnet1\" Port \"ovsbr0\" Interface \"ovsbr0\" type: internal ovs_version: \"2.8.4\" vnet1和vnet3是libvirtd在VM启动时创建的, 并会在VM关闭时销毁. 此时, 在VM里面配置好ip, 经验证两个VM可以互相ping通. 给VM添加vhost接口 这里要添加两个类型是dpdkvhostuser的端口 #增加两个port, 类型是dpdkvhostuser sudo ovs-vsctl add-port ovsbr0 dpdkvhostuser0 -- set Interface dpdkvhostuser0 type=dpdkvhostuser sudo ovs-vsctl add-port ovsbr0 dpdkvhostuser1 -- set Interface dpdkvhostuser1 type=dpdkvhostuser 手工修改xml文件virsh edit , 加入: hvm /usr/local/bin/qemu-system-aarch64 另外一个VM也相应修改, 注意dpdkvhostuser0变成dpdkvhostuser1 troubleshooting 访问socket权限错误: Failed to connect socket /usr/local/var/run/openvswitch/dpdkvhostuser1: Permission denied 解决方法:sudo chmod a+rw /usr/local/var/run/openvswitch/ 参考: https://bugzilla.redhat.com/show_bug.cgi?id=1431556 默认的qemu-kvm不支持vhost-user Parameter 'type' expects a netdev backend type 解决方法: 源码编译qemu, 在xml里用qemu-system-aarch64替代默认的qemu-kvm, 见上面 参考: https://bugzilla.redhat.com/show_bug.cgi?id=1539823 两个VM互相ping不通, VM里面收发包统计一直为0 无效的尝试: 去掉选项里面的 改为 去掉 解决方法: hugepage对应的node要配置memAccess='shared' 参考: https://bugzilla.redhat.com/show_bug.cgi?id=1516114 参考 URI格式说明 remote URI "},"notes/qemu使用.html":{"url":"notes/qemu使用.html","title":"Qemu使用(old)","keywords":"","body":" 一些文档 kvm xen virtualbox比较 ubuntu guide suse doc 非常详尽 qemu使用 kvm只是个qemu的封装 安装 启动命令 从virtualbox迁移到kvm 关于-nodefaults vnc spice配合-vga qxl qemu gdb 一些文档 kvm xen virtualbox比较 http://www.phoronix.com/scan.php?page=article&item=intel_haswell_virtualization&num=1 ubuntu guide https://help.ubuntu.com/community/KVMhttp://www.howtogeek.com/117635/how-to-install-kvm-and-create-virtual-machines-on-ubuntu/ suse doc 非常详尽 http://doc.opensuse.org/products/draft/SLES/SLES-kvm_sd_draft/book.kvm.htmlhttp://doc.opensuse.org/products/draft/SLES/SLES-kvm_sd_draft/cha.qemu.guest_inst.html#cha.qemu.guest_inst.qemu-kvmhttp://doc.opensuse.org/products/draft/SLES/SLES-kvm_sd_draft/cha.qemu.running.html qemu使用 kvm只是个qemu的封装 /usr/bin/kvm -nographic -monitor stdio -hda windowsC.img -m 2048 -smp 2 -cdrom Win2003.iso -boot c -soundhw es1370 -no-acpi -localtime -usb -usbdevice tablet -net nic,vlan=0,model=rtl8139 -net tap,vlan=0,ifname=tap0 $ cat /usr/bin/kvm #! /bin/shexec qemu-system-x86_64 -enable-kvm \"$@\" 安装 #检查cpu是否支持虚拟化 egrep -c ‘(svm|vmx)’ /proc/cpuinfo #安装 sudo apt-get install qemu-kvm libvirt-bin bridge-utils virt-manager virt-viewer #加入组 sudo adduser `id -un` libvirtd #查看当前虚拟机 virsh -c qemu:///system list 启动命令 qemu-kvm -name \"sles11\" -M pc-0.12 -m 768 \\ -smp 2 -boot c \\ -drive file=/images/sles11/hda,if=virtio,index=0,media=disk,format=raw \\ -net nic,model=virtio,macaddr=52:54:00:05:11:11 \\ -vga cirrus -balloon virtio -writeconfig cfg_file qemu-system-x86_64 -enable-kvm -name ubuntutest -m 2048 -balloon virtio -hda ubuntutest.qcow2 -vnc :19 -net nic,model=virtio -net tap,ifname=tap0,script=no,downscript=n -monitor stdio qemu-system-x86_64 -enable-kvm -name ubuntutest -m 2048 -balloon virtio -drive file=ubuntutest.qcow2,if=virtio -vnc :19 -net nic,model=virtio -net tap,ifname=tap0,script=no,downscript=n -monitor stdio 从virtualbox迁移到kvm 先转换磁盘格式, 即使是raw格式, 注意只要host文件系统支持hole, 也只会占用实际大小.比如我这个win7磁盘是64G的, 转换以后win7.raw只有19G, 其余的空间并不占用host磁盘. 但ls -l显示这个文件还是64G qemu-img convert win7.vdi -O raw win7.raw 关于-nodefaults 默认的q35机器配置 # 00.0 - Host bridge # 1f.0 - ISA bridge / LPC # 1f.2 - SATA (AHCI) controller # 1f.3 - SMBus controller 但启动qemu的时候, qemu会根据传入的参数, 添加默认的device. 但如果参数指定了具体的同类设备, 默认的就没了. vnc 带vnc参数时, 需要vnc到指定端口才有\"视频\"输出比如 -vnc :1 注:vnc的默认端口从5900开始另开一个窗口gvncviewer 0.0.0.0:1或gvncviewer 127.0.0.1:1 spice配合-vga qxl kvm -name GentooGuest -machine q35,usb=off -cpu host -smp 4 -m 2048 -realtime mlock=off -balloon virtio -drive file=gentoo-rootfs.ext4,if=virtio -vga qxl -nographic -spice port=5900,addr=127.0.0.1,disable-ticketing -writeconfig gentooguest.cfg 需要开个spice client apt install spice-client spicec -h 127.0.0.1 -p 5900 按shift+f12退出guest窗口 qemu gdb In order to use gdb, launch QEMU with the '-s' option. It will wait for a gdb connection: qemu-system-i386 -s -kernel arch/i386/boot/bzImage -hda root-2.4.20.img \\ -append \"root=/dev/hda\" Connected to host network interface: tun0 Waiting gdb connection on port 1234 Then launch gdb on the 'vmlinux' executable: > gdb vmlinux In gdb, connect to QEMU: (gdb) target remote localhost:1234 Then you can use gdb normally. For example, type 'c' to launch the kernel: (gdb) c Here are some useful tips in order to use gdb on system code: Use info reg to display all the CPU registers.Use x/10i $eip to display the code at the PC position.Use set architecture i8086 to dump 16 bit code. Then usex/10i $cs*16+$eip to dump the code at the PC position. "},"notes/as_title_networking.html":{"url":"notes/as_title_networking.html","title":"计算机网络相关","keywords":"","body":"如题 "},"notes/networking_杂记1.html":{"url":"notes/networking_杂记1.html","title":"networking杂记1","keywords":"","body":" 内核报文流程, 以tcp为例 发送流程 接收流程 关于sk_buff Openstack vlan组网 远程mount nfs超时错误 试了网上的命令 在server上查看 使用nmap扫描端口情况 问题原因 testpmd收发包数一样, 但字节数对不上 mlx两个网卡跑dpdk有杂包 关于零丢包 关于NIC描述符和ring latency和光速 从testpmd转发看VF特性 ethtool统计详解 tcpdump查看完整包 802.1ad 双tag 802.1q也是双tag, 但TPID是0x8100 tun tap和veth 多播 修改网卡速率 创建tap设备 socket_raw原始报文 SOCK_RAW, SOCK_DGRAM和SOCK_PACKET区别 socket怎么知道要用哪个接口? 交换机access与trunk口 PPS计算 NVME-oF(NVM Express over Fabrics) NFV SDN和NFV的关系 DPDK DPDK要解决的问题 主要技术 主核和从核 OVS SDN OVS场景举例 架构 Openvswitch数据库表结构 controller有很多 内核报文流程, 以tcp为例 https://wiki.linuxfoundation.org/networking/kernel_flow 发送流程 5. Session layer (sockets and files) write (memory data to a file descriptor) sendto (memory data to a socket) sendmsg (a composite message to a socket) __sock_sendmsg() 4. Transport layer (TCP) tcp_sendmsg() 找个sk_buff, 一般每个socket都有预先分配的sk_buff, 如果不够了就新分配 调用skb_add_data(), 把数据从用户空间拷贝到内核空间; 在此按照MSS(Maximum Segment Size)大小分片(一般小于mtu) 调用tcp_transmit_skb()向tcp queue发送 tcp_transmit_skb()创建tcp头, clone sk_buff用于向下层传递; 调用下层的queue_xmit发送函数指针 3. Network layer (IPv4) ip_queue_xmit(): 找路由, 创建ip头 nf_hook()会在几个地方调用, 用于network filtering(firewall, NAT...) 路由的结果保存在dst_entry对象, 然后调用dst_entry的output虚函数 调用ip_output() (或者其他的output方法, 比如tunneling); 做post routing filtering; tcp的分片会小于mtu, 所以一般不需要在ip层分片; 对udp来说应该是要的. 调用虚函数dev_queue_xmit()发送到device 2. Link layer (e.g. Ethernet) 调度packet发送, 使用的是queueing discipline (struct Qdisc)对象, 来做流控 Documentation/networking/multiqueue.txt 用qdisc->enqueue虚方法 如果dev不支持scattered数据, 就要copy数据到连续地址 qdisc有几种调度策略, 常用的是pfifo_fast 最后调用dev_hard_start_xmit()来发送报文 发送失败的话, 这个skb要重新enqueue, 用netif_schedule()来触发软中断来重新发送, 由ksoftirqd来执行net_tx_action() dev_hard_start_xmit()会调用dev_queue_xmit_nit(), 检查是否有ETH_P_ALL的handler, 用于tcpdump 发送完成则free skbuff 接收流程 2. Link layer (e.g. Ethernet) NIC驱动会提前申请好一定数量的sk_buff, 放在可以DMA的地址上; 中断收包, 在NAPI出现之前, 是netif_rx(). 这里重点看NAPI方式怎么收包: 在中断处理函数里, 调用netif_rx_schedule(), 该函数把这个device加到softnet_data的poll_list, 然后触发软中断NET_RX_SOFTIRQ, 然后返回. ksoftirqd内核线程执行net_rx_action(), 在里面调用device的poll方法, 该方法管理device的buffer, 调用netif_receive_skb()来处理每个skb, 在必要时申请sk_buffer, 最后调用netif_rx_complete() 在netif_receive_skb()找到如何传递sk_buffer到上层layer: * 调用netpoll_rx(), 处理Netpoll API * 调用ETH_P_ALL处理, 用来tcpdump * 调用handle_ing(), 处理入队列 * 调用handle_bridge(), 处理网桥 * 调用handle_macvlan(), 处理vlan * deliver_skb() 调用3层注册的handler 3. Network layer (IPv4, ARP) 报文是ARP, 调用arp_rcv(), 解析arp信息, 保存在neighbour cache, 申请新的sk_buffer发送reply 普通的IPv4报文, 调用ip_rcv(), 解析报文头, 调用ip_route_input()找到相应的方法, 包括: * ip_mr_input() 多播 * ip_forward() 目的不是本机, 查路由后直接调用neighbour的output接口 * ip_local_deliver() 本地报文, 先调用raw_local_deliver()来给raw socket deliver报文, 然后调用4层处理函数 4. Transport layer (TCP) tcp_v4_rcv(), tcp协议处理 收到的报文可以包含应答字段, 从而触发后续的报文发送, tcp_data_snd_check() or of acknowledgements tcp_ack_snd_check() 向上层传递报文, tcp_rcv_established() and tcp_data_queue(), 用skb_copy_datagram_iovec()拷贝报文到用户态, 用sk_data_ready()来唤醒等待这个socket的用户态进程. 5. Session layer (sockets and files) socket收包 read (memory data from a file descriptor) recvfrom (memory data from a socket) recvmsg (a composite message from a socket) 都是调用__sock_recvmsg() 关于sk_buff sk_buff是个很大的结构体, 表示报文, 包含了报文的所有控制信息, 被组织为双向链表. sk_buff_head代表的链表头表示一个queue, 包含sk_buff_head的结构体都可以是个queue, 比如struct sock就包含了接收和发送的queue. 分配sk_buff时, 其data空间也在kernel space分配好了. 分配函数是alloc_skb()或者dev_alloc_skb(), 驱动使用dev_alloc_skb(); data空间分成head area和data area, 这样可以不用到处拷贝data area skb_network_header(skb), skb_transport_header(skb) and skb_mac_header(skb) return pointer to the header. transport_header 4层, tcp udp network_header 3层, ip ipv6 arp mac_header 2层 每个layer都有自己私有的sk_buff, 报文在layer间移动的时候, 要clone sk_buff, 但报文的data, 即payload, 不会被反复拷贝. Openstack vlan组网 http://docs.ocselected.org/openstack-manuals/kilo/networking-guide/content/under_the_hood_openvswitch.html 远程mount nfs超时错误 在bj的机器上, 想mount我的nfs server: $ sudo mount -v 10.64.17.45:/home/bai/share /home/bai/share mount.nfs: timeout set for Thu Dec 6 18:03:08 2018 mount.nfs: trying text-based options 'vers=4.1,addr=10.64.17.45,clientaddr=10.5.21.125' mount.nfs: mount(2): Connection timed out mount.nfs: Connection timed out 总是显示超时, 但ping 10.64.17.45是能通的, 稳定在30ms左右的延迟. 试了网上的命令 $ showmount -e 10.64.17.45 clnt_create: RPC: Port mapper failure - Timed out #这个也超时 $ sudo mount -t nfs -o vers=3,nolock,proto=tcp 10.64.17.45:/home/bai/share /home/bai/share 在server上查看 Linux Mint 19 Tara $ rpcinfo -p program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100005 1 udp 55012 mountd 100005 1 tcp 37303 mountd 100005 2 udp 43996 mountd 100005 2 tcp 51595 mountd 100005 3 udp 36041 mountd 100005 3 tcp 54355 mountd 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 3 tcp 2049 100003 3 udp 2049 nfs 100227 3 udp 2049 100021 1 udp 51384 nlockmgr 100021 3 udp 51384 nlockmgr 100021 4 udp 51384 nlockmgr 100021 1 tcp 46129 nlockmgr 100021 3 tcp 46129 nlockmgr 100021 4 tcp 46129 nlockmgr 使用nmap扫描端口情况 # -A:打开额外的激进扫描选项 # -T: 时间策略, 有-T paranoid|sneaky|polite|normal|aggressive|insane, 可用数字表示(0-5) # 下面显示 111和2049都被防火墙filter掉了, 所以连不上 $ sudo nmap -p 111,2049 -T4 -A 10.64.17.45 Starting Nmap 6.40 ( http://nmap.org ) at 2018-12-06 18:06 CST Nmap scan report for localhost (10.64.17.45) Host is up (0.029s latency). PORT STATE SERVICE VERSION 111/tcp filtered rpcbind 2049/tcp filtered nfs Too many fingerprints match this host to give specific OS details Network Distance: 5 hops TRACEROUTE (using proto 1/icmp) HOP RTT ADDRESS 1 0.46 ms 10.5.21.1 2 0.50 ms 10.128.254.145 3 1.00 ms 10.128.254.26 4 29.87 ms localhost (10.64.249.2) 5 28.99 ms localhost (10.64.17.45) OS and Service detection performed. Please report any incorrect results at http://nmap.org/submit/ . Nmap done: 1 IP address (1 host up) scanned in 6.32 seconds 问题原因 上面nmap的扫描可以看到, 111和2049都被防火墙filter掉了, 所以连不上. 这两个端口是rpcbind和nfs用的. testpmd收发包数一样, 但字节数对不上 每个包差了4个字节的CRC RX-packets: 957875678 RX-missed: 0 RX-bytes: 68719476735 TX-packets: 957875678 TX-errors: 0 TX-bytes: 64887974023 mlx两个网卡跑dpdk有杂包 通过打开pktgen的capture功能, 可以看到这些杂包是ipv6相关的, 有多播也有广播, 广播主要是ICMP; 因为mlx网卡, 内核驱动和dpdk是同时工作的, 虽然在跑dpdk程序, 但上层协议栈也是能发包的. 用nmtui把相应的interface的ipv6关掉就好了. 关于零丢包 转发是先RX再TX的过程, 线速零丢包的RX本质上是要把wire上的\"满满的data\"(线速意味着wire上的数据包是最大限度的一个挨一个的, 测试仪能保证)都一个不漏的收上来, 要求buffer足够, CPU及时收包; 在TX方向上, 要求CPU要及时把包填到硬件发送队列里, 保证wire永远被\"塞满\", 要求不能让硬件发送队列空闲, 因为其一旦空闲, 则wire空闲, 那就肯定不能到线速. 另外很重要的点: Pktgen只能以平均99.99%的速率发包, 并不能完全线速, 而丢包就是在那0.01%的速率差异上. 关于NIC描述符和ring https://stackoverflow.com/questions/36625892/descriptor-concept-in-nichttps://blog.csdn.net/chen98765432101/article/details/69367633https://blog.csdn.net/bestboyxie/article/details/53414731 latency和光速 光速是300000km/s, 那么光速带来的latency是3.33 us/km, 在光纤中, 因为反射, 光速要慢点, 大约是5 us/km 从testpmd转发看VF特性 两个机器A和B网卡直连(Mellanox CX4), A跑pktgen发包, B跑testpmd转发. 打开网卡的SR-IOV功能后, VF上跑testpmd, 有如下问题: pktgen构造报文的目的mac必须为VF的mac, VF才能收到包, 即使VF打开了混杂模式也一样. testpmd中, fwd为默认的io, VF能够收发报文, 但对端pktgen收不到报文. 原因是: 构造报文时, DST MAC为VF的MAC, VF才能够收到并转发该报文; 在--forward-mode=io模式下, 并不修改报文, 其DST MAC仍旧为VF的MAC, 虽然VF能够发出该报文, 但PF不会把报文从端口真正发送, 因为其目的mac为VF自身. 结论: SR-IOV网卡相当于内建了一个switch, VF接收/发送的报文都要查switch的转发表才能进出; VF收发的报文也是走PF, 在PF的rx_packets_phy和tx_packets_phy能查到. ethtool统计详解 参考: https://community.mellanox.com/docs/DOC-2532 watch -d -n1 'ethtool -S enP5p1s0 | grep packet | grep -v \": 0\"' tcpdump查看完整包 #抓ping包 $ sudo tcpdump -i enP5p1s0 -vvnnXXSs 1514 tcpdump: listening on enP5p1s0, link-type EN10MB (Ethernet), capture size 1514 bytes 17:07:25.486422 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.1.101 tell 192.168.1.200, length 46 0x0000: ffff ffff ffff 5254 00f0 25e4 0806 0001 ......RT..%..... 0x0010: 0800 0604 0001 5254 00f0 25e4 c0a8 01c8 ......RT..%..... 0x0020: 0000 0000 0000 c0a8 0165 0000 0000 0000 .........e...... 0x0030: 0000 0000 0000 0000 0000 0000 ............ #简化版 $ sudo tcpdump -i enP5p1s0 -vvXX # -e也可以打印L2 header, icmp指定协议 tcpdump -n -i enp0s9 icmp -e -vv # 更常用 sudo tcpdump -i eth1 -nevvxx -vv: 详细模式 -nn: 端口号不转换成名字 -XX: 打印header和payload, 包括二层头 -S: absolute-tcp-sequence-numbers -s snaplen: 默认抓262144字节, 尽量改小点 802.1ad 双tag s-tag: Service VLAN tag c-tag: Customer VLAN tag 802.1q也是双tag, 但TPID是0x8100 #define ETH_TYPE_IP 0x0800 #define ETH_TYPE_ARP 0x0806 #define ETH_TYPE_TEB 0x6558 #define ETH_TYPE_VLAN_8021Q 0x8100 #define ETH_TYPE_VLAN ETH_TYPE_VLAN_8021Q #define ETH_TYPE_VLAN_8021AD 0x88a8 #define ETH_TYPE_IPV6 0x86dd #define ETH_TYPE_LACP 0x8809 #define ETH_TYPE_RARP 0x8035 #define ETH_TYPE_MPLS 0x8847 #define ETH_TYPE_MPLS_MCAST 0x8848 #define ETH_TYPE_NSH 0x894f tun tap和veth tun：点对点的设备，tun设备模拟网络层设备，处理三层报文，如IP报文。tun设备完全不需要物理地址的。它收到和发出的包不需要arp，也不需要有数据链路层的头。 tap：是一个普通的以太网设备，tap设备模拟链路层设备，处理二层报文，比如以太网帧。tap设备需要有完整的物理地址和完整的以太网帧 TUN用于路由，而TAP用于创建网桥。 VETH设备总是成对出现，送到一端请求发送的数据总是从另一端以请求接收的形式出现。创建并配置正确后，向其一端输入数据，VETH会改变数据的方向并将其送入内核网络子系统，完成数据的注入，而在另一端则能读到此数据。 多播 https://blog.csdn.net/dengjin20104042056/article/details/52357122 修改网卡速率 #一定要加上autoneg off 关闭自协商 ethtool -s ens817 speed 10000 autoneg off 创建tap设备 命令 ip tuntap add mode tap port_name 代码 You are supposed to open the /dev/net/tun device. A subsequent ioctl on the open fd will create the tun0 (or whatever you wish to name it) network interface. 参考https://stackoverflow.com/questions/1003684/how-to-interface-with-the-linux-tun-driverhttps://www.kernel.org/doc/Documentation/networking/tuntap.txt socket_raw原始报文 Use AF_INET if you want to communicate using Internet protocols: TCP or UDP. This is by far the most common choice, and almost certainly what you want.Use PF_PACKET if you want to send and receive messages at the most basic level, below the Internet protocol layer, for example because you are implementing the protocol yourself. Your process must run as root (or with a special capability) to use PF_PACKET. This is a very advanced option. socket PF_PACKET 发送过程: sock_raw:->packet_create->dev_add_pack Modify ptype_base|ptype_all:二层协议 接收过程: netif_rx->process_backlog->netif_receive_skb:ptype_base|ptype_all socket AF_INET 发送过程: sock_raw:->inet_create (list_for_each_entry_rcu(answer, &inetsw[sock->type], list) find SOCK_RAW )->hash(raw_hash_sk) Modify raw_v4_hashinfo:IP上层协议 接受过程: ip_rcv->ip_rcv_finish->ip_route_input->ip_local_deliver->ip_local_deliver_finish->raw_local_deliver:raw_v4_hashinfo sock_raw（注意一定要在root下使用）原始套接字编程可以接收到本机网卡上的数据帧或者数据包,对于监听网络的流量和分析是很有作用的.一共可以有3种方式创建这种socket socket(AF_INET, SOCK_RAW, IPPROTO_TCP|IPPROTO_UDP|IPPROTO_ICMP)发送接收ip数据包，不能用IPPROTO_IP，因为如果是用了IPPROTO_IP，系统根本就不知道该用什么协议。 socket(PF_PACKET, SOCK_RAW, htons(ETH_P_IP|ETH_P_ARP|ETH_P_ALL))发送接收以太网数据帧 socket(AF_INET, SOCK_PACKET, htons(ETH_P_IP|ETH_P_ARP|ETH_P_ALL))过时了,不要用啊 SOCK_RAW, SOCK_DGRAM和SOCK_PACKET区别 在socket的第一个参数使用PF_PACKET的时候，上述三种socket的类型都可以使用。但是有区别。 使用SOCK_RAW发送的数据必须包含链路层的协议头，接受得到的数据包，包含链路层协议头。而使用SOCK_DGRAM则都不含链路层的协议头。 SOCK_PACKET也是可以使用的，但是已经废弃，以后不保证还能支持，不推荐使用。 在使用SOCK_RAW或SOCK_DGRAM和SOCK_PACKET时，在sendto和recvfrom中使用的地址类型不同，前两者使用sockaddr_ll类型的地址，而后者使用sockaddr类型的地址。 如socket的第一个参数使用PF_INET，第二个参数使用SOCK_RAW，则可以得到原始的IP包。 socket怎么知道要用哪个接口? man sendto里, send sendto sendmsg用于发送: #include #include ssize_t send(int sockfd, const void *buf, size_t len, int flags); ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen); ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags); send用于有连接的发送, 因为之前bind过, 接口已经确定. 和write()效果一样. sendto用sockaddr *dest_addr比如{sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"5.5.5.22\")}来确定 sendmsg用struct msghdr *msg中的msg.msg_name来指定接口 struct msghdr { void *msg_name; /* optional address */ socklen_t msg_namelen; /* size of address */ struct iovec *msg_iov; /* scatter/gather array */ size_t msg_iovlen; /* # elements in msg_iov */ void *msg_control; /* ancillary data, see below */ size_t msg_controllen; /* ancillary data buffer len */ int msg_flags; /* flags (unused) */ }; 对AF_PACKET来说, msg_name是struct sockaddr_ll, 其中的ifindex就指定了接口, 这个index是可以根据接口名称如eth0获取的. struct sockaddr_ll sll = { .sll_family = AF_PACKET, .sll_ifindex = ifindex }; 对AF_INET来说, msg_name是struct sockaddr, 比如{msg_name(16)={sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"5.5.5.22\") 参考: https://docs.oracle.com/cd/E88353_01/html/E37851/pf-packet-4p.html 交换机access与trunk口 都是对交换机来说的, PC/服务器的普通报文都没有vlan tag. Access口: 进出交换机access口的报文都没有vlan tag(有tag的直接丢弃), 进来时根据配好的规则, 按交换的端口号打vlan tag, 并在交换机内部安装vlan进行转发, 出交换机去掉vlan. Access 端口可接受并转发的数据来源：来自PC的无VLAN信息数据包；从一个Access口入打上VLAN标记在交换机内转交给相同VLAN的access口去掉标记的无VLAN信息数据包； Access 端口发送出去的数据包无VLAN信息，可被PC接受或Access口接受。 Trunk口: 可以转发带vlan tag的报文, 一般用于交换机和交换机连接. 结合Trunk口接受与发送数据包特点，可以得出经双绞线相连的一对Trunk口或同一交换机上的两个Trunk口可以将数据包（有或无VLAN信息）原封不动的从一端传到另一端。 PPS计算 1G的网口, 理论pps的计算方法(通常用64字节小包): 1000*1000*1000bps/8bit/(64+8+12)=1.488Mpps 所以, 比如40G网口理论pps为40G/1G * 1.488=59.5Mpps NVME-oF(NVM Express over Fabrics) Nvme命令跑在RDMA的Fabric上 NFV CPU有虚拟化了，计算能力大大提高；但网络在拖后腿 2012年10月，13家运营商发起在ETSI组织下正式成立网络功能虚拟化工作组，即ETSI ISG NFV，致力于实现网络虚拟化的需求定义和系统架构制定。 NFV就是基于大型共享的OTS（Off-The-Shelf，成品）服务器，通过软件定义的方式，探索网络实体的虚拟化使用。在NFV中使用的虚拟机（Virtual Machines，VMs）技术是虚拟化技术的一种。 基于软件定义的虚拟机部署成本低，而且可以快速适应网络需求变化。虚拟机就像是将所有能想到的东西都放在一台物理服务器（physical server）上，有了云计算和虚拟化，那些冗余的服务器都可以部署在独立的物理服务器上，不但可以并行处理，满足网络峰值需求，还可以根据网络需求随时释放资源，方便部署，利于故障管理，快速升级，快速满足市场需求。 NFV技术颠覆了传统电信封闭专用平台的思想，同时引入灵活的弹性资源管理理念，因此，ETSI NFV提出了突破传统网元功能限制、全新通用的NFV架构下图所示 NFV技术主要由3个部分构成：VNF（虚拟网络层，Virtualized Network Function）、NFVI（网络功能虚拟化基础设施NFVI，NFV Infrastructure)和MANO（NFV管理与编排，Management and Orchestration)。 （1）虚拟网络层是共享同一物理OTS服务器的VNF集。对应的就是各个网元功能的软件实现，比如EPC网元、IMS网元等的逻辑实现。 （2）NFVI，你可以将它理解为基础设施层，从云计算的角度看，就是一个资源池。NFVI需要将物理计算/存储/交换资源通过虚拟化转换为虚拟的计算/存储/交换资源池。NFVI映射到物理基础设施就是多个地理上分散的数据中心，通过高速通信网连接起来。 （3）NFVMANO。基于不同的服务等级协议（Service Level Agreements ，SLAs），NFVMANO运营支撑层负责“公平”的分配物理资源，同时还负责冗余管理、错误管理和弹性调整等，相当于目前目前的OSS/BSS系统。 这样一来，现在的移动通信网络结构就变成了这样： 上图顶部的VNF对应了网元功能的逻辑实现，比如，由多个VNF组成的VNF-FG（VNF Forwarding Graph）定义了LTE网络服务。 SDN和NFV的关系 NFV负责各种网元的虚拟化，而SDN负责网络本身的虚拟化（比如，网络节点和节点之间的相互连接）。我们先来看看一个典型的网络结构图。 如上图，一个网络由网络节点和节点间的链路组成。每一个节点都有一个控制面和与其它节点交换的网络信息。在上图中，右边的H节点获知一个新的网络（10.2.3.x/24）存在，现在它需要将这一信息告诉给网络中的其它节点。然而，节点H只和节点F和G直接相连，节点H通过链路状态通告（Link State Advertisements ，LSAs）通知节点F和G，F和G再将信息传递给它们的邻近节点，最终该消息传达到整个网络。这样，网络内每个节点都会更新自己的路由表，以确保数据可以传送到网络10.2.3.x/24。 如果节点C和E之间的链路中断，尽管C和E知道C-E链路中断，但节点A并不知道，节点A会继续通过C-E链路向网络10.2.3.x/24传送数据包。由于节点的“近视”，导致数据堆积在该节点，这需要花一些时间来向整个网络传送网络状态更新信息和完成纠错。网络越大，这种情况就越容易发生。 我们再来看看节点G，它由控制面（control plane）和数据面（data plane）组成。为了适应快速更新，控制面基本上是基于软件的，这实际上意味着控制消息的处理时长比基于硬件的逻辑单元（比如，数据面）要慢5到10倍。一直以来，我们仅要求控制面能够灵活更新，但对时延要求并不是太高。相对于数据面来讲，时延要求较高，我们希望数据包能够传送得越快越好，所以它必须是基于硬件来实现的。尽管基于软件实现的控制面能满足目前的要求，但随着设备的大量接入，特别是物联网的应用，控制面的时延也需要进一步提升。 软件定义网络（SDN）负责分离控制面和数据面，将网络控制面整合于一体。这样，网络控制面对网络数据面就有一个宏观的全面的视野。路由协议交换、路由表生成等路由功能均在统一的控制面完成。实现控制平面与数据平面分离的协议叫OpenFlow，OpenFlow是SDN一个网络协议。如下图所示： 从上图中，我们可以看到，首先需要通过OpenFlow将网络拓扑镜像到控制面，控制面初始化网络拓扑，初始化完成后，控制面会实时更新网络拓扑。 控制面完成初始化后，会向每个转发节点发送转发表，根据转发表用户数据在网络内传送。假设现在节点H获知新的网络 (10.2.3.x/24)。节点H将通过OpenFlow告知控制面，因为控制面统领全局，它可以快速的为每一个转发节点创建新的路由表，这样用户数据就可以传送到这个新网络。 DPDK DPDK要解决的问题 与Linux网络协议栈相比, DPDK有下面的改进： 异步，IO复用（poll，select，epoll） --> 用户态轮询 ，绑定核：解决频繁上下文切换的问题 控制面走linux，数据面走DPDK：提高PPS十倍以上 多核的锁竞争 --> 无锁技术；每个核维护自己的数据，各干各的。 TLB频繁失效 --> 大页，显著减小page table大小和TLB需求数；避免访问跨NUMA的内存 主要技术 UIO，hugepage，无锁循环队列 主核和从核 OVS SDN Application layer又叫北向(Northbound), Infrastructure layer又叫南向(Sourthbound). OVS场景举例 架构 Openvswitch数据库表结构 controller有很多 Beacon，Floodlight，Maestro，NodeFlow，NOX， POX，Trema "},"notes/as_title_qemu_ovs.html":{"url":"notes/as_title_qemu_ovs.html","title":"Qemu OVS和DPDK","keywords":"","body":"如题 "},"notes/OVS_DPDK_编译运行.html":{"url":"notes/OVS_DPDK_编译运行.html","title":"OVS-DPDK编译运行","keywords":"","body":" openvswitch 下载和编译 编译ovs2.9.3 运行 使用 常用命令 配flow举例 性能相关 实例 PHY-PHY PHY-VM-PHY (vHost Loopback) PHY-VM-PHY (vHost Loopback) (Kernel Forwarding) PHY-VM-PHY (vHost Multiqueue) OVS选项pmd-cpu-mask 关于多队列 DPDK多队列 默认情况 配成2个queue OVS配置 绑定pmd VM配置 Vlan和QinQ QoS OVS简介 参考 openvswitch 下载和编译 OVS主页有下载链接, 官方也有git: https://github.com/openvswitch/ovs也可以用内部的OVS git库, 和官网是同步的: git clone ssh://youraccount@bjsss013.hxtcorp.net:29418/networking/ovs #注意OVS和DPDK的版本搭配, 比如OVS2.8.4配合DPDK17.08 git checkout v2.8.4 #记录一下安装准备 sudo yum group install \"Development Tools\" #安装推荐的组件(可选), 详见http://docs.openvswitch.org/en/latest/intro/install/general/#build-requirements yum install libcap-ng-devel openssl-devel unbound-devel #生成configure(直接下载的tar包, 可略过这一步) ./boot.sh #编译, 用with-dpdk参数. 需要dpdk已经成功编译并安装 #详见http://docs.openvswitch.org/en/latest/intro/install/dpdk/ DPDK_ROOT=/home/bai/repo/hxt/mainstream-dpdk-stable/ ./configure --with-dpdk=$DPDK_ROOT/arm64-armv8a-linuxapp-gcc LIBS=-libverbs #或者, 加上debug, 调试时使用 ./configure --with-debug --with-dpdk=$DPDK_ROOT/arm64-armv8a-linuxapp-gcc LIBS=-libverbs CFLAGS=\"-g -O0\" #开始编译 make -j sudo make install configure阶段, 要加参数LIBS=-libverbs, 因为Mellanox驱动会用到/lib64/libibverbs.so.1, 它是ofed驱动包安装到系统的. 编译ovs2.9.3 ./boot.sh sudo yum install python2-pip sudo pip install --upgrade pip sudo pip install six DPDK_ROOT=/home/bai/repo/hxt/mainstream-dpdk-stable/ ./configure --with-dpdk=$DPDK_ROOT/arm64-armv8a-linuxapp-gcc LIBS=\"-libverbs -lmlx5 -lmlx4\" make -j sudo make install 注: ofed4.4.1里面, 把ibverbs又分成了mlx4和mlx5 drivers/net/mlx4/Makefile:LDLIBS += -libverbs -lmlx4 drivers/net/mlx5/Makefile:LDLIBS += -libverbs -lmlx5 运行 export PATH=$PATH:/usr/local/share/openvswitch/scripts sudo ovs-ctl start #打开dpdk初始化 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true #看log sudo cat /usr/local/var/log/openvswitch/ovs-vswitchd.log 如果sudo命令找不到ovs相关的命令, 需要sudo visudo然后注释掉secure_path所在行, 否则出于安全原因, sudo会reset环境变量到secure_path提供的值. 使用 #新增一个bridge sudo ovs-vsctl add-br ovsbr0 -- set bridge ovsbr0 datapath_type=netdev #查看现有bridge sudo ovs-vsctl show sudo ovs-vsctl list-br #列出Open_vSwitch表(root 表) sudo ovs-vsctl list Open_vSwitch #总表下面, 还有默认的几个表: Bridge Port Interface Controller等等 sudo ovs-vsctl list Bridge sudo ovs-vsctl list Port sudo ovs-vsctl list Interface #先查看一下系统上的网卡情况 sudo ibdev2netdev -v #添加dpdk端口 sudo ovs-vsctl add-port ovsbr0 dpdkp0 -- set Interface dpdkp0 type=dpdk options:dpdk-devargs=0005:01:00.0 #查看所有端口 sudo ovs-vsctl list-ports ovsbr0 #查看端口信息, 包括mtu, mac, 收发包统计 sudo ovs-vsctl list interface dpdkp1 #删除端口 sudo ovs-vsctl del-port dpdkp0 关于datapath_type sudo ovs-vsctl add-br ovsbr0 -- set bridge ovsbr0 datapath_type=netdev执行后, ifconfig -a看到多出来两个interface, ovsbr0和ovs-netdev, 用ethtool -i查看, 发现driver都是tun datapath_type=netdev是DPDK要求的方式, 详见http://docs.openvswitch.org/en/latest/howto/dpdk/ 如果不设置, 默认是system方式. 和下面命令效果相同 sudo ovs-vsctl set bridge ovsbr0 datapath_type=system 会发现ovs-netdev变成了ovs-system, driver也由tun变成了openvswitch 对一个PCIe, 两个port的网卡, 用下面的命令添加端口 ovs-vsctl add-port br0 dpdk-p0 -- set Interface dpdk-p0 type=dpdk options:dpdk-devargs=\"class=eth,mac=00:11:22:33:44:55\" ovs-vsctl add-port br0 dpdk-p0 -- set Interface dpdk-p1 type=dpdk options:dpdk-devargs=\"class=eth,mac=00:11:22:33:44:56\" 添加一个dpdk的port后, ovs会使一个core进入poll模式, 占用CPU100%. 调用路径是netdev_dpdk_rxq_recv()@ovs/lib/netdev-dpdk.c->rte_eth_rx_burst()@dpdk/lib/librte_ether/rte_ethdev.h $ sudo pstack 10320 Thread 1 (process 10320): #0 mlx5_rx_burst (dpdk_rxq=0xfff78da20b80, pkts=, pkts_n=) at /home/bai/share/mint/repo/hxt/dpdk/drivers/net/mlx5/mlx5_rxtx.c:1805 #1 0x00000000007a3504 in rte_eth_rx_burst (nb_pkts=32, rx_pkts=0xffff08f9e2a0, queue_id=0, port_id=1 '\\001') at /home/bai/share/mint/repo/hxt/dpdk/arm64-armv8a-linuxapp-gcc/include/rte_ethdev.h:2747 #2 netdev_dpdk_rxq_recv (rxq=, batch=0xffff08f9e290) at lib/netdev-dpdk.c:1733 #3 0x00000000006fbda8 in netdev_rxq_recv (rx=rx@entry=0xfff78d9ff600, batch=0xffff08f9e290, batch@entry=0xffff08f9e2b0) at lib/netdev.c:701 #4 0x00000000006d8740 in dp_netdev_process_rxq_port (pmd=pmd@entry=0xffff602e0010, rx=0xfff78d9ff600, port_no=2) at lib/dpif-netdev.c:3113 #5 0x00000000006d89c8 in pmd_thread_main (f_=0xffff602e0010) at lib/dpif-netdev.c:3854 #6 0x000000000074c9ec in ovsthread_wrapper (aux_=) at lib/ovs-thread.c:348 #7 0x0000ffff8aa17bb8 in start_thread () from /lib64/libpthread.so.0 #8 0x0000ffff8a7ffb50 in thread_start () from /lib64/libc.so.6 常用命令 #关闭megaflows, 默认打开的; 关闭后flow信息更详细 #根据文档, megaflow是microflow后面的一级cache, 应该算是第一级别的cache, 再后面是dpcls $ sudo ovs-appctl list-commands | grep mega upcall/disable-megaflows upcall/enable-megaflows #老化时间配置成一个小时, 默认是10秒 ovs-vsctl --no-wait set Open_vSwitch . other_config:max-idle=3600000 #统计 sudo ovs-appctl dpctl/show --statistics #dump自动生成的flow信息, 哪个core, 什么action; 和上面命令对应着看 sudo ovs-appctl dpctl/dump-flows #删除所有flow sudo ovs-appctl dpctl/del-flows #openflow信息, 能看link状态, 端口mac地址; sudo ovs-ofctl show ovsbr0 #rxq具体信息: 哪个core在pulling哪个port的哪个q sudo ovs-appctl dpif-netdev/pmd-rxq-show #pmd的信息, 包括emc hits, idle cycles, processing cycles等信息 sudo ovs-appctl dpif-netdev/pmd-stats-show sudo ovs-appctl dpif-netdev/pmd-stats-clear -pmd 33 sudo ovs-appctl dpif-netdev/pmd-stats-show -pmd 33 #pmd的性能信息: 很好用, 但新版本才有; pps, emc hit等等 sudo ovs-appctl dpif-netdev/pmd-stats-clear && sleep 1 && sudo ovs-appctl dpif-netdev/pmd-perf-show -pmd 34 sudo ovs-appctl dpif-netdev/pmd-stats-clear && sleep 1 && sudo ovs-appctl dpif-netdev/pmd-perf-show | grep -E \"thread|Rx|Tx\" #多个命令入口 sudo ovs-appctl list-commands $ sudo ovs-appctl list-commands | grep show autoattach/show-isid [bridge] bfd/show [interface] bond/show [port] cfm/show [interface] coverage/show dpctl/ct-stats-show [dp] [zone=N] dpctl/show [dp...] dpif-netdev/pmd-perf-show [-nh] [-it iter-history-len] [-ms ms-history-len] [-pmd core] [dp] dpif-netdev/pmd-rxq-show [-pmd core] [dp] dpif-netdev/pmd-stats-show [-pmd core] [dp] dpif/show dpif/show-dp-features bridge fdb/show bridge fdb/stats-show bridge lacp/show [port] lacp/show-stats [port] mdb/show bridge memory/show ovs/route/show qos/show interface qos/show-types interface rstp/show [bridge] stp/show [bridge] tnl/arp/show tnl/neigh/show tnl/ports/show -v upcall/show #看看log sudo tail -f /usr/local/var/log/openvswitch/ovs-vswitchd.log #openflow信息, 能看link状态, 端口mac地址(交换机的端口也有mac地址?) sudo ovs-ofctl show ovsbr0 #端口统计 sudo ovs-ofctl dump-ports ovsbr0 #底层端口信息: q个数, rxd/txd个数, mtu等 sudo ovs-appctl dpif/show #修改mtu sudo ovs-vsctl set Interface ovsbr0 mtu_request=4000 sudo ovs-vsctl set Interface dpdkp1 mtu_request=4000 sudo ovs-vsctl set Interface dpdkvhostuser0 mtu_request=4000 sudo ovs-vsctl set Interface dpdkvhostuser1 mtu_request=4000 配flow举例 #查现有flow(配置的flow, 不包括自动生成的flow) $ sudo ovs-ofctl dump-flows ovsbr0 # 接口的name和ofport对应关系 sudo ovs-vsctl -- --columns=name,ofport list Interface #从vhostuser0到vhostuser1 sudo ovs-vsctl set interface dpdkp1 ofport_request=1 sudo ovs-vsctl set interface \"dpdkvhostuser0\" ofport_request=2 sudo ovs-vsctl set interface \"dpdkvhostuser1\" ofport_request=3 sudo ovs-ofctl add-flow ovsbr0 in_port=2,action=output:3 sudo ovs-ofctl add-flow ovsbr0 in_port=3,action=output:2 #清除所有flow sudo ovs-ofctl del-flows ovsbr0 #清除flow后加回默认flow, 否则端口都不通 sudo ovs-ofctl add-flow ovsbr0 actions=NORMAL #查看详细的flow sudo ovs-appctl dpif/dump-flows ovsbr0 #更详细的flow, 哪个core, 什么action sudo ovs-appctl dpctl/dump-flows Open vSwitch Cheat Sheet 详细命令参考 性能相关 参考: http://docs.openvswitch.org/en/latest/intro/install/dpdk/#performance-tuning 需要考虑: OS level的优化, 比如: CPU频率, cmdline里面isolcpus=<> 编译选项的优化 多队列, 多核, 多描述符: pmd-cpu-mask, options:n_rxq=, options:n_rxq_desc= 亲和性: taskset, 或者在qemu配置里面vcpu pin 修改EMC(Exact Match Cache)大小: 这个EMC保存着fast path的流表, 修改EM_FLOW_HASH_SHIFT in lib/dpif-netdev.c, 适当改大点 关闭Rx Mergeable Buffers, 这个是给jumbo报文用的, reserve 描述符. mrg_rxbuf=off 报文缓存发送: Output Packet Batching: 对于中断驱动的VM的virtio driver来说, 发送报文是多缓存一些, 集中发送能显著减少中断次数, 从而显著减小guest里面的中断开销, 增大throughput, 但会增加latency. pmd绑定核 http://docs.openvswitch.org/en/latest/topics/dpdk/pmd/ #配置txq和rxq描述符个数: 通常描述符少则latency低, 但throughput不高; 描述符多则相反 ovs-vsctl set Interface dpdk0 options:n_rxq_desc= ovs-vsctl set Interface dpdk0 options:n_txq_desc= #配置flush时间为50us, 默认为0; ovs-vsctl set Open_vSwitch . other_config:tx-flush-interval=50 #另外还可尝试tx-burst-interval 实例 PHY-PHY 两个dpdk的物理port # Add userspace bridge $ ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev # Add two dpdk ports $ ovs-vsctl add-port br0 phy0 -- set Interface phy0 type=dpdk \\ options:dpdk-devargs=0000:01:00.0 ofport_request=1 $ ovs-vsctl add-port br0 phy1 -- set Interface phy1 type=dpdk options:dpdk-devargs=0000:01:00.1 ofport_request=2 配置flow, phy0和phy1互相转发 # Clear current flows $ ovs-ofctl del-flows br0 # Add flows between port 1 (phy0) to port 2 (phy1) $ ovs-ofctl add-flow br0 in_port=1,action=output:2 $ ovs-ofctl add-flow br0 in_port=2,action=output:1 PHY-PHY示意图 PHY-VM-PHY (vHost Loopback) 两个dpdk (PHY) ports, 两个dpdkvhostuser ports: vhost-user模式下(dpdkvhostuser), ovs是server, qemu是client; 新版本推荐使用同样是c-s模式的vhost-user-client模式(dpdkvhostuserclient), 此时ovs是client, qemu是server ```bash Add userspace bridge $ ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev Add two dpdk ports $ ovs-vsctl add-port br0 phy0 -- set Interface phy0 type=dpdk \\ options:dpdk-devargs=0000:01:00.0 ofport_request=1 $ ovs-vsctl add-port br0 phy1 -- set Interface phy1 type=dpdk options:dpdk-devargs=0000:01:00.1 ofport_request=2 Add two dpdkvhostuser ports $ ovs-vsctl add-port br0 dpdkvhostuser0 \\ -- set Interface dpdkvhostuser0 type=dpdkvhostuser ofport_request=3 $ ovs-vsctl add-port br0 dpdkvhostuser1 \\ -- set Interface dpdkvhostuser1 type=dpdkvhostuser ofport_request=4 > 增加vhost-user端口后, 会在`/usr/local/var/run/openvswitch/`下生成socket文件, 起VM的时候要作为参数传给qemu 配流: ```bash # Clear current flows $ ovs-ofctl del-flows br0 # Add flows $ ovs-ofctl add-flow br0 in_port=1,action=output:3 $ ovs-ofctl add-flow br0 in_port=3,action=output:1 $ ovs-ofctl add-flow br0 in_port=4,action=output:2 $ ovs-ofctl add-flow br0 in_port=2,action=output:4 # Dump flows $ ovs-ofctl dump-flows br0 用qemu起一个VM $ export VM_NAME=vhost-vm $ export GUEST_MEM=3072M $ export QCOW2_IMAGE=/root/CentOS7_x86_64.qcow2 $ export VHOST_SOCK_DIR=/usr/local/var/run/openvswitch $ taskset 0x20 qemu-system-x86_64 -name $VM_NAME -cpu host -enable-kvm \\ -m $GUEST_MEM -drive file=$QCOW2_IMAGE --nographic -snapshot \\ -numa node,memdev=mem -mem-prealloc -smp sockets=1,cores=2 \\ -object memory-backend-file,id=mem,size=$GUEST_MEM,mem-path=/dev/hugepages,share=on \\ -chardev socket,id=char0,path=$VHOST_SOCK_DIR/dpdkvhostuser0 \\ -netdev type=vhost-user,id=mynet1,chardev=char0,vhostforce \\ -device virtio-net-pci,mac=00:00:00:00:00:01,netdev=mynet1,mrg_rxbuf=off \\ -chardev socket,id=char1,path=$VHOST_SOCK_DIR/dpdkvhostuser1 \\ -netdev type=vhost-user,id=mynet2,chardev=char1,vhostforce \\ -device virtio-net-pci,mac=00:00:00:00:00:02,netdev=mynet2,mrg_rxbuf=off 前面说过, ovs的vhost-user port会生成socket文件, 要传到qemu; 另外, qemu必须在hugetlbfs上为vm分配大页内存, 让vhost-user port能够访问在大页上的vitio-net设备的虚拟rings和packet buffers 配多个queue举例: -chardev socket,id=char2,path=/usr/local/var/run/openvswitch/vhost-user-2 -netdev type=vhost-user,id=mynet2,chardev=char2,vhostforce,queues=$q -device virtio-net-pci,mac=00:00:00:00:00:02,netdev=mynet2,mq=on,vectors=$v 在VM里面编译DPDK, 运行testpmd $ cd $DPDK_DIR/app/test-pmd; $ ./testpmd -c 0x3 -n 4 --socket-mem 1024 -- \\ --burst=64 -i --txqflags=0xf00 --disable-hw-vlan $ set fwd mac retry $ start PHY-VM-PHY示意图: 这个例子中, 在VM里跑testpmd, 跑完了要还原vNIC的driver到kernel $ $DPDK_DIR/usertools/dpdk-devbind.py --bind=virtio-pci 0000:00:03.0 $ $DPDK_DIR/usertools/dpdk-devbind.py --bind=virtio-pci 0000:00:04.0 PHY-VM-PHY (vHost Loopback) (Kernel Forwarding) 在VM不用dpdk-testpmd, 而是用kernel协议栈做转发 $ ip addr add 1.1.1.2/24 dev eth1 $ ip addr add 1.1.2.2/24 dev eth2 $ ip link set eth1 up $ ip link set eth2 up $ systemctl stop firewalld.service $ systemctl stop iptables.service $ sysctl -w net.ipv4.ip_forward=1 $ sysctl -w net.ipv4.conf.all.rp_filter=0 $ sysctl -w net.ipv4.conf.eth1.rp_filter=0 $ sysctl -w net.ipv4.conf.eth2.rp_filter=0 $ route add -net 1.1.2.0/24 eth2 $ route add -net 1.1.1.0/24 eth1 $ arp -s 1.1.2.99 DE:AD:BE:EF:CA:FE $ arp -s 1.1.1.99 DE:AD:BE:EF:CA:EE PHY-VM-PHY (vHost Multiqueue) 部分命令: $ ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0xc $ ovs-vsctl set Interface phy0 options:n_rxq=2 $ ovs-vsctl set Interface phy1 options:n_rxq=2 对虚拟网卡dpdkvhostuser也可以设置n_rxq 请参考: http://docs.openvswitch.org/en/latest/howto/dpdk/#phy-vm-phy-vhost-multiqueue OVS选项pmd-cpu-mask #dpdk-lcore-mask是指DPDK可以运行在哪些core上(有可能运行dpdk的core), 这里是core 0, 22, 44不能跑dpdk #非datapath线程运行的core, 对性能影响不大 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-lcore-mask=0xffffefffffbffffe #这是个2 node的numa系统, 每个node分配1024M内存 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"1024,1024\" #pmd-cpu-mask是指哪些core运行PMD(实际运行PMD的core), core 1 2 3 4, 45 46 47 48跑DPDK sudo ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=1E0000000001E The dpdk-lcore-mask is a core bitmask that is used during DPDK initialization and it is where the non-datapath OVS-DPDK threads such as handler and revalidator threads run. As these are for non-datapath operations, the dpdk-lcore-mask does not have any significant performance impact on multiple NUMA systems. There is a nice default option when dpdk-lcore-mask is not set, in which the OVS-DPDK cpuset will be used for handler and revalidator threads. This means they can be scheduled across multiple cores by the Linux scheduler. 参考: https://software.intel.com/en-us/articles/set-up-open-vswitch-with-dpdk-on-ubuntu-server https://developers.redhat.com/blog/2017/06/28/ovs-dpdk-parameters-dealing-with-multi-numa/ 关于多队列 DPDK多队列 参考: http://docs.openvswitch.org/en/latest/topics/dpdk/pmd/ 默认情况 默认情况下, OVS和VM之间使用一个Queue(一对RX和TX), 如下图: vhost-user 默认配置: 更进一步讲, 物理NIC默认也是一个Queue来从物理网卡发送和接收(图中NIC部分的Q0); Host上的OVS-DPDK, 也是用一个Queue, 通过一个PMD来收发包; VM上的VNIC, 一般是内核态的virtio驱动(也可以是用户态dpdk的), 也是用一个queue. 配成2个queue 可以配置2个queue, 如下图: vhost-user 2 queue: OVS配置 #Add a bridge ‘br0’ as type netdev. ovs-vsctl add-br br0 -- set Bridge br0 datapath_type=netdev #Add physical port as ‘dpdk0’, as port type=dpdk and request ofport=1. ovs-vsctl add-port br0 dpdk0 -- set Interface dpdk0 type=dpdk ofport_request=1 #Add vHost-user port as ‘vhost-user0’, as port type= dpdkvhostuser and request ofport=2. ovs-vsctl add-port br0 vhost-user0 -- set Interface vhost-user0 type=dpdkvhostuser ofport_request=2 #Delete any existing flows on the bridge. ovs-ofctl del-flows br0 #Configure flow to send traffic received on port 1 to port 2 (dpdk0 to vhost-user0). ovs-ofctl add-flow br0 in_port=1,action=output:2 #Set the PMD thread configuration to two threads to run on the host. In this example core 2 and core 3 on the host are used. ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=C #Set the number of receive queues for the physical port ‘dpdk0’ to 2. ovs-vsctl set Interface dpdk0 options:n_rxq=2 #Set the number of receive queues for the vHost-user port ‘vhost-user0’ to 2. ovs-vsctl set Interface vhost-user0 options:n_rxq=2 绑定pmd #配置多queue, 绑定pmd core sudo ovs-vsctl set Interface dpdkvhostuser0 options:n_rxq=2 other_config:pmd-rxq-affinity=\"0:34,1:35\" VM配置 ./qemu/x86_64-softmmu/qemu-system-x86_64 -cpu host -smp 2,cores=2 -hda /root/fedora-22.img -m 2048M --enable-kvm -object memory-backend-file,id=mem,size=2048M,mem-path=/dev/hugepages,share=on -numa node,memdev=mem -mem-prealloc -chardev socket,id=char1,path=/usr/local/var/run/openvswitch/vhost-user0 -netdev type=vhost-user,id=mynet1,chardev=char1,vhostforce,queues=2 -device virtio-net-pci,mac=00:00:00:00:00:01,netdev=mynet1,mq=on,vectors=6 vectors的个数是2*queue个数+2 参数如下: 参考: https://software.intel.com/en-us/articles/configure-vhost-user-multiqueue-for-ovs-with-dpdk Vlan和QinQ 测试示意图: 在Host B上: #将两个VM对应的port加入vlan 100(外层 vlan), 并配置cvlan(内层 vlan)为0和10 sudo ovs-vsctl set port dpdkvhostuser0 vlan_mode=dot1q-tunnel tag=100 cvlans=0,10 other-config:qinq-ethtype=802.1q sudo ovs-vsctl set port dpdkvhostuser1 vlan_mode=dot1q-tunnel tag=100 cvlans=0,10 other-config:qinq-ethtype=802.1q #将物理网卡加入vlan 100, native-tagged是说出端口带tag, 详见ofproto/ofproto.h sudo ovs-vsctl set port dpdkp1 vlan_mode=native-tagged tag=100 other-config:qinq-ethtype=802.1q 在VM01上 #配置vm01的接口IP为192.168.1.100 ip addr add 192.168.1.100/24 dev eth1 #增加vlan 10 ip link add link eth1 name eth1.10 type vlan id 10 #给vlan 10子接口配置IP ip addr add 192.168.10.100/24 dev eth1.10 ip link set up dev eth1.10 在VM02上 #配置vm01的接口IP为192.168.1.200 ip addr add 192.168.1.200/24 dev eth1 #增加vlan 10 ip link add link eth1 name eth1.10 type vlan id 10 #给vlan 10子接口配置IP ip addr add 192.168.10.200/24 dev eth1.10 ip link set up dev eth1.10 QoS OVS支持Ingress和Egress两个方向的流控, 注意: 方向是对OVS来说的. Egress OVS的出方向, 限制OVS发包速率 参考: https://software.intel.com/en-us/articles/qos-configuration-and-usage-for-open-vswitch-with-dpdk #查看port支持的qos类型, 目前只有egress-policer for OVS-DPDK $ sudo ovs-appctl -t ovs-vswitchd qos/show-types \"dpdkp1\" QoS type: egress-policer bai@CentOS-43 ~/share/mint/repo/hxt/packages/libvirt-4.6.0 $ sudo ovs-appctl -t ovs-vswitchd qos/show-types \"dpdkvhostuser0\" QoS type: egress-policer #查看当前QoS配置 ovs-appctl -t ovs-vswitchd qos/show vhost-user2 #清除QoS配置 ovs-vsctl -- destroy QoS vhost-user2 -- clear Port vhost-user2 qos #限制vhost-user2这个port发送到对端VM的速率, cir是Byte/second, 换算成bit是10M bps ovs-vsctl set port vhost-user2 qos=@newqos -- --id=@newqos create qos type=egress-policer other-config:cir=1250000 other-config:cbs=2048 其中: type= egress-policer: The QoS type to set on the port. In this case ‘egress-policer’. other_config=cir: Committed Information Rate, the maximum rate (in bytes) that the port should be allowed to send. other_config=cbs: Committed Burst Size measured in bytes and represents a token bucket. At a minimum should be set to the expected largest size packet. Ingress OVS的入方向, 对它所连接的对端eg.VM来说, 是限制其发速率 ingress_policing_rate the maximum rate (in Kbps) that this VM should be allowed to send ingress_policing_burst a parameter to the policing algorithm to indicate the maximum amount of data (in Kb) that this interface can send beyond the policing rate.#限制入口速率1 Mbps $ ovs-vsctl set interface tap0 ingress_policing_rate=1000 $ ovs-vsctl set interface tap0 ingress_policing_burst=100 #查看配置 ovs-vsctl list interface tap0 #清除速度限制 $ ovs-vsctl set interface vhost-user0 ingress_policing_rate=0 OVS简介 参考 dpdk howto dpdk vhost-user DPDK在OpenStack中的实现 "},"notes/OVS_架构和代码.html":{"url":"notes/OVS_架构和代码.html","title":"OVS架构和代码","keywords":"","body":" vhost-user协议 KVM的irqfd和ioeventfd irqfd ioeventfd eventfd在ovs的使用 qemu的通知机制 qemu里的eventfd eventfd由qemu通过msg传递到ovs-dpdk 在dpdk的vhost驱动中, 写eventfd 补充: 通过unix socket来传递文件描述符 关于OVS流匹配 smc-enable: signature match cache vhost协议 vhost架构 补充 OVS-DPDK关键路径 dpdkp0的关键路径 vhostuser0的关键路径 buffer到底多大 datapath kernel datapath userspace datapath Architecure vport类型 netdev internal patch tunnel vports: vxlan, gre, etc 用户态通过物理口发包流程 使用perf抓关键路径 结论 vhost-user协议 详见qemu/docs/interop/vhost-user.txt Qemu是master, ovs是slave; master通过unix socket共享它的虚拟队列(virtqueues); 当OVS的虚拟port是dpdkvhostuserclient模式时, 这个socket由Qemu来listen, 由OVS来connect 在socket上, 传递消息的格式如下: ------------------------------------ | request | flags | size | payload | ------------------------------------ * Request: 32-bit type of the request * Flags: 32-bit bit field: - Lower 2 bits are the version (currently 0x01) - Bit 2 is the reply flag - needs to be sent on each reply from the slave - Bit 3 is the need_reply flag - see VHOST_USER_PROTOCOL_F_REPLY_ACK for details. * Size - 32-bit size of the payload payload格式会根据不同的类型来改变, 比如vring状态描述, vring地址描述, 内存地址描述, IOTLB消息, virtio配置空间 //消息定义 typedef struct VhostUserMsg { VhostUserRequest request; uint32_t flags; uint32_t size; union { uint64_t u64; struct vhost_vring_state state; struct vhost_vring_addr addr; VhostUserMemory memory; VhostUserLog log; struct vhost_iotlb_msg iotlb; VhostUserConfig config; }; } QEMU_PACKED VhostUserMsg; vhost-user用的消息协议跑在socket上, 但大部分消息兼容kernel的vhost驱动用的ioctl中的消息;master发request, slave发reply, 但大部分消息不需要reply, 除了以下几个: * VHOST_USER_GET_FEATURES * VHOST_USER_GET_PROTOCOL_FEATURES * VHOST_USER_GET_VRING_BASE * VHOST_USER_SET_LOG_BASE (if VHOST_USER_PROTOCOL_F_LOG_SHMFD) 有些消息传的是fd * VHOST_USER_SET_MEM_TABLE * VHOST_USER_SET_LOG_BASE (if VHOST_USER_PROTOCOL_F_LOG_SHMFD) * VHOST_USER_SET_LOG_FD * VHOST_USER_SET_VRING_KICK * VHOST_USER_SET_VRING_CALL * VHOST_USER_SET_VRING_ERR * VHOST_USER_SET_SLAVE_REQ_FD KVM的irqfd和ioeventfd irqfd Allows an fd to be used to inject an interrupt to the guest KVM provides a complete virtual system environment for guests, including support for injecting interrupts modeled after the real exception/interrupt facilities present on the native platform (such as the IDT on x86). Virtual interrupts can come from a variety of sources (emulated devices, pass-through devices, etc) but all must be injected to the guest via the KVM infrastructure. This patch adds a new mechanism to inject a specific interrupt to a guest using a decoupled eventfd mechnanism: Any legal signal on the irqfd (using eventfd semantics from either userspace or kernel) will translate into an injected interrupt in the guest at the next available interrupt window. 在kvm_vm_ioctl()中, 增加KVM_ASSIGN_IRQFD, 调用kvm_assign_irqfd(), 把irqfd_wakeup()加到底下eventfd的回调里面, 通过workqueue调用irqfd_inject()完成中断注入https://patchwork.kernel.org/patch/23307/ ioeventfd Allow an fd to be used to receive a signal from the guesthttps://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=d34e6b175e61821026893ec5298cc8e7558df43a ioeventfd is a mechanism to register PIO/MMIO regions to trigger an eventfd signal when written to by a guest. Host userspace can register any arbitrary IO address with a corresponding eventfd and then pass the eventfd to a specific end-point of interest for handling. Normal IO requires a blocking round-trip since the operation may cause side-effects in the emulated model or may return data to the caller. Therefore, an IO in KVM traps from the guest to the host, causes a VMX/SVM \"heavy-weight\" exit back to userspace, and is ultimately serviced by qemu's device model synchronously before returning control back to the vcpu. However, there is a subclass of IO which acts purely as a trigger for other IO (such as to kick off an out-of-band DMA request, etc). For these patterns, the synchronous call is particularly expensive since we really only want to simply get our notification transmitted asychronously and return as quickly as possible. All the sychronous infrastructure to ensure proper data-dependencies are met in the normal IO case are just unecessary overhead for signalling. This adds additional computational load on the system, as well as latency to the signalling path. Therefore, we provide a mechanism for registration of an in-kernel trigger point that allows the VCPU to only require a very brief, lightweight exit just long enough to signal an eventfd. This also means that any clients compatible with the eventfd interface (which includes userspace and kernelspace equally well) can now register to be notified. The end result should be a more flexible and higher performance notification API for the backend KVM hypervisor and perhipheral components. eventfd在ovs的使用 qemu的通知机制 //qemu/util/event_notifier-posix.c int event_notifier_init(EventNotifier *e, int active) //如果有定义CONFIG_EVENTFD ret = eventfd(0, EFD_NONBLOCK | EFD_CLOEXEC); e->rfd = e->wfd = ret; //否则用pipe()和fcntl()的组合来产生两个非阻塞的fd, 最后e->rfd = fds[0];e->wfd = fds[1]; int event_notifier_get_fd(const EventNotifier *e) return e->rfd; //写fd int event_notifier_set(EventNotifier *e) ret = write(e->wfd, &value, sizeof(value)); //读fd int event_notifier_test_and_clear(EventNotifier *e) read(e->rfd, buffer, sizeof(buffer)); qemu里的eventfd //qemu/hw/virtio/vhost-user.c //这里我要找file->fd; 那关键是要找调用了这个函数的file参数 vhost_user_set_vring_call(struct vhost_dev *dev, struct vhost_vring_file *file) vhost_set_vring_file(dev, VHOST_USER_SET_VRING_CALL, file); if (ioeventfd_enabled() && file->fd > 0) fds[0] = file->fd; vhost_user_write(dev, &msg, fds,1) //qemu/hw/virtio/vhost-user.c qemu_chr_fe_set_msgfds() qemu_chr_fe_write_all() //qemu/chardev/char.c qemu_chr_write() qemu_chr_write_buffer() //其实是tcp_chr_write() cc->chr_write() tcp_chr_write() io_channel_send_full() //qemu/io/channel.c qio_channel_writev_full() //其实是qio_channel_socket_writev() klass->io_writev() //qemu/io/channel-socket.c qio_channel_socket_writev() cmsg->cmsg_type = SCM_RIGHTS; memcpy(CMSG_DATA(cmsg), fds, fdsize); sendmsg(sioc->fd, &msg, 0); //qemu/hw/virtio/vhost-user.c const VhostOps user_ops = { .backend_type = VHOST_BACKEND_TYPE_USER, .vhost_backend_init = vhost_user_init, ... .vhost_set_vring_call = vhost_user_set_vring_call, ... } //qemu/hw/virtio/vhost-backend.c //注册vhost user在qemu里面的驱动 int vhost_set_backend_type(struct vhost_dev *dev, VhostBackendType backend_type) switch (backend_type) case VHOST_BACKEND_TYPE_KERNEL: dev->vhost_ops = &kernel_ops; case VHOST_BACKEND_TYPE_USER: dev->vhost_ops = &user_ops; //下面要找dev->vhost_ops->vhost_set_vring_call //在qemu/hw/virtio/vhost.c static int vhost_virtqueue_init() //生成eventfd event_notifier_init(&vq->masked_notifier, 0); //vq->masked_notifier->rfd就是刚才生成的eventfd file.fd = vq->masked_notifier->rfd dev->vhost_ops->vhost_set_vring_call(dev, &file); //也有可能是vq->guest_notifier->rfd void vhost_virtqueue_mask() file.fd = vq->guest_notifier->rfd; eventfd由qemu通过msg传递到ovs-dpdk //在OVS代码里面的netdev_dpdk_vhost_client_reconfigure被调用 //ovs/lib/netdev-dpdk.c static const struct netdev_class dpdk_vhost_client_class = NETDEV_DPDK_CLASS( \"dpdkvhostuserclient\", NULL, netdev_dpdk_vhost_client_construct, netdev_dpdk_vhost_destruct, netdev_dpdk_vhost_client_set_config, NULL, netdev_dpdk_vhost_send, netdev_dpdk_vhost_get_carrier, netdev_dpdk_vhost_get_stats, NULL, NULL, netdev_dpdk_vhost_user_get_status, netdev_dpdk_vhost_client_reconfigure, netdev_dpdk_vhost_rxq_recv); //DPDK的代码: //lib/librte_vhost/socket.c rte_vhost_driver_start(const char *path) vsocket = find_vhost_user_socket(path); //起一个线程, 不断poll fdsets pthread_create(&fdset_tid, NULL, fdset_event_dispatch, &vhost_user.fdset); //fdset_event_dispatch()@lib/librte_vhost/fd_man.c while(1) poll(pfdset->rwfds, numfds, 1000 /* millisecs */); //for pfdentry in pfdset //该fd没有event if (!pfd->revents) continue; fd = pfdentry->fd; rcb = pfdentry->rcb; wcb = pfdentry->wcb; dat = pfdentry->dat; if (rcb && pfd->revents & (POLLIN | FDPOLLERR)) rcb(fd, dat, &remove1); //读unix socket的回调函数 vhost_user_read_cb(int connfd, void *dat, int *remove) //lib/librte_vhost/vhost_user.c vhost_user_msg_handler(int vid, int fd) read_vhost_message(fd, &msg); //这个函数有讲究: 通过unix socket的SCM_RIGHTS可以传递文件描述符!详见下小节 //lib/librte_vhost/socket.c read_fd_message() struct msghdr msgh; recvmsg(sockfd, &msgh, 0); cmsg = CMSG_FIRSTHDR(&msgh) //fd是在msgh的cmsg的CMSG_DATA字段里 memcpy(fds, CMSG_DATA(cmsg), fdsize); vhost_user_check_and_alloc_queue_pair(dev, &msg); switch (msg.request.master) case VHOST_USER_SET_MEM_TABLE: case VHOST_USER_SET_VRING_NUM: case VHOST_USER_SET_VRING_ADDR: case VHOST_USER_SET_VRING_BASE: case VHOST_USER_SET_VRING_CALL: vhost_user_set_vring_call(dev, &msg); //看起来这个fd是msg传过来的 file.fd = pmsg->fds[0]; vq->callfd = file.fd; if (wcb && pfd->revents & (POLLOUT | FDPOLLERR)) wcb(fd, dat, &remove2); vhost_user_start_client(struct vhost_user_socket *vsocket) //unix domain socket int fd = vsocket->socket_fd; const char *path = vsocket->path; vhost_user_connect_nonblock(fd, (struct sockaddr *)&vsocket->un,sizeof(vsocket->un)); vhost_user_add_connection(int fd, struct vhost_user_socket *vsocket) conn = malloc(sizeof(*conn)); vid = vhost_new_device(); vsocket->notify_ops->new_connection(vid); //fd加入fd_sets, 当fd可读时, 调用回调函数; 由上面的fdset_event_dispatch线程调用 ret = fdset_add(&vhost_user.fdset, fd, vhost_user_read_cb, NULL, conn); 在dpdk的vhost驱动中, 写eventfd //lib/librte_vhost/virtio_net.c //进队列 rte_vhost_enqueue_burst() virtio_dev_rx() eventfd_write(vq->callfd, (eventfd_t)1); //出队列 rte_vhost_dequeue_burst() update_used_idx() eventfd_write(vq->callfd, (eventfd_t)1); 补充: 通过unix socket来传递文件描述符 我们知道文件描述符是个比较小的整数, 一般是不能共享给其他unrelated进程的, 大部分共享文件描述符的操作出现在父子进程通信场景下, 但在上面的分析中, qemu通过unix socket把eventfd传递给OVS, 怎么做到的呢? 这用到了unix socket的特性, SCM_RIGHTS, unix socket可以传递文件描述符数组给其他进程, 就像它们被dup()过一样. man 7 unix搜索SCM_RIGHTSman 3 cmsg里有示例代码: struct msghdr msg = { 0 }; struct cmsghdr *cmsg; int myfds[NUM_FD]; /* Contains the file descriptors to pass */ int *fdptr; char iobuf[1]; struct iovec io = { .iov_base = iobuf, .iov_len = sizeof(iobuf) }; union { /* Ancillary data buffer, wrapped in a union in order to ensure it is suitably aligned */ char buf[CMSG_SPACE(sizeof(myfds))]; struct cmsghdr align; } u; msg.msg_iov = &io; msg.msg_iovlen = 1; msg.msg_control = u.buf; msg.msg_controllen = sizeof(u.buf); cmsg = CMSG_FIRSTHDR(&msg); cmsg->cmsg_level = SOL_SOCKET; cmsg->cmsg_type = SCM_RIGHTS; cmsg->cmsg_len = CMSG_LEN(sizeof(int) * NUM_FD); fdptr = (int *) CMSG_DATA(cmsg); /* Initialize the payload */ memcpy(fdptr, myfds, NUM_FD * sizeof(int)); 注意: 通过unix socket传递的fd, 实际指向的文件是同一个, 但发送方和接收方的fd号其实是不一样的, 类似于fd被dup()过. 关于OVS流匹配 https://www.cnblogs.com/neooelric/p/7160222.htmlhttps://blog.csdn.net/u010378472/article/details/79146557 smc-enable: signature match cache 实测性能提高10% - 15%https://mail.openvswitch.org/pipermail/ovs-dev/2018-July/348978.html It is generally beneficial to turn on SMC but turn off EMC when traffic flow count is much larger than EMC size. vhost协议 https://blog.csdn.net/me_blue/article/details/77854595https://blog.csdn.net/me_blue/article/details/77969084https://access.redhat.com/solutions/3394851 控制信息走socket文件, 包括交换内存map表, door bell通知对方数据已经放到virtio queue里面了通过socket以\"door bell\"方式通知对方是有代价的, 读写socket需要系统调用, 双方还要处理对应的中断. 数据面直接用DMA(direct memory access), 或者说是共享内存方式读写. VM启动的时候, 内存初始化为shared VM的virtio-net驱动把内存分配给virtio queue, 相关的结构体符合virtio规范 qemu把这个内存地址通过控制面socket给OVS-DPDK DPDK把这个地址映射成符合virtio规范的结构体, 所以内存一样, 结构体也一样. 通过以上几步, DPDK就可以直接读写VM在其hugepage上分配的virtio queue 一方把data放到virtio ring后, 数据通知方式有两种: 另外一方不断的poll, 比如DPDK的pmd 通过\"中断\"方式通知对方 利用KVM提供的ioeventfd和irqfd机制 In the end the vhost instance only knows about the guest memory mapping, a kick eventfd, and a call eventfd. vhost架构 http://blog.allenx.org/2013/09/09/vhost-architecture 补充 https://access.redhat.com/documentation/en-us/reference_architectures/2017/html/deploying_mobile_networks_using_network_functions_virtualization/performance_and_optimization 相关代码: Here are the main points to begin exploring the code: drivers/vhost/vhost.c - common vhost driver code drivers/vhost/net.c - vhost-net driver virt/kvm/eventfd.c - ioeventfd and irqfd The QEMU userspace code shows how to initialize the vhost instance: hw/vhost.c - common vhost initialization code hw/vhost_net.c - vhost-net initialization OVS-DPDK关键路径 先查pmd的q分布 sudo ovs-appctl dpif-netdev/pmd-rxq-show dpdkp0的关键路径 发现dpdkp0的q跑在core 33上, 是从dpdkp0物理口收包, 再发到vhostuser0的过程.抓关键路径 sudo perf top -g -a -C 33 --no-children 结合代码, 调用路径如下: start_thread ovsthread_wrapper pmd_thread_main #一个core可以poll多个rxq poll_cnt = pmd_load_queues_and_ports(pmd, &poll_list); for (i = 0; i rx, &batch) netdev_dpdk_rxq_recv #收包函数占比8.9%, 排名第四 mlx5_rx_burst_vec #再转发, 交换机的基本逻辑 dp_netdev_input(pmd, &batch, port_no) #emc是exact match cache:pmd->flow_cache emc_processing #对每个packet执行 pkt_metadata_init #解析报文, 解析类似五元组的信息 #这个占比第三高, 16% miniflow_extract #在flow cache里查找key(由miniflow_extract得来), 用memcmp; flow = emc_lookup(flow_cache, key) fast_path_processing #多流(L2或L3变化)的时候, 该函数占比最高 dpcls_lookup packet_batch_per_flow_execute dp_netdev_pmd_flush_output_packets(pmd, false) dp_netdev_pmd_flush_output_on_port netdev_send #实际执行__netdev_dpdk_vhost_send #占比第二高, %17 netdev_dpdk_vhost_send #该函数单条流时占比最高, 22% rte_vhost_enqueue_burst vhostuser0的关键路径 发现vhostuser0的q跑在core 36上, 是vhostuser0从VM收包, 再发到dpdkp0物理口的过程.抓关键路径 sudo perf top -g -a -C 36 --no-children 结合代码, 调用路径如下: start_thread ovsthread_wrapper pmd_thread_main #一个core可以poll多个rxq poll_cnt = pmd_load_queues_and_ports(pmd, &poll_list); for (i = 0; i rx, &batch) netdev_dpdk_rxq_recv netdev_dpdk_vhost_rxq_recv #占比最高, 22.36% rte_vhost_dequeue_burst #再转发, 交换机的基本逻辑 dp_netdev_input(pmd, &batch, port_no) #emc是exact match cache:pmd->flow_cache emc_processing #对每个packet执行 pkt_metadata_init #解析报文, 解析类似五元组的信息 #这个占比第三高, 14% miniflow_extract #只有从vhostuser收包才有这个函数, 占比11%, 可以优化吗???? #x86有SSE4优化 miniflow_hash_5tuple #在flow cache里查找key(由miniflow_extract得来), 用memcmp; flow = emc_lookup(flow_cache, key) fast_path_processing #多流(L2或L3变化)的时候, 该函数占比最高 dpcls_lookup packet_batch_per_flow_execute dp_netdev_pmd_flush_output_packets(pmd, false) dp_netdev_pmd_flush_output_on_port netdev_send netdev_dpdk_eth_send #该函数单条流时占比10% mlx5_tx_burst_mpw buffer到底多大 参考: https://developers.redhat.com/blog/2018/03/16/ovs-dpdk-hugepage-memory/ 同一个numa上的ports可以共享mem pool, 但受多方面因素影响, 例如两个物理dpdk port, 同在一个numa上, MTU都是1500, 那很有可能是在一个mem pool里.mem pool一般是预先分好的, 比如 #MTU是1500时: Total size per buffer = 3008 Bytes #MTU是9000时: Total size per buffer = 10176 Bytes 开始的时候, OVS预先申请256k个buffer, 那么 #MTU是1500时: Size of requested mempool = 3008 Bytes * 256K Size of requested mempool = 788 MBytes #MTU是9000时: Size of requested mempool = 10176 Bytes * 256K Size of requested mempool = 2668 MBytes #那么OVS一共要分: Size of mempools = 788 MBytes + 2668 MBytes Size of mempools = 3456 MBytes #初始化时 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=4096,4096 datapath OVS有两种datapath: kernel datapath和userspace datapath: kernel datapath 默认即kernel datapath, 使用内核模块openvswitch.ko 代码实现在lib/dpif-netlink.c When a packet arrives on a vport, the kernel module processes it by extracting its flow key and looking it up in the flow table. If there is a matching flow, it executes the associated actions. If there is no match, it queues the packet to userspace for processing (as part of its processing, userspace will likely set up a flow to handle further packets of the same type entirely in-kernel). kernel datapath框图 userspace datapath 在创建bridge的时候, 加选项datapath_type=netdev, 就打开了userspace模式 代码实现在lib/dpif-netdev.c userspace datapath架构图 Architecure _ | +-------------------+ | | ovs-vswitchd |ovsdb-server | +-------------------+ | | ofproto |OpenFlow controllers | +--------+-+--------+ _ | | netdev | |ofproto-| | userspace | +--------+ | dpif | | | | netdev | +--------+ | | |provider| | dpif | | | +---||---+ +--------+ | | || | dpif | | implementation of | || |provider| | ofproto provider |_ || +---||---+ | || || | _ +---||-----+---||---+ | | | |datapath| | kernel | | +--------+ _| | | | |_ +--------||---------+ || physical NIC vport类型 netdev .send = dev_queue_xmit dev_queue_xmit(skb) will transmit the packet on a physical network device eventually internal .send = internal_dev_recv the send method will call netif_rx(skb) insert the skb into TCP/IP stack, and packet will eventually be transmitted by stack patch .send = patch_send() the send method will just pass the skb pointer to the peer vport tunnel vports: vxlan, gre, etc tunnel xmit method in kernel, e.g. .send = vxlan_xmit for vxlan 用户态通过物理口发包流程 当使用ovs创建的bridge配置成用户态datapath模式时(datapath_type=netdev), 把物理口添加到ovs上: sudo ovs-vsctl del-port ovsbr0 enP5p1s0 发现vm01(5.5.5.44)是ping不通host B(5.5.5.22)的, 但可以ping通对端host A(5.5.5.11) 使用perf抓关键路径 重点关注lib/netdev-linux.c里面的netdev_linux_send()函数, 在Host B上面抓ovs-vswitchd的调用路径. #回顾一下perf probe用法: 解析符号表 perf probe -x /lib64/libibverbs.so.1 -F perf probe -x `which ovs-vswitchd` -F | grep ibv #结合代码, 加动态probe点 sudo perf probe -x `which ovs-vswitchd` --add netdev_linux_send sudo perf probe -x `which ovs-vswitchd` --add wrap_sendmmsg #开始抓调用, 30秒. sudo perf record -e probe_ovs:netdev_linux_send -e probe_ovs:wrap_sendmmsg -e net:net_dev_start_xmit -p 3470 -g -o perf-ping22.data -- sleep 30 #分析结果 sudo perf report -i perf-no-ping.data -n Vm01 上不ping任何IP结果:没有采样--正常 Vm01 ping Host A场景在Vm01上:ping -f 5.5.5.11 结果:netdev_linux_send和wrap_sendmmsg都有99K次数被采样到:结合代码, 调用路径如下: start_thread ovsthread_wrapper pmd_thread_main dp_netdev_process_rxq_port #先收包 netdev_rxq_recv #再转发, 交换机的基本逻辑 dp_netdev_input packet_batch_per_flow_execute dp_netdev_execute_actions dp_execute_cb case OVS_ACTION_ATTR_OUTPUT: netdev_send netdev_linux_send #使用AF_PACKET socket, 直接pypass掉ip协议栈 sock = socket(AF_PACKET, SOCK_RAW, 0); int ifindex = netdev_get_ifindex(netdev_); netdev_linux_sock_batch_send(sock, ifindex, batch); sendmsg ---------- #kernel space sock_sendmsg packet_sendmsg packet_snd.isra.64 dev_queue_xmit sch_direct_xmit dev_hard_start_xmit Vm01 ping Host B场景在Vm01上:ping -f 5.5.5.22结果:采样次数比上个场景少很多, 因为发包少很多, 进一步讲, ping一直等不到回应, 超时了才发下一个.基本路径和上一个场景一样; 但有时会抓到另一种路径: _start __libc_start_main main bridge_run bridge_run__ ofproto_type_run type_run dpif_netdev_run dp_netdev_process_rxq_port dp_netdev_input__ fast_path_processing odp_execute_actions dp_execute_cb netdev_linux_send 结论 在userspace datapath(datapath_type=netdev)模式下, ovs通过socket(AF_PACKET, SOCK_RAW, 0)发包, 不走ip协议栈. 参考: https://arthurchiao.github.io/blog/ovs-deep-dive-3-datapath/ https://github.com/openvswitch/ovs/blob/master/Documentation/topics/porting.rst https://github.com/openvswitch/ovs/blob/master/Documentation/topics/datapath.rst "},"notes/OVS_DPDK_performance_HXT_ARM_server.html":{"url":"notes/OVS_DPDK_performance_HXT_ARM_server.html","title":"OVS-DPDK for ARM server 性能测试环境","keywords":"","body":" 环境 测试场景 配置 OVS配置 VM配置 4q测试 环境 服务器 DUT(HXT server) Note Socket 1 单socket CPU 46core@2.6G AW2.1 sudo dmidecode -t processor MEM 96G free -h NIC MLX CX4121A 10G 0004:01:00.0 enP4p1s0f0 ibdev2netdev -v OS CentOS 7.5.1804 CentOS 7.5.1804 cat /etc/redhat-release kernel 4.14.62-5.hxt.aarch64 uname -r Mellanox OFED version 4.4-1.0.0.0 ofed_info -s QEMU version 2.12.1 qemu-system-aarch64 --version 源码编译 DPDK version 17.11.4 源码编译 OVS(with DPDK) version 2.10.1(with dpdk 17.11.4) sudo ovs-vsctl show 源码编译 libvirt version 4.6.0 源码编译 virt-manager version 1.5.1 源码安装 测试场景 配置 预置条件: DPDK, OVS, Qemu, libvirt, virt-manager已经成功编译安装 OVS配置 OVS选项 值 说明 dpdk-init true bridge ovsbr0 pmd-cpu-mask FF00000000 8个core 32 33 34 35 36 37 38 39 dpdk-socket-mem 16384 单socket 16G vhost-user port 0 dpdkvhostuser0 vhost-user port 1 dpdkvhostuser1 dpdk port0 dpdkp0 MLX CX4 10G NIC #增加32G的hugepage sudo bash -c \"echo 64 > /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages\" #开始ovs export PATH=$PATH:/usr/local/share/openvswitch/scripts sudo ovs-ctl start #打开硬件offload, 2.10才有; 2.9.3没效果 #miniflow_extract, dpcls_lookup等函数被offload了 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:hw-offload=true #多条流会有用? :提高~15% sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:smc-enable=true #打开dpdk初始化 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true #新增一个bridge, 使用用户态datapath模式 sudo ovs-vsctl add-br ovsbr0 -- set bridge ovsbr0 datapath_type=netdev #配置pmd跑在8个核上, core 32 33 34 35 36 37 38 39 sudo ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=FF00000000 #配置使用node0的16G内存. sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"16384\" #增加物理port, 对应物理NIC sudo ovs-vsctl add-port ovsbr0 dpdkp0 -- set Interface dpdkp0 type=dpdk options:dpdk-devargs=0004:01:00.0 #增加两个port, 类型是dpdkvhostuserclient sudo ovs-vsctl add-port ovsbr0 dpdkvhostuser0 -- set Interface dpdkvhostuser0 type=dpdkvhostuserclient sudo ovs-vsctl set Interface dpdkvhostuser0 options:vhost-server-path=\"/tmp/dpdkvhostuser0\" sudo ovs-vsctl add-port ovsbr0 dpdkvhostuser1 -- set Interface dpdkvhostuser1 type=dpdkvhostuserclient sudo ovs-vsctl set Interface dpdkvhostuser1 options:vhost-server-path=\"/tmp/dpdkvhostuser1\" #以下为可选配置 #配置多queue, 绑定pmd core sudo ovs-vsctl set Interface dpdkvhostuser0 options:n_rxq=2 other_config:pmd-rxq-affinity=\"0:34,1:35\" sudo ovs-vsctl set Interface dpdkvhostuser1 options:n_rxq=2 other_config:pmd-rxq-affinity=\"0:36,1:37\" #设置物理port使用2个q sudo ovs-vsctl set Interface dpdkp0 options:n_rxq=2 other_config:pmd-rxq-affinity=\"0:32,1:33\" VM配置 预置条件: libvirtd已经成功启动 先用virt-manager创建2个VM, 配置如下: VM | VM01 | VM02 ---|---|---| CPUs | 4 | 4 Memory | 8192M | 8192M Disk | hxt-centos7.5-01.qcow2 32G | hxt-centos7.5-02.qcow2 32G NIC(for external Net) | virtio NAT | virtio NAT NIC(for test) | vhostuser0 | vhostuser0 NIC(for test) | vhostuser1 | vhostuser1 #启动libvirtd服务 sudo systemctl start libvirtd #启动virt-manager sudo xauth add $(xauth -f /home/bai/.Xauthority list | tail -1) sudo virt-manager -c 'qemu+unix:///system?socket=/usr/local/var/run/libvirt/libvirt-sock' #启动一个VM, 这个VM配置如上 sudo virsh create ~/repo/save/vm/2vhostuser.xml 4q测试 # 4个q ## OVS默认一个core一条流, dpdkp0上几个core决定几条流(或者几个q真正干活) sudo ovs-vsctl set Interface dpdkp0 options:n_rxq=4 other_config:pmd-rxq-affinity=\"0:32,1:33,2:34,3:35\" ## dpdkvhostuser0实际空跑, 绑定核和dpdkp0一样 sudo ovs-vsctl set Interface dpdkvhostuser0 options:n_rxq=4 other_config:pmd-rxq-affinity=\"0:32,1:33,2:34,3:35\" ## 瓶颈在dpdkvhostuser1, 绑定4个独立的core sudo ovs-vsctl set Interface dpdkvhostuser1 options:n_rxq=4 other_config:pmd-rxq-affinity=\"0:36,1:37,2:38,3:39\" # VM ## 要用3个core, 1个core最大转发6.9Mpps; 但3个core, 有一个core是\"空闲\"的 sudo arm64-armv8a-linuxapp-gcc/app/testpmd --socket-mem 1024 -- --stat 2 --nb-cores=3 --rxq=4 --txq=4 --rxd=512 --txd=512 --burst=64 --port-topology=paired --forward-mode=mac -a "},"notes/DPDK_Mellanox.html":{"url":"notes/DPDK_Mellanox.html","title":"DPDK Mellanox","keywords":"","body":" DPDK代码梳理 PMD说明 MLX的txq macswap段错误 程序框架, 以IO FWD为例 pkt_burst_io_forward mlx 10G NIC DPDK上手 Mallanox OFED安装 OFED简介 安装 相关问题解决 DPDK编译安装 高版本pktgen编译 运行testpmd 在VM上运行testpmd 在Mellanox PMD(Poll-Mode Driver)上运行testpmd testpmd交互模式 testpmd命令记录 testpmd常用命令 testpmd常用的fwd模式 运行多个testpmd实例 testpmd查看统计 l3fwd 编译 运行 pktgen pktgen命令记录 图示 测试结果 Mellanox DPDK性能优化 关闭流控 huagepage KVM用hugepage isolcpu 关闭透明大页 关闭中断均衡 mlx网卡的一些特殊优化 用taskset pin vm的核 Mellanox驱动/FW的命令汇总 mod option 常用命令 mst tool(Mellanox Software Tools) SR-IOV相关 SR-IOV使能, 重启不保存 disable sriov, 配置VF要在disable sriov以后做 允许的vf数 SR-IOV使能, 重启保存(未验证) step 1 step 2 step 3 step 4 重启机器 VF相关命令 Vlan tag VF的mac link状态 VF的包统计 VF和port的关系 VF的QoS vxlan相关 QinQ VM相关 两个VM用vhost-user连OVS virt-manager 性能相关 grub选项 intel报告 on Platinum 8180 testpmd L3fwd DPDK代码梳理 PMD说明 https://doc.dpdk.org/guides/prog_guide/poll_mode_drv.html MLX的txq 一次实际的txq (gdb) bt #0 mlx5_tx_complete (txq=0xfff7bff79500) at /home/bai/share/repo/hxt/mainstream-dpdk-stable/drivers/net/mlx5/mlx5_rxtx.h:478 #1 mlx5_tx_burst_mpw (dpdk_txq=0xfff7bff79500, pkts=0xebf30a , pkts_n=1761) at /home/bai/share/repo/hxt/mainstream-dpdk-stable/drivers/net/mlx5/mlx5_rxtx.c:940 #2 0x000000000049e57c in pkt_burst_mac_swap () #3 0x0000000000476f50 in run_pkt_fwd_on_lcore () #4 0x0000000000477030 in start_pkt_forward_on_core () #5 0x00000000005153c4 in eal_thread_loop () #6 0x0000ffffa86d7bb8 in start_thread () from /lib64/libpthread.so.0 #7 0x0000ffffa861fb50 in thread_start () from /lib64/libc.so.6 (gdb) p *txq $13 = {elts_head = 63996, elts_tail = 63804, elts_comp = 28, mpw_comp = 0, cq_ci = 21080, wqe_ci = 34108, wqe_pi = 34026, elts_n = 11, cqe_n = 6, wqe_n = 11, tso_en = 0, tunnel_en = 1, swp_en = 0, mpw_hdr_dseg = 0, max_inline = 0, inline_max_packet_sz = 0, qp_num_8s = 1235712, offloads = 0, mr_ctrl = {dev_gen_ptr = 0xfff7bffe9558, cur_gen = 0, mru = 0, head = 1, cache = {{start = 281439006359552, end = 281439543230464, lkey = 2514682368}, {start = 0, end = 0, lkey = 0}, {start = 0, end = 0, lkey = 0}, {start = 0, end = 0, lkey = 0}, {start = 0, end = 0, lkey = 0}, {start = 0, end = 0, lkey = 0}, {start = 0, end = 0, lkey = 0}, {start = 0, end = 0, lkey = 0}}, cache_bh = {len = 2, size = 256, overflow = 0, table = 0xfff7bff78000}}, cqes = 0xfff7a02d0000, wqes = 0xfff7a02a0000, qp_db = 0xfff7bff80184, cq_db = 0xfff7bff80100, bf_reg = 0xffdee4a96800, elts = 0xfff7bff79700, stats = {idx = 0, opackets = 332528124, obytes = 19951687440, oerrors = 0}} /* TX queue descriptor. */ __extension__ struct mlx5_txq_data { uint16_t elts_head; /* Current counter in (*elts)[]. */ uint16_t elts_tail; /* Counter of first element awaiting completion. */ uint16_t elts_comp; /* Counter since last completion request. */ uint16_t mpw_comp; /* WQ index since last completion request. */ uint16_t cq_ci; /* Consumer index for completion queue. */ #ifndef NDEBUG uint16_t cq_pi; /* Producer index for completion queue. */ #endif uint16_t wqe_ci; /* Consumer index for work queue. */ uint16_t wqe_pi; /* Producer index for work queue. */ uint16_t elts_n:4; /* (*elts)[] length (in log2). */ uint16_t cqe_n:4; /* Number of CQ elements (in log2). */ uint16_t wqe_n:4; /* Number of of WQ elements (in log2). */ uint16_t tso_en:1; /* When set hardware TSO is enabled. */ uint16_t tunnel_en:1; /* When set TX offload for tunneled packets are supported. */ uint16_t swp_en:1; /* Whether SW parser is enabled. */ uint16_t mpw_hdr_dseg:1; /* Enable DSEGs in the title WQEBB. */ uint16_t max_inline; /* Multiple of RTE_CACHE_LINE_SIZE to inline. */ uint16_t inline_max_packet_sz; /* Max packet size for inlining. */ uint32_t qp_num_8s; /* QP number shifted by 8. */ uint64_t offloads; /* Offloads for Tx Queue. */ struct mlx5_mr_ctrl mr_ctrl; /* MR control descriptor. */ volatile struct mlx5_cqe (*cqes)[]; /* Completion queue. */ volatile void *wqes; /* Work queue (use volatile to write into). */ volatile uint32_t *qp_db; /* Work queue doorbell. */ volatile uint32_t *cq_db; /* Completion queue doorbell. */ volatile void *bf_reg; /* Blueflame register remapped. */ struct rte_mbuf *(*elts)[]; /* TX elements. */ struct mlx5_txq_stats stats; /* TX queue counters. */ } __rte_cache_aligned; uint16_t mlx5_tx_burst_mpw(void *dpdk_txq, struct rte_mbuf **pkts, uint16_t pkts_n) { struct mlx5_txq_data *txq = (struct mlx5_txq_data *)dpdk_txq; ... } macswap段错误 在里面, 已经分析过, macswap段错误的原因是数组越界这段程序负责回收mbuf, 根据txq->wqe_pi生产者的index, 找到txq->wqes里面对应的wqe, 其对应的硬件定义如下: struct mlx5_wqe_ctrl { uint32_t ctrl0; uint32_t ctrl1; uint32_t ctrl2; uint32_t ctrl3; } __rte_aligned(MLX5_WQE_DWORD_SIZE); elts_tail根据wqe的ctrl3得出 (gdb) p *(volatile struct mlx5_wqe_ctrl *)0xfff7a02b3e00 $52 = {ctrl0 = 251167745, ctrl1 = 81465856, ctrl2 = 134217728, ctrl3 = 63836} elts_tail = ctrl->ctrl3 //从硬件读出来的, 所谓的硬件是wqe elts_tail=63836 elts_free是上一次的txq->elts_tail 591行free数组越界导致段错误: 通常情况下free和tail的增长情况: 通过加打印:950行, 抓到的2次异常情况 testpmd> PANIC in mlx5_tx_complete_m(): Array index exceeding: elts_free:2685 elts_tail:1576 testpmd> PANIC in mlx5_tx_complete(): Array index exceeding: elts_free:40737 elts_tail:39488 下面想知道, 假定elts_tail发生了\"突变\", 那能抓到这次突变吗? 突变的上一次值是多少? 有什么规律?运行sudo perf record -e probe_testpmd:mlx5_tx_complete_m -a -C 34 -- sleep 30抓30秒, 时间太长则数据量太大. 看运气, 多抓几次. 抓到了, 最后一行: 程序框架, 以IO FWD为例 主核: main() rte_eal_init() 每个从核pthread_create执行eal_thread_loop, 主从核通过管道通信 start_packet_forwarding() #通知从核执行start_pkt_forward_on_core launch_packet_forwarding(start_pkt_forward_on_core) for (i = 0; i stopped = 1 #RX-packets: total_recv += stats.ipackets; #TX-packets: total_xmit += stats.opackets; #RX-dropped: total_rx_dropped += stats.imissed; #TX-dropped: #+=fwd_dropped(见pkt_burst_io_forward), 即tx数小于rx数的部分 total_tx_dropped += port->tx_dropped; #RX-nombufs: total_rx_nombuf += stats.rx_nombuf; 从核: thread_start() start_thread() eal_thread_loop() m2s = lcore_config[lcore_id].pipe_master2slave[0]; s2m = lcore_config[lcore_id].pipe_slave2master[1]; while 1: #没有命令下发则阻塞在这 n = read(m2s, &c, 1); lcore_config[lcore_id].state = RUNNING; #ack n = write(s2m, &c, 1); ret = lcore_config[lcore_id].f(lcore_config[lcore_id].arg) start_pkt_forward_on_core() run_pkt_fwd_on_lcore() do #每个stream都运行一次, 比如下面 #Logical Core 1 (socket 0) forwards packets on 2 streams: # RX P=0/Q=0 (socket 0) -> TX P=1/Q=0 (socket 0) peer=02:00:00:00:00:01 # RX P=1/Q=0 (socket 0) -> TX P=0/Q=0 (socket 0) peer=02:00:00:00:00:00 #第一次从P0Q0收包, 发到P1Q0;第二次从P1Q0收包, 发到P0Q0 for (sm_id = 0; sm_id stopped) lcore_config[lcore_id].ret = ret; rte_wmb(); lcore_config[lcore_id].state = FINISHED; pkt_burst_io_forward pkt_burst_io_forward nb_rx = rte_eth_rx_burst(fs->rx_port, fs->rx_queue, pkts_burst, nb_rx); struct rte_eth_dev *dev = &rte_eth_devices[port_id]; nb_rx = (*dev->rx_pkt_burst)(dev->data->rx_queues[queue_id], rx_pkts, nb_pkts); #ixgbe_rxtx_vec_neon.c #最大burst为RTE_IXGBE_MAX_RX_BURST=32 ixgbe_recv_pkts_vec() #return The number of output packets actually stored in transmit descriptors of #the transmit ring. The return value can be less than the value of the #*tx_pkts* parameter when the transmit ring is full or has been filled up. #往设备的tx queue发报文, burst模式, 通常是32个; 入参同时也提供了实际报文的所在的pool, 是由之前rte_pktmbuf_pool_create()申请的; 这样, 有报文地址, 有个数, rte_eth_tx_burst()就可以发送了; 但实际能发送的个数要看这个tx queue里tx ring的可用的tx描述符的个数; #每个报文发送时, 先在transmit ring里取下一个可用的描述符, 释放已经用它发完的buffer, 用报文的rte_mbuf来配这个tx描述符, 然后发送; 如果有分片的话, 会用几个tx描述符 #有个水限tx_free_thresh, 当tx描述符低于这个水限, 则rte_eth_tx_burst()函数要\"尝试\"释放这些已经发送完成的描述符指向的rte_mbuf #rte_eth_txconf::txq_flags有DEV_TX_OFFLOAD_MT_LOCKFREE的话, 多核可以同时在同一个queue上发送, 不用加锁. nb_tx = rte_eth_tx_burst(fs->tx_port, fs->tx_queue, pkts_burst, nb_rx); struct rte_eth_dev *dev = &rte_eth_devices[port_id]; return (*dev->tx_pkt_burst)(dev->data->tx_queues[queue_id], tx_pkts, nb_pkts); ixgbe_xmit_pkts_vec() ixgbe_xmit_fixed_burst_vec() #tx_free_thresh为32 if (txq->nb_tx_free tx_free_thresh) #根据tx_ring里txq->tx_next_dd指向的描述符的DD(descripor done)位的指示, #释放txq->sw_ring里面指向的mbuf, 一次释放txq->tx_rs_thresh个(32个) ixgbe_tx_free_bufs(txq); #见rte_ethdev.h fs->fwd_dropped += (nb_rx - nb_tx); mlx 10G NIC mlx5_rx_burst_vec mlx5_tx_burst_mpw DPDK上手 Mallanox OFED安装 这里我们使用Mallanox ConnectX-4网卡, 首先需要安装Mallanox驱动包OFED OFED简介 OFED(Mellanox OpenFabrics Enterprise Distribution for Linux) OFED是个软件包, 用来支持自家网卡在InfiniBand和Ethernet上支持RDMA, 包括 驱动(mlx4,mlx5), InfiniBand的上层协议(IPoIB, SRP and iSER Initiator) OpenFabrics工具: OpenSM诊断和性能工具 Open MPI stack, 一些benchmark工具(OSU benchmarks, Intel MPI benchmarks, Presta) PGAS HPC的相关软件 其他: ibutils2, ibdump, MFT 安装 到http://www.mellanox.com/page/products_dyn?product_family=26&mtag=linux下载安装包 sudo yum install gcc-gfortran sudo ./mlnxofedinstall -h ./mlnxofedinstall --add-kernel-support #如果提示版本不对 sudo ./mlnxofedinstall --add-kernel-support --skip-distro-check #经验证, ofed4.4.1和dpdk18.05要用下面的配置安装 sudo ./mlnxofedinstall --add-kernel-support --skip-distro-check --dpdk --upstream-libs sudo ./mlnxofedinstall --add-kernel-support --skip-distro-check --dpdk --with-mlnx-ethtool --with-mft --with-mstflint --upstream-libs #安装完成 sudo /etc/init.d/openibd restart #看版本 ofed_info -s 注: 应该会最大化安装, --add-kernel-support会安装kernel驱动 相关问题解决 ibv_exp_query_device错误 运行app, 比如testpmd, 提示PMD: net_mlx5: ibv_exp_query_device() failed 解决: 重装ofed, 注意版本和DPDK版本要匹配, 而且安装参数也有影响 DPDK编译安装 #clone代码 git clone ssh://1680532@bjsss013.hxtcorp.net:29418/networking/dpdk git checkout hxt-dev-v17.08 #使用gcc6.3编译 scl enable devtoolset-6 bash cd dpdk export RTE_TARGET=arm64-armv8a-linuxapp-gcc #打开common_base一个就行了 sed -i 's/\\(CONFIG_RTE_LIBRTE_MLX5_PMD=\\)n/\\1y/g' config/common_base make config T=${RTE_TARGET} #使能Mlx网卡 sed -ri 's,(LIBRTE_MLX5_PMD=).*,\\1y,' build/.config #编译 make -j24 #带-g编译, 可用gdb调试 make -j EXTRA_CFLAGS=-g make -j EXTRA_CFLAGS=\"-g -O1 -Wno-error\" #编译加安装 make install T=${RTE_TARGET} -j make install -j EXTRA_CFLAGS=\"-g\" T=${RTE_TARGET} #编译l3fwd make examples T=$RTE_TARGET -j #pktgen #和dpdk版本, ofed版本都有关 wget http://www.dpdk.org/browse/apps/pktgen-dpdk/snapshot/pktgen-dpdk-pktgen-3.4.5.zip export RTE_SDK=[the path to the DPDK SW root folder] export RTE_TARGET=arm64-armv8a-linuxapp-gcc cd pktgen-dpdk-pktgen-3.4.5 make -j24 make EXTRA_CFLAGS=\"-Wno-error\" -j #高版本pktgen sudo yum install lua-devel.aarch64 readline-devel.aarch64 #经过验证的版本: dpdk 17.08 & pktgen3.4.5 dpdk 17.11 & pktgen3.4.9(编不过要把rte_memcpy删掉) dpdk 18.05 & pktgen3.5.6(打多流大包有问题) 高版本pktgen编译 #先编译lua5.3 curl -R -O http://www.lua.org/ftp/lua-5.3.5.tar.gz tar zxf lua-5.3.5.tar.gz cd lua-5.3.5 make linux test #默认装到/user/local下面 sudo make install #添加lua5.3.pc $ cat /usr/local/lib/pkgconfig/lua5.3.pc V= 5.3 R= 5.3.5 prefix= /usr/local exec_prefix= ${prefix} libdir= ${prefix}/lib includedir= ${prefix}/include Name: Lua Description: An Extensible Extension Language Version: ${R} Requires: Libs: -L${libdir} -llua -lm -ldl Cflags: -I${includedir} #比如编译pktgen-3.5.6 #修改Makefile $ git diff diff --git a/app/Makefile b/app/Makefile index 3949e51..a78ea7d 100644 --- a/app/Makefile +++ b/app/Makefile @@ -89,8 +89,8 @@ endif ifeq ($(CONFIG_RTE_LIBRTE_LUA),y) else -MYLIBS += -lpktgen_lua -MYLIB_PATH += -L$(LUA_LIB) $(shell pkg-config --libs lua5.3) +MYLIBS += -lpktgen_lua $(shell pkg-config --libs-only-l lua5.3) +MYLIB_PATH += -L$(LUA_LIB) $(shell pkg-config --libs-only-L lua5.3) CFLAGS += -I/usr/include/lua5.3 CFLAGS += -I$(RTE_SRCDIR)/../lib/lua/ #编译pktgen export RTE_SDK=[the path to the DPDK SW root folder] export RTE_TARGET=arm64-armv8a-linuxapp-gcc PKG_CONFIG_PATH=/usr/local/lib/pkgconfig make -j 运行testpmd 在VM上运行testpmd sudo modprobe uio sudo modprobe uio_pci_generic #VM最大内存才8G, 这里分4G给testpmd sudo bash -c \"echo 8 > /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages\" #绑定到uio_pci_generic sudo ./usertools/dpdk-devbind.py --status sudo ./usertools/dpdk-devbind.py -b uio_pci_generic 04:00.0 sudo ./usertools/dpdk-devbind.py -b uio_pci_generic 05:00.0 sudo ./build/app/testpmd -- -i #intel 82599绑定回kernel driver ./usertools/dpdk-devbind.py -b ixgbe 0005:01:00.0 0005:01:00.1 virtio pmd参考: https://doc.dpdk.org/guides/nics/virtio.html 在Mellanox PMD(Poll-Mode Driver)上运行testpmd 不需要unbind kernel driver, 一般文档会要求先unbind pci device, 再bind到uio_pci_generic或vfio-pci上, 比如: ./usertools/dpdk-devbind.py --status #要先加载uio_pci_generic.ko modprobe uio_pci_generic ./usertools/dpdk-devbind.py --bind=uio_pci_generic 04:00.1 或 ./usertools/dpdk-devbind.py --bind=uio_pci_generic eth1 #unbind ./usertools/dpdk-devbind.py -u 04:00.1 Mellanox网卡不需要 unbind, 因为它家的kernel module本身就支持UIO/VFIO #用PCI地址制定网口 ./testpmd -c 0x1ff -n 4 -w 0000:08:00.0 -w 0000:08:001 --socket-mem=2048,0 -- --port-numa-config=0,0,1,0 --socketnum=0 --burst=64 --txd=1024 --rxd=256 --mbcache=512 --rxq=4 --txq=4 --nbcores=8 --i #强制dpdk使用1G大页, 否则会用满可用大页 ./testpmd --socket-mem 1024 #ConnectX-4 NIC最好性能举例: ./testpmd -c 0x1ff -n 4 -w 0000:08:00.0,txq_inline=128 -w 0000:08:00.1,txq_inline=128 --socket-mem=2048,0 -- --port-numaconfig=0,0,1,0 --socket-num=0 --burst=64 --txd=1024 --rxd=256 --mbcache=512 --rxq=4 --txq=4 --nb-cores=8 --rss-udp --i #一个port举例 ./testpmd -c 0xe000 -n 4 -w 0000:08:00.0,txq_inline=200 --socketmem=2048,0 -- --port-numa-config=0,0,1,0 --socket-num=0 --burst=64 -- txd=4096 --rxd=1024 --mbcache=512 --rxq=4 --txq=4 --nb-cores=4 --rss-udp --i #hxt的例子 ./build/app/testpmd -l 3-43 -n 6 txq_inline=256,rxq_cqe_comp_en=1,txqs_min_inline=8, txq_mpw_en=1 -- -i --txq=16 --rxq=16 --mbcache=512 --rxd=4096 --txd=4096 -l是说run在哪些core上, 每个core一个线程 -n 6是说6个mem channel testpmd交互模式 这里默认两台机器直连, 都运行testpmd 在RX侧: ~]# testpmd> set nbcore 32 ~]# testpmd> port config all rss ether # 反压, 不关闭默认是开启的; 这是个网卡的配置, 关一次就好了. ~]# testpmd> set flow_ctrl tx off 0 ~]# testpmd> set flow_ctrl rx off 0 ~]# testpmd> set fwd rxonly ~]# testpmd> start 在TX侧: ~]# testpmd> set nbcore 32 ~]# testpmd> set flow_ctrl tx off 0 ~]# testpmd> set flow_ctrl rx off 0 ~]# testpmd> set fwd flowgen ~]# testpmd> start nbcore就是说用多少个core RX侧是rxonly模式, tx侧是flowgen模式, 都关闭流控 用set txpkts [length]配置报文大小 testpmd命令记录 #使用前准备 #关闭流控 $ sudo ethtool -A enP5p1s0 rx off tx off #关闭透明大页 见下面 #用-w指定pci设备 $ sudo ./build/app/testpmd -l 42-45 -w 0005:01:00.0 -- -i $ sudo ./build/app/testpmd -l 42-45 -w 0005:01:00.0 --log-level=9 -- -i $ sudo ./build/app/testpmd -l 42-45 -w 0005:01:00.0 -- -i --txq=16 --rxq=16 $ sudo ./build/app/testpmd -l 37-45 -w 0005:01:00.0 -- -i testpmd常用命令 #port信息 testpmd> show port info all #关闭混杂模式 testpmd> set promisc all off #看统计 testpmd> show port stats all testpmd> show port xstats all #清统计 testpmd> clear port stats all watch -dn1 'ethtool -S enP5p1s0 | egrep \"rx_packets|tx_packets|dis|pause\"' #收发包配置信息: packet size,nbcore, queue个数, 描述符个数 testpmd> show config rxtx #看转发配置 testpmd> show config fwd #修改包大小, 默认64字节小包 testpmd> set txpkts 512 #fwd模式 set fwd (io|mac|macswap|flowgen|rxonly|txonly|csum|icmpecho) #设置port的queue和描述符个数 port config all (rxq|txq|rxd|txd) (value) testpmd> port config all rxq 2 testpmd> port config all txq 2 testpmd> port config all txd 2048 testpmd> port config all rxd 2048 #改完配置要stop再start port port stop all port start all #设置rss(receive side scaling)模式 port config all rss (all|ip|tcp|udp|sctp|ether|port|vxlan|geneve|nvgre|none) #从上面看到burst默认32个? port config all burst (value) #设置port的mtu port config mtu X value 因为不经过kernel网络栈, 用ip或ifconfig命令看不到收发包统计; 用ethtool -S可以看到物理层收发包统计. 最简单的一条流(stream)的基本的逻辑是: 一个CPU从一个rxq里收包, 做处理, 再发包到txq rxq/txq的数目和cpu的数目(nbcore)要匹配, testpmd会自动安排多流, 比如配了2个core, 4个qtestpmd> show config fwd flowgen packet forwarding - ports=1 - cores=2 - streams=4 - NUMA support enabled, MP over anonymous pages disabled Logical Core 38 (socket 0) forwards packets on 2 streams: RX P=0/Q=0 (socket 0) -> TX P=0/Q=0 (socket 0) peer=02:00:00:00:00:00 RX P=0/Q=1 (socket 0) -> TX P=0/Q=1 (socket 0) peer=02:00:00:00:00:00 Logical Core 39 (socket 0) forwards packets on 2 streams: RX P=0/Q=2 (socket 0) -> TX P=0/Q=2 (socket 0) peer=02:00:00:00:00:00 RX P=0/Q=3 (socket 0) -> TX P=0/Q=3 (socket 0) peer=02:00:00:00:00:00 根据经验, 一般一个core配一到两个q性能最好; Rx only模式, 单核能到16~17Mpps(小包) 4个核能到43Mpps, 再增加核也上不去了; 经分析, 瓶颈应该在PMD通过Mlx_core驱动(提供的类似UIO的接口?)访问硬件上, 此时可以看到ethtool -S enP5p1s0里面, 接收方rx_discards_phy不断增加, 说明硬件在丢弃报文(有问题, TX一直发, RX stop, 而rx_discards_phy不增加; 很可能是驱动层丢包)--进一步说明硬件的buffer满了, 而上层来不及处理; 但增加core数不管用, 所以瓶颈应该在用户态访问硬件上. testpmd常用的fwd模式 https://doc.dpdk.org/guides/testpmd_app_ug/testpmd_funcs.html?highlight=set%20fwd io: 默认fwd模式, 不改报文 macswap: 交换报文的目的mac和源mac flowgen: 产生多条流, 目的IP变化 rxonly: 只收 txonly: 构造报文发送, 不收 运行多个testpmd实例 默认dpdk的程序不能多开, 会报错. 用下面命令可以多开: lcore不一样, prefix不一样, -w设备不一样 sudo ./build/app/testpmd -l 36-41 -m 512 --file-prefix pg1 -w 0004:01:00.0 --proc-type auto txq_inline=256,rxq_cqe_comp_en=1,txqs_min_inline=8,txq_mpw_en=1 -- -i sudo ./build/app/testpmd -l 42-45 -m 512 --file-prefix pg2 -w 0005:01:00.0 --proc-type auto txq_inline=256,rxq_cqe_comp_en=1,txqs_min_inline=8,txq_mpw_en=1 -- -i testpmd查看统计 ~]# testpmd> show port stats all ~]# testpmd> show port xstats all l3fwd 编译 export RTE_TARGET=arm64-armv8a-linuxapp-gcc make examples T=$RTE_TARGET 运行 #主要是配--config, 这是个map表. l3fwd没有交互模式, 所以配多少个q, 怎么map, 都在这个表里了.(port, q, core) sudo examples/l3fwd/arm64-armv8a-linuxapp-gcc/l3fwd -l 37-45 -w 0005:01:00.0 -n 6 -- -p 0x1 --config '(0,0,37),(0,1,38)' #多了--不一样哦 examples/l3fwd/arm64-armv8a-linuxapp-gcc/l3fwd --help examples/l3fwd/arm64-armv8a-linuxapp-gcc/l3fwd -- --help #要加混杂模式才能转发? -P examples/l3fwd/arm64-armv8a-linuxapp-gcc/l3fwd -l 37-45 -w 0004:01:00.1 -n 6 -- -P -p 0x1 --config '(0,0,37),(0,1,38)' -p PORTMASK: Hexadecimal bitmask of ports to configure -P : Enable promiscuous mode -E : Enable exact match -L : Enable longest prefix match (default) --config (port,queue,lcore): Rx queue configuration --eth-dest=X,MM:MM:MM:MM:MM:MM: Ethernet destination for port X --enable-jumbo: Enable jumbo frames --max-pkt-len: Under the premise of enabling jumbo, maximum packet length in decimal (64-9600) --no-numa: Disable numa awareness --hash-entry-num: Specify the hash entry number in hexadecimal to be setup --ipv6: Set if running ipv6 packets --parse-ptype: Set to use software to analyze packet type pktgen 编译见上面 cd ~/repo/hxt/pktgen/pktgen-dpdk-pktgen-3.4.5 #运行, [42:43].0的意思是rx用core42,tx用core43, core41为显示和timer #-T是打开颜色, -P是混杂模式 sudo app/arm64-armv8a-linuxapp-gcc/pktgen -l 41-45 -w 0005:01:00.0 -- -T -P -m \"[42:43].0\" #43和44两个核做tx, 能64字节线速 sudo app/arm64-armv8a-linuxapp-gcc/pktgen -l 41-45 -w 0005:01:00.0 -- -T -P -m \"[42:43-44].0\" sudo app/arm64-armv8a-linuxapp-gcc/pktgen -l 31-35 -w 0005:01:00.0 -- -T -P -m \"[32-33:34-35].0\" #运行testpmd sudo ./build/app/testpmd -l 37-45 -n 6 -w 0005:01:00.0 txq_inline=256,rxq_cqe_comp_en=1,txqs_min_inline=8,txq_mpw_en=1 -- -i pktgen命令记录 #启动pktgen, rx用core42,tx用core43, core41为显示和timer sudo app/arm64-armv8a-linuxapp-gcc/pktgen -l 41-45 -w 0005:01:00.0 -- -T -P -m \"[42:43].0\" #配目的IP(组播IP) Pktgen:/> set 0 dst ip 224.0.0.1 #改变目的组播IP Pktgen:/> set 0 dst ip 225.0.0.1 #配目的mac(组播MAC) Pktgen:/> set 0 dst mac 01:00:5e:00:00:01 #配目的组播mac Pktgen:/> set 0 dst mac 01:00:5e:00:00:02 #设置发包速率为1%, 太大会丢包 Pktgen:/> set 0 rate 1 #开始发送 Pktgen:/> start all #停止发送 Pktgen:/> stop all #设置发包数目 Pktgen:/> set 0 count 1000000000 #设置发包大小 Pktgen:/> set 0 size 1024 #清除统计 Pktgen:/> clr #burst数目 set 0 burst 32 #多流 start min max inc range 0 dst ip 0.0.0.0 0.0.0.0 1.2.3.4 0.0.1.0 range 0 dst ip 0.0.0.0 0.0.0.0 0.0.0.1 0.0.0.1 range 0 size 64 64 64 0 range 0 size 256 256 256 0 range 0 size 512 512 512 0 range 0 size 1024 1024 1024 0 enable 0 range #抓包 #开始 enable 0 capture #结束抓, 此时会在当前目录下生成.pcap文件 disable 0 capture #查看该pcap文件 tcpdump -vvXX -r pktgen-20181113-160020-0.pcap 图示 测试结果 A对B打流, A:pktgen, 1core tx, 1core rx; B:testpmd, fwd=io, nbcore=1, rxtx配置如下 pps L1 Dcache miss note rxq= 1 txq=1 rxd=128 txd=512 Rx~=Tx=16.1M rx_out_of_buffer不断增长 rxq= 1 txq=1 rxd=256 txd=256 Rx:17.9M Tx:14.9M rx_out_of_buffer不断增长 rxq= 1 txq=1 rxd=512 txd=512 Rx~=Tx=16.3M 2.80% rx_out_of_buffer不断增长 rxq= 1 txq=1 rxd=1024 txd=1024 Rx~=Tx=13.6M 3.08% rx_out_of_buffer不断增长 rxq= 1 txq=1 rxd=2048 txd=2048 Rx~=Tx=12.6M 3.19% rx_out_of_buffer不断增长 rxq= 1 txq=1 rxd=4096 txd=4096 Rx~=Tx=12.6M rx_out_of_buffer不断增长 rxq= 2 txq=2 rxd=128 txd=512 Rx~=Tx=15.5M rx_out_of_buffer不断增长 rxq= 2 txq=2 rxd=2048 txd=2048 Rx~=Tx=12.3M rx_out_of_buffer不断增长 Mellanox DPDK性能优化 关闭流控 ethtool -A eth16 rx off tx off huagepage 用/dev/hugepages KVM用hugepage -mem-path /dev/hugepages isolcpu in grub kernel cmdline isolcpus=4-43 关闭透明大页 echo never > /sys/kernel/mm/transparent_hugepage/defrag echo never > /sys/kernel/mm/transparent_hugepage/enabled echo 0 > /sys/kernel/mm/transparent_hugepage/khugepaged/defrag sysctl -w vm.swappiness=0 sysctl -w vm.zone_reclaim_mode=0 关闭中断均衡 service irqbalance stop #手动配置中断 set_irq_affinity_cpulist.sh 0-1 ethX mlx网卡的一些特殊优化 $ sudo ibdev2netdev -v 0004:01:00.0 mlx5_0 (MT4115 - MT1738K12272) CX413A - ConnectX-4 QSFP28 fw 12.22.1002 port 1 (ACTIVE) ==> enP4p1s0 (Up) 0005:01:00.0 mlx5_1 (MT4115 - MT1738K12253) CX413A - ConnectX-4 QSFP28 fw 12.22.1002 port 1 (ACTIVE) ==> enP5p1s0 (Up) mlxconfig -d mlx5_0 q mlxconfig -d mlx5_0 s CQE_COMPRESSION=1 #for ConnectX-4: txq_inline=200, txqs_min_inline=4 ./testpmd -c 0x1ff -n 4 -w 0000:08:00.0,txq_inline=128,txqs_min_inline=4 -w 0000:08:00.1,txq_inline=128,txqs_min_inline=4 --socket-mem=2048,0 -- --port-numa-config=0,0,1,0 --socket-num=0 --burst=64 --txd=1024 -- rxd=256 --mbcache=512 --rxq=4 --txq=4 --nb-cores=8 -i 用taskset pin vm的核 Mellanox驱动/FW的命令汇总 mod option 在/etc/modprobe.d/mlx4.conf里面可以添加参数 options mlx4_core parameter= options mlx4_en parameter= 比如下面的参数, 详见Mellanox Linux user manual.pdf debug_level enable_sys_tune num_vfs probe_vf set_4k_mtu enable_qos probe_vf 常用命令 #进入 退出混杂模式 ifconfig eth2 promisc ifconfig eth2 -promisc #查看mlx网卡信息 ibstat #看mlx网卡收发包实际速度(ethtool) mlnx_perf -i enP4p1s0f0 #PCI设备和接口对应 $ sudo ibdev2netdev -v 0004:01:00.0 mlx5_0 (MT4115 - MT1738K12240) CX413A - ConnectX-4 QSFP28 fw 12.22.1002 port 1 (ACTIVE) ==> enP4p1s0 (Up) 0005:01:00.0 mlx5_1 (MT4115 - MT1717X06081) CX413A - ConnectX-4 QSFP28 fw 12.22.1002 port 1 (ACTIVE) ==> enP5p1s0 (Up) #FW mlxfwmanager #当前版本 $ ethtool -i enP5p1s0 driver: mlx5_core version: 4.3-3.0.2 firmware-version: 12.22.1002 (MT_2120110027) expansion-rom-version: bus-info: 0005:01:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes #offload功能状态 $ sudo ethtool -k enP5p1s0 #配置offload功能 ethtool -K eth [rx on|off] [tx on|off] [sg on|off] [tso on|off] [lro on|off] #看q和irq的数目, 用L(大写)可以配置 $ ethtool -l enP5p1s0 #中断合并, 参数大C可以配置 $ ethtool -c enP5p1s0 #查看pause帧配置 $ ethtool -a enP5p1s0 #ring大小 $ ethtool -g enP5p1s0 #查看详细统计, 详细到每个q ethtool -S enP5p1s0 #qos $ mlnx_qos -i enP5p1s0 #traffic class $ sudo tc_wrap.py -i enP5p1s0 #拥塞控制工具 mlnx_qcn mst tool(Mellanox Software Tools) $ sudo mst start $ sudo mst status SR-IOV相关 SR-IOV使能, 重启不保存 sudo bash -c \"echo 1 > /sys/module/mlx5_core/parameters/probe_vf\" sudo bash -c \"echo 2 > /sys/class/infiniband/mlx5_0/device/mlx5_num_vfs\" #或者用 /sys/bus/pci/devices/0004:01:00.0/sriov_numvfs 执行成功后, 用ifconfig和ibdev2netdev能看到新加的接口 eg. 在原来的接口enP4p1s0基础上, 新增了enP4p1s0f1和enP4p1s0f2 如果probe_vf为0, 则不创建net device, 即没有enP4p1s0f1和enP4p1s0f2, 但PCI的VF是有的 这种情况很常用, 因为VF最终会被bind到VM里. 根据比较新的kernel版本 /sys/class/net/enP4p1s0/device/sriov_drivers_autoprobe也有相同的功能 详见 https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/diff/Documentation/ABI/testing/sysfs-bus-pci?id=0e7df22401a3dfd403b26dea62dd00e0598b538b 好几个文件都有相同的功能: /sys/class/net/enP4p1s0/device/sriov_numvfs /sys/class/net/enP4p1s0/device/mlx5_num_vfs 并且, 如果当前有VF已经分配了, 此时想改numvfs数的话, 要先disable sriov. disable sriov, 配置VF要在disable sriov以后做 sudo bash -c \"echo 0 > /sys/class/infiniband/mlx5_0/device/mlx5_num_vfs\" 注: 此时查看ifconfig, VF的接口消失了 允许的vf数 cat /sys/class/net/enP4p1s0/device/sriov_totalvfs 如果要增加vf数, 要参考下面的mlxconfig s命令 SR-IOV使能, 重启保存(未验证) 需要mst启动, 用mst status查看mst_device step 1 $ sudo mlxconfig -d /dev/mst/mt4115_pciconf0 s SRIOV_EN=1 NUM_OF_VFS=8 Device #1: ---------- Device type: ConnectX4 Name: N/A Description: N/A Device: /dev/mst/mt4115_pciconf0 Configurations: Next Boot New SRIOV_EN True(1) True(1) Apply new Configuration? ? (y/n) [n] : y Applying... Done! -I- Please reboot machine to load new configurations. step 2 #查询一下并重启fw $ sudo mlxconfig -d /dev/mst/mt4115_pciconf0 q Device #1: ---------- Device type: ConnectX4 Name: N/A Description: N/A Device: /dev/mst/mt4115_pciconf0 Configurations: Next Boot MEMIC_BAR_SIZE 0 MEMIC_SIZE_LIMIT _256KB(1) ROCE_NEXT_PROTOCOL 254 NON_PREFETCHABLE_PF_BAR False(0) NUM_OF_VFS 8 FPP_EN True(1) SRIOV_EN True(1) PF_LOG_BAR_SIZE 5 VF_LOG_BAR_SIZE 0 NUM_PF_MSIX 63 NUM_VF_MSIX 11 INT_LOG_MAX_PAYLOAD_SIZE AUTOMATIC(0) SW_RECOVERY_ON_ERRORS False(0) RESET_WITH_HOST_ON_ERRORS False(0) CQE_COMPRESSION BALANCED(0) IP_OVER_VXLAN_EN False(0) LRO_LOG_TIMEOUT0 6 LRO_LOG_TIMEOUT1 7 LRO_LOG_TIMEOUT2 8 LRO_LOG_TIMEOUT3 12 LOG_DCR_HASH_TABLE_SIZE 14 DCR_LIFO_SIZE 16384 ROCE_CC_PRIO_MASK_P1 255 ROCE_CC_ALGORITHM_P1 ECN(0) #mstflint是个fw tool, 读mlx网卡的flash sudo mstflint -d 0004:01:00.0 dc nv_config.global.pci.settings.log_vf_uar_bar_size = 0x0 nv_config.global.pci.settings.sriov_en = 1 nv_config.global.pci.settings.total_vfs = 8 en_dis_exprom_through_nv = 1 #重启FW mlxfwreset -d /dev/mst/mt4115_pciconf0 reset step 3 #修改/etc/modprobe.d/mlx4_core.conf如下 options mlx4_core num_vfs=5 port_type_array=1,2 probe_vf=1 注: num_vfs=n 是说使能8个vf到每个卡? num_vfs=00:04.0-5;6;7,00:07.0-8;9;10 驱动会使能: * HCA positioned in BDF 00:04.0 5 single VFs on port 1 6 single VFs on port 2 7 dual port VFs * HCA positioned in BDF 00:07.0 8 single VFs on port 1 9 single VFs on port 2 10 dual port VFs port_type_array=1,2 是说两个port的类型 1-ib, 2-eth, 3-auto, 4-N/A probe_vf=1 是说在host上开1个vf 在mlx5_core上, 这是个bool值 step 4 重启机器 VF相关命令 一般用iproute2里面的tool Vlan tag #配置出guest加tag, 入guest去tag ip link set dev vf vlan [qos ] #其中: * NUM = 0..max-vf-num * vlan_id = 0..4095 * qos = 0..7ip VF的mac link状态 ip link set dev vf mac ip link set dev vf spoofchk [on | off] ip link set dev vf state [enable| disable| auto] VF的包统计 cat /sys/class/infiniband/mlx5_2/device/sriov/2/stats VF和port的关系 ip link 61: p1p1: mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 00:02:c9:f1:72:e0 brd ff:ff:ff:ff:ff:ff vf 0 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto vf 37 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto vf 38 MAC ff:ff:ff:ff:ff:ff, vlan 65535, spoof checking off, link-state disable vf 39 MAC ff:ff:ff:ff:ff:ff, vlan 65535, spoof checking off, link-state disable 或者 mlnx_get_vfs.pl VF的QoS 需要驱动支持, module 参数 enable_vfs_qos=1, ip link set dev vf rate vxlan相关 Vxlan: 把二层报文用UDP封装, 可以理解成扩大了二层范围 Mlx网卡有offload Vxlan的功能 $ ethtool -k eth0 | grep udp_tnl tx-udp_tnl-segmentation: on # ip link add vxlan0 type vxlan id 10 group 239.0.0.10 ttl 10 dev ens1f0 dstport 4789 # ip addr add 192.168.4.7/24 dev vxlan0 # ip link set up vxlan0 注意: Vxlan增加了50 bytes (14-eth + 20-ip + 8-udp + 8-vxlan), 需要修改MTU QinQ 比如两个机器A和B, 分别起了VM VMA和VMB, A和B有vlan100, 而VMA和VMB在Vlan40里面, 此时VMA要发报文给VMB, 要配置VF是VST QinQ, 从它发出来的报文会被网卡加上vlan100, 发给VMB以后, B上的网卡再去掉vlan100, 这样VMA和VMB都还以为自己在Vlan40里面 #获取priv-flags $ sudo ethtool --show-priv-flags enP5p1s0 Private flags for enP5p1s0: rx_cqe_moder : on tx_cqe_moder : off rx_cqe_compress : off sniffer : off dropless_rq : off per_channel_stats: on #在hypervisor里面使能QinQ ethtool --set-priv-flags ens2 phv-bit on #为每个VF的port设置S-Vlan tag(100) ip link set dev ens2 vf 0 vlan 100 proto 802.1ad #也可以设置qof ip link set dev ens2 vf 0 vlan 100 qos 3 proto 802.1ad #给VF配自己的vlan和IP ip link add link ens5 ens5.40 type vlan protocol 802.1q id 40 ip addr add 42.134.135.7/16 brd 42.134.255.255 dev ens5.40 ip link set dev ens5.40 up VM相关 两个VM用vhost-user连OVS 详见https://software.intel.com/en-us/articles/using-open-vswitch-with-dpdk-for-inter-vm-nfv-applications #ovs配置 cd $OVS_DIR sudo ./utilities/ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev sudo ./utilities/ovs-vsctl add-port br0 vhost-user1 -- set Interface vhost-user1 type=dpdkvhostuser sudo ./utilities/ovs-vsctl add-port br0 vhost-user2 -- set Interface vhost-user2 type=dpdkvhostuser #qemu配置 sudo qemu-system-x86_64 -m 1024 -smp 4 -cpu host -hda /home/user/centos7vm1.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char1,path=/run/openvswitch/vhost-user1 \\ -netdev type=vhost-user,id=mynet1,chardev=char1,vhostforce \\ -device virtio-net-pci,mac=00:00:00:00:00:01,netdev=mynet1 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on -numa node,memdev=mem -mem-prealloc sudo qemu-system-x86_64 -m 1024 -smp 4 -cpu host -hda /home/user/centosvm2.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char2,path=/run/openvswitch/vhost-user2 \\ -netdev type=vhost-user,id=mynet2,chardev=char2,vhostforce \\ -device virtio-net-pci,mac=00:00:00:00:00:02,netdev=mynet2 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on -numa node,memdev=mem -mem-prealloc virt-manager 用MLX网卡的VF, 要选PCI Host Device列表里面的VF, 并在VM里面安装MLNX_OFED 增加网卡VF的xml举例如下: 性能相关 grub选项 根据intel性能报告提到不仅要在grub里面isolcpus, 还要配置nohz_full和rcu_nocbs, 以关掉相应核的中断 default_hugepagesz=1G hugepagesz=1G hugepages=16 isolcpus=1-21,28-48 nohz_full=1-21,28-48 rcu_nocbs=1-21,28-48 Note: nohz_full and rcu_nocbs is to disable Linux system interrupts, and it’s important for zero-packet loss test. Generally, 1G huge pages are used for performance test. intel报告 on Platinum 8180 详见: http://fast.dpdk.org/doc/perf/DPDK_17_08_Intel_NIC_performance_report.pdf DUT配置 testpmd testpmd单核 L3fwd l3fwd单核 "},"notes/OVS_phy-vm-phy.html":{"url":"notes/OVS_phy-vm-phy.html","title":"OVS PHY-VM-PHY","keywords":"","body":" 环境 配置 OVS配置 VM配置 实验1 OVS全部1Q 拓扑模型 拓扑1 拓扑2 性能结果 拓扑1运行数据 拓扑1多流(TCP)运行数据 pktgen命令参考 性能数据 可能的瓶颈 配flow 解决vm里面testpmd性能不够问题 拓扑2运行数据 环境 服务器 Host A 10.129.41.129 Host B 10.129.41.130 Note Socket 1 1 单socket CPU 48core@2.6G HXT1.0 46core@2.6G HXT1.0 sudo dmidecode -t processor MEM 96G 96G free -h NIC MLX CX4121A 10G 0004:01:00.0 enP4p1s0f0 MLX CX4121A 10G 0004:01:00.0 enP4p1s0f0 ibdev2netdev -v NIC MLX CX4121A 10G 0004:01:00.1 enP4p1s0f1 MLX CX4121A 10G 0004:01:00.1 enP4p1s0f1 ibdev2netdev -v OS CentOS 7.5.1804 CentOS 7.5.1804 cat /etc/redhat-release kernel 4.14.62-5.hxt.aarch64 4.14.62-5.hxt.aarch64 uname -r Mellanox OFED version 4.4-1.0.0.0 4.4-1.0.0.0 ofed_info -s QEMU version NA 2.12.1 qemu-system-aarch64 --version 源码编译 DPDK version 17.11.4 17.11.4 源码编译 pktgen version 3.4.9 NA 源码编译 OVS(with DPDK) version NA 2.10.1(with dpdk 17.11.4) sudo ovs-vsctl show 源码编译 libvirt version NA 4.6.0 源码编译 virt-manager version NA 1.5.1 源码安装 配置 预置条件: DPDK, OVS, Qemu, libvirt, virt-manager已经成功编译安装 OVS配置 OVS选项 值 说明 dpdk-init true bridge ovsbr0 pmd-cpu-mask FF00000000 8个core 32 33 34 35 36 37 38 39 dpdk-socket-mem 16384 单socket 16G vhost-user port 0 dpdkvhostuser0 vhost-user port 1 dpdkvhostuser1 dpdk port0 dpdkp0 MLX CX4 10G NIC #增加32G的hugepage sudo bash -c \"echo 64 > /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages\" #开始ovs export PATH=$PATH:/usr/local/share/openvswitch/scripts sudo ovs-ctl start #打开硬件offload, 2.10才有; 2.9.3没效果 #miniflow_extract, dpcls_lookup等函数被offload了 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:hw-offload=true #多条流会有用? :提高~15% sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:smc-enable=true #打开dpdk初始化 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true #新增一个bridge, 使用用户态datapath模式 sudo ovs-vsctl add-br ovsbr0 -- set bridge ovsbr0 datapath_type=netdev #配置pmd跑在8个核上, core 32 33 34 35 36 37 38 39 sudo ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=FF00000000 #配置使用node0的16G内存. sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"16384\" #增加物理port, 对应物理NIC sudo ovs-vsctl add-port ovsbr0 dpdkp0 -- set Interface dpdkp0 type=dpdk options:dpdk-devargs=0004:01:00.0 #增加两个port, 类型是dpdkvhostuserclient sudo ovs-vsctl add-port ovsbr0 dpdkvhostuser0 -- set Interface dpdkvhostuser0 type=dpdkvhostuserclient sudo ovs-vsctl set Interface dpdkvhostuser0 options:vhost-server-path=\"/tmp/dpdkvhostuser0\" sudo ovs-vsctl add-port ovsbr0 dpdkvhostuser1 -- set Interface dpdkvhostuser1 type=dpdkvhostuserclient sudo ovs-vsctl set Interface dpdkvhostuser1 options:vhost-server-path=\"/tmp/dpdkvhostuser1\" #以下为可选配置 #配置多queue sudo ovs-vsctl set Interface dpdkvhostuser0 options:n_rxq=2 sudo ovs-vsctl set Interface dpdkvhostuser1 options:n_rxq=2 #设置物理port使用2个q sudo ovs-vsctl set Interface dpdkp0 options:n_rxq=2 VM配置 预置条件: libvirtd已经成功启动 先用virt-manager创建2个VM, 配置如下: VM | VM01 | VM02 ---|---|---| CPUs | 4 | 4 Memory | 8192M | 8192M Disk | hxt-centos7.5-01.qcow2 32G | hxt-centos7.5-02.qcow2 32G NIC(for external Net) | virtio NAT | virtio NAT NIC(for test) | vhostuser0 | vhostuser0 NIC(for test) | vhostuser1 | vhostuser1 #启动libvirtd服务 sudo systemctl start libvirtd #启动virt-manager sudo xauth add $(xauth -f /home/bai/.Xauthority list | tail -1) sudo virt-manager -c 'qemu+unix:///system?socket=/usr/local/var/run/libvirt/libvirt-sock' #启动一个VM, 这个VM配置如上 sudo virsh create ~/repo/save/vm/2vhostuser.xml 实验1 OVS全部1Q $ sudo ovs-vsctl set Interface dpdkp0 options:n_rxq=1 $ sudo ovs-vsctl set Interface dpdkvhostuser0 options:n_rxq=1 $ sudo ovs-vsctl set Interface dpdkvhostuser1 options:n_rxq=1 $ sudo ovs-appctl dpif/show netdev@ovs-netdev: hit:8540019443 missed:406 ovsbr0: dpdkp0 3/4: (dpdk: configured_rx_queues=1, configured_rxq_descriptors=2048, configured_tx_queues=9, configured_txq_descriptors=2048, lsc_interrupt_mode=false, mtu=1500, requested_rx_queues=1, requested_rxq_descriptors=2048, requested_tx_queues=9, requested_txq_descriptors=2048, rx_csum_offload=true) dpdkvhostuser0 1/2: (dpdkvhostuser: configured_rx_queues=1, configured_tx_queues=1, mtu=1500, requested_rx_queues=1, requested_tx_queues=1) dpdkvhostuser1 2/3: (dpdkvhostuser: configured_rx_queues=1, configured_tx_queues=1, mtu=1500, requested_rx_queues=1, requested_tx_queues=1) ovsbr0 65534/1: (tap) 拓扑模型 拓扑1 pktgen发64字节小包, 单条流, 目的mac随意, 经过OVS到VM, VM跑testpmd macswap转发, 只经过一个vhost user port 在VM上执行 sudo arm64-armv8a-linuxapp-gcc/app/testpmd -w 04:00.0 -- -i --forward-mode=macswap 拓扑1: 经过1个vhostuser port, 单q 拓扑2 在VM上执行 sudo arm64-armv8a-linuxapp-gcc/app/testpmd -- -i --forward-mode=macswap 拓扑2: 经过2个vhostuser port, 单q 性能结果 上面两个图性能差不多, 大约如下: Frame Size (Bytes) Max Line Rate (Mpps) Throughput (Mpps) % Line Rate 64 14.88 3.85 25.87% 128 8.45 3.33 39.40% 256 4.53 3.25 71.74% 512 2.35 2.35 100% 1024 1.20 1.20 100% 1518 0.81 0.81 100% 拓扑1运行数据 稳定状态下OVS的pps统计 调查对象 运行状况 rxq分布 core 33统计 core 34统计 拓扑1多流(TCP)运行数据 case# L2 flow# L3 flow# dst ip src ip dst port src port dst mac src mac pktgen default single 1 1 192.168.1.1 192.168.0.1 5678 1234 00:00:00:00:00:00 ec:0d:9a:d9:89:86 pktgen default range 1 254*255*255 = 16516350 192.168.1.{1..254} 192.168.0.1 {0..254} {0..254} 00:00:00:00:00:00 ec:0d:9a:d9:89:86 case 1 1 2*255*255 = 130050 192.168.1.{1..2} 192.168.0.1 {0..254} {0..254} 00:00:00:00:00:00 ec:0d:9a:d9:89:86 case 2 2 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:00:{01 .. 02} case 3 4 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:00:{01 .. 04} case 4(CMCC) 10000 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:{00:01 .. 27:10} case 5 8 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:00:{01 .. 08} case 6 255 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:00:{01 .. ff} case 7 2000 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:{00:01 .. 07:d0} case 8 4000 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:{00:01 .. 0f:a0} case 9 3000 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:{00:01 .. 0b:b8} case 10 2500 1 192.168.1.1 192.168.0.1 0 0 00:00:00:00:00:00 00:00:00:00:{00:01 .. 09:c4} 注: case 4(CMCC): 是CMCC NFV测试规范里的要求配置: CMCC NFV测试规范: 7.3.5.1 基于10GE网卡的网络转发性能 pktgen命令参考 enable 0 range #从默认状态的range, IP变化2个 range 0 dst ip max 192.168.1.2 #从默认状态的range, 改为range模式的单流 range 0 dst ip inc 0.0.0.0 range 0 dst port inc 0 range 0 src port inc 0 #改变src mac, 10000个 range 0 src mac 00:00:00:00:00:01 00:00:00:00:00:01 00:00:00:00:27:10 00:00:00:00:00:01 #改变src mac, 2个 range 0 src mac 00:00:00:00:00:01 00:00:00:00:00:01 00:00:00:00:00:02 00:00:00:00:00:01 #改变src mac, 2000个 range 0 src mac 00:00:00:00:00:01 00:00:00:00:00:01 00:00:00:00:07:d0 00:00:00:00:00:01 性能数据 case L2 flow# L3 flow# Throughput@64B (Mpps) OVS flow# Note pktgen default single 1 1 3.82 2 case 1 1 2*255*255 = 130050 3.32 2 pktgen default range 1 254*255*255 = 16516350 2.31 2 case 2 2 1 3.00 4 case 3 4 1 2.58 8 case 5 8 1 2.31 16 case 6 255 1 1.89 510 case 7 2000 1 1.77 4000 以上case的htop ---- ---- ---- ---- 以下case OVS开始\"不正常\" ---- ---- ---- ---- case 10 2500 1 1.30 5000 case 9 3000 1 1.10 6000 case 8 4000 1 0.88 8000 case 4(CMCC) 10000 1 0.68 20000 注: 查OVS的flow数: sudo ovs-appctl dpctl/show --statistics dpdkvhostuser0时不时有TX dropped 可能的瓶颈 首先是VM里面的testpmd? 其次是dpdkp0收包? 配flow #配flow sudo ovs-vsctl set interface dpdkp0 ofport_request=1 sudo ovs-vsctl set interface \"dpdkvhostuser0\" ofport_request=2 sudo ovs-vsctl set interface \"dpdkvhostuser1\" ofport_request=3 sudo ovs-ofctl add-flow ovsbr0 in_port=1,action=output:2 sudo ovs-ofctl add-flow ovsbr0 in_port=2,action=output:1 手动配端口flow之后, 源mac变化的所有case里, 自动生成的flow个数一直保持为2, 性能的下降也变小了很多; 对比之前没有配置port flow规则的时候, 生成的flow个数和源mac个数正相关. $ sudo ovs-ofctl dump-flows ovsbr0 cookie=0x0, duration=735.584s, table=0, n_packets=2560466336, n_bytes=153627980160, in_port=dpdkp0 actions=output:dpdkvhostuser0 cookie=0x0, duration=734.856s, table=0, n_packets=1393435005, n_bytes=83606100300, in_port=dpdkvhostuser0 actions=output:dpdkp0 cookie=0x0, duration=232599.667s, table=0, n_packets=20420892318, n_bytes=1225253539080, priority=0 actions=NORMAL $ sudo ovs-appctl dpctl/dump-flows flow-dump from pmd on cpu core: 39 recirc_id(0),in_port(1),packet_type(ns=0,id=0),eth_type(0x0800),ipv4(frag=no), packets:109149103, bytes:6548946180, used:0.000s, flags:., actions:3 flow-dump from pmd on cpu core: 34 recirc_id(0),in_port(3),packet_type(ns=0,id=0),eth_type(0x0800),ipv4(frag=no), packets:54566419, bytes:3273985140, used:0.000s, flags:., actions:1 解决vm里面testpmd性能不够问题 拓扑2运行数据 pktgen发64字节小包, 经过OVS到VM, VM跑testpmd macswap转发, 经过2个vhost user port dpdkp0收包 dpdkp0: RX packets: 4310802 pps dpdkp0: TX packets: 3918137 pps dpdkvhostuser0: RX packets: 3688044 pps dpdkvhostuser0: TX packets: 250301 pps dpdkvhostuser1: RX packets: 229745 pps dpdkvhostuser1: TX packets: 4060117 pps "},"notes/networking_网络虚拟化用例记录.html":{"url":"notes/networking_网络虚拟化用例记录.html","title":"网络虚拟化用例记录","keywords":"","body":" 测试前准备 服务器配置 测试示意图 测试前准备 启动并配置OVS 创建并配置VM 5.4 网络虚拟化 5.4.1.1 OVS采用DPDK 测试结果 -- PASS 5.4.1.2 OVS支持numa感知 测试结果 -- PASS 5.4.2 支持SR-IOV网卡 5.4.2.1 SR-IOV链路状态感知测试 测试link up -- PASS 测试link down -- FAIL Fail分析 5.4.2.2 Hypervisor允许只使用一对物理网口 测试结果 -- PASS issue讨论 5.4.2.3 SR-IOV与虚拟交换机互通 测试结果 -- 待澄清测试场景后测试 5.4.3 SR-IOV网卡的桥接功能 测试结果 -- PASS 5.4.4 支持vNIC多队列 测试结果 -- PASS 5.4.5 网络隔离与地址复用 5.4.5.1 OVS 网络隔离与地址复用 测试结果 -- PASS 5.4.5.2 SR-IOV 网络隔离与地址复用 测试结果 -- PASS 5.4.6 MTU可调整 5.4.6.1 OVS MTU可调整（vxlan网络只在SDN场景下测试） 测试结果 -- PASS 疑问: dpdkp1的txq个数 5.4.6.2 SR-IOV MTU可调整（vxlan网络只在SDN场景下测试） 测试结果 -- PASS 5.4.7 支持VLAN透传 5.4.7.1 OVS 支持VLAN透传 测试结果 -- PASS 5.4.7.2 SR-IOV 支持VLAN透传 测试结果 -- FAIL ToDo: 切换至OFED 4.4版本重新测试 5.4.8 带宽分配比例设定 5.4.8.1 OVS 支持带宽分配比例设定 测试结果 -- PASS 问题: netperf over OVS-DPDK性能差 5.4.8.2 SR-IOV 支持带宽分配比例设定 测试结果 -- PASS(to be clarified) 如何实现调整带宽比例到150%或50%? 5.4.9 支持组播 5.4.9.1 OVS支持正确转发组播包 测试结果 -- PASS 5.4.9.2 SR-IOV支持正确转发组播包 测试结果 -- PASS 7.3 虚拟层性能 7.3.5 网络转发性能 7.3.5.1 基于10GE网卡的网络转发性能 7.3.6 网络转发质量 7.3.6.1 基于10GE网卡的网络转发质量 Troubleshooting OVS异常重启导致vhostuser不通 待解决问题 netperf over OVS-DPDK性能差 VM同时添加sriov和vhost-user导致OVS崩溃重启 本文在HXT ARM服务器上做网络虚拟化的测试, 首先考察功能是否完备, 有无功能缺失和异常; 其次考察性能, 但不对比X86数据. 测试前准备 服务器配置 文中使用的HXT ARM服务器2台, 简称Host A, Host B信息如下: 服务器 Host A Host B Note Socket 1 1 单socket CPU 46core@2.6G AW2542 v2.1 46core@2.6G AW2542 v2.1 sudo dmidecode -t processor MEM 96G 96G free -h NIC 2 * Mellanox ConnectX-4 2 * Mellanox ConnectX-4 lspci OS CentOS 7.5.1804 CentOS 7.5.1804 cat /etc/redhat-release kernel 4.14.36-4.hxt.aarch64 4.14.36-4.hxt.aarch64 uname -r Mellanox OFED version 4.3-3.0.2 4.3-3.0.2 ethtool -i enP4p1s0 QEMU version NA 2.12 qemu-system-aarch64 --version 源码编译 DPDK version hxt-dev-v17.08 hxt-dev-v17.08 源码编译 OVS(with DPDK) version NA 2.8.4 sudo ovs-vsctl show 源码编译 libvirt version NA 4.6.0 源码编译 virt-manager version NA 1.5.1 源码安装 对应测试编号5.1.1 Hypervisor版本要求 要求如下: Host OS的版本 ≥ 3.10.0 kvm-qemu的版本 ≥ 2.3.0 openvswitch ≥ 2.5.0 dpdk的版本 ≥ 16.11.0 注: 如果没有特别说明, 测试均在Host B上执行 测试示意图 测试前准备 预置条件: DPDK, OVS, Qemu, libvirt, virt-manager已经成功编译安装 启动并配置OVS OVS选项 值 说明 dpdk-init true bridge ovsbr0 pmd-cpu-mask FF00000000 8个core 32 33 34 35 36 37 38 39 dpdk-socket-mem 4096 单socket 4G vhost-user port 0 dpdkvhostuser0 n_rxq=2 vhost-user port 1 dpdkvhostuser1 n_rxq=2 export PATH=$PATH:/usr/local/share/openvswitch/scripts sudo ovs-ctl start #打开dpdk初始化 sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true #新增一个bridge, 使用用户态datapath模式 sudo ovs-vsctl add-br ovsbr0 -- set bridge ovsbr0 datapath_type=netdev #配置pmd跑在8个核上, core 32 33 34 35 36 37 38 39 sudo ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=FF00000000 #配置使用node0的4G内存. sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"4096\" #增加两个port, 类型是dpdkvhostuser sudo ovs-vsctl add-port ovsbr0 dpdkvhostuser0 -- set Interface dpdkvhostuser0 type=dpdkvhostuser sudo ovs-vsctl add-port ovsbr0 dpdkvhostuser1 -- set Interface dpdkvhostuser1 type=dpdkvhostuser #配置多queue sudo ovs-vsctl set Interface dpdkvhostuser0 options:n_rxq=2 sudo ovs-vsctl set Interface dpdkvhostuser1 options:n_rxq=2 创建并配置VM 预置条件: libvirtd已经成功启动 先用virt-manager创建2个VM, 配置如下: VM | VM01 | VM02 ---|---|---| CPUs | 4 | 4 Memory | 8192M | 8192M Disk | hxt-centos7.5-01.qcow2 32G | hxt-centos7.5-02.qcow2 32G NIC1(optional) | virtio(NAT to eth0) | virtio(NAT to eth0) NIC1是用来连接外网的, 还需要增加NIC2用来连接OVS的dpdkvhostuser port: 手工修改xml文件virsh edit , 适当增加如下修改: hvm /usr/local/bin/qemu-system-aarch64 另外一个VM也相应修改 需要提前配置hugetlbfs, CentOS上dev-hugepages.mount服务会默认开启, mount到/dev/hugepages 如果大页内存不足, 可以适当增加hugepage数目: sudo bash -c \"echo 64 > /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages\" 5.4 网络虚拟化 5.4.1.1 OVS采用DPDK 测试结果 -- PASS 启动VM, 验证连通性 配了IP后, 两个VM可以互相ping通 结果: $ sudo ovs-vsctl show f81a1c52-91c1-40a2-b993-6dfeea09317e Bridge \"ovsbr0\" Port \"dpdkvhostuser0\" Interface \"dpdkvhostuser0\" type: dpdkvhostuser Port \"ovsbr0\" Interface \"ovsbr0\" type: internal Port \"dpdkvhostuser1\" Interface \"dpdkvhostuser1\" type: dpdkvhostuser ovs_version: \"2.8.4\" 5.4.1.2 OVS支持numa感知 测试结果 -- PASS $ sudo ovs-vsctl list Open_vSwitch | grep other_config other_config : {dpdk-init=\"true\", dpdk-socket-mem=\"4096\", pmd-cpu-mask=\"FF00000000\"} 注: 本系统为单socket, 所以分配的CPU和Mem并不存在跨numa node问题 5.4.2 支持SR-IOV网卡 5.4.2.1 SR-IOV链路状态感知测试 测试link up -- PASS 本文使用PCI地址为0005:01:00.0的Mellanox物理网卡enP5p1s0做测试 #初始状态如下: $ sudo ibdev2netdev -v 0004:01:00.0 mlx5_0 (MT4115 - MT1738K12272) CX413A - ConnectX-4 QSFP28 fw 12.22.1002 port 1 (ACTIVE) ==> enP4p1s0 (Up) 0005:01:00.0 mlx5_1 (MT4115 - MT1738K12253) CX413A - ConnectX-4 QSFP28 fw 12.22.1002 port 1 (ACTIVE) ==> enP5p1s0 (Up) #新增2个sriov的VF, link状态为UP $ sudo bash -c \"echo 2 > /sys/class/net/enP5p1s0/device/sriov_numvfs\" $ sudo ibdev2netdev -v 0004:01:00.0 mlx5_0 (MT4115 - MT1738K12272) CX413A - ConnectX-4 QSFP28 fw 12.22.1002 port 1 (ACTIVE) ==> enP4p1s0 (Up) 0005:01:00.0 mlx5_1 (MT4115 - MT1738K12253) CX413A - ConnectX-4 QSFP28 fw 12.22.1002 port 1 (ACTIVE) ==> enP5p1s0 (Up) 0005:01:00.1 mlx5_2 (MT4116 - NA) fw 12.22.1002 port 1 (ACTIVE) ==> enP5p1s0f1 (Up) 0005:01:00.2 mlx5_3 (MT4116 - NA) fw 12.22.1002 port 1 (ACTIVE) ==> enP5p1s0f2 (Up) 使用virt-manager在vm01上添加PCI Host Device, 选择0005:01:00:1 在vm01上查看link状态为UP: [root@localhost ~]# ip link show dev enp5s0 5: enp5s0: mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 26:35:26:59:32:b9 brd ff:ff:ff:ff:ff:ff 在host上, vf的link为auto $ ip link show dev enP5p1s0 10: enP5p1s0: mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether ec:0d:9a:89:4a:e6 brd ff:ff:ff:ff:ff:ff vf 0 MAC 00:00:00:00:00:00, spoof checking off, link-state auto, trust off, query_rss off vf 1 MAC 00:00:00:00:00:00, spoof checking off, link-state auto, trust off, query_rss off 测试link down -- FAIL 在host上link down PF: #down掉PF $ sudo ip link set dev enP5p1s0 down #vf的link为auto $ ip link show dev enP5p1s0 10: enP5p1s0: mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether ec:0d:9a:89:4a:e6 brd ff:ff:ff:ff:ff:ff vf 0 MAC 00:00:00:00:00:00, spoof checking off, link-state auto, trust off, query_rss off vf 1 MAC 00:00:00:00:00:00, spoof checking off, link-state auto, trust off, query_rss off 在vm01上, link还是UP [root@localhost ~]# ip link show dev enp5s0 5: enp5s0: mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 26:35:26:59:32:b9 brd ff:ff:ff:ff:ff:ff Fail分析 结论: vm能够感知host上对vf的disable和enable操作, 但link-state为auto时, 不能感知PF的link状态. VF的link-state有几种配置: 参考: https://docs.oracle.com/cd/E93554_01/E69348/html/uek4-net.html Support for SR-IOV VF link state control by using the ip command. Previously, VF links were always on, regardless of the physical link status, which allows VMs on the same virtual Ethernet bridge to communicate even if the physical function (PF) link state is down. However, if the VFs were bonded in active/standby mode, this configuration prevented failover when the physical link used by a VF went down. You can now use the ip link set command to configure the behavior of a VF link: # ip link set device vf number state { auto | enable | disable } The possible settings are: auto The VF link state is determined by the PF link state. This setting is suitable for VFs that are bonded in active/standby mode. disable The VF link state is permanently down. enable The VF link state is permanently up. This is the default setting. 如果在host上,配置vf的state为disable #配置vf0的link-state为disable $ sudo ip link set dev enP5p1s0 vf 0 state disable $ ip link show dev enP5p1s0 10: enP5p1s0: mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether ec:0d:9a:89:4a:e6 brd ff:ff:ff:ff:ff:ff vf 0 MAC 00:00:00:00:00:00, spoof checking off, link-state disable, trust off, query_rss off vf 1 MAC 00:00:00:00:00:00, spoof checking off, link-state auto, trust off, query_rss off 此时vm01上, 有linkw down的提示, 并且显示link为DOWN [root@localhost ~]# [ 2863.441859] mlx5_core 0000:05:00.0 enp5s0: Link down [root@localhost ~]# ip link show dev enp5s0 5: enp5s0: mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether 26:35:26:59:32:b9 brd ff:ff:ff:ff:ff:ff 在host上把vf的state重新改为enable, vm01的能够感知并up. 5.4.2.2 Hypervisor允许只使用一对物理网口 测试结果 -- PASS 在Host B上, vm01配成vhost-user, vm02配成sriov模式, 然后 #将enP5p1s0添加到ovsbr0 sudo ovs-vsctl add-port ovsbr0 enP5p1s0 $ sudo ovs-vsctl show f81a1c52-91c1-40a2-b993-6dfeea09317e Bridge \"ovsbr0\" Port \"enP5p1s0\" Interface \"enP5p1s0\" Port \"dpdkvhostuser0\" Interface \"dpdkvhostuser0\" type: dpdkvhostuser options: {n_rxq=\"2\"} Port \"ovsbr0\" Interface \"ovsbr0\" type: internal Port \"dpdkvhostuser1\" Interface \"dpdkvhostuser1\" type: dpdkvhostuser options: {n_rxq=\"2\"} ovs_version: \"2.8.4\" 物理网卡以普通方式添加到OVS示意图: 连通性: ping to | 5.5.5.11(peer) | 5.5.5.22(host) | 5.5.5.33(VM02) | 5.5.5.44(VM01) ----| ----| ----| ----|----| 5.5.5.11(peer) | self | Y | Y | Y 5.5.5.22(host) | Y | self | Y | N 5.5.5.33(VM02) | Y | Y | self | N 5.5.5.44(VM01) | Y | N | N | self 即从外部(5.5.5.11)ping vm01(5.5.5.44)和vm02(5.5.5.33)能ping通. issue讨论 22和44互相ping不通, 为什么? 答: 这是正常的. 在userspace datapath(datapath_type=netdev)模式下, ovs通过socket(AF_PACKET, SOCK_RAW, 0)发包, 不走ip协议栈. ToDo: 验证一下内核datapath模式下, 本机物理网口和vm之间的连通性. 如何理解物理网卡被add到OVS上(非DPDK方式)? 如果是DPDK方式呢? DPDK方式#删掉接口enP5p1s0 sudo ovs-vsctl del-port ovsbr0 enP5p1s0 #以dpdk方式添加到ovsvr0 sudo ovs-vsctl add-port ovsbr0 dpdkp1 -- set Interface dpdkp1 type=dpdk options:dpdk-devargs=0005:01:00.0 sudo ovs-vsctl set Interface dpdkp1 options:n_rxq=2 $ sudo ovs-vsctl show f81a1c52-91c1-40a2-b993-6dfeea09317e Bridge \"ovsbr0\" Port \"dpdkvhostuser0\" Interface \"dpdkvhostuser0\" type: dpdkvhostuser options: {n_rxq=\"2\"} Port \"ovsbr0\" Interface \"ovsbr0\" type: internal Port \"dpdkp1\" Interface \"dpdkp1\" type: dpdk options: {dpdk-devargs=\"0005:01:00.0\", n_rxq=\"2\"} Port \"dpdkvhostuser1\" Interface \"dpdkvhostuser1\" type: dpdkvhostuser options: {n_rxq=\"2\"} ovs_version: \"2.8.4\" 物理网卡以DPDK方式添加到OVS 连通性: 和之前相比, 到5.5.5.22的报文都不通. 说明OVS-DPDK的PMD完全把enP5p1s0接管了. ping to | 5.5.5.11 | 5.5.5.22 | 5.5.5.33 | 5.5.5.44 ----| ----| ----| ----|----| 5.5.5.11 | self | N | Y | Y 5.5.5.22 | N | self | N | N 5.5.5.33 | Y | N | self | N 5.5.5.44 | Y | N | N | self 5.4.2.3 SR-IOV与虚拟交换机互通 根据上个测试用例, vm02(sr-iov port)和vm01(ovs port)是不通的.vm01发出的ping报文, 直接走Host B网卡的PF wire到Host A; vm02发出的ping报文也直接通过网卡的VF到Host A. 如果将vm02移至Host A上运行, 则理论上能够ping通(待测试). 测试结果 -- 待澄清测试场景后测试 5.4.3 SR-IOV网卡的桥接功能 配置两个vm都使用sriov, 并分别配置同网段ip. 测试结果 -- PASS 物理网卡(Mellanox ConnectX-4)默认桥接VF. 两个VM能够互相ping通. 在对端(Host A)上抓包, 没有相关的 ICMP报文. 5.4.4 支持vNIC多队列 在小节中, OVS和VM的接口dpdkvhostuser已经配置了multi queue(2个); VM也配置了2个queue 在Host B上有: $ sudo ovs-appctl dpif/show netdev@ovs-netdev: hit:458 missed:71 ovsbr0: dpdkvhostuser0 2/3: (dpdkvhostuser: configured_rx_queues=2, configured_tx_queues=2, mtu=1500, requested_rx_queues=2, requested_tx_queues=2) dpdkvhostuser1 1/2: (dpdkvhostuser: configured_rx_queues=2, configured_tx_queues=2, mtu=1500, requested_rx_queues=2, requested_tx_queues=2) ovsbr0 65534/1: (tap) 在VM里面有: [root@localhost ~]# ethtool -l eth1 Channel parameters for eth1: Pre-set maximums: RX: 0 TX: 0 Other: 0 Combined: 2 Current hardware settings: RX: 0 TX: 0 Other: 0 Combined: 2 在VM里面, 分别配置2对IP: vm01(5.5.5.51, 6.6.6.61), vm02(5.5.5.52, 6.6.6.62) 测试结果 -- PASS 2对IP能够分别ping通. cat /proc/interrupts看到virtio中断在4个CPU的2个上有中断; tc看到2个发送队列都有发包统计. [root@localhost ~]# tc -s class show dev eth1 class mq :1 root Sent 7706 bytes 79 pkt (dropped 0, overlimits 0 requeues 0) backlog 0b 0p requeues 0 class mq :2 root Sent 20222 bytes 183 pkt (dropped 0, overlimits 0 requeues 0) backlog 0b 0p requeues 0 5.4.5 网络隔离与地址复用 5.4.5.1 OVS 网络隔离与地址复用 前面已经验证了连通性, 下面只验证隔离性. 新增ovsbr1, 并在ovsbr1上增加dpdkvhostuser类型的port:dpdkvhostuser2, 并把dpdkvhostuser2挂到VM02上 sudo ovs-vsctl add-br ovsbr1 -- set bridge ovsbr1 datapath_type=netdev sudo ovs-vsctl add-port ovsbr1 dpdkvhostuser2 -- set Interface dpdkvhostuser2 type=dpdkvhostuser $ sudo ovs-appctl dpif/show netdev@ovs-netdev: hit:481 missed:89 ovsbr0: dpdkvhostuser0 2/3: (dpdkvhostuser: configured_rx_queues=2, configured_tx_queues=2, mtu=1500, requested_rx_queues=2, requested_tx_queues=2) dpdkvhostuser1 1/2: (dpdkvhostuser: configured_rx_queues=2, configured_tx_queues=2, mtu=1500, requested_rx_queues=2, requested_tx_queues=2) ovsbr0 65534/1: (tap) ovsbr1: dpdkvhostuser2 1/5: (dpdkvhostuser: configured_rx_queues=1, configured_tx_queues=1, mtu=1500, requested_rx_queues=1, requested_tx_queues=1) ovsbr1 65534/4: (tap) 在VM02上配置IP, 和VM01同网段, 但并不是一个ovs bridge. 测试结果 -- PASS VM02 ping不通VM01 5.4.5.2 SR-IOV 网络隔离与地址复用 前面已经验证过, 两个VM分别连同一个PF的两个VF, 默认能够ping通. 下面修改VF的vlan sudo ip link set dev enP5p1s0 vf 0 vlan 100 sudo ip link set dev enP5p1s0 vf 1 vlan 101 测试结果 -- PASS 修改后, 两个VF分别属于vlan100和vlan101, 互相ping不通. $ ip link show dev enP5p1s0 10: enP5p1s0: mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether ec:0d:9a:89:4a:e6 brd ff:ff:ff:ff:ff:ff vf 0 MAC 00:00:00:00:00:00, vlan 100, spoof checking off, link-state enable, trust off, query_rss off vf 1 MAC 00:00:00:00:00:00, vlan 101, spoof checking off, link-state auto, trust off, query_rss off 5.4.6 MTU可调整 5.4.6.1 OVS MTU可调整（vxlan网络只在SDN场景下测试） 在小节配置的基础上, 在OVS上添加物理网卡: #添加业务物理口, type=DPDK $ sudo ovs-vsctl add-port ovsbr0 dpdkp1 -- set Interface dpdkp1 type=dpdk options:dpdk-devargs=0005:01:00.0 #设置该物理口的q为2 $ sudo ovs-vsctl set Interface dpdkp1 options:n_rxq=2 #此时OVS配置如下: $ sudo ovs-vsctl show f81a1c52-91c1-40a2-b993-6dfeea09317e Bridge \"ovsbr0\" Port \"dpdkp1\" Interface \"dpdkp1\" type: dpdk options: {dpdk-devargs=\"0005:01:00.0\", n_rxq=\"2\"} Port \"dpdkvhostuser0\" Interface \"dpdkvhostuser0\" type: dpdkvhostuser options: {n_rxq=\"2\"} Port \"ovsbr0\" Interface \"ovsbr0\" type: internal Port \"dpdkvhostuser1\" Interface \"dpdkvhostuser1\" type: dpdkvhostuser options: {n_rxq=\"2\"} ovs_version: \"2.8.4\" #默认的mtu为1500 $ sudo ovs-appctl dpctl/show netdev@ovs-netdev: lookups: hit:8 missed:17 lost:4 flows: 0 port 0: ovs-netdev (tap) port 1: ovsbr0 (tap) port 2: dpdkvhostuser1 (dpdkvhostuser: configured_rx_queues=1, configured_tx_queues=1, mtu=1500, requested_rx_queues=1, requested_tx_queues=1) port 3: dpdkvhostuser0 (dpdkvhostuser: configured_rx_queues=1, configured_tx_queues=1, mtu=1500, requested_rx_queues=1, requested_tx_queues=1) port 4: dpdkp1 (dpdk: configured_rx_queues=2, configured_rxq_descriptors=2048, configured_tx_queues=9, configured_txq_descriptors=2048, lsc_interrupt_mode=false, mtu=1500, requested_rx_queues=2, requested_rxq_descriptors=2048, requested_tx_queues=9, requested_txq_descriptors=2048, rx_csum_offload=true) 根据用例, 这里验证修改OVS的mtu是否有效: 修改bridge的MTU到4000 sudo ovs-vsctl set Interface ovsbr0 mtu_request=4000 sudo ovs-vsctl set Interface dpdkp1 mtu_request=4000 sudo ovs-vsctl set Interface dpdkvhostuser0 mtu_request=4000 sudo ovs-vsctl set Interface dpdkvhostuser1 mtu_request=4000 分别修改VM01和VM02的MTU到4000 ip link set dev eth1 mtu 4000 测试结果 -- PASS VM01和VM02互相ping, 小于4000的报文能ping通, 大于4000的ping不通. 说明修改OVS的mtu有效. 疑问: dpdkp1的txq个数 上面sudo ovs-appctl dpctl/show显示: configured_rx_queues=2, configured_tx_queues=9, 为什么txq的个数是9? 参考: http://docs.openvswitch.org/en/latest/topics/dpdk/pmd/ 5.4.6.2 SR-IOV MTU可调整（vxlan网络只在SDN场景下测试） 根据要求, 先设置物理网卡的mtu sudo ip link set dev enP5p1s0 mtu 4000 测试结果 -- PASS VM的mtu默认值还是1500, ping报文大小为2048时ping不通. 修改vm的mtu为4000, ping -s 2048 -M do 5.5.5.22可以ping通. 修改host和vm的mtu为9000, 小于(9000-28)字节的报文可以ping通. 5.4.7 支持VLAN透传 5.4.7.1 OVS 支持VLAN透传 测试示意图:在Host B上: #将两个VM对应的port加入vlan 100(外层 vlan), 并配置cvlan(内层 vlan)为0和10 sudo ovs-vsctl set port dpdkvhostuser0 vlan_mode=dot1q-tunnel tag=100 cvlans=0,10 other-config:qinq-ethtype=802.1q sudo ovs-vsctl set port dpdkvhostuser1 vlan_mode=dot1q-tunnel tag=100 cvlans=0,10 other-config:qinq-ethtype=802.1q #将物理网卡加入vlan 100, native-tagged是说出端口带tag, 详见ofproto/ofproto.h sudo ovs-vsctl set port dpdkp1 vlan_mode=native-tagged tag=100 other-config:qinq-ethtype=802.1q 在VM01上 #配置vm01的接口IP为192.168.1.100 ip addr add 192.168.1.100/24 dev eth1 #增加vlan 10 ip link add link eth1 name eth1.10 type vlan id 10 #给vlan 10子接口配置IP ip addr add 192.168.10.100/24 dev eth1.10 ip link set up dev eth1.10 在VM02上 #配置vm01的接口IP为192.168.1.200 ip addr add 192.168.1.200/24 dev eth1 #增加vlan 10 ip link add link eth1 name eth1.10 type vlan id 10 #给vlan 10子接口配置IP ip addr add 192.168.10.200/24 dev eth1.10 ip link set up dev eth1.10 测试结果 -- PASS VM01在192.168.1.0和192.168.10.0网段分别ping VM02, 可以ping通. 在对端(Host A)上抓包, ping 192.168.1.101时有单层vlan tag, id为10018:02:10.058754 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.1.101 tell 192.168.1.200, length 42 0x0000: ffff ffff ffff 5254 00f0 25e4 8100 0064 ......RT..%....d 0x0010: 0806 0001 0800 0604 0001 5254 00f0 25e4 ..........RT..%. 0x0020: c0a8 01c8 0000 0000 0000 c0a8 0165 0000 .............e.. 0x0030: 0000 0000 0000 0000 0000 0000 在对端(Host A)上抓包, ping 192.168.10.101时有双层vlan tag, 外层为100, 内层为1018:04:39.898836 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.10.101 tell 192.168.10.200, length 38 0x0000: ffff ffff ffff 5254 00f0 25e4 8100 0064 ......RT..%....d 0x0010: 8100 000a 0806 0001 0800 0604 0001 5254 ..............RT 0x0020: 00f0 25e4 c0a8 0ac8 0000 0000 0000 c0a8 ..%............. 0x0030: 0a65 0000 0000 0000 0000 0000 .e.......... 5.4.7.2 SR-IOV 支持VLAN透传 #给每个VF设置vlan tag sudo ip link set dev enP5p1s0 vf 0 vlan 100 sudo ip link set dev enP5p1s0 vf 1 vlan 100 在VM01上 #配置vm01的接口IP为5.5.5.100 ip addr add 5.5.5.100/24 dev enp4s0 #增加vlan 10 ip link add link enp4s0 name enp4s0.10 type vlan id 10 #给vlan 10子接口配置IP ip addr add 5.5.10.100/24 dev enp4s0.10 ip link set up dev enp4s0.10 在VM02上 #配置vm01的接口IP为5.5.5.200 ip addr add 5.5.5.200/24 dev enp4s0 #增加vlan 10 ip link add link enp4s0 name enp4s0.10 type vlan id 10 #给vlan 10子接口配置IP ip addr add 5.5.10.200/24 dev enp4s0.10 ip link set up dev enp4s0.10 测试结果 -- FAIL VM01和VM02用enp4s0.10互相ping, 不通 注: 本文使用Mellanox ConnectX-4, 在OFED 4.3-3.0.2版本中, 其用户手册明确说明ConnectX-4不支持QinQ. 在OFED 4.4版本中, 新增了ConnectX-4的QinQ支持. ToDo: 切换至OFED 4.4版本重新测试 5.4.8 带宽分配比例设定 5.4.8.1 OVS 支持带宽分配比例设定 主要考察OVS能否对物理网卡进行带宽限制, 这里做一下简化如下图:Host A和Host B上的VM01做iperf测试带宽, 不设置带宽比例时: 性能在10.3 Gbits/sec, 主要瓶颈点在Host A上的kernel协议栈, 以及VM01上的kernel协议栈. [ 4] 15.00-16.00 sec 718 MBytes 6.02 Gbits/sec 7267 1024 KBytes [ 6] 15.00-16.00 sec 366 MBytes 3.07 Gbits/sec 6713 76.4 KBytes [ 8] 15.00-16.00 sec 30.0 MBytes 252 Mbits/sec 1281 90.5 KBytes [ 10] 15.00-16.00 sec 110 MBytes 923 Mbits/sec 2711 139 KBytes [SUM] 15.00-16.00 sec 1.20 GBytes 10.3 Gbits/sec 17972 对Host B上的OVS-DPDK做带宽限制, 主要限制对接物理网卡enP5p1s0对应的port dpdkp1 Egress 在Host B上:#限制dpdkp1的出口速率为5G bps, cir的单位为Bps sudo ovs-vsctl set port dpdkp1 qos=@newqos -- --id=@newqos create qos type=egress-policer other-config:cir=625000000 other-config:cbs=2048 Ingress#限制入口速率5G bps, ingress_policing_rate单位为k bps $ sudo ovs-vsctl set interface dpdkp1 ingress_policing_rate=5000000 $ sudo ovs-vsctl set interface dpdkp1 ingress_policing_burst=500000 测试结果 -- PASS 配置带宽限制后, 再次用iperf测试, 结果为4.85 Gbits/sec [ 4] 5.00-6.00 sec 259 MBytes 2.17 Gbits/sec 8313 264 KBytes [ 6] 5.00-6.00 sec 53.8 MBytes 451 Mbits/sec 2579 11.3 KBytes [ 8] 5.00-6.00 sec 154 MBytes 1.29 Gbits/sec 2293 91.9 KBytes [ 10] 5.00-6.00 sec 112 MBytes 944 Mbits/sec 1467 591 KBytes [SUM] 5.00-6.00 sec 579 MBytes 4.85 Gbits/sec 14652 问题: netperf over OVS-DPDK性能差 netperf是单进程应用, traffic通过OVS-DPDK后, 相比两个物理机直连, 性能下降很多; 下面是各种场景下的性能数据: 测试类型 | Throughput(min) GBytes /s | Throughput(max) GBytes /s -- | -- | -- | 两个物理网卡直连 | 1.27 | 3.23 物理网卡到VM | 0.37 | 0.71 VM02到VM01过OVS-DPDK | 0.26 | 0.33 VM02到VM01过OVS-DPDK with flow | 0.25 | 0.26 VM02到VM01过linux bridge | 2.17 | 2.84 初步结论: OVS-DPDK是瓶颈, rxq个数不够? descriptor个数不够? critical path代码问题? --配flow, 配mac学习 --貌似不行 我以前也对比测过，大概是这原因，如果把bridge的tso/csum关闭，测出的数据和ovs-dpdk是差不多的。 for bridge mode, the VM's virtio nic was enabled tso/csum defaultly for OVS-dpdk, the VM's virtio nic cannot enable tso/csum, although DPDK vhost lib can support tso etc. Already has a Patch for OVS2.6.1 to support TSO, but this patch is not intended for upstreaming.see :https://mail.openvswitch.org/pipermail/ovs-dev/2016-June/316414.html https://lists.linux-foundation.org/pipermail/ovs-dev/2017-April/330703.html OVS模式下VM的offload功能 [root@localhost ~]# ethtool -k eth1 Features for eth1: rx-checksumming: on [fixed] tx-checksumming: off tx-checksum-ipv4: off [fixed] tx-checksum-ip-generic: off [fixed] tx-checksum-ipv6: off [fixed] tx-checksum-fcoe-crc: off [fixed] tx-checksum-sctp: off [fixed] scatter-gather: off tx-scatter-gather: off [fixed] tx-scatter-gather-fraglist: off [fixed] tcp-segmentation-offload: off tx-tcp-segmentation: off [fixed] tx-tcp-ecn-segmentation: off [fixed] tx-tcp-mangleid-segmentation: off [fixed] tx-tcp6-segmentation: off [fixed] udp-fragmentation-offload: off generic-segmentation-offload: off [requested on] generic-receive-offload: on large-receive-offload: off [fixed] rx-vlan-offload: off [fixed] tx-vlan-offload: off [fixed] ntuple-filters: off [fixed] receive-hashing: off [fixed] highdma: on [fixed] rx-vlan-filter: on [fixed] vlan-challenged: off [fixed] tx-lockless: off [fixed] netns-local: off [fixed] tx-gso-robust: off [fixed] tx-fcoe-segmentation: off [fixed] tx-gre-segmentation: off [fixed] tx-gre-csum-segmentation: off [fixed] tx-ipxip4-segmentation: off [fixed] tx-ipxip6-segmentation: off [fixed] tx-udp_tnl-segmentation: off [fixed] tx-udp_tnl-csum-segmentation: off [fixed] tx-gso-partial: off [fixed] tx-sctp-segmentation: off [fixed] tx-esp-segmentation: off [fixed] fcoe-mtu: off [fixed] tx-nocache-copy: off loopback: off [fixed] rx-fcs: off [fixed] rx-all: off [fixed] tx-vlan-stag-hw-insert: off [fixed] rx-vlan-stag-hw-parse: off [fixed] rx-vlan-stag-filter: off [fixed] l2-fwd-offload: off [fixed] hw-tc-offload: off [fixed] esp-hw-offload: off [fixed] esp-tx-csum-hw-offload: off [fixed] rx-udp_tunnel-port-offload: off [fixed] Linux bridge模式下的offload功能: [root@localhost ~]# ethtool -k eth0 Features for eth0: rx-checksumming: on [fixed] tx-checksumming: on tx-checksum-ipv4: off [fixed] tx-checksum-ip-generic: on tx-checksum-ipv6: off [fixed] tx-checksum-fcoe-crc: off [fixed] tx-checksum-sctp: off [fixed] scatter-gather: on tx-scatter-gather: on tx-scatter-gather-fraglist: off [fixed] tcp-segmentation-offload: on tx-tcp-segmentation: on tx-tcp-ecn-segmentation: on tx-tcp-mangleid-segmentation: off tx-tcp6-segmentation: on udp-fragmentation-offload: off generic-segmentation-offload: on generic-receive-offload: on large-receive-offload: off [fixed] rx-vlan-offload: off [fixed] tx-vlan-offload: off [fixed] ntuple-filters: off [fixed] receive-hashing: off [fixed] highdma: on [fixed] rx-vlan-filter: on [fixed] vlan-challenged: off [fixed] tx-lockless: off [fixed] netns-local: off [fixed] tx-gso-robust: on [fixed] tx-fcoe-segmentation: off [fixed] tx-gre-segmentation: off [fixed] tx-gre-csum-segmentation: off [fixed] tx-ipxip4-segmentation: off [fixed] tx-ipxip6-segmentation: off [fixed] tx-udp_tnl-segmentation: off [fixed] tx-udp_tnl-csum-segmentation: off [fixed] tx-gso-partial: off [fixed] tx-sctp-segmentation: off [fixed] tx-esp-segmentation: off [fixed] fcoe-mtu: off [fixed] tx-nocache-copy: off loopback: off [fixed] rx-fcs: off [fixed] rx-all: off [fixed] tx-vlan-stag-hw-insert: off [fixed] rx-vlan-stag-hw-parse: off [fixed] rx-vlan-stag-filter: off [fixed] l2-fwd-offload: off [fixed] hw-tc-offload: off [fixed] esp-hw-offload: off [fixed] esp-tx-csum-hw-offload: off [fixed] rx-udp_tunnel-port-offload: off [fixed] 测试类型: #两个物理网卡直连 HostA(enP4p1s0) HostB(enP4p1s0) #物理网卡到VM HostA HostB VM01 enP5p1s0 enP5p1s0 (dpdk@2rxq)OVS-DPDK(vhost-user@2rxq) eth1(virtio) #VM02到VM01过OVS-DPDK VM02(eth1-virtio) (vhost-user@2rxq)OVS-DPDK(vhost-user@2rxq) VM01(eth1-virtio) #VM02到VM01过OVS-DPDK with flow VM02(eth1-virtio) (vhost-user@2rxq)OVS-DPDK(vhost-user@2rxq) VM01(eth1-virtio) #VM02到VM01过linux bridge VM02(eth1-virtio) (tap)linux bridge(tap) VM01(eth1-virtio) 注: netperf带宽波动很大, 这里取5次测试结果的最大最小值 参考命令: 在server上: cd netperf-master/src ./netserver 在client上: cd netperf-master/src ./netperf -H 5.5.5.51 -l 30 -cC -fG 在HostB上配flow #从vhostuser0到vhostuser1 sudo ovs-vsctl set interface dpdkp1 ofport_request=1 sudo ovs-vsctl set interface \"dpdkvhostuser0\" ofport_request=2 sudo ovs-vsctl set interface \"dpdkvhostuser1\" ofport_request=3 sudo ovs-ofctl add-flow ovsbr0 in_port=2,action=output:3 sudo ovs-ofctl add-flow ovsbr0 in_port=3,action=output:2 #查现有flow $ sudo ovs-ofctl dump-flows ovsbr0 cookie=0x0, duration=525.681s, table=0, n_packets=0, n_bytes=0, in_port=dpdkvhostuser0 actions=output:dpdkvhostuser1 cookie=0x0, duration=517.418s, table=0, n_packets=0, n_bytes=0, in_port=dpdkvhostuser1 actions=output:dpdkvhostuser0 cookie=0x0, duration=64912.051s, table=0, n_packets=262755553, n_bytes=364138797710, priority=0 actions=NORMAL #清除所有flow sudo ovs-ofctl del-flows ovsbr0 #清除flow后加回默认flow, 否则端口都不通 sudo ovs-ofctl add-flow ovsbr0 actions=NORMAL 5.4.8.2 SR-IOV 支持带宽分配比例设定 将用例描述的测试场景分解: 用限制SRIOV网卡的VF max_tx_rate来测试 但只能配置tx_rate, 对rx并没有限制 参考: https://community.mellanox.com/docs/DOC-2565 #在Host B上的两个Mellanox Connect-4物理网卡上, 打开SR-IOV, VF数目分别是2 sudo bash -c \"echo 2 > /sys/class/net/enP4p1s0/device/sriov_numvfs\" sudo bash -c \"echo 2 > /sys/class/net/enP5p1s0/device/sriov_numvfs\" #物理网卡实际link速率为56000Mb/s #使vf 0为30G, vf 1为40G #对应用例要求的port1, 对应VM01 sudo ip link set dev enP4p1s0 vf 0 max_tx_rate 30000 min_tx_rate 25000 #对应用例要求的port3, 对应VM02 sudo ip link set dev enP4p1s0 vf 1 max_tx_rate 40000 min_tx_rate 35000 #对应用例要求的port2, 对应VM01 sudo ip link set dev enP5p1s0 vf 0 max_tx_rate 30000 min_tx_rate 25000 #对应用例要求的port4, 对应VM02 sudo ip link set dev enP5p1s0 vf 1 max_tx_rate 40000 min_tx_rate 35000 #配置好以后的状态: $ ip link show dev enP4p1s0; ip link show dev enP5p1s0 9: enP4p1s0: mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether ec:0d:9a:89:4c:36 brd ff:ff:ff:ff:ff:ff vf 0 MAC 00:00:00:00:00:00, tx rate 30000 (Mbps), max_tx_rate 30000Mbps, min_tx_rate 25000Mbps, spoof checking off, link-state auto, trust off, query_rss off vf 1 MAC 00:00:00:00:00:00, tx rate 40000 (Mbps), max_tx_rate 40000Mbps, min_tx_rate 35000Mbps, spoof checking off, link-state auto, trust off, query_rss off 10: enP5p1s0: mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether ec:0d:9a:89:4a:e6 brd ff:ff:ff:ff:ff:ff vf 0 MAC 00:00:00:00:00:00, tx rate 30000 (Mbps), max_tx_rate 30000Mbps, min_tx_rate 25000Mbps, spoof checking off, link-state auto, trust off, query_rss off vf 1 MAC 00:00:00:00:00:00, tx rate 40000 (Mbps), max_tx_rate 40000Mbps, min_tx_rate 35000Mbps, spoof checking off, link-state auto, trust off, query_rss off VM interface VF@设定tx_rate PF@实际rate VM01 enp4s0 0@tx30G enP5p1s0@56G VM01 enp5s0 0@tx30G enP4p1s0@56G VM02 enp4s0 1@tx40G enP5p1s0@56G VM02 enp5s0 1@tx40G enP4p1s0@56G 配置VM如下: 分别添加Host PCI设备到VM 测试结果 -- PASS(to be clarified) 两个VM成功启动 最大带宽测试结果 在mtu为默认1500时, 文中4核VM并不能达到40G线速; 故这里在两个VM上分别修改接口的mtu为9000 在两个VM上:#MTU增大到9000 ip link set dev enp4s0 mtu 9000 #server端 iperf -s #client端, 用4个线程打满带宽 iperf -c 2.2.2.1 -i 1 -t 360 -l 128K -P 4 Server | Client | 理论带宽(VF tx_rate) | 实际带宽 | 所属PF --- | --- | --- | --- | --- | VM01 | VM02 | 40 G | 39.4 G | enp5s0 VM02 | VM01 | 30 G | 29.7 G | enp5s0 如何实现调整带宽比例到150%或50%? 5.4.9 支持组播 5.4.9.1 OVS支持正确转发组播包 参考下图配置:在两个VM上 # 查看多播地址 [root@localhost ~]# ip maddr show dev eth1 4: eth1 link 01:00:5e:00:00:01 link 33:33:00:00:00:01 link 33:33:ff:c0:69:37 inet 224.0.0.1 inet6 ff02::1:ffc0:6937 inet6 ff02::1 inet6 ff01::1 # VF有两个方法能够收到未知多播, promiscuous和allmulticast, 这里使用后者 ip link set dev eth1 allmulticast on 在Host A上, 运行pktgen, 构造ip和mac #启动pktgen, rx用core42,tx用core43, core41为显示和timer sudo app/arm64-armv8a-linuxapp-gcc/pktgen -l 41-45 -w 0005:01:00.0 -- -T -P -m \"[42:43].0\" #配目的IP(组播IP) Pktgen:/> set 0 dst ip 224.0.0.1 #改变目的组播IP Pktgen:/> set 0 dst ip 225.0.0.1 #配目的mac(组播MAC) Pktgen:/> set 0 dst mac 01:00:5e:00:00:01 #配目的组播mac Pktgen:/> set 0 dst mac 01:00:5e:00:00:02 #设置发包速率为1%, 太大会丢包 Pktgen:/> set 0 rate 1 #开始发送 Pktgen:/> start all #停止发送 Pktgen:/> stop all 发送中在VM01和VM02上抓包 [root@localhost ~]# tcpdump -i eth1 -vvnnXX 12:45:39.316576 IP (tos 0x0, ttl 4, id 52062, offset 0, flags [none], proto TCP (6), length 46) 192.168.0.1.1234 > 224.0.0.1.5678: Flags [.], cksum 0xe18e (correct), seq 0:6, ack 1, win 8192, length 6 0x0000: 0100 5e00 0001 248a 07b5 7070 0800 4500 ..^...$...pp..E. 0x0010: 002e cb5e 0000 0406 4ac1 c0a8 0001 e000 ...^....J....... 0x0020: 0001 04d2 162e 1234 5678 1234 5690 5010 .......4Vx.4V.P. 0x0030: 2000 e18e 0000 7778 797a 3031 ......wxyz01 12:45:39.331109 IP (tos 0x0, ttl 4, id 49983, offset 0, flags [none], proto TCP (6), length 46) 192.168.0.1.1234 > 224.0.0.1.5678: Flags [.], cksum 0xe18e (correct), seq 0:6, ack 1, win 8192, length 6 0x0000: 0100 5e00 0001 248a 07b5 7070 0800 4500 ..^...$...pp..E. 0x0010: 002e c33f 0000 0406 52e0 c0a8 0001 e000 ...?....R....... 0x0020: 0001 04d2 162e 1234 5678 1234 5690 5010 .......4Vx.4V.P. 0x0030: 2000 e18e 0000 7778 797a 3031 ......wxyz01 测试结果 -- PASS 多播dst IP / MAC VM01 是否收到 VM02 是否收到 224.0.0.1 / 01:00:5e:00:00:01 是 是 225.0.0.2 / 01:00:5e:00:00:02 是 是 5.4.9.2 SR-IOV支持正确转发组播包 配置连个SR-IOV模式的VM01和VM02. 两个VM都默认有多播IP224.0.0.1和MAC01:00:5e:00:00:01 [root@localhost ~]# ip maddr show dev enp4s0 4: enp4s0 link 01:00:5e:00:00:01 link 33:33:00:00:00:01 inet 224.0.0.1 inet6 ff02::1 inet6 ff01::1 参考: https://community.mellanox.com/docs/DOC-2473 为了能使VM收到未知组播, 在Host B上: #使能vf的trust sudo ip link set enP5p1s0 vf 0 trust on sudo ip link set enP5p1s0 vf 1 trust on 在两个VM上: #VF有两个方法能够收到未知多播, promiscuous和allmulticast, 这里使用后者 ip link set dev enp4s0 allmulticast on 在Host A上, 运行pktgen, 构造ip和mac #启动pktgen, rx用core42,tx用core43, core41为显示和timer sudo app/arm64-armv8a-linuxapp-gcc/pktgen -l 41-45 -w 0005:01:00.0 -- -T -P -m \"[42:43].0\" #配目的IP(组播IP) Pktgen:/> set 0 dst ip 224.0.0.1 #配目的mac(组播MAC) Pktgen:/> set 0 dst mac 01:00:5e:00:00:01 #设置发包速率为1%, 太大会丢包 Pktgen:/> set 0 rate 1 #开始发送 Pktgen:/> start all #停止发送 Pktgen:/> stop all 发送结束后在Host B和VM01 VM02上分别统计多播报文个数: $ sudo ethtool -S enP5p1s0 | grep multicast_packets rx_vport_multicast_packets: 8981036150 tx_vport_multicast_packets: 112483059 rx_vport_rdma_multicast_packets: 0 tx_vport_rdma_multicast_packets: 0 测试结果 -- PASS 多播dst IP / MAC Host B 是否收到 VM01 是否收到 VM02 是否收到 224.0.0.1 / 01:00:5e:00:00:01 是 是 是 225.0.0.2 / 01:00:5e:00:00:02 是 是 是 7.3 虚拟层性能 7.3.5 网络转发性能 7.3.5.1 基于10GE网卡的网络转发性能 7.3.6 网络转发质量 7.3.6.1 基于10GE网卡的网络转发质量 Troubleshooting OVS异常重启导致vhostuser不通 vhostuser是c-s模式, ovs是server, qemu是client. 当OVS由于某种异常自动重启时, 会因为无法打开vhost端口的socket导致vhost不通. 此时要先关闭VM, 再重启OVS. 待解决问题 netperf over OVS-DPDK性能差 见上面5.4.8.1 VM同时添加sriov和vhost-user导致OVS崩溃重启 现象是单独添加sriov或vhost-user正常, 同时添加则ovs-vswitchd会马上崩溃重启 error log 2018-09-07T05:20:01.091Z|00074|dpdk|INFO|VHOST_CONFIG: read message VHOST_USER_SET_VRING_KICK 2018-09-07T05:20:01.091Z|00075|dpdk|INFO|VHOST_CONFIG: vring kick idx:3 file:110 2018-09-07T05:20:01.091Z|00076|dpdk|INFO|VHOST_CONFIG: virtio is now ready for processing. 2018-09-07T05:20:01.091Z|00077|netdev_dpdk|INFO|vHost Device '/usr/local/var/run/openvswitch/dpdkvhostuser0' has been added on numa node 0 2018-09-07T05:20:01.091Z|00078|dpdk|INFO|VHOST_CONFIG: read message VHOST_USER_SET_VRING_CALL 2018-09-07T05:20:01.091Z|00079|dpdk|INFO|VHOST_CONFIG: vring call idx:3 file:118 2018-09-07T05:20:01.091Z|00080|dpdk|INFO|VHOST_CONFIG: read message VHOST_USER_SET_VRING_ENABLE 2018-09-07T05:20:01.091Z|00081|dpdk|INFO|VHOST_CONFIG: set queue enable: 1 to qp idx: 0 2018-09-07T05:20:01.091Z|00082|netdev_dpdk|INFO|State of queue 0 ( tx_qid 0 ) of vhost device '/usr/local/var/run/openvswitch/dpdkvhostuser0'changed to 'enabled' 2018-09-07T05:20:01.091Z|00083|dpdk|INFO|VHOST_CONFIG: read message VHOST_USER_SET_VRING_ENABLE 2018-09-07T05:20:01.091Z|00084|dpdk|INFO|VHOST_CONFIG: set queue enable: 1 to qp idx: 1 2018-09-07T05:20:01.091Z|00085|dpdk|INFO|VHOST_CONFIG: read message VHOST_USER_SET_VRING_ENABLE 2018-09-07T05:20:01.091Z|00086|dpdk|INFO|VHOST_CONFIG: set queue enable: 1 to qp idx: 2 2018-09-07T05:20:01.091Z|00087|netdev_dpdk|INFO|State of queue 2 ( tx_qid 1 ) of vhost device '/usr/local/var/run/openvswitch/dpdkvhostuser0'changed to 'enabled' 2018-09-07T05:20:01.091Z|00088|dpdk|INFO|VHOST_CONFIG: read message VHOST_USER_SET_VRING_ENABLE 2018-09-07T05:20:01.091Z|00089|dpdk|INFO|VHOST_CONFIG: set queue enable: 1 to qp idx: 3 2018-09-07T05:20:03.356Z|00090|dpdk|INFO|VHOST_CONFIG: read message VHOST_USER_SET_MEM_TABLE 2018-09-07T05:20:03.359Z|00091|dpdk|INFO|VHOST_CONFIG: guest memory region 0, size: 0x200000000 guest physical addr: 0x40000000 guest virtual addr: 0xfffd80000000 host virtual addr: 0x400000000000 mmap addr : 0x400000000000 mmap size : 0x200000000 mmap align: 0x20000000 mmap off : 0x0 2018-09-07T05:20:08.275Z|00002|daemon_unix(monitor)|ERR|1 crashes: pid 44572 died, killed (Segmentation fault), core dumped, restarting 2018-09-07T05:20:08.282Z|00003|ovs_numa|INFO|Discovered 46 CPU cores on NUMA node 0 2018-09-07T05:20:08.282Z|00004|ovs_numa|INFO|Discovered 1 NUMA nodes and 46 CPU cores 2018-09-07T05:20:08.282Z|00005|memory|INFO|18880 kB peak resident set size after 224.0 seconds "},"notes/networking_网络虚拟化操作记录.html":{"url":"notes/networking_网络虚拟化操作记录.html","title":"网络虚拟化操作记录","keywords":"","body":" CentOS换kernel 环境 改grub OVS-DPDK virt-manager libvirt qemu pktgen testpmd nfs mount perf ping perf ovs OVS 记录线程30秒 内存屏障 查看详细的cache统计 hugepage DPDK会用尽所有hugepage kernel cmdline方式(old) CentOS换kernel sudo rpm -ivh kernel-devel-4.14.62-5.hxt.aarch64.rpm kernel-debuginfo-4.14.62-5.hxt.aarch64.rpm kernel-debuginfo-common-aarch64-4.14.62-5.hxt.aarch64.rpm 环境 服务器 Host A 10.129.41.129 Host B 10.129.41.130 Note Socket 1 1 单socket CPU 48core@2.6G HXT1.0 46core@2.6G HXT1.0 sudo dmidecode -t processor MEM 96G 96G free -h NIC MLX CX4121A 10G 0004:01:00.0 enP4p1s0f0 MLX CX4121A 10G 0004:01:00.0 enP4p1s0f0 ibdev2netdev -v NIC MLX CX4121A 10G 0004:01:00.1 enP4p1s0f1 MLX CX4121A 10G 0004:01:00.1 enP4p1s0f1 ibdev2netdev -v OS CentOS 7.5.1804 CentOS 7.5.1804 cat /etc/redhat-release kernel 4.14.62-5.hxt.aarch64 4.14.62-5.hxt.aarch64 uname -r Mellanox OFED version 4.4-1.0.0.0 4.4-1.0.0.0 ofed_info -s QEMU version NA 2.12.1 qemu-system-aarch64 --version 源码编译 DPDK version 17.11.4 17.11.4 源码编译 pktgen version 3.4.9 NA 源码编译 OVS(with DPDK) version NA 2.9.3 sudo ovs-vsctl show 源码编译 libvirt version NA 4.6.0 源码编译 virt-manager version NA 1.5.1 源码安装 改grub sudo vi /etc/default/grub sudo grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg OVS-DPDK sudo bash -c \"echo 64 > /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages\" export PATH=$PATH:/usr/local/share/openvswitch/scripts sudo ovs-ctl start sudo ovs-ctl stop virt-manager cd ~/repo/hxt/packages/virt-manager-1.5.1 sudo xauth add $(xauth -f /home/bai/.Xauthority list | tail -1) sudo ./virt-manager #启动一个新的vm sudo tools/virsh create ~/repo/save/vm/2vhostuser.xml libvirt cd ~/repo/hxt/packages/libvirt-4.6.0 sudo src/virtlogd & sudo src/libvirtd & qemu sudo qemu-system-x86_64 -m 1024 -smp 4 -cpu host -hda /home/user/us17_04vm1.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char1,path=/run/openvswitch/vhost-user1 \\ -netdev type=vhost-user,id=mynet1,chardev=char1,vhostforce \\ -device virtio-net-pci,mac=00:00:00:00:00:01,netdev=mynet1 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on -numa node,memdev=mem -mem-prealloc \\ -virtfs local,path=/home/user/iperf_debs,mount_tag=host0,security_model=none,id=vm1_dev pktgen sudo bash -c \"echo 16 > /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages\" cd ~/repo/hxt/pktgen/pktgen-dpdk-pktgen-3.4.5 #运行, [42:43].0的意思是rx用core42,tx用core43, core41为显示和timer #-T是打开颜色, -P是混杂模式 sudo app/arm64-armv8a-linuxapp-gcc/pktgen -l 41-45 -w 0005:01:00.0 -- -T -P -m \"[42:43].0\" Pktgen:/pktgen/bin/> stop all Pktgen:/pktgen/bin/> set 0 dst mac 01:00:00:00:00:01 Pktgen:/pktgen/bin/> set 0 dst ip 1.0.0.1 testpmd sudo ./build/app/testpmd -l 36-41 -m 512 --file-prefix pg1 -w 0004:01:00.0 --proc-type auto txq_inline=256,rxq_cqe_comp_en=1,txqs_min_inline=8,txq_mpw_en=1 -- -i sudo ./build/app/testpmd -l 42-45 -m 512 --file-prefix pg2 -w 0005:01:00.0 --proc-type auto txq_inline=256,rxq_cqe_comp_en=1,txqs_min_inline=8,txq_mpw_en=1 -- -i sudo app/arm64-armv8a-linuxapp-gcc/pktgen -l 41-45 -w 0005:01:00.0 -- -T -P -m \"[42:43].0\" 135 ./app/app/${target}/pktgen -l 2-11 -n 3 --proc-type auto \\ 136 --socket-mem 512,512 --file-prefix pg1 \\ 137 -b 09:00.0 -b 09:00.1 -b 83:00.1 -b 06:00.0 \\ 138 -b 06:00.1 -b 08:00.0 -b 08:00.1 -- \\ 139 -T -P -m \"[4:6].0, [5:7].1, [8:10].2, [9:11].3\" \\ 140 -f themes/black-yellow.theme 141 142 ./app/app/${target}/pktgen -l 2,4-11 -n 3 --proc-type auto \\ 143 --socket-mem 512,512 --file-prefix pg2 \\ 144 -b 09:00.0 -b 09:00.1 -b 83:00.1 -b 87:00.0 \\ 145 -b 87:00.1 -b 89:00.0 -b 89:00.1 -- \\ 146 -T -P -m \"[12:16].0, [13:17].1, [14:18].2, [15:19].3\" \\ 147 -f themes/black-yellow.theme -e L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,L1-dcache-store-misses,branch-loads,branch-load-misses,qcom_pmuv3_0/l2d_cache/,qcom_pmuv3_0/l2d_cache_allocate/ stop port stop all port config all txd 1024 port config all rxd 1024 port start all start show port stats all nfs mount sudo yum install nfs-utils sudo umount -f -l /home/bai/share sudo mount 10.64.17.45:/home/bai/share /home/bai/share perf ping sudo perf record -g ping 5.5.5.1 sudo perf script | ~/repo/FlameGraph/stackcollapse-perf.pl | ~/repo/FlameGraph/flamegraph.pl > ping-localhost.svg watch -n1 -d 'sudo ethtool -S enP5p1s0 | grep -v \": 0\" | grep packets' perf ovs sudo perf probe -x `which ovs-vswitchd` --add netdev_linux_send sudo perf probe -x `which ovs-vswitchd` --add netdev_linux_tap_batch_send sudo perf probe --add dev_hard_start_xmit sudo perf record -e probe_ovs:netdev_linux_send -e probe_ovs:netdev_linux_tap_batch_send -e probe:dev_hard_start_xmit -e net:net_dev_start_xmit -p 3470 -g -o perf-ping22.data -- sleep 30 perf probe -x /lib64/libibverbs.so.1 -F perf probe -x `which ovs-vswitchd` -F | grep ibv sudo perf report -n sudo ovs-appctl dpif/show sudo tcpdump -i extbr0 --direction=out OVS ovs-dpdk收发包代码 The existing netdev implementations may serve as useful examples during a port: • lib/netdev-linux.c implements netdev functionality for Linux network devices, using Linux kernel calls. It may be a good place to start for full-featured netdev implementations. ovs-dpdk 把enP4p1s0作为一个system接口来处理，通过raw socket从userpace层面从enP4p1s0收发报文,还是走的userpace datapath,最终报文是通过enp4P1s0的drive收发的。 ovs-dpdk 收报文 main() 循环执行type_run() -> dpif_run() -> dpif_netdev_run()  !netdev_is_pmd(port->netdev))--- dp_netdev_process_rxq_port() //system port不会启pmd thread dpnetdevprocess_rxq_port() • netdev_rxq_recv()->rx->netdev->netdev_class->rxq_recv(rx,batch) -=>netdev_linux_rxq_recv() -> netdev_linux_rxq_recv_sock()从socket绑定的eth口上收报文 • dp_netdev_input() -> dp_netdev_input() : 查emc，dpcls，upcall的流程 • dp_netdev_pmd_flush_output_packets()-> dp_netdev_pmd_flush_output_on_port()->netdev_send() //从egress port出报 ovs-dpdk发报文(如果egress port是system接口) netdev_send ()->netdev_linux_send()  netdev_linux_sock_batch_send() 调用raw socket直接从eth口发出去 记录线程30秒 sudo perf record -g -t 3546 -- sleep 30 内存屏障 ~/share/repo/hxt/linux/Documentation/memory-barriers.txt 查看详细的cache统计 #注意是三个-d sudo perf stat -d -d -d -C 38 hugepage #检查hugepage的配置 $ grep Huge /proc/meminfo AnonHugePages: 0 kB ShmemHugePages: 0 kB HugePages_Total: 32 HugePages_Free: 8 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 524288 kB 需要修改的话, 用root执行: sudo bash -c \"echo 64 > /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages\" 这里我们为64个512M的hugepage预留了32G的内存,用free命令能够看到, 这32G的内存被算做\"used\" $ free -h total used free shared buff/cache available Mem: 95G 33G 61G 50M 1.0G 54G Swap: 4.0G 0B 4.0G hugetable需要mount为hugetlbfs才能够使用, 一般的教程会说 mount -t hugetlbfs nodev /mnt/huge 但在centos7.5上, 已经有个systemd的unit专门干这事, 以后就用/dev/hugepages就行 $ systemctl status dev-hugepages.mount ● dev-hugepages.mount - Huge Pages File System Loaded: loaded (/usr/lib/systemd/system/dev-hugepages.mount; static; vendor preset: disabled) Active: active (mounted) since Mon 2018-07-30 08:49:38 CST; 3 weeks 0 days ago Where: /dev/hugepages What: hugetlbfs Docs: https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt http://www.freedesktop.org/wiki/Software/systemd/APIFileSystems Process: 869 ExecMount=/bin/mount hugetlbfs /dev/hugepages -t hugetlbfs (code=exited, status=0/SUCCESS) Tasks: 0 DPDK会用尽所有hugepage 而且推出时并不会release这些大页, 在/dev/hugepages里面rtemap_就是, 整好是配的64个大页. bai@CentOS-43 /dev/hugepages $ ls libvirt rtemap_11 rtemap_15 rtemap_19 rtemap_22 rtemap_26 rtemap_3 rtemap_33 rtemap_37 rtemap_40 rtemap_44 rtemap_48 rtemap_51 rtemap_55 rtemap_59 rtemap_62 rtemap_9 rtemap_0 rtemap_12 rtemap_16 rtemap_2 rtemap_23 rtemap_27 rtemap_30 rtemap_34 rtemap_38 rtemap_41 rtemap_45 rtemap_49 rtemap_52 rtemap_56 rtemap_6 rtemap_63 rtemap_1 rtemap_13 rtemap_17 rtemap_20 rtemap_24 rtemap_28 rtemap_31 rtemap_35 rtemap_39 rtemap_42 rtemap_46 rtemap_5 rtemap_53 rtemap_57 rtemap_60 rtemap_7 rtemap_10 rtemap_14 rtemap_18 rtemap_21 rtemap_25 rtemap_29 rtemap_32 rtemap_36 rtemap_4 rtemap_43 rtemap_47 rtemap_50 rtemap_54 rtemap_58 rtemap_61 rtemap_8 但不会影响DPDK下次运行, 它会直接找rtemap_使用. 手动删除这些文件会把大页还回给系统 用pmap -x查看testpmd进程, 可以看到类似下面的map Address Kbytes RSS Dirty Mode Mapping 0000fff780000000 524288 0 0 rw-s- /dev/hugepages/rtemap_63 0000fff7a0000000 524288 0 0 rw-s- /dev/hugepages/rtemap_62 kernel cmdline方式(old) #reserve 4G, 每个1G, 如果系统有两个CPU node, 好像是平分 default_hugepagesz=1G hugepagesz=1G hugepages=4 "},"notes/as_title_virtnet.html":{"url":"notes/as_title_virtnet.html","title":"虚拟化网络","keywords":"","body":"如题 "},"notes/networking_tc_filter_连接veth和tap.html":{"url":"notes/networking_tc_filter_连接veth和tap.html","title":"Connecting a veth device to tap","keywords":"","body":" 背景知识 Connecting a veth device to tap Redirecting traffic between the two devices 解释 应用场景 Networking go代码 背景知识 tc是iproute2安装的命令行工具, 用于配置linux 出端口流量shaping系统, 一般叫tc filter. Connecting a veth device to tap veth device from CNI/CNM plugin: eth0 tap device that connects to the VM: tap0 Redirecting traffic between the two devices tc qdisc add dev eth0 ingress tc filter add dev eth0 parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev tap0 tc qdisc add dev tap0 ingress tc filter add dev tap0 parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev eth0 tc qdisc add dev eth0 ingress Add a queuing discipline on dev eth0 attach the ingress qdisc Here the handle defaults to ffff: tc filter add dev eth0 parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev tap0 Add a filter to device dev eth0 to parent (class) handle to which we are attaching, ffff: i.e. ingress which we created before (there is no need for tc class add in the ingress case as it does not support classful queuing discplines). protocol all classifier u32 parameters to the classifier u8 0 0, and the first byte of the packet with 0 and if the result is 0 (which it always will be) (i.e. always true) action mirred egress redirect dev eth0, redirect the packet to egress of dev eth0 解释 create a new qdisc called \"ingress\". qdiscs normally don't work on ingress so this is really a special qdisc that you can consider an \"alternate root\" for inbound packets add a new filter, and attach it to node \"ffff:\". The ID \"ffff:\" is the fixed ID of the ingress qdisc we use the \"u32\" matcher, with arguments \"u8 0 0\". This means match any packet where the first byte, when ANDed with the value 0, returns 0. In other words, all packets are selected 据说能mirror所有流量. Hi, thank you for that nice trick. Do you know if this tc based redirect will allow transparent forwarding of layer 2 frames which are filtered on linux bridge by default (like stp bpdu and LACP frames)? @hellt yes. All traffic should passthro. We use this in Kata containers will all types of CNI interfaces without issues. The performance drop is negleible. 应用场景 Networking Containers will typically live in their own, possibly shared, networking namespace. At some point in a container lifecycle, container engines will set up that namespace to add the container to a network which is isolated from the host network, but which is shared between containers In order to do so, container engines will usually add one end of a virtual ethernet (veth) pair into the container networking namespace. The other end of the veth pair is added to the host networking namespace. This is a very namespace-centric approach as many hypervisors/VMMs cannot handle veth interfaces. Typically, TAP interfaces are created for VM connectivity. To overcome incompatibility between typical container engines expectations and virtual machines, kata-runtime networking transparently connects veth interfaces with TAP ones using Traffic Control: With TC filter in place, a redirection is created between the container network and the virtual machine. As an example, the CNI may create a device, eth0, in the container's network namespace, which is a VETH device. Kata Containers will create a tap device for the VM, tap0_kata, and setup a TC redirection filter to mirror traffic from eth0's ingress to tap0_kata's egress, and a second to mirror traffic from tap0_kata's ingress to eth0's egress. Kata maintains support for MACVTAP, which was an earlier implementation used in Kata. TC-filter is the default because it allows for simpler configuration, better CNI plugin compatibility, and performance on par with MACVTAP. Kata has deprecated support for bridge due to lacking performance relative to TC-filter and MACVTAP. Kata Containers supports both CNM and CNI for networking management. go代码 package main import ( \"fmt\" \"github.com/vishvananda/netlink\" \"os\" \"strconv\" \"golang.org/x/sys/unix\" ) func main() { args := os.Args[1:] if len(args) != 2 { fmt.Println(\"Incorrect number of args\") os.Exit(1) } index1, _ := strconv.Atoi(args[0]) index2, _ := strconv.Atoi(args[1]) fmt.Printf(\"network index1 : %d\\n\", index1) fmt.Printf(\"network index2 : %d\\n\", index2) qdisc1 := &netlink.Ingress{ QdiscAttrs: netlink.QdiscAttrs{ LinkIndex: index1, Parent: netlink.HANDLE_INGRESS, }, } err := netlink.QdiscAdd(qdisc1) if err != nil { fmt.Printf(\"Failed to add qdisc for index %d : %s\", index1, err) os.Exit(1) } qdisc2 := &netlink.Ingress{ QdiscAttrs: netlink.QdiscAttrs{ LinkIndex: index2, Parent: netlink.HANDLE_INGRESS, }, } err = netlink.QdiscAdd(qdisc2) if err != nil { fmt.Printf(\"Failed to add qdisc for index %d : %s\", index2, err) os.Exit(1) } filter1 := &netlink.U32{ FilterAttrs: netlink.FilterAttrs{ LinkIndex: index1, Parent: netlink.MakeHandle(0xffff, 0), Protocol: unix.ETH_P_ALL, }, Actions: []netlink.Action{ &netlink.MirredAction{ ActionAttrs: netlink.ActionAttrs{ Action: netlink.TC_ACT_STOLEN, }, MirredAction: netlink.TCA_EGRESS_REDIR, Ifindex: index2, }, }, } if err := netlink.FilterAdd(filter1); err != nil { fmt.Printf(\"Failed to add filter for index %d : %s\", index1, err) os.Exit(1) } filter2 := &netlink.U32{ FilterAttrs: netlink.FilterAttrs{ LinkIndex: index2, Parent: netlink.MakeHandle(0xffff, 0), Protocol: unix.ETH_P_ALL, }, Actions: []netlink.Action{ &netlink.MirredAction{ ActionAttrs: netlink.ActionAttrs{ Action: netlink.TC_ACT_STOLEN, }, MirredAction: netlink.TCA_EGRESS_REDIR, Ifindex: index1, }, }, } if err := netlink.FilterAdd(filter2); err != nil { fmt.Printf(\"Failed to add filter for index %d : %s\", index2, err) os.Exit(1) } } "},"notes/networking_multicast_vxlan_flannel.html":{"url":"notes/networking_multicast_vxlan_flannel.html","title":"multicast vxlan和flannel","keywords":"","body":" multicast介绍 组播网段 组播特点 组播举例 LLDP也是组播的一种 vxlan介绍 https://zhuanlan.zhihu.com/p/130277008 VXLAN 协议原理 vxlan报文格式 VTEP有转发表 k8s网络cni介绍 在集群的扁平网络里run nginx pods, 直接ip:port访问 使用service来处理IP变化 flannel使用vxlan做backend 代码 历史 版本1 版本2 版本3 创建vxlan设备 用到的库 vxlan如何broadcast 一个专门的实现 multicast介绍 参考: https://tldp.org/HOWTO/Multicast-HOWTO-2.html http://www.steves-internet-guide.com/introduction-multicasting/ 组播网段 ipv4里面, 传统上分为A B C D类地址, 组播属于D类: Bit --> 0 31 Address Range: +-+----------------------------+ |0| Class A Address | 0.0.0.0 - 127.255.255.255 +-+----------------------------+ +-+-+--------------------------+ |1 0| Class B Address | 128.0.0.0 - 191.255.255.255 +-+-+--------------------------+ +-+-+-+------------------------+ |1 1 0| Class C Address | 192.0.0.0 - 223.255.255.255 +-+-+-+------------------------+ +-+-+-+-+----------------------+ |1 1 1 0| MULTICAST Address | 224.0.0.0 - 239.255.255.255 +-+-+-+-+----------------------+ +-+-+-+-+-+--------------------+ |1 1 1 1 0| Reserved | 240.0.0.0 - 247.255.255.255 +-+-+-+-+-+--------------------+ D类地址里面也分了多个区间, 见:https://en.wikipedia.org/wiki/Multicast_address一般239.0.0.0 to 239.255.255.255用的比较多. 特别的: 224.0.0.1 is the all-hosts group. If you ping that group, all multicast capable hosts on the network should answer, as every multicast capable host must join that group at start-up on all it's multicast capable interfaces. 224.0.0.2 is the all-routers group. All multicast routers must join that group on all it's multicast capable interfaces. 224.0.0.4 is the all DVMRP routers, 224.0.0.5 the all OSPF routers, 224.0.013 the all PIM routers, etc. 组播特点 组播是基于UDP的, datagram 不需要加入组播组就可以发送多播 发送多播的datagram只需要open一个udp的socket, 填入多播的group地址就可以发送了 想接收组播报文, 必须先join一个组播组 目的mac是01-00-5e-00-00-00 to 01-00-5e-ff-ff-ff会被认为是组播报文 router会转发组播报文, 而广播报文不会被router转发 组播举例 The SSDP (Simple Service Discovery Protocol) uses multicast address 239.255.255.250 on UDP port 1900. LLDP也是组播的一种 vxlan介绍 vxlan是一种overlay的网络技术, 在udp之上封装了vxlan的报文, 通过ip层传输, 组成一个跨网络的L2子网. #在eth0接口(我这里是192.168.0.14)的udp port上建立vxlan0 #上层来的到vxlan0的报文会被加上外层L2, src ip是192.168.0.14, dst port是4789 #kernel接收到dst port是4789的udp报文, 会被kernel\"发送到\"这个vxlan0接口 sudo ip link add vxlan0 type vxlan id 42 dev eth0 dstport 4789 1. Create vxlan device # ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth1 dstport 4789 This creates a new device named vxlan0. The device uses the multicast group 239.1.1.1 over eth1 to handle traffic for which there is no entry in the forwarding table. The destination port number is set to the IANA-assigned value of 4789. The Linux implementation of VXLAN pre-dates the IANA's selection of a standard destination port number and uses the Linux-selected value by default to maintain backwards compatibility. # Delete vxlan device ip link delete vxlan0 # Show vxlan info ip -d link show vxlan0 # 用bridge命令可以查看vxlan0的转发表 bridge fdb show dev vxlan0 # 转发表fdb可以add和remove # bridge fdb add dev dst bridge fdb add to 00:17:42:8a:b4:05 dst 192.19.0.2 dev vxlan0 bridge fdb delete 00:17:42:8a:b4:05 dev vxlan0 这里的vxlan0就是VTEP https://zhuanlan.zhihu.com/p/130277008 VXLAN 不仅支持一对一，也支持一对多，一个 VXLAN 设备能通过像网桥一样的学习方式学习到其他对端的 IP 地址，还可以直接配置静态转发表。 VXLAN 将二层以太网帧封装在 UDP 中（上面说过了），相当于在三层网络上构建了二层网络。这样不管你物理网络是二层还是三层，都不影响虚拟机（或容器）的网络通信，也就无所谓部署在哪台物理设备上了，可以随意迁移。 VXLAN 协议原理 VTEP（VXLAN Tunnel Endpoints，VXLAN 隧道端点） VXLAN 网络的边缘设备，用来进行 VXLAN 报文的处理（封包和解包）。VTEP 可以是网络设备（比如交换机），也可以是一台机器（比如虚拟化集群中的宿主机）。 VNI（VXLAN Network Identifier，VXLAN 网络标识符） VNI 是每个 VXLAN 段的标识，是个 24 位整数，一共有 $2^{24} = 16777216$（一千多万），一般每个 VNI 对应一个租户，也就是说使用 VXLAN 搭建的公有云可以理论上可以支撑千万级别的租户。 vxlan报文格式 VXLAN Header : 在原始二层帧的前面增加 8 字节的 VXLAN 的头部，其中最主要的是 VNID，占用 3 个字节（即 24 bit），类似 VLAN ID，可以具有 $2^{24}$ 个网段。 UDP Header : 在 VXLAN 和原始二层帧的前面使用 8 字节 UDP 头部进行封装（MAC IN UDP），目的端口号缺省使用 4789，源端口按流随机分配（通过 MAC，IP，四层端口号进行 hash 操作）， 这样可以更好的做 ECMP。 IANA（Internet As-signed Numbers Autority）分配了 4789 作为 VXLAN 的默认目的端口号。 在上面添加的二层封装之后，再添加底层网络的 IP 头部（20 字节）和 MAC 头部（14 字节），这里的 IP 和 MAC 是宿主机的 IP 地址和 MAC 地址。 同时，这里需要注意 MTU 的问题，传统网络 MTU 一般为 1500，这里加上 VXLAN 的封装多出的（36+14/18，对于 14 的情况为 access 口，省去了 4 字节的 VLAN Tag）50 或 54 字节，需要调整 MTU 为 1550 或 1554，防止频繁分包。 VTEP有转发表 哪些 VTEP 需要加到一个相同的 VNI 组？ 发送方如何知道对方的 MAC 地址？ 如何知道目的服务器在哪个节点上（即目的 VTEP 的地址）？ 第一个问题简单，VTEP 通常由网络管理员来配置。要回答后面两个问题，还得回到 VXLAN 协议的报文上，看看一个完整的 VXLAN 报文需要哪些信息： 内层报文 : 通信双方的 IP 地址已经明确，只需要 VXLAN 填充对方的 MAC 地址，因此需要一个机制来实现 ARP 功能。 VXLAN 头部 : 只需要知道 VNI。一般直接配置在 VTEP 上，要么提前规划，要么根据内层报文自动生成。 UDP 头部 : 需要知道源端口和目的端口，源端口由系统自动生成，目的端口默认是 4789。 IP 头部 : 需要知道对端 VTEP 的 IP 地址，这个是最关键的部分。 实际上，VTEP 也会有自己的转发表，转发表通过泛洪和学习机制来维护，对于目标 MAC 地址在转发表中不存在的未知单播，广播流量，都会被泛洪给除源 VTEP 外所有的 VTEP，目标 VTEP 响应数据包后，源 VTEP 会从数据包中学习到 MAC，VNI 和 VTEP 的映射关系，并添加到转发表中，后续当再有数据包转发到这个 MAC 地址时，VTEP 会从转发表中直接获取到目标 VTEP 地址，从而发送单播数据到目标 VTEP。 VTEP 转发表的学习可以通过以下两种方式： 多播 外部控制中心（如 Flannel、Cilium 等 CNI 插件） MAC 头部 : 确定了 VTEP 的 IP 地址，后面就好办了，MAC 地址可以通过经典的 ARP 方式获取。 k8s网络cni介绍 k8s的每个pod都有个ip, 用这个ip可以和集群里的其他pod通信(不经过NAT), pod内的containers共享这个epod的网络, 通过localhost通信. Every Pod in a cluster gets its own unique cluster-wide IP address. This means you do not need to explicitly create links between Pods and you almost never need to deal with mapping container ports to host ports. Kubernetes gives every pod its own cluster-private IP address, so you do not need to explicitly create links between pods or map container ports to host ports. This means that containers within a Pod can all reach each other's ports on localhost, and all pods in a cluster can see each other without NAT. pods can communicate with all other pods on any other node without NAT agents on a node (e.g. system daemons, kubelet) can communicate with all pods on that node 参考: https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model 正是因为所有pod都在一个扁平的ip网络里, pod内部用localhost, pod之间直接用ip:port来通信, 如果通信的各方都在一个集群内, 那完全不需要port exposing. 这点和docker不一样. docker的container是在\"子网\"里面, 每个container的ns都不一样. port需要转换才能在host上使用. 而k8s的每个pod都有独立的IP, 内部container的port能直接作用在这个IP上, 而且这个IP可以被集群内部访问到. 在集群的扁平网络里run nginx pods, 直接ip:port访问 用下面的yaml run-my-nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 开启两个nginx的pod, 每个pod都有独立IP, 而且都使用相同的80端口 kubectl apply -f ./run-my-nginx.yaml kubectl get pods -l run=my-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE my-nginx-3800858182-jr4a2 1/1 Running 0 13s 10.244.3.4 kubernetes-minion-905m my-nginx-3800858182-kna2y 1/1 Running 0 13s 10.244.2.5 kubernetes-minion-ljyd 使用service来处理IP变化 设想一个场景, 我用pod ip:80访问一个nginx好好的, 但突然node挂了, 里面的pods也挂了, k8s重新拉起来新的pods, 但IP变了. 我该怎么继续访问? k8s用service来解决这个问题: A Kubernetes Service is an abstraction which defines a logical set of Pods running somewhere in your cluster, that all provide the same functionality. When created, each Service is assigned a unique IP address (also called clusterIP). This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the Service, and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service. 即多个pod可以同时提供同样的一个功能. k8s为这个功能分配一个IP, 所有访问这个IP的通信会被负载均衡的分发到后面的某个pod上去. 用命令: kubectl expose deployment/my-nginx 或者kubectl apply -f下面的yaml apiVersion: v1 kind: Service metadata: name: my-nginx labels: run: my-nginx spec: ports: - port: 80 protocol: TCP selector: run: my-nginx 查看svc # kubectl get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx ClusterIP 10.0.162.149 80/TCP 21s 详细信息: # kubectl describe svc my-nginx Name: my-nginx Namespace: default Labels: run=my-nginx Annotations: Selector: run=my-nginx Type: ClusterIP IP: 10.0.162.149 Port: 80/TCP Endpoints: 10.244.2.5:80,10.244.3.4:80 Session Affinity: None Events: # kubectl get ep my-nginx NAME ENDPOINTS AGE my-nginx 10.244.2.5:80,10.244.3.4:80 1m 需要说明的是, service的IP是虚拟的, 详见service proxy flannel使用vxlan做backend 入下图所示, 用的是vxlan做为backend的flannel网络: 上图的flannel.1就是VTEP. When pod1 on node1 requests pod2 on node2, the traffic goes as follows. the process in pod1 initiates the request and sends an IP packet. the IP packet enters the cni0 bridge based on the veth device pair in pod1. since the destination ip of the IP packet is not on node1, it enters flannel.1 according to the routing rules created by the flannel on the node. flannel.1 encapsulates the original IP packet with a destination MAC address into a layer 2 data frame; the kernel then encapsulates the data frame into a UDP packet; finally, through the gateway on node1, to node2. 参考: https://www.sobyte.net/post/2022-03/how-the-kubernetes-network-plugin-works/ 注: flannel.1(VTEP)相当于路由 在node上能看到其他node的arp, 比如在node1上能看到node2.flannel.1.mac对应的IP是10.244.1.0. 没错, 对vxlan来说, 是网段的IP(x.x.x.0)对应一个MAC, 这个mac就是flannel.1的mac fdb里面记录的是flannel.1的mac对应node IP(192.168.50.x) 代码 https://github.com/flannel-io/flannel/blob/master/backend/vxlan/vxlan.go 历史 版本1 The first versions of vxlan for flannel registered the flannel daemon as a handler for both \"L2\" and \"L3\" misses When a container sends a packet to a new IP address on the flannel network (but on a different host) this generates an L2 miss (i.e. an ARP lookup) The flannel daemon knows which flannel host the packet is destined for so it can supply the VTEP MAC to use. This is stored in the ARP table (with a timeout) to avoid constantly looking it up. 这里说的是内层的L的目的MAC The packet can then be encapsulated but the host needs to know where to send it. This creates another callout from the kernal vxlan code to the flannel daemon to get the public IP that should be used for that VTEP (this gets called an L3 miss). The L2/L3 miss hooks are registered when the vxlan device is created. At the same time a device route is created to the whole flannel network so that non-local traffic is sent over the vxlan device. 内核需要封装外层L2的时候, 需要知道发送到哪里. 内核产生L3 miss到flannel daemon, 因为daemon注册了L2/L3 miss的hook(通过netlink消息). In this scheme the scaling of table entries (per host) is: 1 route (for the configured network out the vxlan device) One arp entry for each remote container that this host has recently contacted One FDB entry for each remote host 版本2 The second version of flannel vxlan removed the need for the L3MISS callout. When a new remote host is found (either during startup or when it's created), flannel simply adds the required entries so that no further lookup/callout is required. 某个新的remote host启动的时候, 应该会通知其他的daemon, daemon就马上添加一个entry. 版本3 The latest version of the vxlan backend removes the need for the L2MISS too, which means that the flannel deamon is not listening for any netlink messages anymore. This improves reliability (no problems with timeouts if flannel crashes or restarts) and simplifies upgrades. How it works: Create the vxlan device but don't register for any L2MISS or L3MISS messages Then, as each remote host is discovered (either on startup or when they are added), do the following Create routing table entry for the remote subnet. It goes via the vxlan device but also specifies a next hop (of the remote flannel host). Create a static ARP entry for the remote flannel host IP address (and the VTEP MAC) Create an FDB entry with the VTEP MAC and the public IP of the remote flannel daemon. In this scheme the scaling of table entries is linear to the number of remote hosts - 1 route, 1 arp entry and 1 FDB entry per host 创建vxlan设备 if config.EnableIPv4 { devAttrs := vxlanDeviceAttrs{ vni: uint32(cfg.VNI), name: fmt.Sprintf(\"flannel.%v\", cfg.VNI), vtepIndex: be.extIface.Iface.Index, vtepAddr: be.extIface.IfaceAddr, vtepPort: cfg.Port, gbp: cfg.GBP, learning: cfg.Learning, } //调用netlink包创建vxlan dev, err = newVXLANDevice(&devAttrs) if err != nil { return nil, err } dev.directRouting = cfg.DirectRouting } vxlan设备提供AddFDB()和DelFDB()等API来管理转发表. func (dev *vxlanDevice) AddFDB(n neighbor) error { log.V(4).Infof(\"calling AddFDB: %v, %v\", n.IP, n.MAC) return netlink.NeighSet(&netlink.Neigh{ LinkIndex: dev.link.Index, State: netlink.NUD_PERMANENT, Family: syscall.AF_BRIDGE, Flags: netlink.NTF_SELF, IP: n.IP.ToIP(), HardwareAddr: n.MAC, }) } 用到的库 import ( \"encoding/json\" \"fmt\" \"net\" \"sync\" \"github.com/flannel-io/flannel/backend\" \"github.com/flannel-io/flannel/pkg/ip\" \"github.com/flannel-io/flannel/subnet\" \"golang.org/x/net/context\" log \"k8s.io/klog\" ) vxlan如何broadcast https://techhub.hpe.com/eginfolib/networking/docs/switches/5710/5200-5004_vxlan_cg/content/517705095.htm https://vincent.bernat.ch/en/blog/2017-vxlan-linux#without-multicast 基本上有两种方式来处理BUM报文(broadcast, unknown, multicast): 组播 head end方式 We can replace multicast by head-end replication of BUM frames to a statically configured lists of remote VTEPs: ip -6 link add vxlan100 type vxlan \\ id 100 \\ dstport 4789 \\ local 2001:db8:1::1 bridge fdb append 00:00:00:00:00:00 dev vxlan100 dst 2001:db8:2::1 bridge fdb append 00:00:00:00:00:00 dev vxlan100 dst 2001:db8:3::1 The VXLAN is defined without a remote multicast group. Instead, all the remote VTEPs are associated with the all-zero address: a BUM frame will be duplicated to all these destinations. The VXLAN device will still learn remote addresses automatically using source-address learning. It is a very simple solution. With a bit of automation, you can keep the default FDB entries up-to-date easily. However, the host will have to duplicate each BUM frame (head-end replication) as many times as there are remote VTEPs. This is quite reasonable if you have a dozen of them. This may become out-of-hand if you have thousands of them. Cumulus vxfld daemon is an example of this strategy (in the head-end replication mode). “Keepalived and unicast over multiple interfaces” shows another usage. 比如在三个pod中(比如A B C), 由pod ip生成vxlan100, 经过验证这个方法可以: BUM可以在是3个pod中广播 A ping B, B可以学到A的vxlan100的mac, 会在fdb表中自动学习. 一个专门的实现 https://github.com/CumulusNetworks/vxfld "},"notes/networking_virtio网络介绍.html":{"url":"notes/networking_virtio网络介绍.html","title":"virtio网络介绍(网摘): vhost-net virtio-net vhost-user SRIOV","keywords":"","body":" 参考(按顺序阅读): vhost-net后端和virtio-net前端介绍 virtio场景 inside qemu vhost-net vhost-user 前端也可以在guest用户态 VFIO pci passthrough 深入理解virtio和vhost QEMU and device emulation KVM Virtio specification: devices and drivers Virtio specification: virtqueues virtio net device inside qemu Vhost protocol vhost-net in host kernel OVS Communication with the outside world sriov和vPDA SR-IOV for isolating VM traffic SR-IOV for mapping NIC to guest Virtio full HW offloading vDPA - standard data plane Comparing virtio architectures 参考(按顺序阅读): Introduction to virtio-networking and vhost-net Deep dive into Virtio-networking and vhost-net Hands on vhost-net: Do. Or do not. There is no try Virtio devices and drivers overview: The headjack and the phone Achieving network wirespeed in an open standard manner: introducing vDPA vhost-net后端和virtio-net前端介绍 virtio场景 virtio的前端是driver, 后端是概念上的device. 前端没啥好说的, 就是guest里面的virtio-net 后端就有很多场景: inside qemu 这个device是qemu模拟的. 数据在qemu用户态和guest kernel申请的ring buffer中流动. qemu通过kvm来实现中断通知. Since the notification now needs to travel from the guest (KVM), to QEMU, and then to the kernel for the latter to forward the network frame, we can spawn a thread in the kernel with access to the guest’s shared memory mapping and then let it handle the virtio dataplane. In that context, QEMU initiates the device using the virtio dataplane, and then forwards the virtio device status to vhost-net, delegating the data plane to it. In this scenario, KVM will use an event file descriptor (eventfd) to communicate the device interruptions, and expose another one to receive CPU interruptions. The guest does not need to be aware of this change, it will operate as the previous scenario. vhost-net Also, in order to increase the performance, we created an in-kernel virtio-net device (called vhost-net) to offload the data plane directly to the kernel, where packet forwarding takes place: 为了提高性能, 数据流动发生在guest kernel和host kernel中, 但通知还是要经过qemu和kvm. vhost-user Later on, we moved the virtio device from the kernel to an userspace process in the host (covered in the post \"A journey to the vhost-users realm\") that can run a packet forwarding framework like DPDK. The protocol to set all this up is called virtio-user. vhost-user后端是另外一个进程, ovs-dpdk中有使用. 前端也可以在guest用户态 It even allows guests to run virtio drivers in guest’s userland, instead of the kernel! In this case, virtio names driver the process that is managing the memory and the virtqueues, not the kernel code that runs in the guest. 和上图的区别是, 这里guest里的virtio-net驱动是在用户态的. VFIO pci passthrough Lastly, we can directly do a virtio device passthrough with the proper hardware. If the NIC supports the virtio data plane, we can expose it directly to the guest with proper hardware (IOMMU device, able to translate between the guest’s and device’s memory addresses) and software (for example, VFIO linux driver, that enables the host to directly give the control of a PCI device to the guest). The device uses the typical hardware signals for notifications infrastructure, like PCI and CPU interruptions (IRQ). If a hardware NIC wants to go this way, the easiest approach is to build its driver on top of vDPA, also explained in earlier posts of this series. 深入理解virtio和vhost 原文一定要读! QEMU and device emulation QEMU is a hosted virtual machine emulator that provides a set of different hardware and device models for the guest machine. For the host, qemu appears as a regular process scheduled by the standard Linux scheduler, with its own process memory. In the process, QEMU allocates a memory region that the guest see as physical, and executes the virtual machine’s CPU instructions. To perform I/O on bare metal hardware, like storage or networking, the CPU has to interact with physical devices performing special instructions and accessing particular memory regions, such as the ones that the device is mapped to. When the guests access these memory regions, control is returned to QEMU, which performs the device’s emulation in a transparent manner for the guest. KVM Kernel-based Virtual Machine (KVM) is an open source virtualization technology built into Linux. It provides hardware assist to the virtualization software, using built-in CPU virtualization technology to reduce virtualization overheads (cache, I/O, memory) and improving security. With KVM, QEMU can just create a virtual machine with virtual CPUs (vCPUs) that the processor is aware of, that runs native-speed instructions. When a special instruction is reached by KVM, like the ones that interacts with the devices or to special memory regions, vCPU pauses and informs QEMU of the cause of pause, allowing hypervisor to react to that event. In the regular KVM operation, the hypervisor opens the device /dev/kvm, and communicates with it using ioctl calls to create the VM, add CPUs, add memory (allocated by qemu, but physical from the virtual machine’s point of view), send CPU interrupts (as an external device would send), etc. For example, one of these ioctl runs the actual KVM vCPU,, blocking QEMU and making the vCPU run until it found an instruction that needs hardware assistance. In that moment, the ioctl returns (this is called vmexit) and QEMU knows the cause of that exit (for example, the offending instruction). For special memory regions, KVM follows a similar approach, marking memory regions as Read Only or not mapping them at all, causing a vmexit with the KVM_EXIT_MMIO reason. Virtio specification: devices and drivers Virtio is an open specification for virtual machines' data I/O communication, offering a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms. It uses the fact that the guest can share memory with the host for I/O to implement that. The virtio specification is based on two elements: devices and drivers. In a typical implementation, the hypervisor exposes the virtio devices to the guest through a number of transport methods. By design they look like physical devices to the guest within the virtual machine. The most common transport method is PCI or PCIe bus. However, the device can be available at some predefined guest’s memory address (MMIO transport). These devices can be completely virtual with no physical counterpart or physical ones exposing a compatible interface. The typical (and easiest) way to expose a virtio device is through a PCI port since we can leverage the fact that PCI is a mature and well supported protocol in QEMU and Linux drivers. Real PCI hardware exposes its configuration space using a specific physical memory address range (i.e., the driver can read or write the device’s registers by accessing that memory range) and/or special processor instructions. In the VM world, the hypervisor captures accesses to that memory range and performs device emulation, exposing the same memory layout that a real machine would have and offering the same responses. The virtio specification also defines the layout of its PCI Configuration space, so implementing it is straightforward. When the guest boots and uses the PCI/PCIe auto discovering mechanism, the virtio devices identify themselves with with the PCI vendor ID and their PCI Device ID. The guest’s kernel uses these identifiers to know which driver must handle the device. In particular, the linux kernel already includes virtio drivers. The virtio drivers must be able to allocate memory regions that both the hypervisor and the devices can access for reading and writing, i.e., via memory sharing. We call data plane the part of the data communication that uses these memory regions, and control plane the process of setting them up. We will provide further details on the virtio protocol implementation and memory layout in future posts. The virtio kernel drivers share a generic transport-specific interface (e.g: virtio-pci), used by the actual transport and device implementation (such as virtio-net, or virtio-scsi). Virtio specification: virtqueues Virtqueues are the mechanism for bulk data transport on virtio devices. Each device can have zero or more virtqueues (link). It consists of a queue of guest-allocated buffers that the host interacts with either by reading them or by writing to them. In addition, the virtio specification also defines bi-directional notifications: Available Buffer Notification: Used by the driver to signal there are buffers that are ready to be processed by the device Used Buffer Notification: Used by the device to signal that it has finished processing some buffers. In the PCI case, the guest sends the available buffer notification by writing to a specific memory address, and the device (in this case, QEMU) uses a vCPU interrupt to send the used buffer notification. The virtio specification also allows the notifications to be enabled or disabled dynamically. That way, devices and drivers can batch buffer notifications or even actively poll for new buffers in virtqueues (busy polling). This approach is better suited for high traffic rates. In summary, the virtio driver interface exposes: Device’s feature bits (which device and guest have to negotiate) Status bits Configuration space (that contains device specific information, like MAC address) Notification system (configuration changed, buffer available, buffer used) Zero or more virtqueues Transport specific interface to the device virtio net device inside qemu The virtio network device is a virtual ethernet card, and it supports multiqueue for TX/RX. Empty buffers are placed in N virtqueues for receiving packets, and outgoing packets are enqueued into another N virtqueues for transmission. Another virtqueue is used for driver-device communication outside of the data plane, like to control advanced filtering features, settings like the mac address, or the number of active queues. As a physical NIC, the virtio device supports features such as many offloadings, and can let the real host’s device do them. To send a packet, the driver sends to the device a buffer that includes metadata information such as desired offloadings for the packet, followed by the packet frame to transmit. The driver can also split the buffer into multiple gather entries, e.g. it can split the metadata header from the packet frame. These buffers are managed by the driver and mapped by the device. In this case the device is \"inside\" the hypervisor. Since the hypervisor (qemu) has access to all the guests’ memory it is capable of locating the buffers and reading or writing them. The following flow diagram shows the virtio-net device configuration and the sending of a packet using virtio-net driver, that communicates with the virtio-net device over PCI. After filling the packet to be sent, it triggers an \"available buffer notification\", returning the control to QEMU so it can send the packet through the TAP device. Qemu then notifies the guest that the buffer operation (reading or writing) is done, and it does that by placing the data in the virtqueue and sending a used notification event, triggering an interruption in the guest vCPU. The process of receiving a packet is similar to that of sending it. The only difference is that, in this case, empty buffers are pre-allocated by the guest and made available to the device so it can write the incoming data to them. Vhost protocol The previous approach contains a few inefficiencies: After the virtio driver sends an Available Buffer Notification, the vCPU stops running and control is returned to the hypervisor causing an expensive context switch. QEMU additional tasks/threads synchronization mechanisms. The syscall and the data copy for each packet to actually send or receive it via tap (no batching). The ioctl to send the available buffer notification (vCPU interruption). We also need to add another syscall to resume vCPU execution, with all the associated mapping switching, etc. In order to address these limitations, the vhost protocol was designed. The vhost API is a message based protocol that allows the hypervisor to offload the data plane to another component (handler) that performs data forwarding more efficiently. Using this protocol, the master sends the following configuration information to the handler: The hypervisor’s memory layout. This way, the handler can locate the virtqueues and buffer within the hypervisor’s memory space. A pair of file descriptors that are used for the handler to send and receive the notifications defined in the virtio spec. These file descriptors are shared between the handler and KVM so they can communicate directly without requiring the hypervisor’s intervention. Note that this notifications can still be dynamically disabled per virtqueue. After this process, the hypervisor will no longer process packets (read or write to/from the virtqueues). Instead, the dataplane will be completely offloaded to the handler, which can now access the virtqueues’ memory region directly as well as send and receive notifications directly to and from the guest. The vhost messages can be exchanged in any host-local transport protocol, such as Unix sockets or character devices and the hypervisor can act as a server or as a client (in the context of the communication channel). The hypervisor is the leader of the protocol, the offloading device is a handler and any of them can send messages. In order to further understand the benefits of this protocol, we will analyze the details of a kernel-based implementation of the vhost protocol: the vhost-net kernel driver. vhost-net in host kernel The vhost-net is a kernel driver that implements the handler side of the vhost protocol to implement an efficient data plane, i.e., packet forwarding. In this implementation, qemu and the vhost-net kernel driver (handler) use ioctls to exchange vhost messages and a couple of eventfd-like file descriptors called irqfd and ioeventfd are used to exchange notifications with the guest. When vhost-net kernel driver is loaded, it exposes a character device on /dev/vhost-net. When qemu is launched with vhost-net support it opens it and initializes the vhost-net instance with several ioctl(2) calls. These are necessary to associate the hypervisor process with the vhost-net instance, prepare for virtio feature negotiation and pass the guest physical memory mapping to the vhost-net driver. During the initialization the vhost-net kernel driver creates a kernel thread called vhost-$pid, where $pid is the hypervisor process pid. This thread is called the \"vhost worker thread\". A tap device is still used to communicate the VM with the host but now the worker thread handles the I/O events i.e. it polls for driver notifications or tap events, and forwards data. Qemu allocates one eventfd and registers it to both vhost and KVM in order to achieve the notification bypass. The vhost-$pid kernel thread polls it, and KVM writes to it when the guest writes in a specific address. This mechanism is named ioeventfd. This way, a simple read/write operation to a specific guest memory address does not need to go through the expensive QEMU process wakeup and can be routed to the vhost worker thread directly. This also has the advantage of being asynchronous, no need for the vCPU to stop (so no need to do an immediate context switch). On the other hand, qemu allocates another eventfd and registers it to both KVM and vhost again for direct vCPU interruption injection. This mechanism is called irqfd, and it allows any process in the host to inject vCPU interrupts to the guest by writing to it, with the same advantages (asynchronous, no need for immediate context switching, etc). Note that such changes in the virtio packet processing backend are completely transparent to the guest who still uses the standard virtio interface. The following block and flow diagrams show the datapath offloading from qemu to the vhost-net kernel driver: OVS Communication with the outside world We could achieve this by using any forwarding or routing mechanism provided by the kernel networking stack, like standard Linux bridges. However, a more advanced solution is to use a fully virtualized, distributed, managed switch, such as Open Virtual Switch (OVS). As said in the overview post, OVS datapath is running as a kernel module in this scenario, ovs-vswitchd as a userland control and managing daemon and ovsdb-server as the forwarding database. As illustrated in the following figure, OVS datapath is running in the kernel and is forwarding the packets between the physical NIC and the virtual TAP device: sriov和vPDA SR-IOV for isolating VM traffic In the vhost-net/virtio-net and vhost-user/virto-pmd architectures we had a software switch (OVS or other) which could take a single NIC on a phy-facing port and distribute it to several VMs with a VM-facing port per VM. The most straight-forward way to attach a NIC directly to a guest is a device-assignment, where we assign a full NIC to the guest kernel driver. The problem is that we have a single physical NIC on the server exposed through PCI thus the question is how can we create \"virtual ports\" on the physical NIC as well? Single root I/O virtualization (SR-IOV) is a standard for a type of PCI device assignment that can share a single device to multiple virtual machines. In other words, it allows different VMs in a virtual environment to share a single NIC. This means we can have a single root function such as an Ethernet port appear as multiple separated physical devices which address our problem of creating \"virtual ports\" in the NIC. SR-IOV has two main functions: Physical functions (PFs) which are a full PCI device including discovery, managing and configuring as normal PCI devices. There is a single PF pair per NIC and it provides the actual configuration for the full NIC device Virtual functions (VFs) are simple PCI functions that only control part of the device and are derived from physical functions. Multiple VFs can reside on the same NIC. We need to configure both the VFs representing virtual interfaces in the NIC and the PF which is the main NIC interface. For example, we can have a 10GB NIC with a single external port and 8 VF. The speed and duplex for the external port are PF configurations while rate limiters are VF configurations. The hypervisor is the one mapping virtual functions to virtual machines and each VF can be mapped to a single guest at a time (a VM can have multiple VFs). Let’s see how SR-IOV can be mapped to the guest kernel, userspace DPDK or directly to the host kernel: OVS kernel with SR-IOV: we are using the SR-IOV to provide multiple phy-facing ports from the OVS perspective (with separate MACs for example) although in practice we have a single physical NIC. This is done through the VFs. We are a section of the kernel memory (for each VF) to the specific VF on the NIC. OVS DPDK with SR-IOV: Bypassing the host kernel directly from the NIC to the OVS DPDK on the user space which SR-IOV provides. We are mapping the host user space memory to the VF on the NIC. SR-IOV for guests: mapping the memory area from the guest to the NIC (bypassing the host all together). It should be noted that Note that when using device assignment, the ring layout is shared between the physical NIC and the guest. However it’s proprietary to the specific NIC being used, hence it can only be implemented using the specific NIC driver provided by the NIC vendor Another note: there is also a fourth option which is common, and that is to assign the device to a dpdk application inside the guest userspace. 以上说的是虽然VF能够assign给guest, 但guest里面要用这个PF厂家的驱动. 因为只有厂家的驱动才能理解硬件的ring layout SR-IOV for mapping NIC to guest Focusing on SR-IOV to the guest use case, another question is how to effectively send/receive packets to the NIC once memory is mapped directly. We have two approaches to solve this: Using the guest kernel driver: In this approach we use the NIC (vendor specific) driver in the kernel of the guest, while directly mapping the IO memory, so that the HW device can directly access the memory on the guest kernel. Using the DPDK pmd driver in the guest: In this approach we use the NIC (vendor specific) DPDK pmd driver in the guest userspace, while directly mapping the IO memory, so that the HW device can directly access the memory on the specific userspace process in the guest. In this section we will focus on the DPDK pmd driver in the guest approach. The following diagram brings this all together: Note the following points: The data plane is vendor specific and goes directly to the VF. For SRIOV, Vendor NIC specific drivers are required both in the host kernel (PF driver) and the guest userspace (VF PMD) to enable this solution. The host kernel driver and the guest userspace PMD driver don’t communicate directly. The PF/VF drivers are configured through other interfaces (e.g. the host PF driver can be configured by libvirt). The vendor-VF-pmd in the guest userspace is responsible for configuring the NICs VF while the vendor-PF-driver in the host kernel space is managing the full NIC. Summarizing this approach we are able to provide wirespeed from the guest to the NIC by leveraging SR-IOV and DPDK PMD. However, this approach comes at an expense. The solution described is vendor specific, meaning that it requires a match between the drivers running in the guest and the actual physical NIC. This implies for example that if the NIC firmware is upgraded, the guest application driver may need to be upgraded as well. If the NIC is replaced with a NIC from another vendor, the guest must use another PMD to drive the NIC. Moreover, migration of a VM can only be done to a host with the exact same configuration. This implies the same NIC with the same version, in the same physical place and some vendor specific tailored solution for migration So the question we want to address is how to provide the SRIOV wirespeed to the VM while using a standard interface and most importantly, using generic driver in the guest to decouple it from specific host configurations or NICs. In the next two solutions we will show how virtio could be used to solve that problem. Virtio full HW offloading The first approach we present is the virtio full HW offloading were both the virtio data plane and virtio control plane are offloaded to the HW. This implies that the physical NIC (while still using VF to expose multiple virtual interfaces) supports the virtio control spec including discovery, feature negotiation, establishing/terminating the data plane, and so on. The device also support the virtio ring layout thus once the memory is mapped between the NIC and the guest they can communicate directly. 要求硬件支持virtio协议. In this approach the guest can communicate directly with the NIC via PCI so there is no need for any additional drivers in the host kernel. The approach however requires the NIC vendor to implement the virtio spec fully inside its NIC (each vendor with its proprietary implementation) including the control plane implementation (which is usually done in SW on the host OS, but in this case needs to be implemented inside the NIC) The following diagram shows the virtio full HW offloading architecture: Note the following: In reality the control plane is more complicated and requires interactions with memory management units (IOMMU and vIOMMU) which will be described in the next technical deep dive post. There are additional blocks in the host kernel, qemu process and guest kernel which have been dropped in order to simplify the flow. There is also an option to pull the virtio data plane and control plane to the kernel instead of the user space (as described in the SRIOV case as well) which means we use the virtio-net driver in the guest kernel to talk directly with the NIC (instead of using the virtio-pmd in the guest userspace as shown above). vDPA - standard data plane Virtual data path acceleration (vDPA) in essence is an approach to standardize the NIC SRIOV data plane using the virtio ring layout and placing a single standard virtio driver in the guest decoupled from any vendor implementation, while adding a generic control plane and SW infrastructure to support it. Given that it’s an abstraction layer on top of SRIOV it is also future proof to support emerging technologies such as scalable IOV (See the relevant spec.). Similar to the virtio full HW offloading the data plane goes directly from the NIC to the guest using the virtio ring layout. However each NIC vendor can now continue using its own driver (with a small vDPA add-on) and a generic vDPA driver is added to the kernel to translate the vendor NIC driver/control-plane to the virtio control plane. The vDPA is a much more flexible approach than the virtio full HW offloading enabling NIC vendors to support virtio ring layout with significant smaller effort and still achieving wire speed performance on the data plane. The next diagram presents this approach: Note the following: There are additional blocks in the host kernel, QEMU process and guest kernel which have been dropped in order to simplify the flow. Similar to the SRIOV and virtio full HW offloading the data plane and control plane can enter in the guest kernel and not the guest user space (same pros and cons previously mentioned). vDPA has the potential of being a powerful solution for providing wirespeed Ethernet interfaces to VMs: Open public specification—anyone can see, consume and be part of enhancing the specifications (the Virtio specification) without being locked to a specific vendor. Wire speed performance—similar to SRIOV, no mediators or translator between. Future proof for additional HW platform technologies—ready to also support Scalable IOV and similar platform level enhancements. Single VM certification—Single standard generic guest driver independent of a specific NIC vendor means that you can now certify your VM consuming an acceleration interface only once regardless of the NICs and versions used (both for your Guest OS and or for your Container/userspace image). Transparent protection—the guest uses a single interface which is protected on the host side by 2 interfaces (AKA backend-protection). If for example the vDPA NIC is disconnected then the host kernel is able to identify this quickly and automagically switch the guest to use another virtio interface such as a simple vhost-net backend. Live migration—Providing live migration between different vendor NICs and versions given the ring layout is now standard in the guest. Providing a standard accelerated interface for containers—will be discussed in future posts. The bare-metal vision—a single generic NIC driver—Forward looking, the virtio-net driver can be enabled as a bare-metal driver, while using the vDPA SW infrastructure in the kernel to enable a single generic NIC driver to drive different HW NICs (similar, e.g. to the NVMe driver for storage devices). Comparing virtio architectures Summarizing what we have learned in the previous posts in this serious, and in this post, we have covered four virtio architectures for providing Ethernet networks to VMs: vhost-net/virito-net, vhost-user/virito-pmd, virtio full HW offloading and vDPA. Let’s compare the pros and cons of each of the approaches: "},"notes/networking_virtualization_杂记.html":{"url":"notes/networking_virtualization_杂记.html","title":"虚拟化网络杂记","keywords":"","body":" bridge的VLAN filter功能 old way 使用VLAN filter功能 docker内部ping 宿主机外部ip docker内eth0 docker0网桥 host eth0 在docker内ping 10.182.105.138 过程第一步 过程第二步 第三步 在docker内ping 192.168.0.14 linux网桥 veth 等等 ip netns 网络名字空间 veth docker网络 docker网桥命令 docker iptable bridge的VLAN filter功能 参考: https://developers.redhat.com/blog/2017/09/14/vlan-filter-support-on-bridge# old way 如果用linux bridge配置带vlan的网络, 以前的做法是创建多个br, 每个br对应一个vlan, 例如:命令:Create a bond device. # ip link add bond0 type bond # ip link set bond0 type bond miimon 100 mode balance-alb # ip link set eth0 down # ip link set eth0 master bond0 # ip link set eth1 down # ip link set eth1 master bond0 # ip link set bond0 up Create VLANs on bond. We need to create more VLANs if dictated by requirements. # ip link add link bond0 name bond0.2 type vlan id 2 # ip link set bond0.2 up # ip link add link bond0 name bond0.3 type vlan id 3 # ip link set bond0.3 up Add a bridge device, to which we can attach the VLAN interface. # ip link add br0 type bridge # ip link set bond0.2 master br0 # ip link set br0 up # ip link add br1 type bridge # ip link set bond0.3 master br1 # ip link set br1 up Attach tap device to bridge. # ip link set guest_1_tap_0 master br0 # ip link set guest_2_tap_0 master br0 # ip link set guest_2_tap_1 master br1 # ip link set guest_3_tap_0 master br1 使用VLAN filter功能 This feature was introduced in Linux kernel 3.8 and was added to RHEL in version 7.0. Let's take an example that is widely used in virtualization: different subnets on a bridge with bond load balance. Create a bond device, the same as above. # ip link add bond0 type bond # ip link set bond0 type bond miimon 100 mode balance-alb # ip link set eth0 down # ip link set eth0 master bond0 # ip link set eth1 down # ip link set eth1 master bond0 # ip link set bond0 up Create the bridge interface, enable VLAN filter and attach the bond interface to the bridge directly. # ip link add br0 type bridge # ip link set br0 up # ip link set br0 type bridge vlan_filtering 1 # ip link set bond0 master br0 Attach the tap device to the bridge. # ip link set guest_1_tap_0 master br0 # ip link set guest_2_tap_0 master br0 # ip link set guest_2_tap_1 master br0 # ip link set guest_3_tap_0 master br0 Set the tap interface with the VLAN filter. # bridge vlan add dev guest_1_tap_0 vid 2 pvid untagged master # bridge vlan add dev guest_2_tap_0 vid 2 pvid untagged master # bridge vlan add dev guest_2_tap_1 vid 3 pvid untagged master # bridge vlan add dev guest_3_tap_0 vid 3 pvid untagged master # bridge vlan add dev bond0 vid 2 master # bridge vlan add dev bond0 vid 3 master To dump the VLAN information from the bridge interface. # bridge vlan show port vlan ids bond0 1 PVID Egress Untagged 2 3 br0 1 PVID Egress Untagged guest_1_tap_0 1 Egress Untagged 2 PVID Egress Untagged guest_2_tap_0 1 Egress Untagged 2 PVID Egress Untagged guest_2_tap_1 1 Egress Untagged 3 PVID Egress Untagged guest_3_tap_0 1 Egress Untagged 3 PVID Egress Untagged docker内部ping 宿主机外部ip docker内eth0 mac: 02:42:ac:11:00:02 ip: 172.17.0.2/16 默认路由: default via 172.17.0.1 dev eth0 docker0网桥 mac: 02:42:29:ac:5c:40 ip: 172.17.0.1/16 host eth0 mac: fa:16:3e:cf:b4:34 ip: 192.168.0.14/24 默认路由: default via 192.168.0.1 dev eth0 对外ip: 10.182.105.138 在docker内ping 10.182.105.138 现象: 能ping通 过程第一步 在docker0抓包看到: 发送icmp请求 02:42:ac:11:00:02 > 02:42:29:ac:5c:40 172.17.0.2 > 10.182.105.138: ICMP echo request 回复icmp响应: 02:42:29:ac:5c:40 > 02:42:ac:11:00:02 10.182.105.138 > 172.17.0.2: ICMP echo reply 所以第一步: docker内部协议栈没有找到138的路由, 走默认路由, 即docker0网桥, 目的ip为138不变, mac地址改为docker0的mac地址 过程第二步 现在报文来到host的docker0网桥上, 但依然没有找到138的路由. 在通过默认路由前, 需要做NAT转换: 把内网ip(172.17.0.2)换成host ip(192.168.0.14) #这里很有意思的出现了2次icmp request #第一次是host ip发给138 fa:16:3e:cf:b4:34 > fa:16:3e:36:da:c0 192.168.0.14 > 10.182.105.138: ICMP echo request #第二次是138发给host ip fa:16:3e:36:da:c0 > fa:16:3e:cf:b4:34 10.182.105.138 > 192.168.0.14: ICMP echo request #以上出现两次request的原因是, 138和192.168.0.14是同一个主机. #类似的, icmp reply也有两个 fa:16:3e:cf:b4:34 > fa:16:3e:36:da:c0 192.168.0.14 > 10.182.105.138: ICMP echo reply fa:16:3e:36:da:c0 > fa:16:3e:cf:b4:34 10.182.105.138 > 192.168.0.14: ICMP echo reply 第三步 给192.168.0.14的icmp echo reply: 10.182.105.138 > 192.168.0.14经过NAT转换为:10.182.105.138 > 172.17.0.2, 协议栈查到172.17.0.2走docker0这个interface, 随后这报文被送到docker内部的ping程序. 在docker内ping 192.168.0.14 192.168.0.14即host的eth0的ip能ping通, 但在host的eth0抓不到包. 说明报文没有从eth0口出来.从docker0能抓到包 02:42:ac:11:00:02 > 02:42:29:ac:5c:40 172.17.0.2 > 192.168.0.14: ICMP echo request 02:42:29:ac:5c:40 > 02:42:ac:11:00:02 192.168.0.14 > 172.17.0.2: ICMP echo reply 说明报文在docker0网桥的协议栈中, 就知道这个icmp request报文是可以本机处理的, 然后就处理了. linux网桥 veth 等等 ip netns 网络名字空间 创建网络空间： # ip netns add ns1 查看网络空间： # ip netns list 删除网络空间：注意网络空间有引用计数, 最后一个user删除才真正删除 # ip netns del ns1 进入网络空间执行命令： # ip netns exec ns1 `command` ip netns ls能看到同名的netns netns的特性是在外面用ip netns exec命令进入netns空间看, 比如sudo ip netns exec setup_standalone_board_0_1fg247_yingjieb ip a, 和在docker实例里面看的结果, 是一模一样的. netns是一个新的网络栈的拷贝, 有独立的路由, 规则, 和网络设备. man ip netns里面说, 一个有名子的netns对应/var/run/netns/NAME目录下的NAME文件, 打开这个文件并传入setns系统调用, task就关联到这个netns了. ip netns exec自动完成上述netns文件关联到进程的动作, 所以执行的命令是在指定的netns里面执行的. ip netns set NAME NETNSID - assign an id to a peer network namespace #看目标PID对应的netns ip netns identify [PID] - Report network namespaces names for process #看目标netns里面有哪些PID ip netns pids NAME - Report processes in the named network namespace ip netns monitor - Report as network namespace names are added and deleted ip netns list-id - list network namespace ids (nsid) 上面提到, 有的netns是name, 有的是id, 它们有什么关系吗? --可以是一一对应关系: 下面不管是list还是list-id命令, 都能打印name和id的关系 bash-5.0# ip netns list-id nsid 0 nsid 1 (iproute2 netns name: pcta_0) nsid 2 (iproute2 netns name: lt_1) bash-5.0# ip netns list dhcpd_1 dhcpd_0 pcta_0 (id: 1) lt_1 (id: 2) veth veth是个驱动, 可以用来创建虚拟网络设备对. veth总是成对出现, 用来连接网络的两端, 从一端来的报文总是立即被另一端接收.ip link add type veth peer name 上面命令创建了p1-name和p2-name做为veth的两端.这时候把其中的一端move到netns中, 就能在两个netns间通信了:ip link set netns 用ethtool -S或者直接ip link就能看到对应关系 docker网络 docker网络文档写的很好: https://docs.docker.com/network/ 有以下几种: bridge: 最常见的模式. 容器通过bridge连接到host, host提供iptable的规则可以隔离容器, 但又能上外网 又可以分为默认网桥(docker0)和自定义网桥. 自定义网桥有些高级特性 自定义网桥有自动DNS解析 容器可以动态的接入和断开自定义网桥 host: 容器直接使用host网络 overlay: 不同host之间overlay macvlan: 让容器的mac看起来像是网络上的物理接口. 就是说报文在host的协议栈发现mac是容器的, 直接投递给容器, 而不是交给网桥路由(个人理解). bridge模式$ docker network create -d macvlan \\ --subnet=172.16.86.0/24 \\ --gateway=172.16.86.1 \\ -o parent=eth0 pub_net trunk模式$ docker network create -d macvlan \\ --subnet=192.168.50.0/24 \\ --gateway=192.168.50.1 \\ -o parent=eth0.50 macvlan50 ipvlan: 感觉是一个轻量的网络虚拟化实现: 不用网桥, 而是绑定到host的物理口eth, 或eth的vlan比如eth.10. 可以是l2模式也可以是l3模式. # IPvlan (-o ipvlan_mode= Defaults to L2 mode if not specified) $ docker network create -d ipvlan \\ --subnet=192.168.1.0/24 \\ # Start a container on the db_net network $ docker run --net=db_net -it --rm alpine /bin/sh # NOTE: the containers can NOT ping the underlying host interfaces as # they are intentionally filtered by Linux for additional isolation. 支持802.1q trunk模式 docker网桥命令 docker network list docker network create my-net $ docker create --name my-nginx \\ --network my-net \\ --publish 8080:80 \\ nginx:latest docker network connect my-net my-nginx docker network disconnect my-net my-nginx $ docker network inspect bridge docker iptable docker会修改host的iptable, 加了两个chain: DOCKER: 所有的默认规则加在这个chain里 DOCKER-USER: 在DOCKER chain之前, 用户可以配置, 比如配置除了192.168.1.1的报文都丢弃.iptables -I DOCKER-USER -i ext_if ! -s 192.168.1.1 -j DROP "},"notes/networking_linux虚拟网络接口.html":{"url":"notes/networking_linux虚拟网络接口.html","title":"linux虚拟网络接口(网摘)","keywords":"","body":" Introduction Bridge What is a bridge When to use a bridge How to create a bridge Bonding What is bonding When to use bonding How to create a bond Team What is team When to use team How to create team VLAN What is VLAN When to use VLAN How to create VLAN VXLAN What is VXLAN When to use VXLAN How to create vxlan MACVLAN What is MACVLAN When to use MACVLAN How to setup MACVLAN IPVLAN What is IPVLAN When to use IPVLAN How to setup IPVLAN MACVTAP/IPVTAP What is MACVTAP/IPVTAP When to use MACVTAP/IPVTAP How to create MACVTAP MACsec What is MACsec When to use MACsec How to setup MACsec VETH What is VETH When to use VETH How to setup VETH VCAN What is VCAN When to use CAN How to create VCAN VXCAN what is VXCAN When to use VXCAN How to setup VXCAN IPOIB What is it When to use How to create nlmon What is nlmon When to use nlmon How to create nlmon Dummy What is Dummy When to use Dummy How to create Dummy IFB What is IFB When to use IFB How to create an IFB interface Introduction In this post I will give a brief introduction for all commonly used virtual interfaces. There is no code analysis. Only brief introduction and the usage on Linux. So anyone with a network background could have a look at this blog post. The interface list can be obtained with the command ip link help. The first part is some frequently used interfaces and some interfaces that can be easily confused with one another. The second part will be a brief introduction of all kinds of tunnels(TBD). Bridge What is a bridge A linux bridge behaves like a network switch. It forwards packets between interfaces that are connected on it. It’s usually used for forwarding packets on routers, gateways, or between VMs and network namespaces on a host. It also supports STP, VLAN filter and multicast snooping. When to use a bridge When you want to establish communication channels between VMs, containers and your hosts. How to create a bridge # ip link add br0 type bridge # ip link set eth0 master br0 # ip link set tap1 master br0 # ip link set tap2 master br0 # ip link set veth1 master br0 This creates a bridge device named br0 and set two tap devices(tap1, tap2), a veth device(veth1) and a physical device(eth0) as its slaves, as shown in the picture. Bonding What is bonding The Linux bonding driver provides a method for aggregating multiple network interfaces into a single logical “bonded” interface. The behavior of the bonded interfaces depends on the mode; generally speaking, modes provide either hot standby or load balancing services. When to use bonding When you want to increase your link speed or do failover on your server. How to create a bond ip link add bond1 type bond miimon 100 mode active-backup ip link set eth0 master bond1 ip link set eth1 master bond1 This creates a bond interface named bond1 with mode active-backup. For other modes, please see: Kernel bonding doc Team What is team Similar to bonding, the purpose of the team device is to provide a mechanism to team multiple NICs (ports) into one logical one (teamdev) at L2 layer. The main thing to realize is that the Team project is not trying to replicate or mimic the bonding driver. What it does is to solve the same problem using a different approach, using e.g. lockless (RCU) TX/RX path and modular design. But there are also some functional difference between bonding and team. e.g. team supports LACP load-balancing, NS/NA (IPV6) link monitoring, D-Bus interface, etc., that are absent in bonding. For further details about the differences between bonding and Team, see: Bonding vs Team. When to use team When you want to use some features that bonding doesn’t provide. How to create team # teamd -o -n -U -d -t team0 -c '{\"runner\": {\"name\": \"activebackup\"},\"link_watch\": {\"name\": \"ethtool\"}}' # ip link set eth0 down # ip link set eth1 down # teamdctl team0 port add eth0 # teamdctl team0 port add eth1 This creates a team interface named team0 with mode active-backup, and adds eth0, eth1 as team0’s sub-interfaces. More info: a new driver called net_failover has been added to linux recently. It’s another failover master netdev for virtualization and manages a primary (a passthru/vf device) and standby (the original paravirtual interface) slave netdevs. VLAN What is VLAN VLAN, aka virtual LAN. It separates broadcast domains by adding tags to network packets. VLANs allow network administrators to group hosts under the same switch or between different switches. The VLAN header looks like: When to use VLAN When you want to seperate subnet in VMs, namespaces or hosts How to create VLAN # ip link add link eth0 name eth0.2 type vlan id 2 # ip link add link eth0 name eth0.3 type vlan id 3 This add VLAN 2 with name eth0.2 and vlan 3 with name eth0.3. The topology looks like: Note: When configuring a VLAN, you need to make sure the switch connected to the host is able to handle VLAN tags, e.g. by setting the switch port to trunk mode. VXLAN What is VXLAN VXLAN(Virtual eXtensible Local Area Network) protocol is a tunnelling protocol designed to solve the problem of limited VLAN IDs (4096) in IEEE 802.1q. It is described by IETF RFC 7348. With a 24-bit segment ID, aka “VXLAN Network Identifier (VNI)”, VXLAN allows up to 2^24(16 777 216) virtual LANs, which is 4096 times of VLAN capacity. VXLAN encapsulates Layer 2 frames with a VXLAN header into a UDP-IP packet, which looks like: When to use VXLAN VXLAN is typically deployed in data centers on virtualized hosts, which may be spread across multiple racks. How to create vxlan # ip link add vx0 type vxlan id 100 local 1.1.1.1 remote 2.2.2.2 dev eth0 dstport 4789 For reference, you can read the vxlan kernel doc or an other vxlan introduction MACVLAN What is MACVLAN With VLAN, we can create multiple interfaces on top of a single one and filter packages based on VLAN tag. With MACVLAN, we can create multiple interfaces with different Layer 2 (i.e. Ethernet MAC) addresses on top of a single one . Before MACVLAN, if users wanted to connect to physical network from VM or namespace, they would have needed to create TAP/VETH devices and attach one side to bridge, and attach a physical interface to the bridge on the host at the same time. Now, with MACVLAN, they can bind a physical interface, associated to a MACVLAN, directly to namespaces, without the need for a bridge. There are four MACVLAN types: Private: doesn’t allow communication between MACVLAN instances on the same physical interface, even if the external switch supports hairpin mode. VEPA: data from one macvlan instance to the other on the same physical interface is transmitted over the physical interface. Either the attached switch needs to support hairpin mode, or there must be a TCP/IP router forwarding the packets in order to allow communication. Bridge: all endpoints are directly connected to each other with a simple bridge via physical interface. Passthru: allows a single VM to be connected directly to the physical interface. Source: the source mode is used to filter traffic based on a list of allowed source MAC addresses, to create MAC-based VLAN associations. Please see the commit message The type is chosen according to different needs. Bridge mode is the most commonly used When to use MACVLAN Users want to connect directly to physical network from Containers. How to setup MACVLAN # ip link add macv1 link eth0 type macvlan mode bridge # ip link add macv2 link eth0 type macvlan mode bridge # ip netns add net1 # ip netns add net2 # ip link set macvlan1 netns net1 # ip link set macvlan2 netns net2 This creates two new MACVLAN devices in bridge mode. And assigns these two devices to two different namespaces. IPVLAN What is IPVLAN IPVLAN is similar to MACVLAN with the difference being that the endpoints have the same mac address. IPVLAN supports L2 and L3 mode. IPVLAN L2 mode acts like a MACVLAN in bridge mode. The parent interface looks like a bridge or switch. In IPVLAN l3 mode, the parent interface acts like a router and packets are routed between endpoints, which gives better scalability. When to use IPVLAN Quoting the IPVLAN kernel doc MACVLAN and IPVLAN are very similar in many regards and the specific use case could very well define which device to choose. if one of the following situations defines your use case then you can choose to use ipvlan - The Linux host that is connected to the external switch / router has policy configured that allows only one mac per port. No of virtual devices created on a master exceed the mac capacity and puts the NIC in promiscuous mode and degraded performance is a concern. If the slave device is to be put into the hostile / untrusted network namespace where L2 on the slave could be changed / misused. How to setup IPVLAN # ip netns add ns0 # ip link add name ipv1 link eth0 type ipvlan mode l2 # ip link set dev ipvl0 netns ns0 This creates an IPVLAN device named ipvl0 with mode L2, assigned to namespace ns0. MACVTAP/IPVTAP What is MACVTAP/IPVTAP MACVTAP/IPVTAP is a new device driver meant to simplify virtualized bridged networking. When a MACVTAP/IPVTAP instance is created on top of a physical interface, the kernel also creates a character device /dev/tapX to be used just like a TUN/TAP device, which can be directly used by KVM/QEMU. with MACVTAP/IPVTAP, we can replace the combination of TUN/TAP and bridge drivers with a single module. When to use MACVTAP/IPVTAP Typically, this is used to make both the guest and the host show up directly on the switch to which the host is connected. The difference between MACVTAP and IPVTAP is same with MACVLAN/IPVLAN. How to create MACVTAP # ip link add link eth0 name macvtap0 type macvtap MACsec What is MACsec MACsec is an IEEE standard for security in wired Ethernet LANs. Similiar to IPsec, as a layer 2 specification, MACsec can protect not only IP traffic, but also ARP, neighbour discovery, and DHCP. The MACSEC headers looks like When to use MACsec The main use case for MACsec is to secure all messages on a standard LAN, includeing ARP, NS and DHCP message. How to setup MACsec # ip link add macsec0 link eth1 type macsec Note: This only adds a MACsec device macsec0 on interface eth1. For more detailed configurations, please see the “Configuration example” Section in MACsec introduction by Sabrina VETH What is VETH This device is a local Ethernet tunnel. Devices are created in pairs. Packets transmitted on one device in the pair are immediately received on the other device. When either device is down the link state of the pair is down. When to use VETH When namespaces need to communicate to the main host namespace, or between each other. How to setup VETH # ip netns add net1 # ip netns add net2 # ip link add veth1 netns net1 type veth peer name veth2 netns net2 This creates two namespaces net1, net2 and a pair of veth devices, and assigns veth1 to namespace net1, and veth2 to namespace net2. These two namespaces are connected with this veth pair. Assign a pair of IP addresses, and you can ping and communicate between the two namespaces. VCAN What is VCAN Similar to the network loopback devices, VCAN offers a virtual local CAN (Controller Area Network) interface, so users can send/receive can message via VCAN interface. CAN is mostly used in the automotive field nowadays. For more CAN protocol information, please refer to kernel can doc When to use CAN When you want to test a CAN protocol implementation on the local host. How to create VCAN # ip link add dev vcan1 type vcan VXCAN what is VXCAN Similar to the veth(virtual ethernet) driver, vxcan(Virtual CAN Tunnel) implements a local CAN traffic tunnel between two virtual CAN network devices. When creating a vxcan, two VXCAN devices are created as a pair. When one end receives the packet it appears on its pair and vice versa. VXCAN can be used for cross namespace communication. When to use VXCAN When you want to send CAN message across namespaces. How to setup VXCAN # ip netns add net1 # ip netns add net2 # ip link add vxcan1 netns net1 type vxcan peer name vxcan2 netns net2 IPOIB What is it IPOIB a device to support IP-over-InfiniBand protocol. This transports IP packets over InfiniBand so you can use your IB device as a fast NIC. The IPoIB driver supports two modes of operation: datagram and connected. In datagram mode, the IB UD (Unreliable Datagram) transport is used. In connected mode, the IB RC (Reliable Connected) transport is used. Connected mode takes advantage of the connected nature of the IB transport and allows an MTU up to the maximal IP packet size of 64K. For more details, please see ipoib kernel doc When to use When you have an IB device and want to communicate with a remote host via IP. How to create # ip link add ipoib0 type ipoib mode connected nlmon What is nlmon nlmon is a netlink monitor device When to use nlmon When you want to monitor system netlink messages How to create nlmon # ip link add nlmon0 type nlmon # ip link set nlmon0 up # tcpdump -i nlmon0 -w nlmsg.pcap This creates a nlmon device named nlmon0 and sets it up. Use a packet sniffer (e.g. tcpdump) to capture netlink messages. Recent versions of wireshark feature decoding of netlink messages. Dummy What is Dummy The dummy interface is entirely virtual, like e.g. the loopback interface. The purpose of dummy driver is to provide a device to route packets through, without actually transmitting them. When to use Dummy It was used to make inactive SLIP address look like a real address for local programs. Nowadays it’s mostly used for testing and debugging. How to create Dummy # ip link add dummy1 type dummy # ip addr add 1.1.1.1/24 dev dummy1 # ip link set dummy1 up IFB What is IFB The IFB (Intermediate Functional Block) driver supplies a device that allows concentration of traffic from several sources, and shaping incoming traffic instead of dropping it. When to use IFB When you want to queue and shape incoming traffics. How to create an IFB interface IFB is an alternative to tc filters for handling ingress traffic, by redirecting it to a virtual interface and treat is as egress traffic there.You need one ifb interface per physical interface, to redirect ingress traffic from eth0 to ifb0, eth1 to ifb1 and so on. First, add ifb interfaces. # ip link add ifb0 type ifb # ip link set ifb0 up And redirect ingress traffic from the physical interfaces to corresponding ifb interface. For eth0 -> ifb0: # tc qdisc add dev eth0 handle ffff: ingress # tc filter add dev eth0 parent ffff: protocol ip u32 match u32 0 0 action mirred egress redirect dev ifb0 Again, repeat for eth1 -> ifb1, eth2 -> ifb2 and so on, until all the interfaces you want to shape are covered. Now, you can apply all the rules you want. Egress rules for eth0 go as usual in eth0. Let’s limit bandwidth, for example: # tc qdisc add dev eth0 root handle 1: htb default 10 # tc class add dev eth0 parent 1: classid 1:1 htb rate 1mbit # tc class add dev eth0 parent 1:1 classid 1:10 htb rate 1mbit Ingress rules for eth0, now go as egress rules on ifb0 (whatever goes into ifb0 must come out, and only eth0 ingress traffic goes into ifb0). Again, a bandwidth limit example: # tc qdisc add dev ifb0 root handle 1: htb default 10 # tc class add dev ifb0 parent 1: classid 1:1 htb rate 1mbit # tc class add dev ifb0 parent 1:1 classid 1:10 htb rate 1mbit The advantage of this approach is that egress rules are much more flexible than ingress filters. Filters only allow you to drop packets, not introduce wait times, for example. By handling ingress traffic as egress you can setup queue disciplines, with traffic classes and, if need be, filters. You get access to the whole tc tree, not only simple filters. For more IFB qdisc use cases, please refer to: Linux Foundation wiki - IFB Author Hangbin Liu LastMod 2019-09-18 (d761a5f) "},"notes/as_title_arm_server.html":{"url":"notes/as_title_arm_server.html","title":"ARM server","keywords":"","body":"如题 "},"notes/arm_server_杂记.html":{"url":"notes/arm_server_杂记.html","title":"Arm server 杂记","keywords":"","body":" Ali support char类型在arm和x86上不一样 高精度时钟 io优化 openssl CCPI lattency specint 2006 kernel 测试 perf frontend backend stall from ARM streamline from ARM qemu如何进入monitor命令行 更新grub docker kernel team的问题 问题 回答 PXE调试 driver调试手段之trace ssh配置文件 efi grub 和 grub.cfg 几何平均数和算数平均数 kernel补丁 给alibaba提动kernel补丁的过程 启动时加模块的参数 ssd的strim是什么东东? discard? Fstrim/discard 调试RDS tps变为0的问题 网口讨论 关于网络处理的基本流程和概念 编译模块的kernel通用路径 通过其他机器上外网 解决polkitd的问题 sysbench里面随机函数lrand48问题 ssh keepalive git生成patch 添加和删除网关 一次性top coredump 在thunder上的sysdig 好像用不起来... sysdig初体验 ulimit进程数限制 在thunder上做盘 rtc时间 SATA slow sync问题 使用native gcc的变量设置 smp_affinity和rps_cpus mysql调试方法论之各种调试和检测命令 mysql性能提升之网口中断 解决sync flood问题 mysql调优之瓶颈在哪? 理论 mysql调优之innodb_spin_wait_delay = 30 memcpy和strcpy perf调试单进程, 调用栈? numactl 预取 prefetch gdb调试多进程 aj kernel git换remote? 交叉编译kernel pssh可以在多个机器同时执行命令 ssh免密码登录 ssh远程执行命令 cpu offline 动态 dd原理 每个字节都被复制 UUID启动 不用sda sdb 重做fedora fedora网口配置 查看10台系统是否online check cpld 版本 写mac地址 编译gcc 96个核的异常问题 关于rps_cpus xps_cpu 网络时间 irq绑定 tubo错误分析 ethtool可以配置驱动 为什么ulimit -n的值不能改? atomic解决mysql崩溃问题, weak order Ali support 记录我在Alibaba支持ARM64的一些技术点 char类型在arm和x86上不一样 toku db有几个测试项fail rds.tokudb_bug_811, rds.tokudb_file_length, rds.tokudb_ftclose_checkpoint and the result mismatch with rds.tokudb_zstd_compression 原因是: x86的char是有符号的, 而arm上char是无符号的! --- a/storage/tokudb/ft-index/ft/serialize/compress.cc +++ b/storage/tokudb/ft-index/ft/serialize/compress.cc @@ -295,7 +295,7 @@ void toku_decompress (Bytef *dest, uLongf destLen, strm.zalloc = Z_NULL; strm.zfree = Z_NULL; strm.opaque = Z_NULL; - char windowBits = source[1]; + int8_t windowBits = source[1]; int r = inflateInit2(&strm, windowBits); lazy_assert(r == Z_OK); strm.next_out = dest; 高精度时钟 在编译tokudb的代码时, 需要实现一个函数 static inline tokutime_t toku_time_now(void) { uint32_t lo, hi; __asm__ __volatile__ (\"rdtsc\" : \"=a\" (lo), \"=d\" (hi)); return (uint64_t)hi 这里面用了x86的汇编. 需要在ARM64上实现这个功能 具体的实现应该是__asm __volatile__ (\"mrs %[rt],cntvct_el0\" : [rt] \"=r\" (result)); 或者这样 unsigned long long get_ticks(void) { unsigned long long tv; #if defined(AARCH64) asm volatile(\"mrs %0, cntvct_el0\" : \"=r\" (tv)); #elif defined(X64) asm volatile (\"rdtsc\" : \"=a\"(tv)); #endif return tv; } io优化 echo 150 > /sys/block/sda/device/timeout echo deadline > /sys/block/sdb/queue/scheduler echo deadline > /sys/block/sda/queue/scheduler echo 500> /sys/block/sdb/queue/nr_requests /etc/default/irqbalance #Configuration for the irqbalance daemon #Should irqbalance be enabled? ENABLED=\"1\" #Balance the IRQs only once? ONESHOT=\"0\" IRQBALANCE_BANNED_CPUS=\"ffffffffffffffffffffffff\" Here are some potential optimizations to get the best performance: Regarding the MegaRAID controller If you are using SSD as data volumes, use LSI MegaRAID Fast Path and tune the OS as described below. Click here for a KB article with a trial key . Use SSD as caching along with disk drives and LSI MegaRAID CacheCade or CacheCade Pro 2.0 Click here for a trial key If using HDD (Hard Disk Drives) enable write back cache (consider using a BBU in the event of unexpected power loss or unclean shutdown) Linux OS tuning also improves performance for LSI SAS HBAs. Under Windows and Linux using SSDs: When the volume is created, use write through caching (i.e. write cache will be disabled) Use Direct IO (and not Cached IO) Use RAID 0 for the highest performance (note: the loss of a single SSD will result in the loss of all data) In general, the dual core controllers (e.g. 9265/9285) perform faster than the single controllers (e.g. 9260/9280) Under Linux (some of these are specific to SSDs, as indicated below): telinit 3 (Shutdown to runlevel 3/kill Xwindows) echo \"0\" > /sys/block/sda/queue/rotational (Turn off seek reordering-use with SSDs) I/O scheduler To check the current setting: cat /sys/block/sdb/queue/scheduler To tune the setting: echo \"deadline\" > /sys/block/sdb/queue/scheduler (Change to deadline I/O scheduler) Repeat for other devices (e.g. sdb, sdc, etc.) This setting reverts to the default after a system reboot. You may want to create a start up script that will automatically run this tuning parameter after every reboot. Turn up block layer queue depth for sda to 975-use with SSDs or use with HDDs if using a single Logical Drive) To check the current setting: cat /sys/block/sda/queue/nr_requests To tune the setting: echo \"975\" > /sys/block/sdb/queue/nr_requests Repeat for other devices (e.g. sdb, sdc, etc.) This setting reverts to the default after a system reboot. You may want to create a start up script that will automatically run this tuning parameter after every reboot. Turn up driver queue depth for sda to 975-use with SSDs or use with HDDs if using a single Logical Drive) To check the current setting: cat /sys/block/sda/device/queue_depth To tune the setting: echo \"975\" > /sys/block/sda/device/queue_depth Repeat for other devices (e.g. sdb, sdc, etc.) This setting reverts to the default after a system reboot. You may want to create a start up script that will automatically run this tuning parameter after every reboot. echo N > /sys/module/drm_kms_helper/parameters/poll (Turn off kworkd001 CPU eating background kernel thread) Assign Affinity Masks - See A008273-110112 Setup rq_affinity To check the current setting: cat /sys/block/sda/queue/rq_affinity To tune the setting: echo \"0\" > /sys/block/sda/queue/rq_affinity Repeat for other devices (e.g. sdb, sdc, etc.) This setting reverts to the default after a system reboot. You may want to create a start up script that will automatically run this tuning parameter after every reboot. Newer kernels (e.g. RHEL 6.4) allow this setting to be '2', which will cause the block layer softirq to run on the exact same CPU that issued the I/O. openssl I've tested the RSA2048 performance with this cmd openssl speed rsa2048 -multi 32 And intel 2630 got about: 6911, 2650 with 32 core and -multi 32 thread got about 7907.7 runing the same cmd in one of ten machine (2 sockets) on my hand(evb3 maybe) and with option: multi 96 got about 6509.9. Today, I try to run openssl speed rsa2048 on ThunderX CRB-1s (ThunderX CPU @ 2.0GHz , 48Cores). “./openssl speed rsa2048 -multi 32” test result is: 5505.3 “./openssl speed rsa2048 -multi 48” test result is: 8104.6 CCPI lattency The following is the CCPI latency #. On our pass 1.x silicon we are running at 6.25Gbps, pass 2.0 aka the production part will be 10Gbps. You will see latency improvement from 400 cycles to 300 cycles. specint 2006 kernel 测试 LKP Blogbench dbench Ext4-frags Fileio ku-latency kvm-unit-tests nuttcp pft LTP Trinity AIM9 XFS Test unionmount-testsuite Fsync perf frontend backend stall from ARM Hi Bai, As we discussed on-site at Alibaba, I think it would be good to backport the patches that add support for the frontend stall and backend stall performance counters. This was implemented earlier this year by Jan Glauber from Cavium, and appeared in the 4.6 kernel. As discussed, these counters are very valuable when you need to analyze the root cause of a CPU performance problem, following the top-down methodology (see https://moodle.technion.ac.il/pluginfile.php/560599/mod_resource/content/1/Vtune%20%20-%20Top%20Down%20Performance%20Analysis%20,%20by%20Ahmad%20Yasin.pdf for the paper). My understanding is that the following commits introduce this support in the main-line linux kernel: 94085fe - Thu, 18 Feb 2016 17:50:12 +0100 (5 months ago) arm64: dts: Add Cavium ThunderX specific PMU - Jan Glauber c210ae8 - Thu, 18 Feb 2016 17:50:14 +0100 (5 months ago) arm64: perf: Extend event mask for ARMv8.1 - Jan Glauber 7175f05 - Thu, 18 Feb 2016 17:50:13 +0100 (5 months ago) arm64: perf: Enable PMCR long cycle counter bit - Jan Glauber d0aa2bf - Thu, 18 Feb 2016 17:50:11 +0100 (5 months ago) arm64/perf: Add Cavium ThunderX PMU support - Jan Glauber 5f140cc - Thu, 18 Feb 2016 17:50:10 +0100 (5 months ago) arm64: perf: Rename Cortex A57 events - Jan Glauber If the support could be backported to linux kernel 4.2, it means Alibaba could use these counters to more efficiently investigate the root cause of CPU inefficiencies. streamline from ARM DS-5 download https://developer.arm.com/products/software-development-tools/ds-5-development-studio/downloads gator source code https://github.com/ARM-software/gator protocol https://github.com/ARM-software/gator/tree/master/protocol qemu如何进入monitor命令行 Crtl+a c 更新grub vim /etc/default/grub grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg reboot docker https://docs.docker.com/engine/installation/linux/centos/ kernel team的问题 问题 why 100, not 1000？ why noop, not others? why 64k, not 4k? performance comparison? 回答 100hz vs. 1000hz: we see minimal performance difference for most of our applications here. We have performed significant testing using both settings, as well as 250hz and 300hz without any issue. I will direct all future kernels to use 1000hz moving forward, at your preference. noop is generally best for SSDs, which many customer use often However, if your application has a combination of SSDs and HDDs we will gladly switch the default to deadline, which is shown to work well with both disks and all applications. The impact on SSDs is effectively 0, while the improvement with HDDs is significant. The only consideration is that MySQL is known to have poor performance when using the CFQ scheduler, this is true on all platforms, and is a standard note for MySQL performance. 64k vs 4k: 64k has performance benefits in many applications, 5% on average (though many, even most applications see 0 change). However, specifically, 64k is considered the desired default for ARM64, RedHat has specifically asked for 64k to be the standard. 4k works perfectly fine, however. The performance hit isn’t very noticeable in most applications. We would kindly suggest using the default 64k, however, particularly since RedHat and CentOS will be providing 64k based distributions in the future. We chose a number of configuration options based on our experience, or because they were considered recommend by the ARM ecosystem, however these are only defaults, and we understand your use case may differ. Please ask if you have any questions at all, we are very eager to customize your kernel such that it may maximize your application performance, while ensuring a minimal impact on your deployment process. PXE调试 PXE需要配DHCP TFTP FTP server. Thunder机器先是通过DHCP获取IP地址, 然后TFTP下载grub.efi, 目录在/tftpboot下面. 这个efi会在板子上跑起来, 然后会下载grub.cfg和image, initrd等文件, 用这些文件, 系统就能进入最小安装环境(ram os). 起kernel的时候可以在grub里面传参数来指定ISO的位置, 用FTP协议. 这里遇到的问题是系统卡住了, 此时需要tcpdump来调试, 需要知道机器的mac, 有很多方法可以查到. tcpdump -i em1 ether host 02:1f:b7:00:00:00 -vv 下面就可以装了. 用text模式, 选server with GUI. 用BMC可以看输出, 关键是用which javaws来打开那个文件 driver调试手段之trace 调试driver的时候, 经常会加一些打印; 而其实你想过没有, 一些比较成熟的系统, 比如i2c已经有系统性的调试打印了; 在自己动手加打印之前, 先把这些被默认关闭的调试打印打开, 经常会事半功倍, 下面以调试i2c read为例 打开调试信息输出 echo 1 > /sys/kernel/debug/tracing/events/i2c/i2c_read/enable 查看输出 cat /sys/kernel/debug/tracing/trace 这些debug信息在哪里? --i2c框架代码里已经埋好 trace_i2c_read(...) ssh配置文件 baiyingjie@mserver-dell ~/.ssh $ chmod 600 config $ cat config Host ali6 Hostname 10.97.219.6 User root IdentityFile ~/.ssh/id_rsa ServerAliveInterval 60 Host ali55 Hostname 10.97.219.55 User root IdentityFile ~/.ssh/id_rsa ServerAliveInterval 60 Host rdsgserver Hostname 192.168.10.3 User root IdentityFile ~/.ssh/id_rsa ProxyCommand ssh -q -W %h:%p rdsgclient ServerAliveInterval 60 Host rdsgclient Hostname 30.2.47.69 User root IdentityFile ~/.ssh/id_rsa ServerAliveInterval 60 efi grub 和 grub.cfg 这三个东西都是存在第一个分区里面的, 这是个fat32的bootable分区, 好像专门给uefi用的 这个分区会mount到/boot/efi 注意grub.cfg也是在这个分区里面 根分区(一般是第二个分区, ext4)里面也有grub和grub2文件夹, 但里面要么是软链接到/boot/efi的, 要么就是像背景图一样的不重要文件? 几何平均数和算数平均数 算数平均数: n个数的和除以n 几何平均数定义: n个数相乘再开n次方 应用场景: 算数平均数用来表示这n个数的和的平均特征, 意味着你要考察的潜在目标是他们的和 几何平均数是这n个数的积的特征, 考察的潜在目标是乘积. kernel补丁 git format-patch git apply --check newpatch.patch git am --signoff 给alibaba提动kernel补丁的过程 thunder_master基于linux_4.2, 开发了比如500+个commit 同样基于linux_4.2添加新的分支alibaba_4.2 在alibaba_4.2上, 用git cherry-pick commitid从thunder_master上的500+commit里面挑出来有用的, 比如200个左右. git cherry-pick可以把别的分支上的commit应用到当前分支 用alibaba给的centos7.1里面的内核源码新建一个repo 在新建的这个repo里面, git remote add cavium thunder_repo(就是刚才那个) git fetch cavium 把cavium repo取下来, 注意此时并没有更新到工作区 还用git cherry-pick commitB...commitA应用cavium repo alibaba_4.2上的两个commit B和A之间的补丁. B就是linux_4.2, A就是最新的alibaba_4.2; 这里不用merge是因为虽然都是基于4.2, 但这两个repo没有共同的commit, 给ali提供patch的时候, 用git format-patch ali应用patch的时候, 可以git apply 启动时加模块的参数 root@Tfedora /etc/modprobe.d # cat megaraid_sas.conf options megaraid_sas msix_disable=1 ssd的strim是什么东东? discard? Fstrim/discard I suspect the reason for this may be SSD garbage collection. The only known solution to it that I'm aware of is trim: https://en.wikipedia.org/wiki/Trim_%28computing%29 调试RDS tps变为0的问题 When TPS drops to zero, run this: gdb -ex \"set pagination 0\" -ex \"thread apply all bt\" \\ --batch -p $(pidof mysqld) > pmp.txt set pagination 0是说不要分页, 否则还要提示敲回车继续 thread apply all bt是说查所有线程的调用栈 网口讨论 关于网络处理的基本流程和概念 在内核态的整个包处理流程中, 不会内存拷贝 硬件收到的是比特流, 然后通过DMA把报文放在buffer里(这些buffer是驱动在初始化的时候或运行时分配好的, 硬件看到的是指针, 这些buffer都是物理地址连续的), 然后发中断, 但是往那个cpu发呢? 这是由q决定的. Thunder上一个网口设备默认是8个q, 但可以通过ethtool -L tx 16 rx 16 eth0来配. 这里用默认的8个q来分析: 每个q绑定一个core, 中断就发到对应的core上. 网口中断的下半部是NAPI, 也是在同一个core上, NAPI负责把buffer处理成skb, 然后算hash, 并根据/sys/class/net/neP6p1s0f1/queues/rx-0/rps_cpus来决定把这个\"work\"放到那个core的q(TCP/IP的q? 每个core都有一个)上. 注意以上都没有内存拷贝. 然后就是那个core来跑TCP/IP协议栈, 跑完了以后, kernel发现有一个或多个socket的文件描述符在等待, 这时会把这个buffer(或者叫skb好一点)拷贝到用户态, 每个socket都会拷一遍. 据说内核的处理数度是1M pps/core. DPDK是完全在用户态控制硬件收发包 要用DPDK, 要先unbind默认的网口driver, 应该是通过/proc, 有个接口. 然后相当于把PCIE的设备assgin给DPDK. DPDK run的时候, 会直接访问对应的PCIE设备的寄存器(通过PCIE map)来直接控制硬件; buffer的问题也是通过map内核空间的内存来解决物理地址连续的问题; 没有中断, 全是polling. 此时内核是看不到这个网口的, ifconfig看不到. DPDK据说~10M pps/core 目前每个interface最大支持96个q 一些参考数据 Upstream driver performance(internal: Not ready to share with customers (This is to give a rough idea on what would be the max performance on 88xx + pass 2.0 + 2.0 Ghz ) Per core performance: testpmd: 19.2 mpps/core l2fwd: 11.8 mpps/core l3fwd: 9.3 mpps/cor 编译模块的kernel通用路径 /lib/modules${kernel}/build是软链接, 指向本地编译kernel的路径, 有了它, 我们在编译module的时候就能方便的找到kernel的源码树. 通过其他机器上外网 on 192.168.10.2 sysctl -w net.ipv4.ip_forward=1 iptables -t nat -A POSTROUTING -j MASQUERADE -s 192.168.10.0/24 -o enP9p144s0f0 enP10p88s0f0是可以上外网的interface 192.168.10.0/24 是内网网段 on 192.168.10.3route add default gw 192.168.10.2 echo \"nameserver 10.65.0.1\" > /etc/resolv.conf 解决polkitd的问题 root@Tfedora /usr/share/dbus-1/system-services # cat org.freedesktop.PolicyKit1.service [D-BUS Service] #Name=org.freedesktop.PolicyKit1 #Exec=/usr/lib/polkit-1/polkitd --no-debug #User=root #SystemdService=polkit.service or mv org.freedesktop.PolicyKit1.service org.freedesktop.PolicyKit1.service.bak sysbench里面随机函数lrand48问题 在sysbench里面用了lrand48函数, 在多线程跑的时候, 在thunder上有的随机数会重复, 概率很大, 但x86上就没有. man lrand48得知: $ man lrand48 ATTRIBUTES Multithreading (see pthreads(7)) The drand48(), erand48(), lrand48(), nrand48(), mrand48(), jrand48(), srand48(), seed48(), and lcong48() functions record global state information for the random number generator, so they are not thread-safe. 这个lrand48不是线程安全的 可以用lrand48_r, 这个是可重入的 ssh keepalive ssh连接超时问题解决方案： 修改server端的etc/ssh/sshd_config ClientAliveInterval 60 ＃server每隔60秒发送一次请求给client，然后client响应，从而保持连接 ClientAliveCountMax 3 ＃server发出请求后，客户端没有响应得次数达到3，就自动断开连接，正常情况下，client不会不响应 修改client端的etc/ssh/ssh_config添加以下：（在没有权限改server配置的情形下） ServerAliveInterval 60 ＃client每隔60秒发送一次请求给server，然后server响应，从而保持连接 ServerAliveCountMax 3 ＃client发出请求后，服务器端没有响应得次数达到3，就自动断开连接，正常情况下，server不会不响应 git生成patch byj@mint ~/repo/git/thunder/RDS/alisql $ git diff cavium_baseline cavium_optimized > RDS_Cavium.patch 添加和删除网关 ip route show ip route add default via 10.97.219.247 dev eth2 ip route del 10.0.0.0/8 一次性top $ ./alldo.sh 'top -bn1 | head -15' coredump root@ARM ~/src/git/LuaJIT/src # ulimit -c unlimited root@ARM ~/src/git/LuaJIT/src # ./luajit --version Segmentation fault (core dumped) root@ARM ~/src/git/LuaJIT/src # ls core.15262 -lh -rw------- 1 root root 1.6M 1月 12 15:17 core.15262 root@ARM ~/src/git/LuaJIT/src # gdb luajit core.15262 需要先是能core机制 ulimit -c unlimited 默认core文件生成在当前目录下 gdb可以直接debug这个core文件 gdb program core-file-name 在thunder上的sysdig 好像用不起来... cmake -DUSE_BUNDLED_LUAJIT=OFF .. sysdig初体验 安装: 我是去sysdig官网下的deb安装包 基本使用 sysdig不加参数会输出系统实时的系统调用, 所有进程的, 感觉没什么大用. sudo sysdig proc.name=sshd 这种只观察一个进程的格式可能更有用一点, 但其实和strace -p差不多. 所有的输出可以先写到文件里, 然后读出来, 比如sysdig -w mint.scap sysdig -r mint.scap 凿子 sysdig里面有个很有意思的新名词chisel(凿子), 它是sysdig用来分析上面的事件流的一些脚本, 用 sysdig -cl来查看所有支持的chisel, 按app, cpu, net, io, performance等等来分了好几类 比如sudo sysdig -c topprocs_net 显示最占网络的程序 sudo sysdig -c topprocs_time 显示最占时间的文件 sudo sysdig -c topprocs_file 显示读写文件最多的进程 实例 我在局域网用rsync在拷贝一个大文件, 速度大约11M/s, 在服务端: sudo sysdig -c topprocs_file 显示rsync最占文件操作, 11M/s sudo sysdig -c topprocs_net 显示sshd最占网络, 11M/s 那么很显然, rsync负责读取文件内容, sshd负责传输. sudo sysdig -c fileslower 1 显示IO调用延时超过1ms的 sysdig -i fileslower 用-i选项可以查看如何使用 sudo sysdig -c spy_users 这个可以查看系统中所有用户的操作 sudo sysdig -A -c spy_ip 192.168.2.13 这个厉害了, 可以检测和192.168.2.13的数据传输. 但似乎ping报文没法监控. -A的意思是ascii方式显示 显示cat打开的所有文件的名称, 注意evt.arg.name就表示这个名称, -p%是打印格式$ sudo sysdig -p\"%evt.arg.name\" proc.name=cat and evt.type=open /etc/ld.so.cache /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/locale/locale-archive /home/byj/tmp/octeon_edac-lmc.c.patch 注意上面的例子使用了条件, proc.name=和evt.type, sysdig支持的条件可以用sysdig -l来查看 过滤器可以使用比较操作符 如 (=, !=, , >=, contains) 且可以联合布尔操作符 (and, or and not) 还有括号 ulimit进程数限制 vim /etc/security/limits.conf vim /etc/security/limits.d/20-nproc.conf 在thunder上做盘 export THUNDER_ROOT=/root/src/ThunderX-SDK lsblk ./create_disk.sh --raw-disk /dev/sdb rtc时间 date 123010372015 hwclock -s 读rtc到系统时间 hwclock -w 把系统时间写入rtc SATA slow sync问题 工具: iobench 关键词: softirq smp_function_call IPI 关于smp function call, 它的目的是让另外一个cpu来执行一个函数 linux/kernel/linux-aarch64/block/blk-softirq.c 比如在本cpu上执行/* * Setup and invoke a run of 'trigger_softirq' on the given cpu. */ static int raise_blk_irq(int cpu, struct request *rq) { if (cpu_online(cpu)) { struct call_single_data *data = &rq->csd; data->func = trigger_softirq; data->info = rq; data->flags = 0; smp_call_function_single_async(cpu, data); return 0; } return 1; } 它用IPI(核间中断)来通知另一个CPU来执行下面的函数, 这个函数是static void trigger_softirq(void *data) { struct request *rq = data; unsigned long flags; struct list_head *list; local_irq_save(flags); list = this_cpu_ptr(&blk_cpu_done); list_add_tail(&rq->ipi_list, list); if (list->next == &rq->ipi_list) raise_softirq_irqoff(BLOCK_SOFTIRQ); local_irq_restore(flags); } 使用native gcc的变量设置 #!/bin/sh export MY_ROOT_DIR=`pwd` #export PATH=$MY_ROOT_DIR/gcc5-native/bin:$PATH #export LD_LIBRARY_PATH=$MY_ROOT_DIR/gcc5-native/lib64 export PATH=$MY_ROOT_DIR/thunderx-tools-414/aarch64-thunderx-linux-gnu/sys-root/usr/bin:$PATH export LD_LIBRARY_PATH=$MY_ROOT_DIR/thunderx-tools-414/aarch64-thunderx-linux-gnu/sys-root/lib64 smp_affinity和rps_cpus 简单的说, 两个都是均衡网卡中断的, 区别似乎是: smp_affinity是多队列网卡多中断情况下用的, 硬件中断到哪个核处理 rps_cpus是硬件中断搞完了以后, 可以把接下来的下半部交给其他核处理. 如果为0, 则默认还是本核处理 总结下来, 中断来了, 先看smp_affinity到哪个核, 此时是硬中断流程. 硬中断处理完了, 再看rps_cpus, 分发到哪个核上做下半部. mysql调试方法论之各种调试和检测命令 首先要对mysql特别有研究, 对代码, 各种选项, performance-schema=1, 这是第一点. 然后是各种系统工具的使用, 比如vmstat, mpstat, top, ps, 网络工具等等 mpstat -P ALL 查看所有cpu的状态 vmstat -w 然后是用perf来分析hotpot代码 最后是debug代码的手段: 比如pstack和gdb的使用, 如果程序跑到了一个持续的异常状态, 比如sysbench测试的时候每秒统计都是0, 这时需要先pstack来查调用栈, 在pstack执行期间, 这个进程的所有线程都会被停住, 从调用栈里面查看端倪. 下一步就是用gdb -p去跟. pstack -p strace -p gdb -p sysdig lttng 以上所有这些都是手段, 其实真正的东西是, 理解内核, 理解libc, 理解应用, 都要深入理解. vmstat命令 vmstat -w 5 $((3600*3)) 3小时, 输出对齐 获取所有cpu的利用率信息 mpstat -P ALL 5 1 iostat -x /dev/sda pstack命令 pstack命令可以打印进程的调用栈. 基本原理是pstack告诉kernel保存一个这个进程的栈快照, 然后show出来. mysql性能提升之网口中断 We don't have hardware assisted flow director and having more number of queues means interrupts are shared on many CPUs and mysql threads frequently get context switched. Hence low performance. Right from beginning we always saw better performance with leaving cores 0-8 aside and pinning mysql to CPUs 9-47 (of node0). If possible, can you try one experiment (with below settings) and see if that benefits. Replace eth0 with whatever interface you are using. echo 0 > /sys/class/net/eth0/queues/rx-0/rps_cpus echo 0 > /sys/class/net/eth0/queues/rx-1/rps_cpus echo 0 > /sys/class/net/eth0/queues/rx-2/rps_cpus echo 0 > /sys/class/net/eth0/queues/rx-3/rps_cpus echo 0 > /sys/class/net/eth0/queues/rx-4/rps_cpus echo 0 > /sys/class/net/eth0/queues/rx-5/rps_cpus echo 0 > /sys/class/net/eth0/queues/rx-6/rps_cpus echo 0 > /sys/class/net/eth0/queues/rx-7/rps_cpus echo 32768 > /proc/sys/net/core/rps_sock_flow_entries echo 4096 > /sys/class/net/eth0/queues/rx-0/rps_flow_cnt echo 4096 > /sys/class/net/eth0/queues/rx-1/rps_flow_cnt echo 4096 > /sys/class/net/eth0/queues/rx-2/rps_flow_cnt echo 4096 > /sys/class/net/eth0/queues/rx-3/rps_flow_cnt echo 4096 > /sys/class/net/eth0/queues/rx-4/rps_flow_cnt echo 4096 > /sys/class/net/eth0/queues/rx-5/rps_flow_cnt echo 4096 > /sys/class/net/eth0/queues/rx-6/rps_flow_cnt echo 4096 > /sys/class/net/eth0/queues/rx-7/rps_flow_cnt 解决sync flood问题 现象是[142943.335106] TCP: request_sock_TCP: Possible SYN flooding on port 25616. Sending cookies. Check SNMP counters. [143245.955016] TCP: request_sock_TCP: Possible SYN flooding on port 25616. Sending cookies. Check SNMP counters. [143397.874323] TCP: request_sock_TCP: Possible SYN flooding on port 25616. Sending cookies. Check SNMP counters. [146560.278277] TCP: request_sock_TCP: Possible SYN flooding on port 25616. Sending cookies. Check SNMP counters. 解决 Set net.ipv4.tcp_max_syn_backlog and net.core.somaxconn to large enough values (I used 4096 for both) Set back_log in my.cnf to a large enough value (I used 4096) mysql调优之瓶颈在哪? 理论 mysql专家:Aleksei Kopytov Alexey kaamos@yandex.ru 在升级了新的kernel到4.2之后, throughput下降了, 但专家在看了mysql的自有的调试框架的输出之后, 反而觉得新内核更好. 为什么呢? 专家使用了一个很形象的比喻: 这就像是公路上的堵车现象, 原来有两个地方比较窄, 大家一次通过这两个地方, 车流量还是可以的; 但一旦第一个窄口子变宽了之后, 会有更多的车来到第二个窄口, 此时这里的竞争更加激烈, 互不相让, 结果就是车流量反而下降了. mysql调优之innodb_spin_wait_delay = 30 在用了新的内核之后, throughput降低了(50K), 但是通过专家的发现, mutex耗时最长(这里的mutex已经是用户态的spinlock, 用了atomic的东东). 当innodb_spin_wait_delay = 100时, throughput提高了(到70K). 这里这个东西的含义是, 当竞争一个锁的时候, 程序先spin多久, 然后就放弃CPU进入睡眠. 当核多的时候, 大家当然要等的时间多些. memcpy和strcpy 理论上, memcpy肯定比strcpy快. 但我们在调试translator的时候, 发现strcpy反而更快一点. 为什么呢? 因为malloc的对齐问题! 修改m_alloc函数, 这个函数的入参size表示要申请多少个字节, 在m_alloc函数内部, 把这个size强制16字节对齐, ullMemSize = (ullMemSize+16)&~0xf; 结果快很多! 对一个频繁操作字符串或者内存的应用来说, 这点影响至关重要! perf调试单进程, 调用栈? 用-g 选项 numactl numactl可以控制一个进程以及他的所有子进程的numa运行模式, 比如绑定到某个node上. 预取 prefetch 如果 perf发现热点代码在ldr xxx, 说明它在不断load什么东西. 此时用prefetch会好点. #define PRFM(x,y) asm(\"prfm pldl1strm, [%x[a], %[off] ]\" : : [a]\"r\"(x), [off]\"i\"(y)) float Decoder::CalcModelScore( Hypothesis* hypo ) { float score = 0.0; size_t i = 0; std::vector &mfw = m_model->m_featWeights; std::vector::const_iterator mfwi = mfw.begin(); float *hmfs = hypo->m_featureScoreVec; size_t limit = m_model->m_featNumber; PRFM(hmfs, 0); PRFM(hmfs, 128); PRFM(hmfs, 256); PRFM(&(*mfwi), 0); PRFM(&(*mfwi), 128); PRFM(&(*mfwi), 256); if (limit & 1) { score = hmfs[i] * mfw[i]; i++; } #if 1 for( ; i m_featNumber; i++ ) { score += hypo->m_featureScoreVec[i] * m_model->m_featWeights[i]; } #endif #endif return score; } gdb调试多进程 搜索 gdb follow fork 来查看gdb关于folk的选项. gdb 可以同时调试几个进程 比如gdb调试父进程的时候, 如果父进程fork了一次, gdb默认的行为是detach子进程, 只跟踪父进程. 如果子进程里面曾经打了断点, 则子进程会收到SIGTRAP信号而终止(如果没有显式接管), 有个选项可以改变这个默认行为, 使gdb可以也可以跟踪子进程, 但统一时刻只能跟踪一个进程, 未被跟踪的进程是suspended状态. inferior 选择调试哪个父子?进程 info source可以显示当前调试位置的源码 aj kernel git换remote? https://github.com/ajasty-cavium/linux git clone -b aj-alibaba bill@192.168.137.197:src/linux-aarch64 换成官网的 git remote set-url origin git://cagit1.caveonetworks.com/thunder/sdk/linux-aarch64.git 为了防止换了url以后git pull从新的url的default branch来取东西, 因为git pull是fetch远程的默认分支, 在merge到本地分支. git remote set-branches origin aj-alibaba git remote set-head origin aj-alibaba 交叉编译kernel export ARCH=arm64 export CROSS_COMPILE=aarch64-thunderx-linux-gnu- export PATH=/home/byj/repo/git/thunder/new/sdk-master.change/tools/bin:$PATH cp alibaba.config .config make olddefconfig make Image -j4 make modules -j4 pssh可以在多个机器同时执行命令 pssh可以做下面脚本的事情 apt install pssh 在多个服务器上同时执行命令的脚本 在ali有10台机器, 经常要同时执行同样的命令. 一台一台来执行太麻烦, 有没有一个脚本可以同时执行呢? 有, 用到了下面的ssh免密码功能, 以及ssh命令可以后面跟字符串来在远端机器上执行. 以及eval #! /bin/bash B=\"\\033[1;37;40m\" N=\"\\033[0m\" atlocal=F servers=\" yingjie@192.168.85.10 byj@localhost \" cvmservers=\" root@10.97.219.6 root@10.97.219.55 root@10.97.219.21 root@10.97.219.44 root@10.97.219.50 root@10.97.219.53 root@10.97.219.214 root@10.97.219.13 root@10.97.219.47 root@10.97.219.69 \" if [ \"$1\" = \"-l\" ]; then atlocal=T shift fi for i in $servers; do ip=${i#*@} echo echo -e $B\">>>$i\"$N if [ \"$atlocal\" = \"T\" ]; then eval \"$*\" else ssh $i \"$*\" fi done 使用时: 这个命令要在10台机器上执行byj@mint ~/repo/git/thunder/new/alibaba $ ./alldo.sh ls 这个命令要在本地机器执行, 但要对每个服务器的ip进行某种操作byj@mint ~/repo/git/thunder/new/alibaba $ ./alldo.sh -l ping \\$ip 注意需要用\\来转义$, 因为我们希望$ip作为字符串传给脚本, 脚本里面用eval \"$*\"来执行. 注意, 如果这里不用eval的话, 脚本里面的$ip是不会被传递到命令的. >里面有详细的eval的原理的解释.我的理解是: 普通的\"$\"的执行方法, 并不是在执行扩展后的$, 而是省略了命令解析的前面很多步骤, 比如解析特殊符号, 管道符, 分号, 重定向等. 而eval的好处在于, 它把后面的字符串按照命令格式重新解析一遍, 它的效果相当于在shell窗口敲了一遍这个命令. 至于为什么eval会继承$ip, 我认为eval是在当前shell进程执行的. 而\"$*\"会新建进程来执行. ssh免密码登录 \"公私钥\"认证方式简单的解释:首先在客户端上创建一对公私钥 （公钥文件：~/.ssh/id_rsa.pub； 私钥文件：~/.ssh/id_rsa）。然后把公钥放到服务器上（~/.ssh/authorized_keys）, 自己保留好私钥.在使用ssh登录时,ssh程序会发送私钥去和服务器上的公钥做匹配.如果匹配成功就可以登录了。 用A登录B 在A上 ssh-keygen -t rsa -P '' 它在/home/byj/.ssh/下面生成私钥id_rsa和公钥id_rsa.pub 然后把公钥/home/byj/.ssh/id_rsa.pub写到B的文件~/.ssh/authorized_keys里 cat ~/.ssh/id_rsa.pub | ssh yingjie@192.168.85.10 \"cat >> ~/.ssh/authorized_keys\" 需要的话要改B上的访问权限 chmod 700 .ssh; chmod 640 .ssh/authorized_keys 然后就可以直接从A登录到B了 ssh yingjie@192.168.85.10 ssh远程执行命令 ssh会接受一个字符串, 登录后在目的机器执行 ssh xxx@xxx \"command;next command;next command\" cpu offline 动态 #echo 0 > /sys/devices/system/cpu/cpuX/online cat /proc/cpuinfo | grep nid | wc -l dd原理 每个字节都被复制 dd if=/dev/sdX of=/dev/sdY bs=512 conv=noerror,sync This will clone the entire drive, including the MBR (and therefore bootloader), all partitions, UUIDs, and data. noerror instructs dd to continue operation, ignoring all read errors. Default behavior for dd is to halt at any error. sync fills input blocks with zeroes if there were any read errors, so data offsets stay in sync. bs=512 sets the block size to 512 bytes, the \"classic\" block size for hard drives. If and only if your hard drives have a 4 Kib block size, you may use \"4096\" instead of \"512\". Also, please read the warning below, because there is more to this than just \"block sizes\" -it also influences how read errors propagate. UUID启动 不用sda sdb 本来想用 root=UUID=xxx来指定UUID, 但是kernel不认, 系统会一直等待. 这样写就OK了: menuentry 'Thunder Fedora default Boot' { linux /boot/Image root=PARTUUID=5105e002-fe7d-4fc6-8b78-c9fc7cad3d84 console=ttyAMA0,115200n8 earlycon=pl011,0x87e024000000 coherent_pool=16M rootwait rw transparent_hugepage=never boot } 正确的写法是: root=PARTUUID=xxx 这里需要说明的是,UUID和PARTUUID是不一样的, 新版本的blkid能够show出来. UUID是文件系统的ID, 而PARTUUID是分区的ID. UUID在initrd中能被识别, 而PARTUUID在kernel就能识别. 下面的说明很详细 The parameter you have to pass to boot from UUID is PARTUUID. So it should be root=PARTUUID=666c2eee-193d-42db-a490-4c444342bd4e. The documentation explains why it's coming back with unknown-block(0,0): kernel-parameters.txt: root= [KNL] Root filesystem See name_to_dev_t comment in init/do_mounts.c. init/do_mounts.c: /* * Convert a name into device number. We accept the following variants: * * 1) device number in hexadecimal represents itself * 2) /dev/nfs represents Root_NFS (0xff) * 3) /dev/ represents the device number of disk * 4) /dev/ represents the device number * of partition - device number of disk plus the partition number * 5) /dev/p - same as the above, that form is * used when disk name of partitioned disk ends on a digit. * 6) PARTUUID=00112233-4455-6677-8899-AABBCCDDEEFF representing the * unique id of a partition if the partition table provides it. * The UUID may be either an EFI/GPT UUID, or refer to an MSDOS * partition using the format SSSSSSSS-PP, where SSSSSSSS is a zero- * filled hex representation of the 32-bit \"NT disk signature\", and PP * is a zero-filled hex representation of the 1-based partition number. * 7) PARTUUID=/PARTNROFF= to select a partition in relation to * a partition with a known unique id. * * If name doesn't have fall into the categories above, we return (0,0). * block_class is used to check if something is a disk name. If the disk * name contains slashes, the device name has them replaced with * bangs. */ So I guess that the real answer is that the kernel does not support root=UUID, only root=PARTUUID. If you want to use a filesystem UUID, I guess you need an initramfs that can handle mounting filesystems by UUID. Just to clarify UUIDs are the only reliable way for the kernel to identify hard drives. There are two types: UUID, which is stored in the filesystem and is not available to the kernel at boot-time, and PARTUUID, which is stored in the partition table and IS available at boot time. So you have to use root=PARTUUID=SSSSSSSS-PP as /dev/sd?? can change with devices plugged/unplugged. Don't forget to capitalize the hexadecimal number SSSSSSSS-PP you get from blkid! The more easy to use root=LABEL= root=UUID= only work with an initramfs that fetches these identifiers. So, if you use a non-empty initramfs, you can have all three! With an empty initramfs, you only have PARTUUID. 重做fedora 需要root cd /home/byj/repo/git/thunder/new/sdk-master . env-setup BOARD_TYPE=crb-2s cd /home/byj/repo/git/thunder/new/sdk-master/host/bin ./create_disk.sh --raw-disk /dev/sdc cd /home/byj/repo/git/thunder/new/fs mount /dev/sdc2 mnt cd mnt tar xvf ../fedora-with-native-kernel-repo-gcc-update-to-20150423-factory.tar.bz2 cd .. umount mnt cd /home/byj/repo/git/thunder/new/sdk-master/host/bin ./create_disk.sh --install-grub2 /dev/sdc1 /dev/sdc2 /boot ./create_disk.sh --install-modules /dev/sdc2 fedora网口配置 静态IProot@ARM /etc # cat ./sysconfig/network-scripts/ifcfg-enP2p1s0f1 DEVICE=enP2p1s0f1 USERCTL=no ONBOOT=yes BOOTPROTO=none USERCTL=no IPADDR=10.97.219.47 NETMASK=255.255.255.0 GATEWAY=10.97.219.247 DNS1=10.137.59.1 bondroot@ARM /etc/sysconfig/network-scripts # cat ifcfg-bond0 DEVICE=bond0 ONBOOT=no BOOTPROTO=none USERCTL=no BONDING_OPTS=\"mode=4 miimon=100 xmit_hash_policy=1\" root@ARM /etc/sysconfig/network-scripts # cat ifcfg-enP6p1s0f1 DEVICE=enP6p1s0f1 USERCTL=no ONBOOT=no BOOTPROTO=none 查看10台系统是否online $ cat macip.table | cut -d ' ' -f2 | xargs -i ping {} -c 3 | egrep -i \"statistics|transmitted\" check cpld 版本 /usr/local/bin/cpldaccess --get-data 0x29 写mac地址 Enter UEFI shell. setvar N0ETH0 -guid A70B59ED-6228-4883-BBF0-5FD91C14EFF6 -bs -rt -nv =0x123456789abc02 setvar N1ETH0 -guid A70B59ED-6228-4883-BBF0-5FD91C14EFF6 -bs -rt -nv =0x123456789abc01 编译gcc 需要先装一些依赖包yum install gcc-c++ texi2html yum install texi2html -y yum install vim yum install makeinfo yum install texinfo yum install make yum install termcap yum install ncurses yum install m4 yum install flex bison -y yum install ncurses-devel.aarch64 yum install gmp yum install gmp-devel yum install mpfr-devel mpc-dev -y yum install mpc-devel -y yum install gmpc-devel.aarch64 yum install mpc.aarch64 yum install libmpc-devel.aarch64 编译gccINSTALL_DIR=${HOME}/gcc5-native rm -Rf ${INSTALL_DIR} cd binutils rm -Rf objdir mkdir objdir cd objdir ../configure --disable-libsanitizer --prefix=${INSTALL_DIR} --with-cpu=thunderx --enable-languages=c,c++,fortran --disable-werror make -j48 make install cd ../../ cd gcc rm -Rf objdir mkdir objdir cd objdir ../configure --disable-libsanitizer --prefix=${INSTALL_DIR} --with-cpu=thunderx --enable-languages=c,c++,fortran --disable-werror make -j48 make install cd ../../ 96个核的异常问题 现象是96个核跑ODPS的时候, 会有96个worker, 其中有一两个worker会挂掉. 异常打印显示是访问0地址异常, 此时的重要特征是PC为0; 为什么PC都为0了呢? 谁踩了内存吗? 这个问题我们后面再议, 我们先要弄清楚另外一个问题, 我们不知道哪个worker会挂掉, 怎么调试? 首先, 那个异常打印能提供很多信息, 比如pc为0, 但lr指针不为0, 通过这个指针, 可以找到bug在代码里大致的位置. dmesg | tail -30 然后, 通过来调试. 大牛的办法是, 打开core dump功能, 这样当某个worker挂掉的时候, 会产生core文件. 这里我把用到的命令大约列以下: thread apply all bt info sharedlibrary info thread thread 84 up p $x1 //打印寄存器x1 p/x *((long ***)($29+32)) 通过分析bug附近的汇编代码, 大体脉络如下: 最终, 程序是要调用一个类的成员函数: someclass->run() 这就需要先找到someclass的指针, 然后找到run的地址; 但是这个地址是0, 导致pc跳到0去执行. 为什么是0呢? 最后分析到锁的问题. 一般锁的流程是这样的 lock(v) modify(something) unlock(v) 这个问题出现的背景是: Thunder有个write buffer, 会做merge, 导致内存写是out of order的. 锁的本质是从0(可获取锁) 到1(占有锁) 再到0(释放锁, 此时别人可以获取锁) 而按照上面上锁的流程, 这三句的执行顺序是 lock=1 some_var=any lock=0 因为我们有write buffer, 所以有可能lock=1 和lock=0被merge了, 准确的说, lock=1进入 write buffer做为entry 0, 然后修改被保护的变量some_var 做为entry1, 而lock=0进入write buffer的时候, 它可能被 merge到entry0, 导致lock从0到1再0的过程在wirte buffer里只有0. 从外面的核看起来, 这个锁一直都是0. 所以别人也在改那个关键变量, 导致问题. 解决办法是, 在unlock之前加mb(), memory barriar lock(v) modify(something) --在这里加mb(), 对ARM来说是DMB指令. unlock(v) 原来的代码是这样的： easy/src/include/easy_atomic.h easy_atomic_barrier() {__asm__ (\"\" ::: \"memory\");} easy_trylock(lock) (*(lock) == 0 && easy_atomic_cmp_set(lock, 0, 1)) easy_unlock(lock) {__asm__ (\"\" ::: \"memory\"); *(lock) = 0;} 新的代码: easy/src/include/easy_atomic.h easy_atomic_barrier() {__asm__ (\"dmb ish\" ::: \"memory\");} easy_trylock(lock) (*(lock) == 0 && easy_atomic_cmp_set(lock, 0, 1)) easy_unlock(lock) {__asm__ (\"dmb ish\" ::: \"memory\"); *(lock) = 0;} [byj]注:gcc4.9.2提供这个函数 void __atomic_thread_fence (int memmodel) 反汇编也是:dmb ish 0000000000000000 : 0: d5033bbf dmb ish 4: d65f03c0 ret 关于rps_cpus 我们的系统里默认是把rps_cpus打开了, 这样的好处是对网口的一个queue来说, 硬件中断到一个CPU, 但是可以用几个CPU来处理接下来的软中断(应该是在跑linux网络协议栈) 我们vnic有8个queue, 绑定到1~8号core来处理硬中断, 如果不打开rps_cpu, 那就只有这8个CPU来跑接下来的软中断; 而如果打开了rps_cpus, 那接下来的软中断可能跑在所有的CPU上(有个mask来管的). 此时观察到的现象是, 用top来看, si这一项在所有CPU上都有值. 而我们在测试mysql时, 采用的策略是关闭rps_cpu, 为什么呢? 因为我们发现, 测mysql时, 系统的负载并不高, 网络负载也不高. 此时8个CPU处理网络就足够了, 其他core都跑mysql. 所以我们不需要把si都分到所有core上来跑, 这样反而会影响mysql, 比如导致cache miss升高. echo 0 > /sys/class/net/eth4/queues/rx-0/rps_cpus 在测试ODPS的时候, 网络性能很差, 10G的接口只有2G左右的带宽. 在调试的时候发现, rps_cpus是全f cd /sys/class/net/neP6p1s0f1/queues/rx-0 cat rps_cpus ffffffff,fffdffff,ffffffff 这里就有个问题, 这个是2个node的系统, 这里把所有cpu的rps都打开了, 那么会有很大可能是node1来处理node0的软中断, 这样效率低. 所以要这样改: enP6p1s0f1是node1的接口 echo ffffffff,fffd0000,00000000 > /sys/class/net/neP6p1s0f1/queues/rx-0/rps_cpus xps_cpu 对应的, 有个xps_cpu的东东. 我理解, CPU发包, 那么此时CPU是固定的, 下面就是选择往那个q里面发, 这个q一般是hash来的, 这就导致了从全局来看, CPU和q是多对多的关系, 而xps_cpu是一个对每个q的一个CPU set的限制, 也就是说, 一个q, 只能对应这个set里面的CPU, 要获得更好的局部性, 个人觉得可以一对一. 这个patch主要是针对多队列的网卡发送时的优化，当发送一个数据包的时候，它会根据cpu来选择对应的队列，而这个cpu map可以通过sysctl来设置： /sys/class/net/eth/queues/tx-/xps_cpus 这里xps_cpus是一个cpu掩码，表示当前队列对应的cpu。 而xps主要就是提高多对列下的数据包发送吞吐量，具体来说就是提高了发送数据的局部性。按照作者的benchmark，能够提高20%. 原理很简单，就是根据当前skb对应的hash值(如果当前socket有hash，那么就使用当前socket的)来散列到xps_cpus这个掩码所 设置的cpu上，也就是cpu和队列是一个1对1，或者1对多的关系，这样一个队列只可能对应一个cpu，从而提高了传输结构的局部性。 没有xps之前的做法是这样的，当前的cpu根据一个skb的4元组hash来选择队列发送数据，也就是说cpu和队列是一个多对多的关系，而这样自然就会导致传输结构的cache line bouncing。 网络时间 ntpdate ntpdate s2g.time.edu.cn irq绑定 irq_set_affinity() #(eth, irq, desired_core) { DEV=\"$1\" IRQ=\"$2\" VEC=\"$3\" MASK_FILL=\"\" MASK_ZERO=\"00000000\" let \"IDX = $VEC / 32\" for ((i=1; i /proc/irq/$IRQ/smp_affinity } 使用时, 比如对第161号中断, 绑定到core95上, 此时eth0没用 $ irq_set_affinity eth0 161 95 eth0 mask=80000000,00000000,00000000 for /proc/irq/161/smp_affinity q_set_affinity() #(eth, queue_num, desired_core) { DEV=\"$1\" QUE=\"$2\" VEC=\"$3\" MASK_FILL=\"\" MASK_ZERO=\"00000000\" let \"IDX = $VEC / 32\" for ((i=1; i /proc/irq/$IRQ/smp_affinity } 使用时, 对eth9的第15个q绑定到core35上 $ q_set_affinity eth9 15 35 eth9 mask=8,00000000 for /proc/irq/125/smp_affinity tubo错误分析 tubo[85270]: unhandled level 1 translation fault (11) at 0xffffff1c000bc8, esr 0x92000005 >>> dr(0x92000005,[31,26],25,[24,0]) [31-26] Bin: 100100 Dec: 36 Hex: 0x24 [25] Bin: 1 Dec: 1 Hex: 0x1 [24-0] Bin: 0000000000000000000000101 Dec: 5 Hex: 0x5 所以: EC:100100 IL: 1 ISS:0000000000000000000000101 详细解释: arm64体系规定的虚拟地址有三种:39bit 42bit 48bit(numa用) 所以最高几位还有空余, arm规定最高8位可以用作tag, 硬件在地址翻译的时候会忽略掉tag. 但这里发生的情况是: ali自己写的代码里面, 手动修改?(增加然后删除)tag, 但这个tag是16位的. 这个是用户态代码, 用户态的地址空间是0x0000_0000_0000_0000到0x0000_ffff_ffff_ffff 比如原来的地址是0x0000_fxxx_xxxx_xxxx, 在去掉tag的时候, 代码里面?会做最高位的符号扩展, 导致扩展到内核空间?0xffff_fxxx_xxxx_xxxx 最后的修改方案是最高16位强制填0 ethtool可以配置驱动 这个把eth3的q设为8个 ethtool eth3 -L combined 8 用小l可以查当前的配置 [root@cavium tmp]# ethtool -l eth8 Channel parameters for eth8: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 16 为什么ulimit -n的值不能改? 修改file-max echo 6553560 > /proc/sys/fs/file-max sysctl -w \"fs.file-max=34166\"，前面2种重启机器后会恢复为默认值 或vim /etc/sysctl.conf, 加入以下内容, 重启生效 fs.file-max = 6553560 修改ulimit的open file，系统默认的ulimit对文件打开数量的限制是1024 ulimit -HSn 102400 这只是在当前终端有效，退出之后，open files又变为默认值。当然也可以写到/etc/profile中，因为每次登录终端时，都会自动执行/etc/profile 或vim /etc/security/limits.conf 加入以下配置，重启即可生效 ``` soft noproc 65535 hard noproc 65535 soft nofile 65535 hard nofile 65535 ``` atomic解决mysql崩溃问题, weak order 修改原子锁： 正确的修改原子锁方法 vim storage/innobase/include/os0sync.h # define os_atomic_test_and_set_byte(ptr, new_val) \\ __atomic_exchange_n (ptr,new_val, __ATOMIC_ACQ_REL) /*__sync_lock_test_and_set(ptr, (byte) new_val)*/ # define os_atomic_test_and_set_ulint(ptr, new_val) \\ __atomic_exchange_n (ptr,new_val, __ATOMIC_ACQ_REL) /*__sync_lock_test_and_set(ptr, new_val)*/ vim ./include/atomic/gcc_builtins.h #define make_atomic_fas_body(S) \\ v= __atomic_exchange_n (a,v, __ATOMIC_ACQ_REL); /* v= __sync_lock_test_and_set(a, v);*/ "},"notes/as_title_embedded.html":{"url":"notes/as_title_embedded.html","title":"嵌入式系统开发调试","keywords":"","body":"如题 "},"notes/embedded_debugging.html":{"url":"notes/embedded_debugging.html","title":"嵌入式调试杂记","keywords":"","body":" 查看调用栈 内核态 用户态 ping收到杂包问题 现象 ping的基本流程 ping都收到了什么报文? 为什么会收到127.0.0.1的ICMP报文? 结论 查看调用栈 当板子上程序跑死的时候,看调用栈 内核态 /proc/1080 # cat stack [] do_signal_stop+0x140/0x200 [] get_signal+0x270/0x5d8 [] do_signal+0x24/0x210 [] do_notify_resume+0x58/0x70 [] work_notifysig+0x10/0x18 用户态 使用ptrace库和libunwind库 #include #include #include #include 流程 //unwind创建space unw_create_addr_space(&_UPT_accessors, 0); //用ptrace attach到目标tid上 ptrace(PTRACE_ATTACH, tracee_tid, 0, 0) //等待pid ret_pid = waitpid(tracee_tid, &status, __WALL); //用libunwind解析栈 unw_init_remote(&cursor, as, ui); unw_get_reg(&cursor, UNW_REG_IP, &ip) unw_step(&cursor) //打印栈 //销毁space unw_destroy_addr_space(as); 用户态也可以用pstack, 但需要板子上有gdb. ping收到杂包问题 现象 板子的一个脚本里面, 一直运行一个ping命令: ping 169.254.1.253 -W 1 -c 3600 -p 42 -s 16 超时1秒, 3600次, 填充0x42, 报文size 16 不打流时, ping进程的CPU占用非常低, 几乎没有; 但打流的时候, ping进程的CPU占用高到10%以上. ping的基本流程 用strace看ping的流程, 基本上 先用socket(AF_INET, SOCK_RAW, IPPROTO_ICMP) = 3创建socket ping会查dns, 方法是socket(AF_INET, SOCK_DGRAM, IPPROTO_IP) = 5 connect(5, {sa_family=AF_INET, sin_port=htons(1025), sin_addr=inet_addr(\"169.254.1.253\")}, 16) = 0 getsockname(5, {sa_family=AF_INET, sin_port=htons(37650), sin_addr=inet_addr(\"192.168.0.14\")}, [16]) = 0 设置socket属性setsockopt(3, SOL_RAW, ICMP_FILTER, ~(1 发送ICMP请求sendto(3, \"\\10\\0a\\27g\\326\\0\\1&\\220G_\\0\\0\\0\\0\\273!\\6\\0\\0\\0\\0\\0\", 24, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"169.254.1.253\")}, 16) = 24 # 注意, 这里收到了\"192.168.0.1\"的ICMP报文? recvmsg(3, {msg_name={sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"192.168.0.1\")}, msg_namelen=128->16, msg_iov=[{iov_base=\"E\\0\\0,\\351\\332\\0\\0@\\1\\17\\227\\300\\250\\0\\1\\300\\250\\0\\16\\0\\0L\\252Z\\374\\f &\\220G_\"..., iov_len=152}], msg_iovlen=1, msg_control=[{cmsg_len=32, cmsg_level=SOL_SOCKET, cmsg_type=SCM_TIMESTAMP, cmsg_data={tv_sec=1598525478, tv_usec=674988}}], msg_controllen=32, msg_flags=0}, 0) = 44 set filter setsockopt(3, SOL_SOCKET, SO_ATTACH_FILTER, {len=8, filter=0x5588bb473020}, 16) = 0 收到ICMP响应 poll([{fd=3, events=POLLIN|POLLERR}], 1, 513) = 0 (Timeout) sendto(3, \"\\10\\0\\333\\311i\\0\\0\\2J\\221G_\\0\\0\\0\\0\\31C\\10\\0\\0\\0\\0\\0\", 24, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"169.254.1.253\")}, 16) = 24 recvmsg(3, {msg_namelen=128}, 0) = -1 EAGAIN (Resource temporarily unavailable) sendto(3, \"\\10\\0\\252Xi\\0\\0\\3K\\221G_\\0\\0\\0\\0I\\263\\10\\0\\0\\0\\0\\0\", 24, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"169.254.1.253\")}, 16) = 24 recvmsg(3, {msg_namelen=128}, 0) = -1 EAGAIN (Resource temporarily unavailable) ping都收到了什么报文? 用strace看ping进程的系统调用strace -p 18060 -s 256 -x, 发现正常情况下能收到169.254.1.253的ICMP响应报文 --- SIGALRM {si_signo=SIGALRM, si_code=SI_KERNEL} --- clock_gettime(CLOCK_MONOTONIC, {tv_sec=64787, tv_nsec=218738867}) = 0 sendto(0, \"\\x08\\x42\\x81\\xa4\\x46\\x8c\\x0b\\x2f\\x15\\x9e\\x81\\x32\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\", 24, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"169.254.1.253\")}, 28) = 24 rt_sigaction(SIGALRM, {sa_handler=0x10021800, sa_mask=[ALRM], sa_flags=SA_RESTART}, {sa_handler=0x10021800, sa_mask=[ALRM], sa_flags=SA_RESTART}, 16) = 0 setitimer(ITIMER_REAL, {it_interval={tv_sec=0, tv_usec=0}, it_value={tv_sec=1, tv_usec=0}}, NULL) = 0 rt_sigreturn({mask=[]}) = 6044 recvfrom(0, \"\\x45\\x00\\x00\\x2c\\x50\\xf3\\x00\\x00\\x40\\x01\\xd2\\xdf\\xa9\\xfe\\x01\\xfd\\xa9\\xfe\\x01\\x05\\x00\\x42\\x89\\xa4\\x46\\x8c\\x0b\\x2f\\x15\\x9e\\x81\\x32\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\\x42\", 152, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"169.254.1.253\")}, [16]) = 44 clock_gettime(CLOCK_MONOTONIC, {tv_sec=64787, tv_nsec=221353769}) = 0 write(1, \"24 bytes from 169.254.1.253: seq=2863 ttl=64 time=2.615 ms\\n\", 59) = 59 recvfrom(0, 0x1041f1b0, 152, 0, 0x7f979a80, [16]) = ? ERESTARTSYS (To be restarted if SA_RESTART is set) 打流的情况下, 收到很多127.0.0.1的报文. recvfrom(0, \"\\x45\\xc0\\x00\\xf2\\x92\\x27\\x00\\x00\\x40\\x01\\xe9\\x21\\x7f\\x00\\x00\\x01\\x7f\\x00\\x00\\x01\\x03\\x03\\x1a\\x69\\x00\\x00\\x00\\x00\\x45\\x00\\x00\\xd6\\x58\\x4f\\x40\\x00\\x40\\x11\\xe3\\xc5\\x7f\\x00\\x00\\x01\\x7f\\x00\\x00\\x01\\x9e\\x21\\x02\\x02\\x00\\xc2\\xfe\\xd5\\x3c\\x31\\x35\\x3e\\x41\\x50\\x50\\x5f\\x4e\\x41\\x4d\\x45\\x3a\\x64\\x68\\x63\\x70\\x76\\x34\\x5f\\x72\\x65\\x6c\\x61\\x79\\x5f\\x6c\\x6f\\x67\\x69\\x63\\x5f\\x61\\x70\\x70\\x2c\\x41\\x50\\x50\\x5f\\x56\\x45\\x52\\x53\\x49\\x4f\\x4e\\x3a\\x32\\x30\\x30\\x39\\x2e\\x32\\x35\\x36\\x2c\\x4d\\x4f\\x44\\x55\\x4c\\x45\\x5f\\x4e\\x41\\x4d\\x45\\x3a\\x64\\x68\\x63\\x70\\x5f\\x72\\x65\\x6c\\x61\\x79\\x5f\\x70\\x72\\x6f\\x74\\x6f\\x63\\x6f\\x6c\\x2c\\x41\\x50\\x50\\x5f\\x50\\x48\\x41\", 152, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"127.0.0.1\")}, [16]) = 152 recvfrom(0, \"\\x45\\xc0\\x00\\xf2\\x92\\x28\\x00\\x00\\x40\\x01\\xe9\\x20\\x7f\\x00\\x00\\x01\\x7f\\x00\\x00\\x01\\x03\\x03\\x1a\\x69\\x00\\x00\\x00\\x00\\x45\\x00\\x00\\xd6\\x58\\x50\\x40\\x00\\x40\\x11\\xe3\\xc4\\x7f\\x00\\x00\\x01\\x7f\\x00\\x00\\x01\\x9e\\x21\\x02\\x02\\x00\\xc2\\xfe\\xd5\\x3c\\x31\\x35\\x3e\\x41\\x50\\x50\\x5f\\x4e\\x41\\x4d\\x45\\x3a\\x64\\x68\\x63\\x70\\x76\\x34\\x5f\\x72\\x65\\x6c\\x61\\x79\\x5f\\x6c\\x6f\\x67\\x69\\x63\\x5f\\x61\\x70\\x70\\x2c\\x41\\x50\\x50\\x5f\\x56\\x45\\x52\\x53\\x49\\x4f\\x4e\\x3a\\x32\\x30\\x30\\x39\\x2e\\x32\\x35\\x36\\x2c\\x4d\\x4f\\x44\\x55\\x4c\\x45\\x5f\\x4e\\x41\\x4d\\x45\\x3a\\x64\\x68\\x63\\x70\\x5f\\x72\\x65\\x6c\\x61\\x79\\x5f\\x70\\x72\\x6f\\x74\\x6f\\x63\\x6f\\x6c\\x2c\\x41\\x50\\x50\\x5f\\x50\\x48\\x41\", 152, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"127.0.0.1\")}, [16]) = 152 这些报文都是ICMP报文, 因为socket指定了socket(AF_INET, SOCK_RAW, IPPROTO_ICMP) = 3 额外冲上来的报文的确也都是ICMP的，但是是板内自己的ICMP报文。 应该是当时pppoe_ia_app产生了大量syslog想报给514端口的syslog server，但是server没有开，内核回复了端口不可达的ICMP报文。 报文是从环回口上来的, man recvfrom, src_addr是协议栈填上来的. ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen); ICMP的reply报文, 如果是不可达, 似乎内核会把之前的payload报文封装到不可达报文里. 为什么会收到127.0.0.1的ICMP报文? 答案是本来就要收到.因为socket(AF_INET, SOCK_RAW, IPPROTO_ICMP)的意思就是说IPPROTO_ICMP协议的SOCK_RAW都收上来. busybox版本的ping, 没有实现上面的第5步. 没有filter, 所以ICMP报文都上来了.验证方法也简单, 在ping 169.254.1.253的时候, 再ping 127.0.0.1, 发现第一个ping进程就会收到127.0.0.1的ICMP reply 而iputils的ping, 会set filter. 所以, PC上的ping不会有这个现象 结论 buxybox的ping是精简版, 会收说有的ICMP报文, 除非指定接口, 比如(-I eth0) 通用版本的ping本来就调用了setsockopt(3, SOL_SOCKET, SO_ATTACH_FILTER, {len=8, filter=0x5588bb473020}, 16)接口, 过滤了别的地址的IMCP报文. 用strace抓数据, 放到wireshark里面协议分析是好方法. 参考详细的ping的协议栈流程解析 "},"notes/uboot_杂记.html":{"url":"notes/uboot_杂记.html","title":"uboot杂记","keywords":"","body":" uboot调试 setenv 在uboot下更新uboot fdt命令 load CF卡 找到sda1分区的offset, 在pc上mount uboot调试 setenv #增加reboot=0 setenv kernel_extra_args config_overlay=reboot=0 #修改波特率 setenv baudrate 115200 #设置完成后, uboot会自动把115200传给kernel:console=ttyS1,115200 在uboot下更新uboot #define PKGB_FIT_IMAGE_ADDR 0xffe00000 #define PKGA_FIT_IMAGE_ADDR 0xffd20000 #define UBOOT_FIT_IMAGE_SIZE 0x000e0000 ext2load isam_cf 0:0 0x10000000 /images/uboot/uboot3041.itb //其实不需要protect off protect off all #ubootB erase 0xffe00000 +0x000e0000 cp.b 0x10000000 0xffe00000 0x000e0000 #ubootA erase 0xffd20000 +0x000e0000 cp.b 0x10000000 0xffd20000 0x000e0000 fdt命令 fdt addr 0x11000000 fdt print /memory load CF卡 4G CF卡, 在linux起来后, 看到两个dev, 各有一个分区:/dev/sda /dev/sda1 /dev/sdb /dev/sdb1分区都是ext4格式. 分区表是mbr格式 /dev/sda1 on /mnt/cf type ext4 (rw,relatime,data=ordered) /dev/sdb1 on /mnt/cf2 type ext4 (rw,relatime,data=ordered) 在uboot阶段, 会从cf卡load linux.itb load_image_from_cf_card: load image /images/linuxA/linux.itb to 7a000000 done, size 14013800 prepare_images: load all images from fit sections do_load_images: Kernel section at 0x7a0002a4 size=0x373d07 do_load_images: Current dtb name: dtb3041 do_load_images: Dtb section at 0x7a374090 size=0x95a7 do_load_images: Rootfs section at 0x7a386ea8 size=0x9d61c8 do_load_images: Copy rootfs to ram 0xc10000 这个linux.itb有几个部分, 是vobs/esam/build/lsfx-a/OS/images/fant-f/linux.its定义好的 ./linux.itb.version 0.2K ./uImage 3.4M ./fant-f-y-3041.dtb 38K ./fant-f-y-4040.dtb 39K ./rootfs.cpio.xz 11M 物理上, 只有一个CF卡. 出现两个dev, 是dts里面配的: sda是高2G: 起始字节地址是 0x400000 * 512 sdb是低2G: 起始字节地址是 0x1ec0 * 512 /* compact-flash */ cf@1,0 { compatible = \"alu,cf\"; reg = ; reg-swap = ; /* data bus swapped */ /* isam_cf driver will number * the 1st present logic dev as sda, * the 2nd present logic dev ad sdb, * ... * We'd like to see the high 2G is taken as sda since the low 2G * will not be taken immediately by Linux during the migration. * */ /* sda */ cf-device@0 { /* This covers the whole high 2G */ compatible = \"alu,cf-device\"; /* first LBA sector (if ommitted, from end of previous * virtual device or zero if first virtual device) */ start-sector = ; /* last LBA sector (if ommitted, till end of physical device) */ /* end-sector = ; */ }; /* sdb */ cf-device@1 { /* This covers the /dev/hda1-7(legacy Integrity) in low 2G */ compatible = \"alu,cf-device\"; start-sector = ; /* this is the start LBA of /dev/hda1 */ end-sector = ; /* this is the end LBA of /dev/hda7 */ }; }; 找到sda1分区的offset, 在pc上mount fdisk -l /dev/sda Disk /dev/sda: 1871 MB, 1962704896 bytes, 3833408 sectors 238 cylinders, 255 heads, 63 sectors/track Units: sectors of 1 * 512 = 512 bytes Device Boot StartCHS EndCHS StartLBA EndLBA Sectors Size Id Type /dev/sda1 0,1,1 237,254,63 63 3823469 3823407 1866M 83 Linux 这里是说, sda1分区从sector 63开始, 到sector 3823469结束, 共3823407个 那么, 因为sda是cf卡的高2G, 那sda1所在的分区的offset是: (0x400000 + 63) * 512 = 2147515904 在pc上mount这个分区 #按照计算好的分区offset来mount sudo mount -o offset=2147515904 /dev/sdb mnt #能看到文件 ls mnt/images/ cf_low_2g_reclaim linuxA linuxB #用root用户权限, 拷贝linux.itb cd mnt/images/linuxA/ cp /home/yingjieb/work/share/swms/vobs/esam/build/lsfx-a/OS/images/fant-f/linux.itb . sync 参考: https://superuser.com/questions/694430/how-to-inspect-disk-image "},"notes/buildroot_杂记.html":{"url":"notes/buildroot_杂记.html","title":"buildroot杂记","keywords":"","body":" buildroot修改包代码后编译 常用命令 重编linux OCTEON SDK的kernelconfig buildroot修改包代码后编译 比如我想修改util-linux这个包的文件在第一次编译完后, 直接修改buildrootPOC/output/build/util-linux-2.33/configure强制编译script工具 28399 BUILD_SCRIPT_TRUE= 28400 BUILD_SCRIPT_FALSE='#' 28401 BUILD_SCRIPTREPLAY_TRUE= 28402 BUILD_SCRIPTREPLAY_FALSE='#' 因为我改的是configure文件, 需要重新配置才能生效 #make xxx-reconfigure比rebuild多了configure过程. make util-linux-reconfigure 每个包的精细控制如下: Package-specific: - Build and install and all its dependencies -source - Only download the source files for -extract - Extract sources -patch - Apply patches to -depends - Build 's dependencies -configure - Build up to the configure step -build - Build up to the build step -show-depends - List packages on which depends -show-rdepends - List packages which have as a dependency -show-recursive-depends - Recursively list packages on which depends -show-recursive-rdepends - Recursively list packages which have as a dependency -graph-depends - Generate a graph of 's dependencies -graph-rdepends - Generate a graph of 's reverse dependencies -dirclean - Remove build directory -reconfigure - Restart the build from the configure step -rebuild - Restart the build from the build step -source-check - Check package for valid download URLs -all-source-check - Check package and its dependencies for valid download URLs 常用命令 make update-defconfig make linux-tools-rebuild make linux-tools make rootfs-cpio make linux-dirclean make linux-rebuild 重编linux cd buildroot/output/build/linux-custom rm -f .stamp_* cd buildroot make linux #重新同步代码到linux-custom, 重新配置, 重新编译 #观察发现是增量编译 make linux-rebuild OCTEON SDK的kernelconfig cd /repo/yingjieb/dl/caviumsdk5.1/usr/local/Cavium_Networks/OCTEON-SDK/linux/kernel #用SDK自带的defconfig cp kernel.config linux/.config cd linux/ make menuconfig ARCH=mips #保存def文件 make savedefconfig ARCH=mips "},"notes/toolchain_升级gcc问题解决.html":{"url":"notes/toolchain_升级gcc问题解决.html","title":"升级GCC7.3问题解决","keywords":"","body":" 浮点运算的内核模拟问题 floating-point 排查 解决 so不匹配问题 解决 用python直接修改二进制 指针强转和编译器的 strict aliasing原则 详细解释 代码 编译 参考 cannot find crt1.o 问题 现象 分析 buildroot octeon3用的是lib32-fp gcc4.7和gcc7.3的搜索路径不一样 原因 解决 浮点运算的内核模拟问题 floating-point 在工具链升级后, MIPS的板子在跑一个厂家提供的SDK时, 变得非常慢. 提前剧透一下, 跟floating-point有关. 排查 根据版本比较, 和之前升级的问题经验, 排除了-fno-strict-aliasing的问题. 见下文指针强转和编译器的 strict aliasing原则 过程如下: 启动慢了30秒, 定位到慢的code block, 其中cmpPorts函数是热点 调查这个函数, 用新老toolchain都对照分析, 虽然最后的汇编不太一样, 但执行时间和结果都没有不同 基本排除这个函数, 但要加更多调试打印来看整个code block 用perf抓到很多软浮点函数的调用, 那可能是硬浮点没使能? 确认硬浮点已经使能, 但为什么还有软浮点的函数调用? 解决 上面已经很接近root cause了. 是浮点emulation导致的性能下降.硬浮点已经正确使能了.为什么还要浮点emulation呢?为什么测试程序不复现, 只有sdk代码复现?那关键点在于, 什么情况下, 会使用软浮点? 因为-- 浮点指令ldc1的data不对齐, 会导致浮点异常. 实际上, datasheet上确实要求这个指令要64bit对齐. 如果不对齐, 会触发异常.kernel捕获这个异常, 用软浮点模拟了计算 接下来就要看, sdk代码里, 到底是不是data不对齐造成的?C语言要求malloc返回的地址要8字节对齐, 但SDK没有follow. 解决办法是使能SDK里面已有的预定义宏, 使malloc 8字节对齐. 为什么老的工具链不复现? 因为老的工具链没有用这个浮点指令. so不匹配问题 板子上confd起不来, 显示 /run # confd --start-phase0 Internal error: Failed to load NIF library: '/lib/confd/lib/core/util/priv/syslog_nif.so: cannot open shared object file: No such file or directory' Daemon died status=19 但实际上, 这个so文件是存在的 ~ # ls -lh /lib/confd/lib/core/util/priv/syslog_nif.so -rwxr-xr-x 1 root root 6.3K Sep 9 2019 /lib/confd/lib/core/util/priv/syslog_nif.so 用readelf命令, 可以看到编译时的flags: readelf -h `find -name *.so` | grep Flag 在octeon3系列上, 一般是这样:Flags: 0x808e0827, noreorder, pic, cpic, abi2, octeon3, mips64r2 而/lib/confd/lib/core/util/priv/syslog_nif.so是这样的:Flags: 0x808e0227, noreorder, pic, cpic, abi2, fp64, octeon3, mips64r2多了fp64, 因为toolchain的bug, 这个flag会导致so文件打不开. 在buildroot output目录下, 寻找所有的fp64标记的so for f in `find -name *.so`; do echo $f; readelf -h $f | grep Flag | grep fp64; done > so.log 发现全部都是./build/confd-7.1.1.5/confd/lib/confd下面的so才有fp64标记. 调查结果是: confd的so是自己带的, 不是新的工具链编出来的. 解决 这个问题的root cause是: cavium提供的gcc7.3, 以及binutils, 和老的gcc4.7不兼容 confd是第三方提供的, 他们不肯重新用新gcc来编译.那只能自己直接修改老的so. 把ELF头的flag给改掉. 用python直接修改二进制 先把so mmap, 然后修改其中的字段.这个和ultraedit直接改二进制一样道理. 代码如下: #!/usr/bin/env python2 # Cavium (Marvell) toolchain based on gcc 7.x introduced a change in ELF flags # such that objects from old toolchain and new toolchain are not compatible. # Specifically, the following change was made in glibc to align with binutils # values: # # -#define EF_MIPS_HARD_FLOAT 0x00000200 # -#define EF_MIPS_SINGLE_FLOAT 0x00000400 # +#define EF_MIPS_HARD_FLOAT 0x00000800 # +#define EF_MIPS_SINGLE_FLOAT 0x00001000 # # The normal solution to this problem is to make sure to recompile all objects # with the new toolchain. However, in case of ConfD, the recompilation needs to # happen by the third-party vendor (TailF) which is not open to taking in the # new toolchain. # We can workaround this problem by patching the flag values manually. # Note: below we only change the 'HARD_FLOAT' value (0x200 -> 0x800), we don't # seem to encounter the SINGLE_FLOAT case. import mmap import os import struct import sys ELF32_HEADER_SIZE = 0x34 ELF64_HEADER_SIZE = 0x40 ELF_EI_MAG0 = 0x0 ELF_EI_MAG1 = 0x1 ELF_EI_MAG2 = 0x2 ELF_EI_MAG3 = 0x3 ELF_EI_CLASS = 0x4 ELF_EI_DATA = 0x5 ELF_E_MACHINE = 0x12 #size 2 ELF32_E_FLAGS = 0x24 #size 4 ELF64_E_FLAGS = 0x30 #size 4 if len(sys.argv) H', mm[ELF_E_MACHINE:ELF_E_MACHINE+2])[0] if machine != 0x0008: sys.stderr.write('Error: not a MIPS ELF file: %s\\n' % filename) sys.exit(1) # Patch hard-float flags flags = struct.unpack('>L', mm[flags_offset:flags_offset+4])[0] if flags & 0x200: newflags = flags & ~0x200 | 0x800 print('File %s: changing flags from %x to %x' % (filename, flags, newflags)) mm[flags_offset:flags_offset+4] = struct.pack('>L', newflags) else: print('File %s already has suitable flags: %x' % (filename, flags)) mm.close() 注: mmap提供把文件mmap到内存的功能, 这里返回的mm就像个数组 struct是python的一个包, 用来把二进制和pythone值之间进行转换. 比如struct.unpack(fmt, string)接受一个fmt, 表示要如何进行转换: 指针强转和编译器的 strict aliasing原则 使用gcc7.3导致marvell.user运行失败 对应代码 解释: 135行, 入参cfg_swap是NULL 应该是大端, default_swap应该是1 传入hxctl_cfg_reg_write函数的结构体cfg_swap全是0 -- 为什么? 见下文 导致marvell芯片配置成小端模式, 实际应该是大端模式. 解决办法是加-fno-strict-aliasing 注: 根据解释, 打开strict-aliasing选项后, 编译器会认为不同类型的指针, 不可能指向同一个内存地址. 所以放心的做一些优化. 这要求代码也同样遵守这个规则: 不同类型的指针不能指向同一个内存. 指针强制转换会破坏这个rule, 在-fstrict-aliasing打开的情况下, 会出现未知结果. 所以为了更好的享受编译器优化, 建议不要在代码里做强制类型转换. -fstrict-aliasing在较新的编译器里, 默认打开. 比如这次从gcc4.9升级到gcc7.3, 7.3就默认打开了这个选项. 详细解释 代码 #include typedef unsigned char __uint8_t; typedef unsigned short int __uint16_t; typedef unsigned int __uint32_t; typedef __uint8_t uint8_t; typedef __uint16_t uint16_t; typedef __uint32_t uint32_t; struct configuration { uint8_t value[4]; }; int bar(uint32_t data); int baz(struct configuration x); int foo(struct configuration *cfg_swap) { struct configuration default_swap = { .value[0] = 0xaa, .value[1] = 0xbb, .value[2] = 0xcc, .value[3] = 0xdd, }; int rc; if (!cfg_swap) { //printf(\"foo\\n\"); // adding the print fixes the problem too cfg_swap = &default_swap; } rc = bar(* (uint32_t *)cfg_swap); // the cast here triggers the issue //rc = baz(*cfg_swap); // passing cfg_swap without cast works correctly return rc; } 编译 对代码编译: A: gcc -fpic -O2 -fno-strict-aliasing -c test.c -o test.o反汇编 00000000 : 0: 27bdffe0 addiu sp,sp,-32 4: 3c02aabb lui v0,0xaabb 8: 03a4200a movz a0,sp,a0 c: 3442ccdd ori v0,v0,0xccdd 10: ffbc0010 sd gp,16(sp) 14: 3c1c0000 lui gp,0x0 18: ffbf0018 sd ra,24(sp) 1c: 0399e021 addu gp,gp,t9 20: afa20000 sw v0,0(sp) 24: 279c0000 addiu gp,gp,0 28: 8f990000 lw t9,0(gp) 2c: 0320f809 jalr t9 30: 8c840000 lw a0,0(a0) 34: dfbf0018 ld ra,24(sp) 38: dfbc0010 ld gp,16(sp) 3c: 03e00008 jr ra 40: 27bd0020 addiu sp,sp,32 B: gcc -fpic -O2 -c test.c -o test.o 00000000 : 0: 27bdffe0 addiu sp,sp,-32 4: ffbc0010 sd gp,16(sp) 8: 03a4200a movz a0,sp,a0 c: ffbf0018 sd ra,24(sp) 10: 3c1c0000 lui gp,0x0 14: 0399e021 addu gp,gp,t9 18: 279c0000 addiu gp,gp,0 1c: 8f990000 lw t9,0(gp) 20: 0320f809 jalr t9 24: 8c840000 lw a0,0(a0) 28: dfbf0018 ld ra,24(sp) 2c: dfbc0010 ld gp,16(sp) 30: 03e00008 jr ra 34: 27bd0020 addiu sp,sp,32 补充汇编知识 解释: 在B情况下, 调用bar的入参是寄存器a0, 而a0是前面赋值来的:movz a0,sp,a0, 即如果a0为0, 则把sp赋值给a0; 对应 if (!cfg_swap) cfg_swap = &default_swap; 在foo函数入参是NULL的情况下, a0为sp的值, 而sp指向变量default_swap, 并为其保留了32byte的栈空间 所以if (!cfg_swap)成立, cfg_swap指向栈上的default_swap, 从而bar(* (uint32_t *)cfg_swap)是把栈变量default_swap强转成uint32_t 编译器默认打开了strict-aliasing, 认为(uint32_t *)和struct configuration *cfg_swap不可能是同一块地址, 这是strict-aliasing的原则. 这也要求代码里不要对指针强转.所以编译器认为既然你最后用的是uint32_t *, 没用struct configuration *, 所以对struct configuration类型的default_swap赋值没有意义, 也没有哪里用到. 就优化掉了.从反汇编结果看, 没有aabbccdd的赋值 所以default_swap在栈上分配了空间, 但没有初始化, 其值随机; 到真实的marvell sdk代码, 值应该是1(代表大端), 但因为上述原因, 一般是0(代表小端). 导致配置错误. 对case A来说, 有-fno-strict-aliasing, 关闭了编译器关于strict-aliasing的假设, 编译器还是老老实实的把default_swap赋值成aabbccdd. 参考 https://blog.csdn.net/dbzhang800/article/details/6720141 https://xania.org/200712/cpp-strict-aliasing https://cellperformance.beyond3d.com/articles/2006/06/understanding-strict-aliasing.html cannot find crt1.o 问题 现象 直接把gcc4.7替换成gcc7.3, 有找不到crt1.o错误 make dtc-dirclean make -j1 dtc V=1 前面编译没错误, 但在链接时, 找不到crt1.o; 这个东西之前也见过, 估计是C Run Time的缩写 /repo1/yingjieb/ms/buildroot/output/host/bin/mips64-octeon-linux-gnu-gcc -D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -Os -g2 -fPIC -I libfdt -I . -o convert-dtsv0 srcpos.o util.o convert-dtsv0-lexer.lex.o /repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/../../../../mips64-octeon-linux-gnu/bin/ld: cannot find crt1.o: No such file or directory /repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/../../../../mips64-octeon-linux-gnu/bin/ld: cannot find crti.o: No such file or directory collect2: error: ld returned 1 exit status 分析 /repo1/yingjieb/ms/buildroot/output/host/bin/mips64-octeon-linux-gnu-gcc是buildroot提供的\"wrapper\", 代码在buildroot/toolchain/toolchain-wrapper.c yingjieb@FNSHA190 /repo1/yingjieb/ms/buildroot $ ll /repo1/yingjieb/ms/buildroot/output/host/bin/mips64-octeon-linux-gnu-gcc lrwxrwxrwx 1 yingjieb systemintegration 17 Aug 12 13:57 /repo1/yingjieb/ms/buildroot/output/host/bin/mips64-octeon-linux-gnu-gcc -> toolchain-wrapper 单独运行出错的那行命令, 加环境变量BR2_DEBUG_WRAPPER, 同样能复现: yingjieb@FNSHA190 /repo1/yingjieb/ms/buildroot/output/build/dtc-1.4.7 $ BR2_DEBUG_WRAPPER=1 /repo1/yingjieb/ms/buildroot/output/host/bin/mips64-octeon-linux-gnu-gcc -mabi=n32 -D_LARGEFILE _SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -Os -g2 -fPIC -I libfdt -I . -o convert-dtsv0 srcpos.o util. o convert-dtsv0-lexer.lex.o Toolchain wrapper executing: CCACHE_BASEDIR='/repo1/yingjieb/ms/buildroot/output' '/repo1/yingjieb/ms/buildroot/output/host/bin/ccache' '/repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/mips64-octeon-linux-gnu-gcc' '--sy sroot' '/repo1/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot' '-mabi=n32' '-mnan=legacy' '-EB' '-pipe' '-mno-branch-likely' '-march=octeon3' '-mabi=n32' '-D_LARGEFILE_SOURCE' '-D_LARGEFILE64_SOURCE' '-D_FILE_OFF SET_BITS=64' '-Os' '-g2' '-fPIC' '-I' 'libfdt' '-I' '.' '-o' 'convert-dtsv0' 'srcpos.o' 'util.o' 'convert-dtsv0-lexer.lex.o' /repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/../../../../mips64-octeon-linux-gnu/bin/ld: cannot find crt1.o: No such file or directory /repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/../../../../mips64-octeon-linux-gnu/bin/ld: cannot find crti.o: No such file or directory 说明这个wrapper最后还是调用了/repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/mips64-octeon-linux-gnu-gcc, 默认传入一些参数--sysroot /repo1/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot -mabi=n32 -mnan=legacy -EB -pipe -mno-branch-likely -march=octeon3 -mabi=n32在命令行还原这些参数如下: yingjieb@FNSHA190 /repo1/yingjieb/ms/buildroot/output/build/dtc-1.4.7 $ /repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/mips64-octeon-linux-gnu-gcc --sysroot /repo1/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot '-mabi=n32' '-mnan=legacy' '-EB' '-pipe' '-mno-branch-l ikely' '-march=octeon3' '-mabi=n32' '-D_LARGEFILE_SOURCE' '-D_LARGEFILE64_SOURCE' '-D_FILE_OFFSET_BITS=64' '-Os' '-g2' '-fPIC' '-I' 'libfdt' '-I' '.' '-o' 'convert-dtsv0' 'srcpos.o' 'util.o' 'convert-dtsv0-lexer.lex.o' /repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/../../../../mips64-octeon-linux-gnu/bin/ld: cannot find crt1.o: No such file or directory /repo1/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/7.3.0/../../../../mips64-octeon-linux-gnu/bin/ld: cannot find crti.o: No such file or directory 至此说明问题出在新的GCC7.3 buildroot 编译器在buildroot/output/host/opt/ext-toolchain/mips64-octeon-linux-gnubuildroot会把编译器的sysroot, 拷贝到buildroot/output/host/mips64-buildroot-linux-gnu/sysroot, 但只保留lib octeon3用的是lib32-fp gcc4.7和gcc7.3的搜索路径不一样 用strace可以看出来 strace -o ld47.log /repo/yingjieb/ms/buildrootmlt/output/host/bin/mips64-octeon-linux-gnu-gcc -D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -Os -g2 -fPIC -I libfdt -I . -o convert-dtsv0 srcpos.o util.o convert-dtsv0-lexer.lex.o strace -o ld73.log /repo1/yingjieb/ms/buildroot/output/host/bin/mips64-octeon-linux-gnu-gcc -D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -Os -g2 -fPIC -I libfdt -I . -o convert-dtsv0 srcpos.o util.o convert-dtsv0-lexer.lex.o 对比如下: #前面几个路径类似, 在toolchain目录下找 host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/4.7.0/n32/octeon3/crt1.o host/opt/ext-toolchain/bin/../lib/gcc/n32/octeon3/crt1.o host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/4.7.0/../../../../mips64-octeon-linux-gnu/lib/mips64-octeon-linux-gnu/4.7.0/n32/octeon3/crt1.o host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/4.7.0/../../../../mips64-octeon-linux-gnu/lib/../lib32-fp/crt1.o #下面开始, 在sysroot下面找, sysroot是传参来的 host/mips64-buildroot-linux-gnu/sysroot/lib/mips64-octeon-linux-gnu/4.7.0/n32/octeon3/crt1.o #gcc4.7找的是lib/../lib32-fp host/mips64-buildroot-linux-gnu/sysroot/lib/../lib32-fp/crt1.o #而gcc7.3找的是lib64../lib32-fp host/mips64-buildroot-linux-gnu/sysroot/lib64/../lib32-fp/crt1.o host/mips64-buildroot-linux-gnu/sysroot/usr/lib/mips64-octeon-linux-gnu/4.7.0/n32/octeon3/crt1.o #GCC4.7到这里找到了 host/mips64-buildroot-linux-gnu/sysroot/usr/lib/../lib32-fp/crt1.o #问题在这, lib32-fp里面是由cr1.o的, 但是相对于lib64目录的, 而buildroot没拷贝lib64目录. host/mips64-buildroot-linux-gnu/sysroot/usr/lib64/../lib32-fp/crt1.o 原因 buildroot拷贝sysroot的时候, 做了裁剪, 导致gcc7.3找不到crt1.o 解决 /repo1/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot/usr #加lib64软链接 ln -s lib lib64 "},"notes/kernel_异常打印分析实例.html":{"url":"notes/kernel_异常打印分析实例.html","title":"kernel bug异常打印分析","keywords":"","body":"现象 02-10 01:10:24 [ 81.726789] Kernel bug detected[#1]: 02-10 01:10:24 [ 81.743065] CPU: 1 PID: 699 Comm: watchdog_monito Tainted: G O 3.10.20-rt14-Cavium-Octeon #6 02-10 01:10:24 [ 81.765061] task: 800000008c476af0 ti: 8000000088b28000 task.ti: 8000000088b28000 02-10 01:10:24 [ 81.785229] $ 0 : 0000000000000000 ffffffff8054af84 0000000000000004 0000000000000008 02-10 01:10:24 [ 81.869389] $ 4 : 0000000000000000 800000008c01ce88 00000000ffffff01 00000000ffffff02 02-10 01:10:24 [ 81.953547] $ 8 : 800000008c01cea8 ffffffff80710000 0000000000000001 0000000000000000 02-10 01:10:24 [ 82.037705] $12 : 8000000088b2bc98 800000008c211f18 0000000000000000 0000000000000000 02-10 01:10:24 [ 82.121863] $16 : 800000008c20b340 800000008c07f000 0000000000000009 800000008c01dd80 02-10 01:10:24 [ 82.206021] $20 : 00000000000080d0 0000000000000000 0000000000000009 ffffffff812c0000 02-10 01:10:24 [ 82.290179] $24 : 00000000100c60c8 00000000774e4060 02-10 01:10:25 [ 82.374337] $28 : 8000000088b28000 8000000088b2bcb0 800000008c01ce80 ffffffff802698ac 02-10 01:10:25 [ 82.458496] Hi : 0000000000000001 02-10 01:10:25 [ 82.474754] Lo : 0000000000000000 02-10 01:10:25 [ 82.491022] epc : ffffffff8026997c cache_alloc_refill+0x174/0x7e0 02-10 01:10:25 [ 82.509975] Tainted: G O 02-10 01:10:25 [ 82.526410] ra : ffffffff802698ac cache_alloc_refill+0xa4/0x7e0 02-10 01:10:25 [ 82.545276] Status: 14009ce2 KX SX UX KERNEL EXL 02-10 01:10:25 [ 82.638828] Cause : 00800034 02-10 01:10:25 [ 82.654385] PrId : 000d9602 (Cavium Octeon III) 02-10 01:10:25 [ 82.671685] Modules linked in: hxdrv(O) uio_generic_driver(O) led_cpld(O) reborn_macfilter(O) logbuffer(O) fglt_b_reboot_helper(O) ramoops reed_solomon fglt_b_cpld(O) reborn_class(O) generic_access(O) spi_oak_island(O) 02-10 01:10:25 [ 82.868798] Process watchdog_monito (pid: 699, threadinfo=8000000088b28000, task=800000008c476af0, tls=00000000776404a0) 02-10 01:10:25 [ 82.892353] Stack : 0000000000000001 ffffffff8054abfc 800000008c073100 0000000000000000 02-10 01:10:25 800000008c211c90 ffffffff801a4e1c 800000008c01dd80 00000000000080d0 02-10 01:10:25 00000000000080d0 0000000000000001 ffffffff8016d260 00000000000080d0 02-10 01:10:25 0000000000000000 800000008c2874a0 0000000000000000 ffffffff8026974c 02-10 01:10:25 800000008c3fba00 0000000001200012 0000000000000000 800000008c3fba00 02-10 01:10:25 0000000000000000 ffffffff81230000 0000000077639068 ffffffff8016d260 02-10 01:10:25 fffffff48c073100 ffffffff80295cd8 0000000000000000 0000000000000000 02-10 01:10:26 800000008c287660 8000000088b18000 00000000000002bb 000000007f6b24e0 02-10 01:10:26 0000000001200012 0000000000000002 0000000000000000 0000000000000000 02-10 01:10:26 00000000100cb3e8 00000000100c0000 000000007f6b2500 ffffffff8016dae0 02-10 01:10:26 ... 02-10 01:10:26 [ 83.630192] Call Trace: 02-10 01:10:26 [ 83.645325] [] cache_alloc_refill+0x174/0x7e0 02-10 01:10:26 [ 83.663932] [] kmem_cache_alloc+0x154/0x210 02-10 01:10:26 [ 83.682371] [] copy_process.part.53+0x760/0xea0 02-10 01:10:26 [ 83.701153] [] do_fork+0xa8/0x2c8 02-10 01:10:26 [ 83.718719] [] handle_sys+0x134/0x160 02-10 01:10:26 [ 83.736630] 02-10 01:10:26 [ 83.750804] 02-10 01:10:26 Code: 8e640018 0044202b 2c8a0001 10800023 26c4ffff 12c0006c 0080902d 0809a668 02-10 01:10:26 [ 83.900378] [sched_delayed] sched: RT throttling activated 02-10 01:10:26 [ 83.918593] ---[ end trace a91730027ab35f25 ]--- 02-10 01:10:28 [ 83.937180] Fatal exception: panic in 5 seconds[ 85.896096] EDAC MC0: 1 CE DIMM 1 rank 0 bank 4 row 63553 col 1436 on any memory ( page:0x0 offset:0x0 grain:0 syndrome:0x0) 02-10 01:10:28 [ 85.920145] EDAC MC0: 1 UE DIMM 1 rank 0 bank 4 row 63553 col 1436 on any memory ( page:0x0 offset:0x0 grain:0) 02-10 01:10:31 02-10 01:10:31 [ 88.965613] Kernel panic - not syncing: Fatal exception 02-10 01:10:31 [ 88.997694] reboot_helper: stored panic_counter = 1 02-10 01:10:31 [ 89.005599] [sched_delayed] process 1366 (TICK) no longer affine to cpu0 02-10 01:10:31 [ 89.015598] [sched_delayed] process 1520 (shl1) no longer affine to cpu0 02-10 01:10:31 [ 89.054033] Extra info: CPLD: 10=05 12=3e 92=00 e0=00 02-10 01:10:31 [ 89.065598] [sched_delayed] process 1521 (shl1) no longer affine to cpu0 02-10 01:10:31 [ 89.091240] reboot_helper: isam_reboot_type='warm' 02-10 01:10:31 [ 89.108715] reboot-helper: Enabling preserved ram 02-10 01:10:31 [ 89.126103] flush l2 cache. 02-10 01:10:31 [ 89.141690] reboot_helper: continuing standard linux reboot 02-10 01:10:37 [ 89.159957] Rebooting in 5 seconds.. 内核相关代码 首先要参考笔记 octeon中断根据现场Cause : 00800034, 结合前面的笔记我们知道出现了13号trap异常, 触发中断向量handle_tr, 进而调用do_tr() do_tr()出自下面arch/mips/kernel/traps.c异常打印出自 do_tr(struct pt_regs *regs) do_trap_or_bp(struct pt_regs *regs, unsigned int code, const char *str) switch (code) case BRK_BUG: //如果是kernel模式下, die; die_if_kernel(\"Kernel bug detected\", regs); if (unlikely(!user_mode(regs))) die(str, regs); oops_enter(); printk(\"%s[#%d]:\\n\", str, ++die_counter); show_registers(regs); __show_regs(regs); print_modules(); show_stacktrace(current, regs); show_code((unsigned int __user *) regs->cp0_epc); add_taint(TAINT_DIE, LOCKDEP_NOW_UNRELIABLE); oops_exit(); if (in_interrupt()) panic(\"Fatal exception in interrupt\"); if (panic_on_oops) printk(KERN_EMERG \"Fatal exception: panic in 5 seconds\"); ssleep(5); panic(\"Fatal exception\"); //能到这里说明是user代码出了问题, 发SIGTRAP信号 force_sig(SIGTRAP, current); force_sig_info(sig, SEND_SIG_PRIV, p); "},"notes/as_title_linuxdaily.html":{"url":"notes/as_title_linuxdaily.html","title":"Linux工程实践","keywords":"","body":"如题 "},"notes/linux_日常使用.html":{"url":"notes/linux_日常使用.html","title":"日常linux使用","keywords":"","body":" 使用overlay文件系统 查看是否用了proxy 记录tty输出并回放 编译安装openssh ubuntu打开ssh服务 ubuntu apt用代理 xiaoshujiang代码变整齐 使用sshfs实现远程目录mount pkg-config工具用来检索系统中库文件的信息, 编译和链接的时候会用到. pandoc生成简历 nmap 网络扫描 简单的文件服务器 或者用http-server ubuntu 打开nfs服务 nfs v3 和v4的mount区别 补充, mount fuse文件系统 nfs服务端 在客户端mount 如果这个nfs mount点挂了, 用这个命令umount 编译kernel route SDP通过mini主机上外网 mini主机双网卡同时上内网和外网 最最新的配置文件 最新的配置文件 新机器pci错误 关于网络-域名debug 文本比较工具 dhclient 无线网卡命令 hostname sudo 免密码 不启动GUI wiz的搜索功能 修改grub2默认启动项 解决中文乱码 使用tmux vim支持系统clipboard ubuntu查看系统中已安装的包 locate每天更新数据库 ubuntu基本开发包 vim插件 开机自启动 SSD优化 升级kernel版本 调节笔记本亮度 笔记本省电模式 笔记本触摸板 n7260无线网卡 on linux 查看firmware版本 不跳转的google 使用overlay文件系统 很简单, linux支持overlay文件系统, 它是个uinon多的文件系统, 底层(lower)文件系统可以是只读的, 上层(upper)文件系统可以是tmpfs. 在mount的时候指定 godevtmp=/tmp/godev/$user tmphome=/tmp/godev/home/$user mkdir -p $godevtmp/{upper,work} sudo mount -t overlay overlay -o lowerdir=/home/$user,upperdir=$godevtmp/upper,workdir=$godevtmp/work $tmphome lowerdir: 底层文件系统目录 upperdir: 上层文件系统目录 workdir: 上层work目录 最后的挂载点: 任意已存在目录 查看是否用了proxy #出现Proxy-Agent字样说明用了proxy $ curl --head https://www.google.com/ HTTP/1.1 200 Connection Established Proxy-Agent: Fortinet-Proxy/1.0 记录tty输出并回放 #记录, htop的输出也能记录 script --timing=time.log poc.log #回放, ctrl+s暂停, ctrl+q恢复 scriptreplay --timing=time.log poc.log 编译安装openssh wget https://ftp.yzu.edu.tw/pub/OpenBSD/OpenSSH/portable/openssh-8.0p1.tar.gz cd openssh-8.0p1 ./configure --prefix=$HOME make make install /home/yingjieb/bin/ssh /home/yingjieb/bin/scp /home/yingjieb/sbin/sshd ubuntu打开ssh服务 sudo apt-get install openssh-server 查看是否ssh已经启动 netstat -tlp 或 ps -e | grep ssh 没启动的话要重启一下ssh服务 sudo /etc/init.d/ssh resart ubuntu apt用代理 Linux Mint 19.1 Tessa $ cat /etc/apt/apt.conf.d/proxy.conf Acquire::http::Proxy \"http://135.245.48.34:8000\"; xiaoshujiang代码变整齐 自定义css, 只管导出到wiz笔记的, xsj自己的预览还是一塌糊涂 .preview .xiaoshujiang_code_container>.xiaoshujiang_code_buttons{ display: none; } body code, body .xiaoshujiang_code { font-family:\"Courier New\"; } 使用sshfs实现远程目录mount #在client上 sudo yum install fuse-sshfs #mount远程目录, -C打开压缩, 性能高点 sshfs -C -o allow_other bai@10.64.17.45:/home/bai/share /home/bai/share #umount fusermount -u /home/bai/share #keep alive, 每60秒发\"keep alive\"消息给server #在client的~/.ssh/config里 ServerAliveInterval 60 #or sshfs -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 server:/path/to/mount pkg-config工具用来检索系统中库文件的信息, 编译和链接的时候会用到. 不同的库用.pc文件来管理, ARM64上一般在/usr/lib64/pkgconfig下面 也可以用PKG_CONFIG_PATH环境变量来指定 export PKG_CONFIG_PATH=/opt/gtk/lib/pkgconfig:$PKG_CONFIG_PATH 比如lua的.pc文件如下: $ cat /usr/lib64/pkgconfig/lua.pc V= 5.1 R= 5.1.4 prefix= /usr exec_prefix=${prefix} libdir= /usr/lib64 includedir=${prefix}/include Name: Lua Description: An Extensible Extension Language Version: ${R} Requires: Libs: -llua -lm -ldl Cflags: -I${includedir} 在/usr/local/lib/pkgconfig/下面新建lua.pc pandoc生成简历 git clone https://github.com/mszep/pandoc_resume #有两种方式, 一是本地运行, 而是起一个docker实例 #本地运行 #这里有个bug, 需要安装pandoc最新的版本 wget https://github.com/jgm/pandoc/releases/download/2.2.1/pandoc-2.2.1-1-amd64.deb sudo dpkg -i pandoc-2.2.1-1-amd64.deb make #docker运行类似, 是一个很好的dev-op应用实例 ubuntu 删除老kernel dpkg --list | grep linux-image sudo apt-get purge linux-image-x.x.x-x-generic sudo update-grub2 nmap 网络扫描 扫描单个IP地址 nmap 192.168.56.1 扫描一个网络中IP地址范围 nmap 192.168.56.1-255 扫描目标主机的特定端口 nmap 192.168.56.1 -p 80 扫描特定子网的特殊的端口 nmap 192.168.56.0/24 –p 1-1000 扫描子网, 找到 IP MAC对应关系nmap -sn 10.239.120.1/24; -sn指只ping扫描, 不做端口扫描sudo nmap -sn 10.239.120.1/24 -oG -; -oG是输出格式, 后面-是指stdout更好用的版本sudo nmap -n -sP 10.239.120.1/24 | awk '/Nmap scan report/{printf $5;printf \" \";getline;getline;print $3;}'更更好用的版本sudo nmap -n -sP 10.239.120.1/24 | awk '/Nmap scan report for/{printf $5;}/MAC Address:/{print \" => \"$3;}' | sortsudo nmap -n -sP 192.168.8.100-200 | awk '/Nmap scan report for/{printf $5;}/MAC Address:/{print \" => \"$3;}' | sort 简单的文件服务器 在需要share的文件夹下面执行 nohup python -m SimpleHTTPServer & 或 nohup python -m SimpleHTTPServer 8088 & 默认是8000端口访问的话就 http://serverip:port 或者用http-server apt install npm sudo npm install http-server -g sudo apt install nodejs-legacy http-server -p 8000 ubuntu 打开nfs服务 nfs v3 和v4的mount区别 在server上 /etc/export #注意nfsv4要加fsid=0 /home/yingjieb/work 192.168.2.0/24(rw,sync,insecure,no_subtree_check,no_root_squash,fsid=0) 在client上 #v3的mount, 带路径 mount 192.168.2.11:/home/yingjieb/work/nfsroot/mipsroot /root/remote -o nolock #v4的mount, 不带路径 mount -t nfs4 192.168.2.11:nfsroot/mipsroot /root/remote -o nolock 补充, mount fuse文件系统 nfsv4才支持mount用户态的文件系统, 此时需要用\"fsid=\"来显式指定没研究这个选项, 但似乎就是提供一个id号, 数字的, 好像任意填. cat /etc/exports /home/yingjieb/work/share/output 192.168.2.0/24(rw,sync,insecure,no_subtree_check,no_root_squash,all_squash,anonuid=1000,anongid=1000,fsid=0) 在板子上mount: mkdir -p /root/remote #一定要加nolock选项, 否则会timeout mount 192.168.2.11:/home/yingjieb/work/share/output /root/remote -o nolock 这里解释一下: 我的连接是这样的: board(192.168.2.12) --(nfs)--> (192.168.2.11)nfs server on PC Linux on virtual box(NAT via host) --(sshfs)--> work station(135.251.206.190) yingjieb@yingjieb-VirtualBox ~/work Linux Mint 19.1 Tessa $ ls share tftpd #share是另外一个服务器的目录, 走sshfs yingjieb@135.251.206.190:/repo/yingjieb/ms/buildroot on /home/yingjieb/work/share type fuse.sshfs (rw,nosuid,nodev,relatime,user_id=1000,group_id=1000,allow_other) nfs服务端 sudo apt install nfs-kernel-server cat /etc/exports /home/qdt yingjieb-gv(rw,sync,no_subtree_check,no_root_squash,all_squash,anonuid=1000,anongid=1000) 或 /home/bai/share 10.64.16.0/24(rw,sync,no_subtree_check,no_root_squash,all_squash,anonuid=1000,anongid=1000) 注: uid和gid的1000是指server端的用户qdt /home/bai/share *(rw,sync,insecure,no_subtree_check,no_root_squash,all_squash,anonuid=1000,anongid=1000) 选项insecure可以让其他网络的client有权限连接, 否则会有`ount.nfs: access denied by server while mounting`错误 sudo exportfs -a sudo systemctl start nfs-kernel-server.service sudo systemctl enable nfs-kernel-server.service 注: NFS希望uid和gid在host上和client上是相同的, 要用all_squash和anonuid=1000,anongid=1000来指定, unkown用户的操作都被当成host上的1000用户 1000组(一般是第一个普通用户) 如果在客户端可以mount, 但有权限问题, 要检查host上的共享文件夹的权限, others要可读可写 比如:drwxrwxrwx 3 bai bai 4096 Aug 14 16:55 share 修改/etc/exports要执行sudo exportfs -rav 在客户端mount sudo yum install nfs-utils sudo mount qdt-shlab-pc.ap.qualcomm.com:/home/qdt /local/qdt-shlab-pc/ 如果这个nfs mount点挂了, 用这个命令umount sudo umount -f -l /local/qdt-shlab-pc 编译kernel make mrproper make oldconfig make defoldconfig make -j32 make modules_install -j32 make install -j32 route SDP通过mini主机上外网 在mini主机上, 外网是wlp1s0: 10.75.61.131, 内网是enp0s31f6: 10.239.120.104 原理见笔记 iptables详解 #打开转发 sudo sysctl -w net.ipv4.ip_forward=1 #用nat table, 路由后, 限定awsdp1的ip(10.239.120.133)可以nat方式从wlp1s0转发出去 sudo iptables -t nat -A POSTROUTING -j MASQUERADE -s 10.239.120.133/32 -o wlp1s0 在awsdp1上, 自己的IP是eth0 10.239.120.133 #因为内网防火墙原因, 默认路由无法上外网, 所以先添加所有10.0.0.0网段走内网路由 sudo ip route add 10.0.0.0/8 via 10.239.120.1 #这时可以安全的删除默认路由了(之前的默认路由也是走10.239.120.1), 因为有了上面那句, 所有目的地址是10.0.0.0网段的报文都走10.239.120.1. sudo ip route del default #增加默认路由到mini主机 sudo ip route add default via 10.239.120.104 #如果没有配域名服务器, 还要修改/etc/resolv.conf, 比如 echo \"nameserver 10.65.0.1\" > /etc/resolv.conf qdt-shlab-pc: mini pc 登录: ssh qdt@qdt-shlab-pc 上外网之前要先查看一下ifconfig wlp1s0, 有ip说明已经up了 打开无线网卡上外网: sudo ifup wlp1s0 使用完毕后一定要关闭无线网卡: sudo ifdown wlp1s0 注: 路由表已经配置在文件里, 有线网络不需要操作. 其他机器上外网 在AWSDP1上: sudo ip route add 10.0.0.0/8 via 10.239.120.1 sudo ip route del default sudo ip route add default via 10.239.120.104 此时路由表应该是这样: DestinationGateway Genmask Flags Metric Ref Use Iface 0.0.0.010.239.120.104 0.0.0.0 UG0 0 0 eth0 10.0.0.010.239.120.1 255.0.0.0 UG0 0 0 eth0 10.231.213.1010.239.120.1 255.255.255.255 UGH 100 00 eth0 10.239.120.00.0.0.0 255.255.252.0 U100 0 0 eth0 在qdt-shlab-pc上: sudo sysctl -w net.ipv4.ip_forward=1 sudo iptables -t nat -A POSTROUTING -j MASQUERADE -s 10.239.120.133/32 -o wlp1s0 mini主机双网卡同时上内网和外网 最最新的配置文件 $ cat interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s31f6 iface enp0s31f6 inet dhcp post-up ip route add 10.0.0.0/8 via 10.239.120.1 dev enp0s31f6 # post-up ip route del default 因为后面加了del default, 这里就必须不要; 否则后面再次del的时候, 会因为没有default而报错. auto wlp1s0 iface wlp1s0 inet dhcp pre-up ip route del default pre-up wpa_supplicant -B -D wext -i wlp1s0 -c /etc/wpa_supplicant/wpa.conf post-up sysctl -w net.ipv4.ip_forward=1 post-up iptables -t nat -A POSTROUTING -j MASQUERADE -s 10.239.120.0/22 -o wlp1s0 post-down pkill wpa_supplicant 最新的配置文件 $ cat /etc/network/interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s31f6 iface enp0s31f6 inet dhcp post-up ip route add 10.0.0.0/8 via 10.239.120.1 dev enp0s31f6 post-up ip route del default iface wlp1s0 inet dhcp pre-up wpa_supplicant -B -D wext -i wlp1s0 -c /etc/wpa_supplicant/wpa.conf post-up sysctl -w net.ipv4.ip_forward=1 post-up iptables -t nat -A POSTROUTING -j MASQUERADE -s 10.239.120.0/22 -o wlp1s0 post-down pkill wpa_supplicant qdt@qdt-shlab-pc /etc/iproute2 $ cat /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.253.157.26 nameserver 10.253.157.27 search wlan.qualcomm.com Hydra: K5x48Vz3 sudo systemctl stop network-manager.service sudo systemctl disable network-manager.service sudo systemctl start networking.service sudo systemctl enable networking.service qdt@qdt-shlab-pc ~ $ cat /etc/wpa_supplicant/wpa.conf # reading passphrase from stdin network={ ssid=\"Hydra\" #psk=\"K5x48Vz3\" psk=529cd7878309812d395fb8bd999c203f35a83da1ca0a86fe83c0f7c5cd08efc4 } qdt@qdt-shlab-pc ~ $ cat /etc/network/interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s31f6 iface enp0s31f6 inet dhcp post-up ip route add 10.0.0.0/8 via 10.239.120.1 dev enp0s31f6 post-up ip route del default iface wlp1s0 inet dhcp pre-up wpa_supplicant -B -D wext -i wlp1s0 -c /etc/wpa_supplicant/wpa.conf post-down pkill wpa_supplicant qdt@qdt-shlab-pc ~ $ sudo ifup wlp1s0 qdt@qdt-shlab-pc ~ $ sudo ifup wlp1s0 man interfaces qdt@qdt-shlab-pc ~/Desktop $ cat /etc/network/interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s31f6 iface enp0s31f6 inet dhcp sudo dhclient -r wlp1s0 sudo ip route add 10.0.0.0/8 via 10.239.120.1 dev enp0s31f6 sudo ip route del default sudo ip route add default via 10.75.60.1 dev wlp1s0 qdt@qdt-shlab-pc /etc/iproute2 $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.75.60.1 0.0.0.0 UG 0 0 0 wlp1s0 10.0.0.0 10.239.120.1 255.0.0.0 UG 0 0 0 enp0s31f6 10.75.60.0 0.0.0.0 255.255.254.0 U 0 0 0 wlp1s0 10.239.120.0 0.0.0.0 255.255.252.0 U 0 0 0 enp0s31f6 sudo killall dhclient sudo ip address add 10.75.61.106/255.255.254.0 dev wlp1s0 sudo ip route add default via 10.75.60.1 dev wlp1s0 sudo sh -c 'echo \"nameserver 10.253.157.26\" >> /etc/resolv.conf' ip addr flush dev wlp1s0 sudo wpa_passphrase Hydra >> /etc/wpa_supplicant/wpa.conf qdt@qdt-shlab-pc ~/Desktop $ cat /etc/wpa_supplicant/wpa.conf # reading passphrase from stdin network={ ssid=\"Hydra\" #psk=\"K5x48Vz3\" psk=529cd7878309812d395fb8bd999c203f35a83da1ca0a86fe83c0f7c5cd08efc4 } rfkill unblock all sudo ip link set wlp1s0 up sudo wpa_supplicant -B -D wext -i wlp1s0 -c /etc/wpa_supplicant/wpa.conf sudo dhclient wlp1s0 enp0s31f6: qdt@qdt-shlab-pc /etc/NetworkManager $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.239.120.1 0.0.0.0 UG 100 0 0 enp0s31f6 10.231.213.10 10.239.120.1 255.255.255.255 UGH 100 0 0 enp0s31f6 10.239.120.0 0.0.0.0 255.255.252.0 U 100 0 0 enp0s31f6 169.254.0.0 0.0.0.0 255.255.0.0 U 1000 0 0 enp0s31f6 qdt@qdt-shlab-pc /etc/NetworkManager $ ifconfig enp0s31f6 Link encap:Ethernet HWaddr 40:8d:5c:d1:d1:f1 inet addr:10.239.120.104 Bcast:10.239.123.255 Mask:255.255.252.0 inet6 addr: fe80::428d:5cff:fed1:d1f1/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:15516 errors:0 dropped:197 overruns:0 frame:0 TX packets:709 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:1586576 (1.5 MB) TX bytes:73304 (73.3 KB) Interrupt:16 Memory:df100000-df120000 qdt@qdt-shlab-pc /etc/iproute2 $ cat /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.231.213.10 nameserver 10.253.144.21 search ap.qualcomm.com wlp1s0: qdt@qdt-shlab-pc /etc/NetworkManager $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.75.60.1 0.0.0.0 UG 600 0 0 wlp1s0 10.75.60.0 0.0.0.0 255.255.254.0 U 600 0 0 wlp1s0 10.231.213.10 10.75.60.1 255.255.255.255 UGH 600 0 0 wlp1s0 169.254.0.0 0.0.0.0 255.255.0.0 U 1000 0 0 wlp1s0 qdt@qdt-shlab-pc /etc/NetworkManager $ ifconfig wlp1s0 wlp1s0 Link encap:Ethernet HWaddr 48:d2:24:d4:70:b9 inet addr:10.75.61.74 Bcast:10.75.61.255 Mask:255.255.254.0 inet6 addr: fe80::4ad2:24ff:fed4:70b9/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1355 errors:0 dropped:0 overruns:0 frame:0 TX packets:1278 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:607469 (607.4 KB) TX bytes:154983 (154.9 KB) qdt@qdt-shlab-pc /etc/iproute2 $ cat /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.253.157.26 nameserver 10.253.157.27 search wlan.qualcomm.com qdt@qdt-shlab-pc /etc/NetworkManager $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.239.120.1 0.0.0.0 UG 100 0 0 enp0s31f6 0.0.0.0 10.75.60.1 0.0.0.0 UG 600 0 0 wlp1s0 10.75.60.0 0.0.0.0 255.255.254.0 U 600 0 0 wlp1s0 10.231.213.10 10.239.120.1 255.255.255.255 UGH 100 0 0 enp0s31f6 10.231.213.10 10.75.60.1 255.255.255.255 UGH 600 0 0 wlp1s0 10.239.120.0 0.0.0.0 255.255.252.0 U 100 0 0 enp0s31f6 169.254.0.0 0.0.0.0 255.255.0.0 U 1000 0 0 wlp1s0 qdt@qdt-shlab-pc /etc/iproute2 $ cat /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.231.213.10 nameserver 10.253.144.21 nameserver 10.253.157.26 search ap.qualcomm.com wlan.qualcomm.com 新机器pci错误 1) edit /etc/default/grub and and add pci=noaer to the line starting with GRUB_CMDLINE_LINUX_DEFAULT. It will look like this: GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash pci=noaer\" 2) run \"sudo update-grub\" 3) reboo 关于网络-域名debug host用来查DNS服务器, ip到域名, 域名到ip; nslookup有一样的效果 arp -a用来查本机arp表 网络debug用cat /var/log/messages | egrep \"dhclient|NetworkManager\" 用NetworkManager的话, nmcli也很有用$ host 10.239.120.175 175.120.239.10.in-addr.arpa domain name pointer qdt-shlab-awsdp3-010239120175.qualcomm.com. qdt@qdt-shlab-awsdp2 ~ $ host 10.239.120.12 12.120.239.10.in-addr.arpa domain name pointer qdt-shlab-awsdp4-010239120012.qualcomm.com. qdt@qdt-shlab-awsdp2 ~ $ host 10.239.120.179 179.120.239.10.in-addr.arpa domain name pointer qdt-shlab-awsdp2.qualcomm.com. $ arp -a -i enp0s31f6 pvg-b7-lab-fw-vlan337.qualcomm.com (10.239.120.1) at 00:1b:17:00:37:30 [ether] on enp0s31f6 qdt-shlab-awsdp4-010239120116.qualcomm.com (10.239.120.116) at 8c:fd:f0:06:95:0d [ether] on enp0s31f6 sudo cat /var/log/messages | egrep \"dhclient|NetworkManager\" uuidgen eth0 nmcli con show nslookup qdt-shlab-awsdp2 nmcli connection nmcli con show eth0 nmcli con modify eth0 ipv4.dhcp-hostname `hostname` 文本比较工具 apt install meld dhclient dhclient会获取一个ip, 并持续保持在后台用sudo dhclient -r会释放现在的ip, 并杀掉原后台dhclient; 注意需要用sudo多次执行dhclient会产生多个后台进程 无线网卡命令 基本命令 iw dev iw wlp1s0 link iw wlp1s0 scan 参考How to connect to a WPA/WPA2 WiFi network using Linux command line sudo wpa_passphrase Hydra >> /etc/wpa_supplicant/wpa.conf qdt@qdt-shlab-pc ~/Desktop $ cat /etc/wpa_supplicant/wpa.conf # reading passphrase from stdin network={ ssid=\"Hydra\" #psk=\"K5x48Vz3\" psk=529cd7878309812d395fb8bd999c203f35a83da1ca0a86fe83c0f7c5cd08efc4 } rfkill unblock all sudo ip link set wlp1s0 up sudo wpa_supplicant -B -D wext -i wlp1s0 -c /etc/wpa_supplicant/wpa.conf sudo dhclient wlp1s0 注: wpa_supplicant和dhclient都会在后台一直运行 hostname sudo nano /etc/hostname sudo nano /etc/hosts sudo 免密码 sudo -s visudo 在%sudo那行加 NOPASSWD: # User privilege specification root ALL=(ALL:ALL) ALL # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL # Allow members of group sudo to execute any command %sudo ALL=(ALL:ALL) NOPASSWD:ALL 不启动GUI sudo systemctl disable mdm sudo systemctl start mdm sudo systemctl enable mdm wiz的搜索功能 一般为\"或\"搜索 linux版可以这样搜:\"struct*file\" windows版: s:struct AND file 修改grub2默认启动项 修改/etc/default/grub saved是说让grub记录上次的启动项 $ cat /etc/default/grub # If you change this file, run 'update-grub' afterwards to update # /boot/grub/grub.cfg. # For full documentation of the options in this file, see: # info -f grub -n 'Simple configuration' GRUB_DEFAULT=saved GRUB_SAVEDEFAULT=true 还要运行 $ sudo update-grub2 解决中文乱码 iconv -f gbk -t utf8 readme.txt > out.txt 使用tmux $tmux ls 查看有几个session $tmux attach -t 1 进入第一个session c-b c 新建一个window c-b n 切换到下一个window c-b 0 1 2 3 4 5 6 7 8 9 选择window c-b & 退出当前window c-b % 在当前panel左右新建一个panel c-b \" 在当前panel上下新建一个panel c-b o 在panel之间切换 c-b ! 关闭所有分割出来的pannel c-b x 退出光标所在的panel c-b d 临时退出tmux, 进程不退出. 可用tmux attach再次进入 c-b ? 帮助 在tmux窗口里面敲exit退出当前窗口. tmux kill-server 终止tmux server, 会杀掉所有tmux client 系统操作 ? #列出所有快捷键；按q返回 d #脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话 D #选择要脱离的会话；在同时开启了多个会话时使用 Ctrl+z #挂起当前会话 r #强制重绘未脱离的会话 s #选择并切换会话；在同时开启了多个会话时使用 : #进入命令行模式；此时可以输入支持的命令，例如kill-server可以关闭服务器 [ #进入复制模式；此时的操作与vi/emacs相同，按q/Esc退出 ~ #列出提示信息缓存；其中包含了之前tmux返回的各种提示信息 窗口操作 c #创建新窗口 & #关闭当前窗口 数字键 #切换至指定窗口 p #切换至上一窗口 n #切换至下一窗口 l #在前后两个窗口间互相切换 w #通过窗口列表切换窗口 , #重命名当前窗口；这样便于识别 . #修改当前窗口编号；相当于窗口重新排序 f #在所有窗口中查找指定文本 面板操作 ” #将当前面板平分为上下两块 % #将当前面板平分为左右两块 x #关闭当前面板 ! #将当前面板置于新窗口；即新建一个窗口，其中仅包含当前面板 Ctrl+方向键 #以1个单元格为单位移动边缘以调整当前面板大小 Alt+方向键 #以5个单元格为单位移动边缘以调整当前面板大小 Space #在预置的面板布局中循环切换；依次包括even-horizontal、even-vertical、main-horizontal、main-vertical、tiled q #显示面板编号 o #在当前窗口中选择下一面板 方向键 #移动光标以选择面板 { #向前置换当前面板 } #向后置换当前面板 Alt+o #逆时针旋转当前窗口的面板 Ctrl+o #顺时针旋转当前窗口的面板 vim支持系统clipboard $apt install vim-gtk 在.vimrc里面加 set clipboard=unnamedplus 检查是否带clipboard功能 $vim --version 找+xterm_clipboard ubuntu查看系统中已安装的包 $dpkg -l | grep fcitx 这里的选项是list的意思 locate每天更新数据库 突然发现每天第一次开机硬盘狂转，机器非常慢。 调查发现是locate每天都在更新数据库。 这个文件/etc/crontab， 是个周期任务控制文件。 上面写了要把/etc/cron.daily里面的脚本每天执行一遍， 时间在每天6点25. 这就解释了为什么第一次开机硬盘会狂闪。 解决办法： cp /etc/cron.daily/locate /etc/cron.weekly/ 把每天执行改为每周执行一次 ubuntu基本开发包 apt install build-essential sudo apt-get install libncurses5-dev vim插件 sudo apt- install vim-addon-manager vim-scripts vim-addons install taglist apt install astyle cscope ctags global byj@byj-mint ~/.vim/plugin $ ls autoformat.vim defaults.vim gtags.vim mru.vim taglist.vim 开机自启动 修改/etc/rc.local 加一行 /home/byj/goagent/goagent/local/proxy.sh start SSD优化 修改/etc/rc.local echo 50 > /proc/sys/vm/dirty_ratio echo 10 > /proc/sys/vm/dirty_background_ratio echo 6000 > /proc/sys/vm/dirty_expire_centisecs echo 1000 > /proc/sys/vm/dirty_writeback_centisecs 修改/etc/fstab UUID=d039c104-49d4-464a-b3df-a962574fd46f / ext4 noatime,nodiratime,errors=remount-ro 0 1 升级kernel版本 到https://www.kernel.org 找稳定的版本号 到ubuntu网站上下载对应的版本http://kernel.ubuntu.com/~kernel-ppa/mainline 一共有三个文件linux-headers-3.16.2-031602-generic_3.16.2-031602.201409052035_amd64.deb linux-headers-3.16.2-031602_3.16.2-031602.201409052035_all.deb linux-image-3.16.2-031602-generic_3.16.2-031602.201409052035_amd64.deb mkdir linux-kernel-3.16.2 && mv ~/Downloads/linux-*.deb linux-kernel-3.16.2 安装cd linux-kernel-3.16.2 su dpkg -i *.deb update-grub(可以省略, 上面的命令已包含) 重启 调节笔记本亮度 su echo 80 > /sys/class/backlight/intel_backlight/brightness 笔记本省电模式 安装tlp: https://linrunner.de/tlp/ sudo add-apt-repository ppa:linrunner/tlp sudo apt install tlp tlp-rdw sudo systemctl start tlp sudo systemctl enable tlp 如果装了其他的省电工具 sudo systemctl stop laptop-mode sudo systemctl disable laptop-mode 查看cpu调频的信息: sudo cpupower frequency-info 我发现tlp会自动停掉wifi和蓝牙 笔记本触摸板 临时禁止触摸板：sudo modprobe -r psmouse 开启触摸板：sudo modprobe -a psmouse 永远禁用触摸板：sudo vi /etc/modprobe.d/blacklist.conf blacklist psmouse n7260无线网卡 on linux iwlwifi.ko 禁用8022.11n 临时: 有时不好用su modprobe -r iwlwifi modprobe iwlwifi 11n_disable=1 长久:su echo \"options iwlwifi 11n_disable=1\" >> /etc/modprobe.d/iwlwifi.conf 查看firmware版本 lshw 不跳转的google http://www.google.com/ncr "},"notes/git_日常使用.html":{"url":"notes/git_日常使用.html","title":"日常git使用","keywords":"","body":" git分支操作 查看分支 查看本地分支 查看远程分支 查看所有分支 本地创建新的分支 切换到新的分支 创建+切换分支 将新分支推送到github 删除本地分支 删除github远程分支 github Readme模板 我的git操作记录 修改作者和邮箱 修改commit时间 同步upstream的更改 复杂步骤 简单步骤 使用不同的用户名commit 同步remote已删除的分支 找到当前branch最近的tag 其他分支的修改拿到本分支 diff和patch 从其他git repo导入到新git库 Git global setup Create a new repository Push an existing folder Push an existing Git repository git新建远程分支 git 查看log摘要 git checkout branch git commit set格式 git 修改commit git undo reset git调整commit顺序 linux kernel:找到一个函数/字符串 相关的commit linux kernel: 找到一个commit对应的kernel release版本 保存现场 两个现场切换 git stash git查看某个版本的文件, 不checkout git设置允许push到非bare的库, 在remote端运行 git取消commit git大文件支持 git打包 push到远程repo 单一分支clone git下载远程分支 创建本地分支来跟踪远程分支 git merge本地改动 git比较某次commit的改动 查看单个文件 git比较本地修改 还是犯了remote错误 为什么git clone一个本地的repo没有branch? git切换分支 放弃本地更改 git merge 不commit git tag git log显示文件 git使用kdiff查看改动 git更改remote git 从别人那里pull git恢复文件 git clean, 删除所有未跟踪的文件 git查看clone地址 git分支操作 git clone https://github.com/siskinc/siskinc.github.io 查看分支 查看本地分支 $ git branch * master 查看远程分支 git branch -r 查看所有分支 git branch -a 本地创建新的分支 git branch [branch name] 切换到新的分支 git checkout [branch name] 创建+切换分支 git checkout -b [branch name] 相当于 git branch [branch name] git checkout [branch name] 将新分支推送到github git push origin [branch name] 删除本地分支 git branch -d [branch name] 删除github远程分支 git push origin :[branch name] github Readme模板 https://github.com/golangci/golangci-lint golangci-lint Fast linters runner for Go --- `golangci-lint` is a fast Go linters runner. It runs linters in parallel, uses caching, supports `yaml` config, has integrations with all major IDE and has dozens of linters included. ## Install `golangci-lint` - [On my machine](https://golangci-lint.run/usage/install/#local-installation); - [On CI/CD systems](https://golangci-lint.run/usage/install/#ci-installation). ## Documentation Documentation is hosted at https://golangci-lint.run. ## Badges ![Build Status](https://github.com/golangci/golangci-lint/workflows/CI/badge.svg) [![License](https://img.shields.io/github/license/golangci/golangci-lint)](/LICENSE) [![Release](https://img.shields.io/github/release/golangci/golangci-lint.svg)](https://github.com/golangci/golangci-lint/releases/latest) [![Docker](https://img.shields.io/docker/pulls/golangci/golangci-lint)](https://hub.docker.com/r/golangci/golangci-lint) [![Github Releases Stats of golangci-lint](https://img.shields.io/github/downloads/golangci/golangci-lint/total.svg?logo=github)](https://somsubhra.com/github-release-stats/?username=golangci&repository=golangci-lint) ## Contributors This project exists thanks to all the people who contribute. [How to contribute](https://golangci-lint.run/contributing/quick-start/). 我的git操作记录 修改作者和邮箱 https://stackoverflow.com/questions/750172/how-to-change-the-author-and-committer-name-and-e-mail-of-multiple-commits-in-gi git rebase -i 82350e5 git commit --amend --author \"Bai Yingjie \" git rebase --continue 修改commit时间 git commit --amend --date=\"Tue 20 Dec 2021 14:14:22 AM UTC\" --no-edit 同步upstream的更改 我有crosstool-ng的本地repogit@gitlabe1.ext.net.nokia.com:godevsig/crosstool-ng.git 里面有两个分支, godev和master 现在我想同步上游repo的更改 复杂步骤 # 直接拉上游更改 git checkout master git pull https://github.com/crosstool-ng/crosstool-ng # 上游更新push到组织repo. 此步必须 git push # 第二步, 切换到godev分支, 拉刚才push的master git checkout godev git pull origin master 提示merge失败 简单步骤 # 直接拉上游更改 git checkout master git pull https://github.com/crosstool-ng/crosstool-ng # 第二步, 切换到godev分支, rebase git checkout godev git rebase master # 用--skip可以跳过某个冲突的commit, 后续的commit会继续 git rebase --skip 使用不同的用户名commit git commit --author=\"Bai Yingjie \" 同步remote已删除的分支 # 查看 $ git remote show origin # 删除本地多余分支, 但已经checkout的分支还在 $ git remote prune origin # 手动删除本地checkout的分支 git branch -d [branch_name] 找到当前branch最近的tag yingjieb@9102a93a554e /repo/yingjieb/godevsig/golang-go $ git describe --tags --abbrev=0 go1.13.15 其他分支的修改拿到本分支 比如我在godevsig分支有个commit: e164f53422 2020-10-22 Bai Yingjie godevsig: add support to use gccgo for ppc 现在我想把它拿到release-branch.go1.13分支 # 先更新本地working tree git checkout release-branch.go1.13 # cherry pick后, 自动会commit git cherry-pick e164f53422 # 在release-branch.go1.13分支, 就有这个改动了. 但commit id不一样 222b096725 2020-10-22 Bai Yingjie godevsig: add support to use gccgo for ppc diff和patch 比如A和B都是一个repo的两个拷贝. 某种原因, 我想把A里面还没有commit的改动, 拿到B里面来 #在A中: git diff > ppc.patch #在B中: git apply /path/to/A/ppc.patch 从其他git repo导入到新git库 Git global setup git config --global user.name \"Bai Yingjie\" git config --global user.email \"yingjie.bai@nokia-sbell.com\" Create a new repository git clone git@bhgitlab.int.net.nokia.com:godev/golang-go.git cd golang-go touch README.md git add README.md git commit -m \"add README\" git push -u origin master Push an existing folder cd existing_folder git init git remote add origin git@bhgitlab.int.net.nokia.com:godev/golang-go.git git add . git commit -m \"Initial commit\" git push -u origin master Push an existing Git repository cd existing_repo git remote rename origin old-origin git remote add origin git@bhgitlab.int.net.nokia.com:godev/golang-go.git git push -u origin --all git push -u origin --tags #我的操作 cd crosstool-ng git remote rename origin old-origin git remote add origin git@gitlabe1.ext.net.nokia.com:godevsig/crosstool-ng.git #我只想有master分支 git push -u origin master #我指向有最新的tag git push -u origin crosstool-ng-1.24.0 其他方法 #直接改origin git remote set-url origin git@bhgitlab.int.net.nokia.com:godev/golang-go.git #或者加个remote git remote add gitlab git@bhgitlab.int.net.nokia.com:godev/golang-go.git #把之前默认的origin改成gitlab git push gitlab master git新建远程分支 比如现在在master分支上 # 创建本地分支, 并切换到这个新建的分支 git checkout -b test # 做修改 ... # push到远程分支, 格式git push : git push origin test:test 删除分支 git push origin :test //将一个空分支推送到远程即为删除 //或者 git push origin --delete test git 查看log摘要 git shortlog可以看repo的提交信息, 每个作者名下的commit都是哪些.git shortlog -s不输出具体的commit, 只给出数目统计 比如: $ git shortlog -s 1 Author A 42 Author B 5 Author C git checkout branch 这是个老生常谈的问题了, 用git branch -avv看到的branch 名字比如 remotes/origin/master-FNMS-63345-compatible-package-log-for-vonumgmt-gongc 用下面的命令checkout并跟踪branch, 跟踪的意思是git pull的时候会更新当前的working copygit checkout master-FNMS-62052-Heartbeat_support注意: 不要加-b选项, 我试下来-b会导致不track远程分支 去掉remotes/origin git commit set格式 git的命令很多时候支持传入commit set, 用man gitrevisions可以查到详细的语法格式比如: #生成从A到B的所有commit的patch, 不包括A, 但包括B git format-patch A..B git 修改commit 在本地还没提交的commit, 可以用git rebase -i来修改, 过程和hg histedit一样比如我要修改552476b75408 这个commit 77d3efca122b 2019-12-28 Bai Yingjie powerpc/io: use phys_addr_t in virt_to_phys/phys_to_virt 552476b75408 2019-11-25 Bai Yingjie powerpc/mpc85xx: also write addr_h to spin table for 64bit boot entry d5c2c21e7a74 2019-12-28 Bai Yingjie powerpc32/booke: consistently return phys_addr_t in __pa() 46cf053efec6 2019-12-22 Linus Torvalds Linux 5.5-rc3 那我要找到它的前一个commit:git rebase -i d5c2c21e7a74 把要修改的那条commit的状态从pick改为edit 此时, 修改要修改的文件, commit必须用: git commit --all --amend 确认无误后提交然后用 git rebase --continue 来完成这次修改 git undo reset git reset会把commit丢弃掉, 比如 Linux Mint 19.1 Tessa $ git ls 77d3efca122b 2019-12-28 Bai Yingjie powerpc/io: use phys_addr_t in virt_to_phys/phys_to_virt 552476b75408 2019-11-25 Bai Yingjie powerpc/mpc85xx: also write addr_h to spin table for 64bit boot entry d5c2c21e7a74 2019-12-28 Bai Yingjie powerpc32/booke: consistently return phys_addr_t in __pa() 46cf053efec6 2019-12-22 Linus Torvalds Linux 5.5-rc3 我执行了 git reset 552476b75408 最上面的commit就没了. 如果想把它找回来, 这时候就要用git reflog找到对应的操作历史 Linux Mint 19.1 Tessa $ git reflog 552476b75408 (HEAD -> master) HEAD@{0}: reset: moving to 552476b75408 77d3efca122b HEAD@{1}: rebase -i (finish): returning to refs/heads/master 77d3efca122b HEAD@{2}: rebase -i (pick): powerpc/io: use phys_addr_t in virt_to_phys/phys_to_virt 552476b75408 (HEAD -> master) HEAD@{3}: rebase -i (pick): powerpc/mpc85xx: also write addr_h to spin table for 64bit boot entry d5c2c21e7a74 HEAD@{4}: rebase -i (pick): powerpc32/booke: consistently return phys_addr_t in __pa() 46cf053efec6 (tag: v5.5-rc3, origin/master, origin/HEAD) HEAD@{5}: rebase -i (start): checkout 46cf053efec6 结果如下: yingjieb@yingjieb-VirtualBox ~/repo/linux Linux Mint 19.1 Tessa $ git reset 'HEAD@{1}' yingjieb@yingjieb-VirtualBox ~/repo/linux Linux Mint 19.1 Tessa $ git ls 77d3efca122b 2019-12-28 Bai Yingjie powerpc/io: use phys_addr_t in virt_to_phys/phys_to_virt 552476b75408 2019-11-25 Bai Yingjie powerpc/mpc85xx: also write addr_h to spin table for 64bit boot entry d5c2c21e7a74 2019-12-28 Bai Yingjie powerpc32/booke: consistently return phys_addr_t in __pa() 46cf053efec6 2019-12-22 Linus Torvalds Linux 5.5-rc3 git调整commit顺序 调整前, 我想把3d5ecc00a315 和ca81e2ba497d 顺序调换一下 b0389050a303 2019-12-28 Bai Yingjie powerpc/io: use phys_addr_t in virt_to_phys/phys_to_virt 3d5ecc00a315 2019-12-28 Bai Yingjie powerpc32/booke: consistently return phys_addr_t in __pa() ca81e2ba497d 2019-11-25 Bai Yingjie powerpc/mpc85xx: also write addr_h to spin table for 64bit boot entry 46cf053efec6 2019-12-22 Linus Torvalds Linux 5.5-rc3 用这个命令: #-i表示--interactive, 46cf053efec6是他们公共的父节点 git rebase -i 46cf053efec6 这个命令会跳出编辑框来, 只要把要调整的两行顺序调换一下就好了. 和hg histedit过程差不多 调整后: 77d3efca122b 2019-12-28 Bai Yingjie powerpc/io: use phys_addr_t in virt_to_phys/phys_to_virt 552476b75408 2019-11-25 Bai Yingjie powerpc/mpc85xx: also write addr_h to spin table for 64bit boot entry d5c2c21e7a74 2019-12-28 Bai Yingjie powerpc32/booke: consistently return phys_addr_t in __pa() 46cf053efec6 2019-12-22 Linus Torvalds Linux 5.5-rc3 linux kernel:找到一个函数/字符串 相关的commit 比如, drivers/i2c/busses/i2c-ocores.c里面, 3.10的时候还有of_i2c_register_devices, 但4.9就没有了. 我想找到哪个commit删除了这个函数的使用, 它又用了什么新方法? git log drivers/i2c/busses/i2c-ocores.c -S of_i2c_register_devices # 新修改: -S不能在最后 git log -S __noreturn arch/powerpc/include/asm/machdep.h linux kernel: 找到一个commit对应的kernel release版本 Linux Mint 19.1 Tessa $ git show 687b81d083c08 yingjieb@yingjieb-VirtualBox ~/repo/linux Linux Mint 19.1 Tessa $ git describe --contains 687b81d083c08 v3.12-rc1~140^2~14 保存现场 两个现场切换 git stash # 保存现场, --all包括所有untracked和iggnored文件 Linux Mint 19 Tara $ git stash push --all Saved working directory and index state WIP on hxt-dev-v17.08: 7030d3888 Extand RXD TXD size to 2048 # 可以用-m加说明 Linux Mint 19 Tara $ git stash push --all -m \"aarch64 17.11\" Saved working directory and index state On 17.11: aarch64 17.11 # 看看当前的stash Linux Mint 19 Tara $ git stash list stash@{0}: WIP on hxt-dev-v17.08: 7030d3888 Extand RXD TXD size to 2048 # 看某个stash的改动 Linux Mint 19 Tara $ git stash show stash@{0} config/common_base | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) # 此时我切换到一个新的tag v18.05, 编译, 再stash; Linux Mint 19 Tara $ git stash list stash@{0}: WIP on (no branch): a5dce5555 version: 18.05.0 stash@{1}: WIP on hxt-dev-v17.08: 7030d3888 Extand RXD TXD size to 2048 # 还是切换回hxt-dev-v17.08 git checkout hxt-dev-v17.08 # 这里用apply, 而不是pop; 因为我想反复切换现场 git stash apply stash@{1} #其实还是pop好 git stash pop stash@{1} # show tag with message git tag -ln git查看某个版本的文件, 不checkout git show remotes/origin/releases:drivers/net/mlx5/mlx5_rxtx.h git设置允许push到非bare的库, 在remote端运行 git config receive.denyCurrentBranch ignore git取消commit 如果commit还没有push, 可以直接 git reset changset 此时工作区的内容还是保留的 git大文件支持 wget https://github.com/git-lfs/git-lfs/releases/download/v2.0.0/git-lfs-linux-amd64-2.0.0.tar.gz https://github.com/git-lfs/git-lfs/wiki/Tutorial git lfs track \"*.tar.*\" git add .gitattributes git add * git commit git lfs ls-files 注: 好像不好用 git打包 yingjieb@yingjieb-gv /local/mnt/workspace/common/ls/kernel $ git archive remotes/qserver/qserver-4.9 -o ../../archives/kernel.tar.gz git archive HEAD -o ../dpdk-intel-only-cacheline64.tar.gz push到远程repo 我想把本地的linux的repo push到云服务器上.现在云服务器上创建个空的repo库, 没有这个空的库, 在本地push的时候会说没有远程的库 baiyingjie@caviumtech /home/share/src/alikernel/caviumkernel $ git init Initialized empty Git repository in /home/share/src/alikernel/caviumkernel/.git/ 然后在本地, 先要add一个remote, 网上都是图省事add origin, 但我这里是不行的, 因为origin是我的上游库, 我还要拿来更新 注意这里的ssh库的格式 byj@mint ~/repo/git/cavium/thunder/sdk/sdk-master/linux/kernel/linux-aarch64 $ git remote add caviumtech ssh://baiyingjie@caviumtech.cn/home/share/src/alikernel/caviumkernel $ git remote -v caviumtech ssh://baiyingjie@caviumtech.cn/home/share/src/alikernel/caviumkernel (fetch) caviumtech ssh://baiyingjie@caviumtech.cn/home/share/src/alikernel/caviumkernel (push) origin git://cagit1.caveonetworks.com/thunder/sdk/linux-aarch64.git (fetch) origin git://cagit1.caveonetworks.com/thunder/sdk/linux-aarch64.git (push) 然后就可以push了, 这里面的-u据说是: 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。但我怀疑这里有个副作用, 就是把本地的分支指向了这个远程分支, 而我需要保留这个指向到origin byj@mint ~/repo/git/cavium/thunder/sdk/sdk-master/linux/kernel/linux-aarch64 $ git push caviumtech alibaba-4.2 单一分支clone git clone git://cagit1.caveonetworks.com/thunder/boot/atf.git -b thunder-master --single-branch --depth=10 git下载远程分支 git默认会把remote上的更新都fetch下来的, 但我这里想下的一个分支为什么没有呢? $ git branch -avv * thunder-master efe9731 [origin/thunder-master] i2c: thunderx: update to v8 remotes/origin/thunder-master efe9731 i2c: thunderx: update to v8 $ cat .git/FETCH_HEAD efe97310c4c06b046f2f14006ccc990243da9307 branch 'thunder-master' of git://cagit1.caveonetworks.com/thunder/sdk/linux-aarch64 git fetch origin alibaba-4.2 以上这句会更新.git/FETCH_HEAD, 不推荐用 关键是这里 $ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \"origin\"] url = git://cagit1.caveonetworks.com/thunder/sdk/linux-aarch64.git fetch = +refs/heads/thunder-master:refs/remotes/origin/thunder-master 添加下面这行 fetch = +refs/heads/alibaba-4.2:refs/remotes/origin/alibaba-4.2 [branch \"thunder-master\"] remote = origin merge = refs/heads/thunder-master 这里面origin是个默认的名字, 来指代原始url 以后git branch -avv 就能看到两个 $ git branch -avv * thunder-master efe9731 [origin/thunder-master] i2c: thunderx: update to v8 remotes/origin/alibaba-4.2 0e253af Compiler bug workaround!!! remotes/origin/thunder-master efe9731 i2c: thunderx: update to v8 然后就可以checkout了 $ git checkout alibaba-4.2 创建本地分支来跟踪远程分支 root@CVMX55 ~/src/git/LuaJIT # git branch -avv * master 3d4c9f9 [origin/master] FFI: Fix SPLIT pass for CONV i64.u64. remotes/origin/HEAD -> origin/master remotes/origin/master 3d4c9f9 FFI: Fix SPLIT pass for CONV i64.u64. remotes/origin/v2.1 22e7b00 DynASM/x64: Fix for full VREG support. root@CVMX55 ~/src/git/LuaJIT # git checkout --track remotes/origin/v2.1 Branch v2.1 set up to track remote branch v2.1 from origin. Switched to a new branch 'v2.1' or # git checkout -b v2.1 remotes/origin/v2.1 git merge本地改动 先暂存本地改动 git stash git merge origin/master pop出来的时候, 已经merge好了? git stash pop git比较某次commit的改动 git difftool a450650dd9cbfff454a1f5cd443b8a2a3ed3206f^! git diff 15dc8^..15dc8 git show a450650dd9cbfff454a1f5cd443b8a2a3ed3206f 查看单个文件 git show 6194e7c4 src/omci/mealarm.go git比较本地修改 git add以后就不能直接diff本地修改了, 需要加cached选项 git diff --cached 还是犯了remote错误 背景是我在mint上拷贝了ma的库, 从美国更新, remote是美国, 可以看到有很多remote分支. 但用的命令是git pull, 它只会更新当前branch 服务器上的库是clone mint机器的, 所以此时remote是mint, 只有我曾经在mint上checkout过的两个本地分支;而我只更新了master, 而stable不是当前分支, 所以mint上的local的stable分支从来不变.导致服务器上的remotes/origin/stable分支也不变.所以fae从stable merge也就从来没有新东西. 解决办法是想办法把所有mint的本地分支都更新. 比如checkout后pull或者rebase(感觉rebase更靠谱点) yingjie@cavium /cavium/repo/git/thunder/linux-aarch64 $ git branch -a * fae thunder-master thunder-stable remotes/origin/HEAD -> origin/thunder-master remotes/origin/thunder-master remotes/origin/thunder-stable thunder-stable 和 remotes/origin/thunder-stable不是一回事 前者是local的分支, 后者和origin/thunder-stable是一个东西, 是remote上的分支git log thunder-stable 和 git log origin/thunder-stable是两码事 为什么git clone一个本地的repo没有branch? 现象:从原始repo clone的库(比如叫repo1)用git branch -a看是有很多分支的; 但从repo1再clone为repo2, 就只有master分支了. 原因是: 第一次clone: 对repo1来说, origin是原始库第二次clone: 对repo2来说, origin是repo1而clone会带origin库本地的branch, 第一次原始库里面的branch带到repo1里, 但不在repo1本地;所以第二次clone发现repo1本地没有branch, 就没clone 解决办法: 不能连到原始库:在repo1里面跟踪每个分支 能连原始库:在repo2里面git remote add 原始库 [补充]: git clone的是remote库当前的分支 git fetch是说从remote库拿到最新的东西, \"应该是包括所有分支的东西\" git merge 分支名是说把一个分支merge到当前工作区(也可以说是当前分支), 默认会直接commit git pull 是前面两个动作的组合, fetch都会成功, 但如果本地的当前分支和remote的当前分支不是同一个, git会抱怨不知道merge哪个分支. 在fae分支push, 提示receive.denyCurrentBranch问题, 应该设为ignore git push如果和remote的当前分支一样就应该会成功 git checkout -b fae应该隐含了commit操作, 或类似的操作, 因为本地repo会记住这个分支 git切换分支 本地内容也会切换 git checkout master git checkout fae 放弃本地更改 放弃本分支的修改 git reset --hard 完整的写法是 git reset --hard origin/master git merge 不commit git merge master --no-commit git tag 新增一个tag git tag -a -m \"tag 0.1\" 0.1 push到remote git push --tag git log显示文件 git log --stat git使用kdiff查看改动 $ git difftool 4bc4c5c6fdf06d1d7b8fd1b6dbe1f6d721d05389 965697b896aeb5993e973942aebac19945436e87 -d 也可以和上一次比, 用revision~1表示上次的 $ git difftool bc6c72ff37dd9deb7c83d9bf77e713b747976140 bc6c72ff37dd9deb7c83d9bf77e713b747976140~1 -d git更改remote git remote set-url origin ssh://byj@192.168.1.217/home/byj/repo/git/thunder/ma/sdk/linux/kernel/linux-aarch64/ git remote -v git 从别人那里pull $ git pull ssh://byj@192.168.1.217//home/byj/repo/git/thunder/ma/sdk master 后面的master是分支名 不加的话, 会失败: You asked to pull from the remote 'ssh://byj@192.168.1.217/home/byj/repo/git/thunder/ma/sdk', but did not specify a branch. Because this is not the default configured remote for your current branch, you must specify a branch on the command line. git恢复文件 git checkout filename git clean, 删除所有未跟踪的文件 git clean -fdx 参考: For all git clean commands, add -n for dry-run. Clean all untracked files and directories, except ignored files (listed in .gitignore): git clean -fd Also clean ignored files: git clean -fdx Only clean ignored files (useful for removing build artifacts, to ensure a fresh build from scratch, without discarding new files/directories you haven't committed yet): git clean -fdX git查看clone地址 git remote -v "},"notes/向kernel提交补丁.html":{"url":"notes/向kernel提交补丁.html","title":"向kernel提交补丁","keywords":"","body":" 背景 使用git 准备kernel的git库 patch从hg到git 写git commit 生成用于提交upstream的patch 检查patch格式 测试发送 找到maintainer 正式发送 其实没这么简单 到patchwork上查提交 补充, 用手机开热点可以发送 先要设置gmail, 打开less secure app access 再用git send-email发送 以上不行要打开临时访问, 据说只有10分钟. 参考 背景 我在linux-fsl里面有一个commit, 想提交到社区. 327 32db966721ed tip linux-4.9-preparation 2019-11-21 Bai Yingjie PPC32 smp boot: also write addr_h to spin table for a 64bit boot entry 下面记录一下如何提交补丁到upstream社区 使用git 我们内部使用hg, 但提交kernel的补丁要用git. 所以需要先在kernel的git上生成patch. 先要安装git-email, 用来发送patch apt install git-email 在生成patch之前, 我要先查一下kernel的commit格式. 比如, 针对我要提交patch的文件, 看一下别人都怎么提交的: git ls arch/powerpc/platforms/85xx/smp.c 看了一下, 大部分commiter都用powerpc/mpc85xx来开头, 表示子系统. 提交者主要来自freescale.com, 也有gmail.com, windriver.com等待. OK. 可以准备patch了 准备kernel的git库 需要有个最新的kernel的git库 git clone https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git 或者已经有的库更新到最新: git pull patch从hg到git hg export可以生成hg的patch, 但我没找到直接可以给git用的patch. 所以我们用半手工的方式生成patch #先生成纯内容的patch hg diff --git -c 327 | nc 11985 #在kernel的git库, 这里我用虚拟机clone的 nc 172.24.213.190 11985 > 85xx.patch #patch打到kernel git apply 85xx.patch 写git commit #写commit git add arch/powerpc/platforms/85xx/smp.c #用-s生成Signed-off-by:字段. 内核commit惯例 git commit -s commit之后, 这个patch是这样的: f3f83cac875a 2019-11-25 Bai Yingjie powerpc/mpc85xx: also write addr_h to spin table for 64bit boot entry 219d54332a09 2019-11-24 Linus Torvalds Linux 5.4 b8387f6f3495 2019-11-24 Linus Torvalds Merge branch 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs 3e5aeec0e267 2019-10-19 Maxime Bizon cramfs: fix usage on non-MTD device 生成用于提交upstream的patch kernel的patch都是通过邮件发送的. 所以我要先生成patch. 用 git format-patch HEAD~ 生成一系列patch 这里我只有一个改动, 用: Linux Mint 19.1 Tessa $ git format-patch HEAD~ -o outgoing outgoing/0001-powerpc-mpc85xx-also-write-addr_h-to-spin-table-for-.patch 生成的patch在outgoing文件夹下面. 一次邮件发送多个patch的时候, 用outgoing保存多个patch更方便. 看看这个文件: 它就是个文本格式的邮件. 这些patch用git am 就被社区维护者打到kernel代码里. 注: patch的后续版本可以用 git format-patch -v2 HEAD~来format, 邮件标题会自动加V2 多个patch的例子Linux Mint 19.1 Tessa $ git format-patch HEAD~3 -o outgoing outgoing/0001-powerpc-mpc85xx-also-write-addr_h-to-spin-table-for-.patch outgoing/0002-powerpc32-booke-consistently-return-phys_addr_t-in-_.patch outgoing/0003-powerpc-io-use-phys_addr_t-in-virt_to_phys-phys_to_v.patch 按照惯例, 邮件标题会加上[PATCH], 比如Subject: [PATCH] powerpc/mpc85xx: also write addr_h to spin table for 64bit 发送邮箱就是我gitconfig的邮箱: Linux Mint 19.1 Tessa $ cat ~/.gitconfig [user] name = Bai Yingjie email = byj.tea@gmail.com [sendemail] smtpencryption = tls smtpserver = smtp.gmail.com smtpuser = byj.tea@gmail.com smtpserverport = 587 [core] editor = vim [diff] tool = vimdiff [merge] tool = meld [alias] ls = log '--pretty=format:\"%h %ad%x09%an%x09%s\"' --date=short lg = log --decorate --stat --graph [push] default = matching [http] proxy = http://135.245.48.34:8000 检查patch格式 在邮件发送patch之前, 用./scripts/checkpatch.pl检查一下patch的格式是否符合kernel的惯例. 如果有问题, 用git commit --amend 来做相应修改. 这里我的patch格式没问题: Linux Mint 19.1 Tessa $ ./scripts/checkpatch.pl outgoing/* total: 0 errors, 0 warnings, 14 lines checked outgoing/0001-powerpc-mpc85xx-also-write-addr_h-to-spin-table-for-.patch has no obvious style problems and is ready for submission. 测试发送 git send-email --to byj8389@qq.com outgoing/* 找到maintainer kernel提供了找到相关maintainer的脚本:scripts/get_maintainer.pl Linux Mint 19.1 Tessa $ ./scripts/get_maintainer.pl outgoing/0001-powerpc-mpc85xx-also-write-addr_h-to-spin-table-for-.patch Scott Wood (maintainer:LINUX FOR POWERPC EMBEDDED PPC83XX AND PPC85XX) Kumar Gala (maintainer:LINUX FOR POWERPC EMBEDDED PPC83XX AND PPC85XX) Benjamin Herrenschmidt (supporter:LINUX FOR POWERPC (32-BIT AND 64-BIT)) Paul Mackerras (supporter:LINUX FOR POWERPC (32-BIT AND 64-BIT)) Michael Ellerman (supporter:LINUX FOR POWERPC (32-BIT AND 64-BIT)) linuxppc-dev@lists.ozlabs.org (open list:LINUX FOR POWERPC EMBEDDED PPC83XX AND PPC85XX) linux-kernel@vger.kernel.org (open list) 这个脚本也可以对文件使用: ./scripts/get_maintainer.pl -f arch/powerpc/platforms/85xx/smp.c 效果是一样的. 可以看到, Scott Wood 和Kumar Gala 是maintainer, 是email要to的人. 其他人cc 正式发送 git send-email outgoing/* --cc-cmd './scripts/get_maintainer.pl --norolestats --git outgoing/*' --to 'Scott Wood ' --to 'Kumar Gala ' 其实没这么简单 如果这么简单就好了. 实际上, 我不能通过gmail的smtp发送patch. 在公司, 587端口被防火墙了. 在家, google更是连不上. 但gmail邮箱是我主力提交代码的邮箱, 我还不想换. 所以, 需要曲线救国的方案. 如下: 先注册一个126的邮箱, 而且一定要在其设置里把smtp服务打开. 打开这个服务, 需要再设置个\"授权码\", 用于第三方登录的时候, 代替邮箱密码. 然后用如下命令提交patch, 就是说让126邮箱来发这个patch, 但内容还是我之前生成的, 在patch里用的是gmail邮箱. 先试一试: git send-email 0001-powerpc-mpc85xx-also-write-addr_h-to-sp.patch --smtp-debug=1 --to yingjie.bai@nokia-sbell.com --smtp-encryption=ssl --smtp-server=smtp.126.com --smtp-server-port=994 --smtp-user=yingjie_bai@126.com --from=yingjie_bai@126.com 正式提交版本: git send-email 0001-powerpc-mpc85xx-also-write-addr_h-to-sp.patch --smtp-debug=1 --to 'Scott Wood ' --to 'Kumar Gala ' --cc 'Benjamin Herrenschmidt ' --cc 'Paul Mackerras ' --cc 'Michael Ellerman ' --cc 'linuxppc-dev@lists.ozlabs.org' --cc 'linux-kernel@vger.kernel.org' --smtp-encryption=ssl --smtp-server=smtp.126.com --smtp-server-port=994 --smtp-user=yingjie_bai@126.com --from=yingjie_bai@126.com git send-email outgoing/* --cc-cmd './scripts/get_maintainer.pl --norolestats --git outgoing/*' --to 'Scott Wood ' --to 'Kumar Gala ' --smtp-encryption=ssl --smtp-server=smtp.126.com --smtp-server-port=994 --smtp-user=yingjie_bai@126.com --from=yingjie_bai@126.com 到patchwork上查提交 linux kernel mail list 本次提交 补充, 用手机开热点可以发送 先要设置gmail, 打开less secure app access 再用git send-email发送 多试几次, 有的时候卡住, 有的时候可以输入密码. 第一次会有下面的提示, 按照提示的URL地址打开浏览器, google提示新设备access, 要发手机验证码验证 Password for 'smtp://byj.tea@gmail.com@smtp.gmail.com:587': 5.7.14 5.7.14 Please log in via your web browser and then try again. 5.7.14 Learn more at 5.7.14 https://support.google.com/mail/answer/78754 b65sm47429126pgc.18 - gsmtp 验证好了再次发送. 大部分时间还是不行 偶尔可以 以上不行要打开临时访问, 据说只有10分钟. https://accounts.google.com/DisplayUnlockCaptcha 参考 简单全面的patch提交过程 FirstKernelPatch https://www.kernel.org/doc/html/v4.17/process/submitting-patches.html "},"notes/linux_ssh_relay.html":{"url":"notes/linux_ssh_relay.html","title":"ssh relay","keywords":"","body":" ssh relay on home server login to relay server from any PC ssh keep alive make it persistent ssh relay you want to access your home server behind a firewall or nat from outside. you must have a public server(relay server) 122.235.139.194(sh_cvmx shanghaicavm!), which both home server and outside PC can access. on home server homeserver~$ ssh -fN -R 10022:localhost:22 sh_cvmx@122.235.139.194 10022 is an unused port on relay server login to relay server ssh sh_cvmx@122.235.139.194 shanghaicavm! check 10022 port, shoud be like this: relayserver~$ sudo netstat -nap | grep 10022 tcp 0 0 127.0.0.1:10022 0.0.0.0:* LISTEN 8493/sshd from any PC login to relay server ssh sh_cvmx@122.235.139.194 shanghaicavm! Then on relay server, login to home server: relayserver~$ ssh -p 10022 cvm@localhost cvmx@sh ssh keep alive # vim /etc/ssh/ssh_config Host * ServerAliveInterval 60 ServerAliveCountMax 2 sudo service ssh restart make it persistent passwordless ssh login, on home server ssh-keygen -t rsa -P '' cat ~/.ssh/id_rsa.pub | ssh root@112.74.210.173 \"cat >> ~/.ssh/authorized_keys\" 注: 有的时候还是不能免密登录, 此时要该服务端的权限 chmod 600 authorized_keys yum install autossh homeserver~$ autossh -M 10985 -fN -o \"PubkeyAuthentication=yes\" -o \"StrictHostKeyChecking=false\" -o \"PasswordAuthentication=no\" -o \"ServerAliveInterval 60\" -o \"ServerAliveCountMax 3\" -R 112.74.210.173:11985:localhost:22 root@112.74.210.173 ssh root@112.74.210.173 Cavium@sh ssh -p 11985 cvm@localhost cvmx@sh "},"notes/gitlab_ci.html":{"url":"notes/gitlab_ci.html","title":"gitlab CI","keywords":"","body":" 安装和注册gitlab runner runner介绍 Group runner Set up a group Runner manually 前置条件:安装docker 安装runner 注册runner runner使用和.gitlab-ci.yml runner管理 image和services docker runner和shell runner job和script 全局配置 stages和workflow include其他yml 可配参数参考 预定义的变量 How to do continuous integration like a boss touble shooting 解决docker内git clone/下载失败问题 merge request gitlab支持pipeline, 官方详细参考, 很全, 很细节 docker方式参考, 实用 快速入门 安装和注册gitlab runner runner介绍 gitlab runner简介是用来执行工程根目录下的.gitlab-ci.yml. 有三种类型的runner Shared (for all projects) Group (for all projects in a group) Specific (for specific projects) 不同的job由不同的runner执行 这里我们实用Group runner Group runner group的admin可以创建group runner. 到gitlab Settings页面 CI/CD下面的Runner配置页面, 找下面的信息 比如这个是https://gitlabe1.ext.net.nokia.com/groups/godevsig 的group runner信息 Set up a group Runner manually Install GitLab Runner Specify the following URL during the Runner setup: https://gitlabe1.ext.net.nokia.com/ Use the following registration token during setup: Aprw1hQ6nuxyra5dzVwQ Start the Runner! 前置条件:安装docker docker安装官方参考 安装完毕后, 如果显示docker ps socket权限问题, 则需要把用户加入到docker组, 特别的, 这里要把gitlab-runner加进去 sudo usermod -a -G docker gitlab-runner 安装runner 增加gitlab源 # For Debian/Ubuntu/Mint curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash # For RHEL/CentOS/Fedora curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.rpm.sh | sudo bash 安装 # For Debian/Ubuntu/Mint export GITLAB_RUNNER_DISABLE_SKEL=true; sudo -E apt-get install gitlab-runner # For RHEL/CentOS/Fedora export GITLAB_RUNNER_DISABLE_SKEL=true; sudo -E yum install gitlab-runner 注册runner 根据上面工程的信息, 注册runner sudo gitlab-runner register \\ --url \"https://gitlabe1.ext.net.nokia.com/\" \\ --description \"docker-godevsig\" \\ --registration-token \"Aprw1hQ6nuxyra5dzVwQ\" \\ --executor \"docker\" \\ --tag-list \"docker-generic\" \\ --docker-image alpine:latest 在ubuntu上, 看runner服务的状态 yingjieb@cloud-server-1:~$ systemctl status gitlab-runner ● gitlab-runner.service - GitLab Runner Loaded: loaded (/etc/systemd/system/gitlab-runner.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2020-09-19 16:11:41 UTC; 2 days ago Main PID: 1944 (gitlab-runner) Tasks: 35 (limit: 4915) CGroup: /system.slice/gitlab-runner.service └─1944 /usr/bin/gitlab-runner run --working-directory /home/gitlab-runner --config /etc/gitlab-runner/config.toml --service gitlab-runner --syslog --user gitlab-runner 工作目录是/home/gitlab-runner config文件/etc/gitlab-runner/config.toml 使用gitlab-runner用户 runner使用和.gitlab-ci.yml runner管理 yingjieb@cloud-server-1:~$ gitlab-runner -h 支持很多命令 start stop restart status register unregister install uninstall 等等 image和services image: 可以放在default里面, 表示runner的基础docker镜像 services: 也是docker镜像, 是给image提供服务的镜像 service的玩法是启动一个service镜像, 可以指定镜像, 可以修改镜像的entrypoint, 可以改默认 docker runner和shell runner runner可以run在docker里面, 也可以是实际host的shell docker runner使用说明 shell runner使用说明 job和script job是runner的基础执行单元, job之间是并行执行的. job是用户定义的, 但不能是保留字 job的script是必须的, 可以有其他可选配置 官方例子for go: image: golang:latest variables: # Please edit to your GitLab project REPO_NAME: gitlab.com/namespace/project # The problem is that to be able to use go get, one needs to put # the repository in the $GOPATH. So for example if your gitlab domain # is gitlab.com, and that your repository is namespace/project, and # the default GOPATH being /go, then you'd need to have your # repository in /go/src/gitlab.com/namespace/project # Thus, making a symbolic link corrects this. before_script: - mkdir -p $GOPATH/src/$(dirname $REPO_NAME) - ln -svf $CI_PROJECT_DIR $GOPATH/src/$REPO_NAME - cd $GOPATH/src/$REPO_NAME stages: - test - build - deploy format: stage: test script: - go fmt $(go list ./... | grep -v /vendor/) - go vet $(go list ./... | grep -v /vendor/) - go test -race $(go list ./... | grep -v /vendor/) compile: stage: build script: - go build -race -ldflags \"-extldflags '-static'\" -o $CI_PROJECT_DIR/mybinary artifacts: paths: - mybinary 全局配置 有几个配置可以配成全局的 stages和workflow stages是顺序执行的 stages: - build - test - deploy workflow是全局的执行条件, 是条件规则集合, 规则依次匹配 workflow: rules: - if: '$CI_PIPELINE_SOURCE == \"schedule\"' when: never - if: '$CI_PIPELINE_SOURCE == \"push\"' when: never - when: always This example never allows pipelines for schedules or push (branches and tags) pipelines, but does allow pipelines in all other cases, including merge request pipelines. workflow有官方模板可以参考 include其他yml 有4种include类型 Method Description local Include a file from the local project repository. file Include a file from a different project repository. remote Include a file from a remote URL. Must be publicly accessible. template Include templates that are provided by GitLab. 可配参数参考 Parameter details 预定义的变量 有些变量是预定义的 比如 CI_PROJECT_DIR CI_BUILDS_DIR CI_PIPELINE_SOURCE CI_COMMIT_TAG CI_COMMIT_BRANCH 其中几个在if条件里挺有用: Example rules Details if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"' Control when merge request pipelines run. if: '$CI_PIPELINE_SOURCE == \"push\"' Control when both branch pipelines and tag pipelines run. if: $CI_COMMIT_TAG Control when tag pipelines run. if: $CI_COMMIT_BRANCH Control when branch pipelines run. How to do continuous integration like a boss 参考gitlab本身的ci配置 和这篇攻略 touble shooting 解决docker内git clone/下载失败问题 原因是docker内的mtu设置比host大. 修改方法如下 增加docker的配置文件, 指定mtu yingjieb@cloud-server-1:~$ cat /etc/docker/daemon.json { \"mtu\": 1400 } 然后重启docker daemon sudo systemctl restart docker merge request 一个forked repo的developer给parent发出merge request, 会在forked repo下面执行gitlab ci的. 过程如下 Fork a parent project. Create a merge request from the forked project that targets the master branch in the parent project. A pipeline runs on the merge request. A maintainer from the parent project checks the pipeline result, and merge into a target branch if the latest pipeline has passed. Currently, those pipelines are created in a forked project, not in the parent project. This means you cannot completely trust the pipeline result, because, technically, external contributors can disguise their pipeline results by tweaking their GitLab Runner in the forked project. There are multiple reasons why GitLab doesn’t allow those pipelines to be created in the parent project, but one of the biggest reasons is security concern. External users could steal secret variables from the parent project by modifying .gitlab-ci.yml, which could be some sort of credentials. This should not happen. 目前的状态是, forked repo发出的merge request不能在parent执行. 有个proposal解决这个问题: Allow fork pipelines to run in parent project 但现在的版本12.7.6还没有这个功能. "},"notes/docker_操作记录.html":{"url":"notes/docker_操作记录.html","title":"docker操作记录","keywords":"","body":" docker加proxy docker限制CPU和内存 使用nsenter进入容器 清除不用的image 清除不用的container 重载entrypoint启动 解决连接/var/run/docker.sock权限问题 docker用root登陆 docker国内代理 20210123更新 1、配置镜像地址 2、国内镜像地址 2020 7月 docker ubuntu for golang gentoo docker 清理空间 docker保存镜像 history 进入shell 获取每个container的ip docker run docker commit 容易混淆的概念入门讲解 docker tag可以push到私有库? 在docker里面起systemd? Dockerfile: ENTRYPOINT vs CMD Repository和Registry 用新的command启动一个container image和container的区别? What's an Image? docker加proxy 在docker run命令行加: --env http_proxy=\"http://10.158.100.6:8080/\" docker限制CPU和内存 限制只有2个CPU, 2G内存 docker run --cpus=2 -m 2g --rm --runtime=runsc -it --name=test centos:7 bash 参考: https://docs.docker.com/config/containers/resource_constraints/ 举例: If you have 1 CPU, each of the following commands guarantees the container at most 50% of the CPU every second. $ docker run -it --cpus=\".5\" ubuntu /bin/bash Which is the equivalent to manually specifying --cpu-period and --cpu-quota; $ docker run -it --cpu-period=100000 --cpu-quota=50000 ubuntu /bin/bash 使用nsenter进入容器 docker ps找到container ID, 比如是908592cfba96 找到这个容器的pid docker inspect -f{ {.State.Pid} } 908592cfba96, 比如得到10394 进入容器(需要root): nsenter -m -t 10394 bash 容器里看到的和docker exec -it 908592cfba96 bash一样 注: 这个10394进程一般就是对应containerd-shim的子进程. nsenter - run program with namespaces of other processes 原理大概就是clone子进程的时候加各种new标记, 比如CLONE_NEWNS, CLONE_NEWIPC, CLONE_NEWPID等等. 默认是进入一个新的name space, 并执行命令. 后面执行的命令对host的空间没影响. 用-t选项可以理解为attach到target pid的name space. 清除不用的image The docker image prune command allows you to clean up unused images. By default, docker image prune only cleans up dangling images. A dangling image is one that is not tagged and is not referenced by any container. To remove dangling images: docker image prune 清除不用的container When you stop a container, it is not automatically removed unless you started it with the --rm flag. To see all containers on the Docker host, including stopped containers, use docker ps -a. You may be surprised how many containers exist, especially on a development system! A stopped container’s writable layers still take up disk space. To clean this up, you can use the docker container prune command. $ docker container prune 重载entrypoint启动 有的docker image起不来, 是因为默认的entrypoint启动失败. 此时重载entrypoint到bash一般可启动 docker run -itd --user $(id -u):$(id -g) -v /etc/passwd:/etc/passwd:ro -v /etc/group:/etc/group:ro -v /home/$(whoami):/home/$(whoami) -v /repo/$(whoami):/repo/$(whoami) -w /repo/$(whoami) --entrypoint=/bin/bash docker-registry-remote.artifactory-blr1.int.net.nokia.com/codercom/code-server 对于entrypoint为空的docker docker run --rm -itd --user $(id -u):$(id -g) -v /etc/passwd:/etc/passwd:ro -v /etc/group:/etc/group:ro -v /home/$(whoami):/home/$(whoami) -v /repo/$(whoami):/repo/$(whoami) -w /repo/$(whoami) godevsig/godev-tool /bin/bash fef7fdacb79a78c2e8467e1ff6cfdd38ac0e7072eebfa30df33b8f2805faa020 docker attach fef7fdacb79a78c2e8467e1ff6cfdd38ac0e7072eebfa30df33b8f2805faa020 解决连接/var/run/docker.sock权限问题 将用户加入到docker 组 sudo usermod -a -G docker $USER docker用root登陆 一般的基础镜像比如ubuntu, 不知道root密码, 但可以这样登陆 docker exec -it -u root cf13ed39f0d9 bash docker国内代理 20210123更新 似乎我后来改用了aliyun的镜像... cat /etc/docker/deamon.json { \"registry-mirrors\": [ \"https://ryh6l7pc.mirror.aliyuncs.com\" ] } 1、配置镜像地址 Docker客户端版本大于 1.10.0 的用户 可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器 { \"registry-mirrors\": [ \"https://docker.mirrors.ustc.edu.cn\" ] } 重启docker和deamon sudo systemctl daemon-reload sudo systemctl restart docker 2、国内镜像地址 也可以免费使用阿里云的镜像。登陆阿里云，然后访问：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors 按照官方提供的文档进行操作即可。 镜像地址： 1) 阿里云 docker hub mirror https://registry.cn-hangzhou.aliyuncs.com 2) 腾讯云 docker hub mirror https://mirror.ccs.tencentyun.com 3) 华为云 https://05f073ad3c0010ea0f4bc00b7105ec20.mirror.swr.myhuaweicloud.com 4) docker中国 https://registry.docker-cn.com 5) 网易 http://hub-mirror.c.163.com 6) daocloud http://f1361db2.m.daocloud.io 2020 7月 我用微软的Azure的代理(dockerhub.azk8s.cn)不错: 详见: https://github.com/Azure/container-service-for-azure-china/blob/master/aks/README.md#22-container-registry-proxy docker pull dockerhub.azk8s.cn/yingjieb/godev-vscode:latest 有人总结的国内列表:https://www.jianshu.com/p/5a911f20d93e 镜像加速器 镜像加速器地址 专属加速器？ 其它加速？ Docker 中国官方镜像 https://registry.docker-cn.com Docker Hub DaoCloud 镜像站 http://f1361db2.m.daocloud.io 可登录，系统分配 Docker Hub Azure 中国镜像 https://dockerhub.azk8s.cn Docker Hub、GCR、Quay 科大镜像站 https://docker.mirrors.ustc.edu.cn Docker Hub、GCR、Quay 阿里云 https://.mirror.aliyuncs.com 需登录，系统分配 Docker Hub 七牛云 https://reg-mirror.qiniu.com Docker Hub、GCR、Quay 网易云 https://hub-mirror.c.163.com Docker Hub 腾讯云 https://mirror.ccs.tencentyun.com Docker Hub docker ubuntu for golang godev docker run --rm -itd --privileged -h yubuntu --name yubuntu -v /repo/yingjieb:/repo/$(whoami) -w /repo/$(whoami) yingjieb/godev/ubuntu docker attach yubuntu docker run --rm -itd -h yubuntu -v /repo/yingjieb:/repo/$(whoami) -w /repo/$(whoami) yingjieb/ubuntu-godev docker run --privileged -itd -h yubuntu --name yubuntu -v /repo/yingjieb:/repo/$(whoami) -w /repo/$(whoami) yingjieb/ubuntu-godev #登陆docker后 usermod -u 3030425 godev groupmod -g 1155 godev su godev gentoo docker gentoo的docker image也是基于stage3的包, 不同的是, 它修改了/etc/rc.conf 详见 https://github.com/gentoo/gentoo-docker-images/blob/master/stage3.Dockerfile # This is the subsystem type. # It is used to match against keywords set by the keyword call in the # depend function of service scripts. # # It should be set to the value representing the environment this file is # PRESENTLY in, not the virtualization the environment is capable of. # If it is commented out, automatic detection will be used. # # The list below shows all possible settings as well as the host # operating systems where they can be used and autodetected. # # \"\" - nothing special # \"docker\" - Docker container manager (Linux) # \"jail\" - Jail (DragonflyBSD or FreeBSD) # \"lxc\" - Linux Containers # \"openvz\" - Linux OpenVZ # \"prefix\" - Prefix # \"rkt\" - CoreOS container management system (Linux) # \"subhurd\" - Hurd subhurds (to be checked) # \"systemd-nspawn\" - Container created by systemd-nspawn (Linux) # \"uml\" - Usermode Linux # \"vserver\" - Linux vserver # \"xen0\" - Xen0 Domain (Linux and NetBSD) # \"xenU\" - XenU Domain (Linux and NetBSD) rc_sys=\"docker\" 清理空间 sudo systemctl stop docker #删除前先备份 rm -rf /var/lib/docker docker保存镜像 对image docker save 66389e4e65c5 -o centos-aarch64:7.4.tar docker load -i arm64-gentoo.1.0.tar docker tag f43a088c78f5 arm64-gentoo:1.0 对container docker export 239fc2b54d18 -o arm64-gentoo.2.0.tar docker import -c 'CMD [\"/bin/bash\"]' arm64-gentoo.1.2.0.tar 二者区别是export只导出rootfs, 丢失元数据层 history # 执行docker instance里面的一个命令 sudo docker exec 080443f42a7d ip -s addr docker pull docker-registry.qualcomm.com/yingjieb/centos-aarch64:7.4 docker login -u=\"yingjieb\" -p=\"ReU6z0UtlODAt210c9fn+gSMhszgMT/9Bkr4D6fMAsayPz6wmEKB0tFNN7xaIX2N\" docker-registry.qualcomm.com docker commit bbacde9f6753 docker-registry.qualcomm.com/yingjieb/centos-aarch64:7.4 docker push docker-registry.qualcomm.com/yingjieb/centos-aarch64:7.4 docker run --privileged -ti -u qdt -h dcent docker-registry.qualcomm.com/yingjieb/centos-aarch64:7.4 docker run --privileged -it -u qdt -h dcent --name=ceph-mon1 docker-registry.qualcomm.com/yingjieb/centos-aarch64:7.4 docker run --privileged -dt -u qdt -h dcent --name=ceph-mon2 docker-registry.qualcomm.com/yingjieb/centos-aarch64:7.4 # 上面两个命令只有-it和-dt不一样, -t是说要创建个tty, 而i是要进入交互模式, d是detach模式(后台运行); 需要注意着都和command有关, 这个image默认的command是/bin/bash, 是不退出的(不加选项t会退出, 因为bash必须运行在tty环境下?), 所以it还是dt都会一直运行; # 这里itd可以一起使用, 这样既能在后台继续运行, 又保留的shell交互入口 docker run --privileged -itd -u qdt -h dcent --name=ceph-mon3 docker-registry.qualcomm.com/yingjieb/centos-aarch64:7.4 docker run -itd --privileged -u qdt -h dgentoo --name=dgentoo arm64-gentoo:2.0 docker attach ceph-mon3; 可以进入shell, 退出时ctrl+p ctrl+q docker start -ai ceph-mon1 启动一个创建好的实例, attach并进入交互模式 docker cp linux-4.14.tar.xz agitated_wright:/root 进入shell docker exec -it ceph-mon2 /bin/bash 获取每个container的ip docker inspect -f '{ {.Name} } - { {.NetworkSettings.IPAddress} }' $(docker ps -aq) docker run docker run有限制资源使用的N多选项 比较有用的选项 --cpuset-cpus=\"\" CPUs in which to allow execution (0-3, 0,1) --cpuset-mems=\"\" Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems. --device=[] Add a host device to the container (e.g. --device=/dev/sdc:/dev/xvdc:rwm) -h, --hostname=\"\" Container host name --expose=[] Expose a port, or a range of ports (e.g. --expose=3300-3310) informs Docker that the container listens on the specified network ports at runtime. Docker uses this information to interconnect containers using links and to set up port redirection on the host system. -e, --env=[] Set environment variables This option allows you to specify arbitrary environment variables that are available for the process that will be launched inside of the con‐ tainer. --entrypoint=\"\" Overwrite the default ENTRYPOINT of the image --ip=\"\" Sets the container's interface IPv4 address (e.g. 172.23.0.9) It can only be used in conjunction with --net for user-defined networks -m, --memory=\"\" Memory limit (format: [], where unit = b, k, m or g) --name=\"\" Assign a name to the container --net=\"bridge\" Set the Network mode for the container 'bridge': create a network stack on the default Docker bridge 'none': no networking 'container:': reuse another container's network stack 'host': use the Docker host network stack. Note: the host mode gives the container full access to local system services such as D-bus and is therefore considered insecure. '|': connect to a user-defined network -p, --publish=[] Publish a container's port, or range of ports, to the host. Format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort Both hostPort and containerPort can be specified as a range of ports. When specifying ranges for both, the number of container ports in the range must match the number of host ports in the range. (e.g., docker run -p 1234-1236:1222-1224 --name thisWorks -t busybox but not docker run -p 1230-1236:1230-1240 --name RangeContainer‐ PortsBiggerThanRangeHostPorts -t busybox) With ip: docker run -p 127.0.0.1:$HOSTPORT:$CONTAINERPORT --name CONTAINER -t someimage Use docker port to see the actual mapping: docker port CONTAINER $CONTAINERPORT --privileged=true|false Give extended privileges to this container. The default is false. By default, Docker containers are “unprivileged” (=false) and cannot, for example, run a Docker daemon inside the Docker container. This is because by default a container is not allowed to access any devices. A “privileged” container is given access to all devices. When the operator executes docker run --privileged, Docker will enable access to all devices on the host as well as set some configuration in AppArmor to allow the container nearly all the same access to the host as processes running outside of a container on the host. -u, --user=\"\" Sets the username or UID used and optionally the groupname or GID for the specified command. The followings examples are all valid: --user [user | user:group | uid | uid:gid | user:gid | uid:group ] Without this argument the command will be run as root in the container. -v|--volume[=[[HOST-DIR:]CONTAINER-DIR[:OPTIONS]]] Create a bind mount. If you specify, -v /HOST-DIR:/CONTAINER-DIR, Docker bind mounts /HOST-DIR in the host to /CONTAINER-DIR in the Docker container. If 'HOST-DIR' is omitted, Docker automatically creates the new volume on the host. The OPTIONS are a comma delimited list and can be: 0 item [rw|ro] item [z|Z] item [[r]shared|[r]slave|[r]private] item [nocopy] -w, --workdir=\"\" Working directory inside the container The default working directory for running binaries within a container is the root directory (/). The developer can set a different default with the Dockerfile WORKDIR instruction. The operator can override the working directory by using the -w option. Mapping Ports for External Usage The exposed port of an application can be mapped to a host port using the -p flag. For example, a httpd port 80 can be mapped to the host port 8080 using the following: # docker run -p 8080:80 -d -i -t fedora/httpd Creating and Mounting a Data Volume Container Many applications require the sharing of persistent data across several containers. Docker allows you to create a Data Volume Container that other containers can mount from. For example, create a named container that contains directories /var/volume1 and /tmp/volume2. The image will need to contain these directories so a couple of RUN mkdir instructions might be required for you fedora-data image: # docker run --name=data -v /var/volume1 -v /tmp/volume2 -i -t fedora-data true # docker run --volumes-from=data --name=fedora-container1 -i -t fedora bash Multiple --volumes-from parameters will bring together multiple data volumes from multiple containers. And it's possible to mount the volumes that came from the DATA container in yet another container via the fedora-container1 intermediary container, allowing to abstract the actual data source from users of that data: # docker run --volumes-from=fedora-container1 --name=fedora-container2 -i -t fedora bash Mounting External Volumes To mount a host directory as a container volume, specify the absolute path to the directory and the absolute path for the container directory separated by a colon: # docker run -v /var/db:/data1 -i -t fedora bash docker commit https://docs.docker.com/engine/reference/commandline/commit/ 可以用container生成一个image举例: 普通的commit, 名字随便起? $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c3f279d17e0a ubuntu:12.04 /bin/bash 7 days ago Up 25 hours desperate_dubinsky 197387f1b436 ubuntu:12.04 /bin/bash 7 days ago Up 25 hours focused_hamilton $ docker commit c3f279d17e0a svendowideit/testimage:version3 f5283438590d $ docker images REPOSITORY TAG ID CREATED SIZE svendowideit/testimage version3 f5283438590d 16 seconds ago 335.7 MB commit的时候还能顺便更改配置 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c3f279d17e0a ubuntu:12.04 /bin/bash 7 days ago Up 25 hours desperate_dubinsky 197387f1b436 ubuntu:12.04 /bin/bash 7 days ago Up 25 hours focused_hamilton $ docker inspect -f \"{ { .Config.Env } }\" c3f279d17e0a [HOME=/ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin] $ docker commit --change \"ENV DEBUG true\" c3f279d17e0a svendowideit/testimage:version3 f5283438590d $ docker inspect -f \"{ { .Config.Env } }\" f5283438590d [HOME=/ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBUG=true] 更改CMD和EXPOSE $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c3f279d17e0a ubuntu:12.04 /bin/bash 7 days ago Up 25 hours desperate_dubinsky 197387f1b436 ubuntu:12.04 /bin/bash 7 days ago Up 25 hours focused_hamilton $ docker commit --change='CMD [\"apachectl\", \"-DFOREGROUND\"]' -c \"EXPOSE 80\" c3f279d17e0a svendowideit/testimage:version4 f5283438590d $ docker run -d svendowideit/testimage:version4 89373736e2e7f00bc149bd783073ac43d0507da250e999f3f1036e0db60817c0 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 89373736e2e7 testimage:version4 \"apachectl -DFOREGROU\" 3 seconds ago Up 2 seconds 80/tcp distracted_fermat c3f279d17e0a ubuntu:12.04 /bin/bash 7 days ago Up 25 hours desperate_dubinsky 197387f1b436 ubuntu:12.04 /bin/bash 7 days ago Up 25 hours focused_hamilton 容易混淆的概念入门讲解 http://blog.thoward37.me/articles/where-are-docker-images-stored/ docker tag可以push到私有库? Tagging an image for a private repository To push an image to a private registry and not the central Docker registry you must tag it with the registry hostname and port (if needed). docker tag 0e5574283393 myregistryhost:5000/fedora/httpd:version1.0 在docker里面起systemd? A package with the systemd initialization system is included in the official Red Hat Enterprise Linux base images. This means that applications created to be managed with systemd can be started and managed inside a container. A container running systemd will: NOTE Previously, a modified version of the systemd initialization system called systemd-container was included in the Red Hat Enterprise Linux versions 7.2 base images. Now, the systemd package is the same across systems. Start the /sbin/init process (the systemd service) to run as PID 1 within the container. Start all systemd services that are installed and enabled within the container, in order of dependencies. Allow systemd to restart services or kill zombie processes for services started within the container. The general steps for building a container that is ready to be used as a systemd services is: Install the package containing the systemd-enabled service inside the container. This can include dozens of services that come with RHEL, such as Apache Web Server (httpd), FTP server (vsftpd), Proxy server (squid), and many others. For this example, we simply install an Apache (httpd) Web server. Use the systemctl command to enable the service inside the container. Add data for the service to use in the container (in this example, we add a Web server test page). For a real deployment, you would probably connect to outside storage. Expose any ports needed to access the service. Set /sbin/init as the default process to start when the container runs In this example, we build a container by creating a Dockerfile that installs and configures a Web server (httpd) to start automatically by the systemd service (/sbin/init) when the container is run on a host system. 1 Create Dockerfile: In a separate directory, create a file named Dockerfile with the following contents: FROM rhel7 RUN yum -y install httpd; yum clean all; systemctl enable httpd; RUN echo \"Successful Web Server Test\" > /var/www/html/index.html RUN mkdir /etc/systemd/system/httpd.service.d/; echo -e '[Service]\\nRestart=always' > /etc/systemd/system/httpd.service.d/httpd.conf EXPOSE 80 CMD [ \"/sbin/init\" ] The Dockerfile installs the httpd package, enables the httpd service to start at boot time (i.e. when the container starts), creates a test file (index.html), exposes the Web server to the host (port 80), and starts the systemd init service (/sbin/init) when the container starts. 2 Build the container: From the directory containing the Dockerfile, type the following: # docker build -t mysysd . 3 Run the container: Once the container is built and named mysysd, type the following to run the container: # docker run -d --name=mysysd_run -p 80:80 mysysd From this command, the mysysd image runs as the mysysd_run container as a daemon process, with port 80 from the container exposed to port 80 on the host system. 4 Check that the container is running: To make sure that the container is running and that the service is working, type the following commands: # docker ps | grep mysysd_run de7bb15fc4d1 mysysd \"/sbin/init\" 3 minutes ago Up 2 minutes 0.0.0.0:80->80/tcp mysysd_run # curl localhost/index.html Successful Web Server Test At this point, you have a container that starts up a Web server as a systemd service inside the container. Install and run any services you like in this same way by modifying the Dockerfile and configuring data and opening ports as appropriate. Dockerfile: ENTRYPOINT vs CMD https://www.ctl.io/developers/blog/post/dockerfile-entrypoint-vs-cmd/ 简单来说, CMD和ENTRYPOINT都可以指定进docker以后, 默认的执行程序, 两者都可以被重载, 重载CMD更容易. 重载ENTRYPOINT需要加额外参数 Repository和Registry Repository：本身是一个仓库，这个仓库里面可以放具体的镜像，是指具体的某个镜像的仓库，比如Tomcat下面有很多个版本的镜像，它们共同组成了Tomcat的Repository。 Registry：镜像的仓库，比如官方的是Docker Hub，它是开源的，也可以自己部署一个，Registry上有很多的Repository，Redis、Tomcat、MySQL等等Repository组成了Registry。 用新的command启动一个container Find your stopped container id docker ps -a Commit the stopped container: This command saves modified container state into a new image user/test_image docker commit $CONTAINER_ID user/test_image Start/run with a different entry point: docker run -ti --entrypoint=sh user/test_image image和container的区别? container是image的一个实例 What's an Image? An image is an inert, immutable, file that's essentially a snapshot of a container. Images are created with the build command, and they'll produce a container when started with run. Images are stored in a Docker registry such as registry.hub.docker.com. Because they can become quite large, images are designed to be composed of layers of other images, allowing a miminal amount of data to be sent when transferring images over the network. Local images can be listed by running docker images: REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE ubuntu 13.10 5e019ab7bf6d 2 months ago 180 MB ubuntu 14.04 99ec81b80c55 2 months ago 266 MB ubuntu latest 99ec81b80c55 2 months ago 266 MB ubuntu trusty 99ec81b80c55 2 months ago 266 MB 4ab0d9120985 3 months ago 486.5 MB Some things to note: IMAGE ID is the first 12 characters of the true identifier for an image. You can create many tags of a given image, but their IDs will all be the same (as above). VIRTUAL SIZE is virtual because its adding up the sizes of all the distinct underlying layers. This means that the sum of all the values in that column is probably much larger than the disk space used by all of those images. The value in the REPOSITORY column comes from the -t flag of the docker build command, or from docker tag-ing an existing image. You're free to tag images using a nomenclature that makes sense to you, but know that docker will use the tag as the registry location in a docker push or docker pull. The full form of a tag is [REGISTRYHOST/][USERNAME/]NAME[:TAG]. For ubuntu above, REGISTRYHOST is inferred to be registry.hub.docker.com. So if you plan on storing your image called my-application in a registry at docker.example.com, you should tag that image docker.example.com/my-application. The TAG column is just the [:TAG] part of the full tag. This is unfortunate terminology. The latest tag is not magical, it's simply the default tag when you don't specify a tag. You can have untagged images only identifiable by their IMAGE IDs. These will get the TAG and REPOSITORY. It's easy to forget about them. More info on images is available from the Docker docs and glossary. What's a container? To use a programming metaphor, if an image is a class, then a container is an instance of a class—a runtime object. Containers are hopefully why you're using Docker; they're lightweight and portable encapsulations of an environment in which to run applications. View local running containers with docker ps:CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f2ff1af05450 samalba/docker-registry:latest /bin/sh -c 'exec doc 4 months ago Up 12 weeks 0.0.0.0:5000->5000/tcp docker-registry Here I'm running a dockerized version of the docker registry, so that I have a private place to store my images. Again, some things to note: Like IMAGE ID, CONTAINER ID is the true identifier for the container. It has the same form, but it identifies a different kind of object. docker ps only outputs running containers. You can view all containers (running or stopped) with docker ps -a. NAMES can be used to identify a started container via the --name flag. "},"notes/centos_操作记录.html":{"url":"notes/centos_操作记录.html","title":"centos操作记录","keywords":"","body":" 安装新版本GCC clear disk space 关于permission denied x11 forwarding 查找系统中所有安装的kernel版本 查看rpm包含的文件 安装htop 关闭源的check 开发环境 KVM 修改yum源 安装rpm 安装新版本GCC sudo yum install centos-release-scl-rh sudo yum-config-manager --enable centos-sclo-rh-testing #把centos-sclo-rh disable, 把centos-sclo-rh-testing enable sudo vi /etc/yum.repos.d/CentOS-SCLo-scl-rh.repo #列出devtoolset yum list available | grep devtoolset #安装devtoolset6 yum install devtoolset-6 #切换到gcc6 scl enable devtoolset-6 bash clear disk space https://www.getpagespeed.com/server-setup/clear-disk-space-centos 关于permission denied home目录有额外的权限控制, 默认other没有任何权限; 而有些进程比如qemu是libvirtd启动的, 一般为libvirt-qemu用户, 就不能访问个人home下面的东西; 似乎root用户都不能访问??! 有两套东西管权限, 一个是ACL, 另一个就是臭名昭著的seLinux setfacl -b will remove the ACL on a file. setfattr -x security.selinux will remove the SELinux file context, but you will probably have to boot with SELinux completely disabled. #让other能完全访问home目录 sudo setfacl -m o:rwx /home/bai $ sudo getfacl -e /home/bai getfacl: Removing leading '/' from absolute path names # file: home/bai # owner: bai # group: bai user::rwx group::rwx other::rwx #或者安全点, 指定用户 sudo setfacl -m u:libvirt-qemu:rx /home/murlock x11 forwarding yum install -y xorg-x11-server-Xorg xorg-x11-xauth xorg-x11-apps 查找系统中所有安装的kernel版本 rpm -qa | grep kernel 查看rpm包含的文件 libfdt包括了操作device tree的库函数, 以rpm包的方式被centos收录发行. $ rpm -ql libfdt-devel /usr/include/fdt.h /usr/include/libfdt.h /usr/include/libfdt_env.h /usr/lib64/libfdt.so 安装htop #enable the epel release sudo yum -y install epel-release sudo yum install htop 关闭源的check yum --nogpg install ... 开发环境 # yum groups list | grep -i devel # yum groups install \"C Development Tools and Libraries\" yum --nogpg groupinstall \"Development Tools\" KVM sudo yum groupinstall \"Virtualization*\" 修改yum源 把everything iso mount到/mnt/centos sudo mv CentOS-Base.repo CentOS-Base.repo.bak [qdt@qdt-shlab-awsdp1 yum.repos.d]$ cat CentOS-Local.repo [local] name=local baseurl=file:///mnt/centos enabled=1 gpgcheck=1 gpgkey=file:///mnt/centos/RPM-GPG-KEY-CentOS-7 安装rpm rpm -ivh $url "},"notes/as_title_golang.html":{"url":"notes/as_title_golang.html","title":"Golang","keywords":"","body":"如题 "},"notes/as_title_golang1.html":{"url":"notes/as_title_golang1.html","title":"入门","keywords":"","body":"如题 "},"notes/golang_语法基础.html":{"url":"notes/golang_语法基础.html","title":"Golang 语法基础","keywords":"","body":" 初始化和空值 结构体 声明空切片 new切片 空切片的实例化 总结 字符串支持比较操作符 空白标识符 空白标识符和err 空白标识符和编译unused检查 空白标识符和类型检查 go 是静态类型语言 interface类型的变量可以重复赋值为任意类型 可以在循环里用:语法糖赋值 连续赋值可以支持重复声明 结构体和json反射 结构体定义里的反射字段 反射 Golang的单引号、双引号与反引号 变长参数 flag包用来解析cmd参数 内置len copy 和cap 字符串 字节数组 符文 常量和iota 格式化和scan print scan 减小go可执行文件的size go doc看说明 go内置pacakge go 环境变量 go test框架 远程包 go 工程布局(layout) 典型的go workspace布局 完整布局参考 go知识点 值传递和指针类型 struct 形式 结构体方法 基于指针对象的方法 继承 结构体可以比较 new分配内存 工厂模式初始化 接口 interface 类型断言 类型断言判断对象是否实现了一个接口 goroutine 通道 带缓冲的通道 通道用close来关闭 切片 切片的append map 集合 delete可以删除元素 range 初始化和空值 结构体 先说结论, 初始化时没有赋值的结构体, 其内容是零值, 但这个对象的地址不是nil; 这个对象也不能和nil比较 type MystructStr struct { s string a int } var ms MystructStr //ms是在地址0xc0000d00e0上的24字节大小的结构体 ({ 0 0}, main.MystructStr)@[*0xc0000d00e0, 24] //&ms不是nil, ms本身也不能和nil比较 声明空切片 对于切片, map等对象来说, 变量名代指切片; 切片的地址可以和nil比较, 切片也可以和nil比较, 这是两码事. 切片和nil可以比较是go语法的规定. var sl []int //判断成立, 会打印 //这里应该理解成, sl的内容为空; 注意sl的内容为空, 不是说它本身的地址是nil. //做为一个变量, sl本身的地址不可能为nil. if sl == nil { fmt.Println(\"nillll\") } new切片 对于new返回的地址, sn是真正的地址类似于sn := &[]int{}, 变量名指切片的地址那么sn就不是nil, 但它指向一个空的切片 sn := new([]int) if sn != nil { fmt.Println(sn) } //结果 &[] 空切片的实例化 //si不为nil si := []int{} 总结 //len(s1)=0;cap(s1)=0;s1==nil var s1 []int //len(s1)=0;cap(s1)=0;s2!=nil s2 := []int{} //len(s3)=0;cap(s3)=0;s3!=nil s3 := make([]int, 0) 指针是否为nil说的是指针是否指向东西. 切片等结构体头也类似于指针. 所以切片是否为nil也说的是切片头是否指向实际的东西 只有var s1 []int形式的声明, 声明了一个切片变量, 只是个声明, 没有给\"指针\"赋值, 所以此时s1为nil new也没有实例化, 但new返回指针, 指向interface对象本身, 所以也不是nil 其他形式, 比如:= make, 都会实例化, 即接口的\"指针\"字段都指向实例. 所以都不是nil 字符串支持比较操作符 原生的比较符可以直接用来比较字符串 ==, !=, >=, . 它们都返回bool值 另外一种写法是func Compare(str1, str2 string) int result1 := \"GFG\" > \"Geeks\" fmt.Println(\"Result 1: \", result1) result2 := \"GFG\" = \"for\" fmt.Println(\"Result 3: \", result3) result4 := \"Geeks\" 空白标识符 空白标识符可以占位任何类型 空白标识符和err if _, err := os.Stat(path); os.IsNotExist(err) { fmt.Printf(\"%s does not exist\\n\", path) } 有时为了省事, 直接把err给\"占位\"了, 这样是不对的. // Bad! This code will crash if path does not exist. fi, _ := os.Stat(path) if fi.IsDir() { fmt.Printf(\"%s is a directory\\n\", path) } 空白标识符和编译unused检查 go会检查代码, 没有用的import和变量会报错误. 用占位符可以让编译器happy: 这里fmt和io以及变量fd没有使用, 一般编译会报错, 用占位符就不会. package main import ( \"fmt\" \"io\" \"log\" \"os\" ) var _ = fmt.Printf // For debugging; delete when done. var _ io.Reader // For debugging; delete when done. func main() { fd, err := os.Open(\"test.go\") if err != nil { log.Fatal(err) } // TODO: use fd. _ = fd } 把import的包赋值给占位符, 相当于不用这个包, 但包里的init会被调用. import _ \"net/http/pprof\" 空白标识符和类型检查 go的类型检查发生在编译时, 传入的对象必须和函数的参数类型一致. 但也可以运行时检查: 下面是json的encoder代码, 它检查如果传入的值实现了json.Marshaler接口, 就调用这个值的MarshalJSON方法, 而不是调用标准的MarshalJSON方法. m, ok := val.(json.Marshaler) 这种运行时检查很常见, 比如判断一个值是否实现了某个接口, 可以这样: 用空白标识符来占位返回的值, 只看ok. if _, ok := val.(json.Marshaler); ok { fmt.Printf(\"value %v of type %T implements json.Marshaler\\n\", val, val) } json.Marshaler和json.RawMessage的定义 Linux Mint 19.1 Tessa $ go doc json.Marshaler type Marshaler interface { MarshalJSON() ([]byte, error) } Marshaler is the interface implemented by types that can marshal themselves into valid JSON. yingjieb@yingjieb-VirtualBox ~/repo/myrepo/try Linux Mint 19.1 Tessa $ go doc json.RawMessage type RawMessage []byte RawMessage is a raw encoded JSON value. It implements Marshaler and Unmarshaler and can be used to delay JSON decoding or precompute a JSON encoding. func (m RawMessage) MarshalJSON() ([]byte, error) func (m *RawMessage) UnmarshalJSON(data []byte) error 如果json.Marshaler的定义变了, 那么一个原本实现了这个接口的类型,就不再有效了. 此时只有等到运行时的类型断言才能知道, 有办法在编译时就知道吗? 可以: 用空白标识符可以进行静态检查: 这里用_代替变量名 //这里是个强制转换, 把nil转换为(*RawMessage) //我认为这里转换为(RawMessage)也行. var _ json.Marshaler = (*RawMessage)(nil) 如果json.Marshaler接口变化了, 这段代码就编不过. go 是静态类型语言 虽然go有语法糖, 可以根据右值来自动解析数据类型. 但不要把它当作动态语言来用了. // 编译器自动知道mys是string mys := \"hhhh\" // 给mys赋值64会报错: cannot use 64 (type int) as type string in assignment mys = 64 动态语言, 变量可以随便赋值为不同种类的. interface类型的变量可以重复赋值为任意类型 The interface{} (empty interface) type describes an interface with zero methods. Every Go type implements at least zero methods and therefore satisfies the empty interface. func describe(i interface{}) { fmt.Printf(\"(%v, %T)\\n\", i, i) } var mi interface{} mi = \"a string\" describe(mi) mi = 2011 describe(mi) mi = 2.777 describe(mi) #输出 (a string, string) (2011, int) (2.777, float64) 可以在循环里用:语法糖赋值 func main() { fmt.Println(\"Hello, playground\") for i := 0; i 连续赋值可以支持重复声明 一般在一个语句块里, 不能对单个变量重复用:=声明; 但可以对连续变量重复声明 Unlike regular variable declarations, a short variable declaration may redeclare variables provided they were originally declared earlier in the same block with the same type, and at least one of the non-blank variables is new. As a consequence, redeclaration can only appear in a multi-variable short declaration. Redeclaration does not introduce a new variable; it just assigns a new value to the original. v := 1 v := 2 fmt.Println(v) // 编译时, 只有v的第二次冒号赋值会报错. ./prog.go:18:4: no new variables on left side of := // 但下面是可以的 func main() { a, b := 1, 2 c, b := 3, 4 fmt.Println(a, b, c) } // 一个经常见的例子是 f, err := os.Open(name) if err != nil { return err } d, err := f.Stat() if err != nil { f.Close() return err } codeUsing(f, d) 注: open等调用返回的error是个内建的类型, 必须用if err != nil来判断; 不能用if err来判断, 因为error类型不是bool类型 ./prog.go:11:2: non-bool err (type error) used as if condition 结构体和json反射 结构体定义里的反射字段 先定义和json对应的struct, 要导出的字段首字母大写. json.Unmarshal()把json转为struct json.Marshal()把struct转为json 这里的反射大概是指//可以选择的控制字段有三种： // -：不要解析这个字段 // omitempty：当字段为空（默认值）时，不要解析这个字段。比如 false、0、nil、长度为 0 的 array，map，slice，string // FieldName：当解析 json 的时候，使用这个名字 type StudentWithOption struct { StudentId string //默认使用原定义中的值 StudentName string `json:\"sname\"` // 解析（encode/decode） 的时候，使用 `sname`，而不是 `Field` StudentClass string `json:\"class,omitempty\"` // 解析的时候使用 `class`，如果struct 中这个值为空，就忽略它 StudentTeacher string `json:\"-\"` // 解析的时候忽略该字段。默认情况下会解析这个字段，因为它是大写字母开头的 } //与json数据对应的结构体 type Server struct { ServerName string ServerIP string } // 数组对应slice type ServerSlice struct { Servers []Server } //将JSON数据解析成结构体 package main import ( \"encoding/json\" \"fmt\" ) func main() { var s ServerSlice str := `{\"servers\":[{\"serverName\":\"TianJin\",\"serverIP\":\"127.0.0.1\"}, {\"serverName\":\"Beijing\",\"serverIP\":\"127.0.0.2\"}]}` json.Unmarshal([]byte(str), &s) fmt.Println(s) } ----output----- {[{TianJin 127.0.0.1} {Beijing 127.0.0.2}]} 反射 Reflection（反射）在计算机中表示 程序能够检查自身结构的能力，尤其是类型 func main() { var x float64 = 3.4 fmt.Println(reflect.TypeOf(x)) //float64 t := reflect.TypeOf(x) fmt.Println(t) //float64 // 注: 我认为其输出也可以叫reflect.Type fmt.Println(reflect.TypeOf(t)) //*reflect.rtype //相关代码在 go/src/reflect/value.go v := reflect.ValueOf(x) fmt.Println(v) //3.4 fmt.Println(reflect.TypeOf(v)) //reflect.Value fmt.Println(v.Interface()) //3.4 fmt.Println(v.Type()) //float64 } 变量包括（type, value）两部分 type 包括 static type和concrete type. 简单来说 static type是你在编码是看见的类型(如int、string)，concrete type是runtime系统看见的类型 类型断言能否成功，取决于变量的concrete type，而不是static type. 因此，一个 reader变量如果它的concrete type也实现了write方法的话，它也可以被类型断言为writer. 只有interface类型才有反射一说 每个interface变量都有一个对应pair，pair中记录了实际变量的值和类型 (value, type) reflect包api //ValueOf用来获取输入参数接口中的数据的值，如果接口为空则返回0 func ValueOf(i interface{}) Value {...} //func TypeOf(i interface{}) Type {...} func TypeOf(i interface{}) Type {...} 反射可以大大提高程序的灵活性，使得interface{}有更大的发挥余地 反射必须结合interface才玩得转 变量的type要是concrete type的（也就是interface变量）才有反射一说 反射可以将“接口类型变量”转换为“反射类型对象” 反射使用 TypeOf 和 ValueOf 函数从接口中获取目标对象信息 反射可以将“反射类型对象”转换为“接口类型变量 reflect.value.Interface().(已知的类型) 遍历reflect.Type的Field获取其Field 反射可以修改反射类型对象，但是其值必须是“addressable” 想要利用反射修改对象状态，前提是 interface.data 是 settable,即 pointer-interface 通过反射可以“动态”调用方法 因为Golang本身不支持模板，因此在以往需要使用模板的场景下往往就需要使用反射(reflect)来实现 Golang的单引号、双引号与反引号 Go语言的字符串类型string在本质上就与其他语言的字符串类型不同： Java的String、C++的std::string以及Python3的str类型都只是定宽字符序列 Go语言的字符串是一个用UTF-8编码的变宽字符序列，它的每一个字符都用一个或多个字节表示 即：一个Go语言字符串是一个任意字节的常量序列。 Golang的双引号和反引号都可用于表示一个常量字符串，不同在于： 双引号用来创建可解析的字符串字面量(支持转义，但不能用来引用多行) 反引号用来创建原生的字符串字面量，这些字符串可能由多行组成(不支持任何转义序列)，原生的字符串字面量多用于书写多行消息、HTML以及正则表达式 而单引号则用于表示Golang的一个特殊类型：rune，类似其他语言的byte但又不完全一样，是指：码点字面量（Unicode code point），不做任何转义的原始内容。 变长参数 func append(slice []Type, elems ...Type) []Type //举例 orig, err := ioutil.ReadFile(file) //这里orig后面的三个点, 是展开orig的意思 in := append([]byte{}, orig...) //举例 func sum(nums ...int) { fmt.Print(nums, \" \") //输出如 [1, 2, 3] 之类的数组 total := 0 for _, num := range nums { //要的是值而不是下标 total += num } fmt.Println(total) } func main() { sum(1, 2) sum(1, 2, 3) //传数组 nums := []int{1, 2, 3, 4} //相当于把nums打散成元素, 依次传入sum sum(nums...) } flag包用来解析cmd参数 import \"flag\" //定义flag, flag.Int返回一个指针: *int var ip = flag.Int(\"flagname\", 1234, \"help message for flagname\") flag.Var(&flagVal, \"name\", \"help message for flagname\") //定义完了调用, 调用完了指针才会有内容 flag.Parse() //代码里使用flag fmt.Println(\"ip has value \", *ip) fmt.Println(\"flagvar has value \", flagvar) //cmd参数形式 --flag -flag -flag=x --flag=x -flag x // non-boolean flags only //integer可以是如下形式, 也可以为负数 1234, 0664, 0x1234 //Bool可以是 1, 0, t, f, T, F, true, false, TRUE, FALSE, True, False 内置len copy 和cap // cap是最大容量, len是实际容量 func cap(v Type) int func len(v Type) int func copy(dst, src []Type) int 字符串 字节数组 符文 []byte: byte数组 b := []byte(\"ABC€\") fmt.Println(b) // [65 66 67 226 130 172] //go的string是unicode编码(变长)的byte数组 s := string([]byte{65, 66, 67, 226, 130, 172}) fmt.Println(s) // ABC€ character € is encoded in UTF-8 using 3 bytes string: 本质上是只读的的byte数组, 对string的index返回对应的byte; 对大于255的字符(character)来说, 占多个byte func main() { const placeOfInterest = `⌘` fmt.Printf(\"plain string: \") fmt.Printf(\"%s\", placeOfInterest) fmt.Printf(\"\\n\") fmt.Printf(\"quoted string: \") fmt.Printf(\"%+q\", placeOfInterest) fmt.Printf(\"\\n\") fmt.Printf(\"hex bytes: \") for i := 0; i UTF-8用1到6个字节编码Unicode字符 rune: 是int32的别名 常量和iota 在常量表达式里面使用iota, 从0开始, 每行加一. type Stereotype int const ( TypicalNoob Stereotype = iota // 0 TypicalHipster // 1 TypicalHipster = iota TypicalUnixWizard // 2 TypicalUnixWizard = iota TypicalStartupFounder // 3 TypicalStartupFounder = iota ) //如果两个const的赋值语句的表达式是一样的，那么可以省略后一个赋值表达式。 type AudioOutput int const ( OutMute AudioOutput = iota // 0 OutMono // 1 OutStereo // 2 _ _ OutSurround // 5 ) type Allergen int const ( IgEggs Allergen = 1 格式化和scan print // 定义示例类型和变量 type Human struct { Name string } var people = Human{Name:\"zhangsan\"} 普通占位符 占位符 说明 举例 输出 %v 相应值的默认格式。 Printf(\"%v\", people) {zhangsan}， %+v 打印结构体时，会添加字段名 Printf(\"%+v\", people) {Name:zhangsan} %#v 相应值的Go语法表示 Printf(\"%#v\", people) main.Human{Name:\"zhangsan\"} %T 相应值的类型的Go语法表示 Printf(\"%T\", people) main.Human %% 字面上的百分号，并非值的占位符 Printf(\"%%\") % func describe(i interface{}) { fmt.Printf(\"(%v, %T)\\n\", i, i) } 整数占位符 占位符 说明 举例 输出 %b 二进制表示 Printf(\"%b\", 5) 101 %c 相应Unicode码点所表示的字符 Printf(\"%c\", 0x4E2D) 中 %d 十进制表示 Printf(\"%d\", 0x12) 18 %o 八进制表示 Printf(\"%d\", 10) 12 %q 单引号围绕的字符字面值，由Go语法安全地转义 Printf(\"%q\", 0x4E2D) '中' %x 十六进制表示，字母形式为小写 a-f Printf(\"%x\", 13) d %X 十六进制表示，字母形式为大写 A-F Printf(\"%x\", 13) D %U Unicode格式：U+1234，等同于 \"U+%04X\" Printf(\"%U\", 0x4E2D) U+4E2D 字符串与字节切片 占位符 说明 举例 输出 %s 输出字符串表示（string类型或[]byte) Printf(\"%s\", []byte(\"Go语言\")) Go语言 %q 双引号围绕的字符串，由Go语法安全地转义 Printf(\"%q\", \"Go语言\") \"Go语言\" %x 十六进制，小写字母，每字节两个字符 Printf(\"%x\", \"golang\") 676f6c616e67 %X 十六进制，大写字母，每字节两个字符 Printf(\"%X\", \"golang\") 676F6C616E67 指针 占位符 说明 举例 输出 %p 十六进制表示，前缀 0x Printf(\"%p\", &people) 0x4f57f0 其它标记 占位符 说明 举例 输出 + 总打印数值的正负号；对于%q（%+q）保证只输出ASCII编码的字符。 Printf(\"%+q\", \"中文\") \"\\u4e2d\\u6587\" - 在右侧而非左侧填充空格（左对齐该区域） # 备用格式：为八进制添加前导 0（%#o） Printf(\"%#U\", '中') U+4E2D 为十六进制添加前导 0x（%#x）或 0X（%#X），为 %p（%#p）去掉前导 0x； 如果可能的话，%q（%#q）会打印原始 （即反引号围绕的）字符串； 如果是可打印字符，%U（%#U）会写出该字符的 Unicode 编码形式（如字符 x 会被打印成 U+0078 'x'）。 ' ' (空格)为数值中省略的正负号留出空白（% d）； 以十六进制（% x, % X）打印字符串或切片时，在字节之间用空格隔开 0 填充前导的0而非空格；对于数字，这会将填充移到正负号之后 scan package main import ( \"fmt\" ) func main() { var name string var age int n, err := fmt.Sscanf(\"Kim is 22 years old\", \"%s is %d years old\", &name, &age) if err != nil { panic(err) } fmt.Printf(\"%d: %s, %d\\n\", n, name, age) } 减小go可执行文件的size #之前是15M, 带符号表 go build -ldflags \"-s -w\" main.go #之后是7.3M, 不带符号表 go tool link -h -s disable symbol table -w disable DWARF generation 不影响panic的打印信息 比如, 在hello.go加入一行panic() # 不要符号表, 不要DWARF go build -ldflags \"-s -w\" hello.go # 还是有panic信息 $ ./hello panic: goroutine 1 [running]: main.main() /repo/yingjieb/godev/practice/src/examples/hello.go:39 +0xa3 注: 如果是gccgo, strip符号表会导致panic的打印没有调用栈信息. go doc看说明 #格式化输出的 go doc fmt #命令行解析的 do doc flag #导出变量的, via HTTP at /debug/vars in JSON format ??? go doc expvar go内置pacakge go的内置package在toolchain的src目录下, 都是go文件. yingjieb@yingjieb-VirtualBox ~/repo/gorepo/go/src Linux Mint 19.1 Tessa $ ls all.bash archive builtin clean.rc container debug flag html io make.bat mime os race.bat run.bat strconv testdata unicode all.bat bootstrap.bash bytes cmd context encoding fmt image iostest.bash Make.dist naclmake.bash path reflect run.rc strings testing unsafe all.rc bufio clean.bash cmp.bash crypto errors go index log make.rc nacltest.bash plugin regexp runtime sync text androidtest.bash buildall.bash clean.bat compress database expvar hash internal make.bash math net race.bash run.bash sort syscall time go 环境变量 $GOROOT : go toolchain的目录, 在编译go toolchain时写入默认值为all.bash的上级目录, 比如/root/go-mips64; 可用来在多个go toolchain间切换; buildroot默认在HOST_GO_ROOT = $(HOST_DIR)/lib/go, 见package/go/go.mk $GOOS and $GOARCH : 目标OS和CPU arch, 比如linux 和mips64. 默认是$GOHOSTOS and $GOHOSTARCH $GOPATH : 所有go程序的工作目录, 默认是用户home/go $GOBIN : go可执行二进制目录, 用go命令安装的bin在此. 默认是$GOPATH/bin 比如go get golang.org/x/tools/cmd/godoc下载, 编译, 安装$GOBIN/godoc 参考: go环境变量 Linux Mint 19.1 Tessa $ go env GOARCH=\"amd64\" GOBIN=\"\" GOCACHE=\"/home/yingjieb/.cache/go-build\" GOEXE=\"\" GOHOSTARCH=\"amd64\" GOHOSTOS=\"linux\" GOOS=\"linux\" GOPATH=\"/home/yingjieb/go\" GORACE=\"\" GOROOT=\"/usr/lib/go-1.10\" GOTMPDIR=\"\" GOTOOLDIR=\"/usr/lib/go-1.10/pkg/tool/linux_amd64\" GCCGO=\"gccgo\" CC=\"gcc\" CXX=\"g++\" CGO_ENABLED=\"1\" CGO_CFLAGS=\"-g -O2\" CGO_CPPFLAGS=\"\" CGO_CXXFLAGS=\"-g -O2\" CGO_FFLAGS=\"-g -O2\" CGO_LDFLAGS=\"-g -O2\" PKG_CONFIG=\"pkg-config\" GOGCCFLAGS=\"-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build439003515=/tmp/go-build -gno-record-gcc-switches\" go test框架 go有个集成的test框架, 包括 go test命令 需包含testing包 被test的package下面, 创建xxx_test.go, 包含func TestXxx(t *testing.T) 比如package stringutil里实现了字符串反转的方法, 那么, 它的test要这么写 package stringutil import \"testing\" func TestReverse(t *testing.T) { cases := []struct { in, want string }{ {\"Hello, world\", \"dlrow ,olleH\"}, {\"Hello, 世界\", \"界世 ,olleH\"}, {\"\", \"\"}, } for _, c := range cases { got := Reverse(c.in) if got != c.want { t.Errorf(\"Reverse(%q) == %q, want %q\", c.in, got, c.want) } } } 测试时, #从任意地方运行 go test github.com/user/stringutil #如果从这个package下面运行 go test 远程包 有的包在github上, 用go get命令可以从远程repo下载 编译 安装指定包. $ go get github.com/golang/example/hello $ $GOPATH/bin/hello Hello, Go examples! 被import的远程包, 本地没有的, 会被自动下载到workspace. go 工程布局(layout) 参考: https://golang.org/doc/code.html go开发约定: go的所有程序都放在一个workspace下面 这个workspace下面放了很多repo, 用git或hg管理起来 package name就是其package路径的basename, 比如crypto/rot13的package name就是rot13 go不要求package name是唯一的, 但要求其路径是唯一的. 一个repo包括一个或多个package 一个package放在一个目录下面, 包括一个或多个go源文件 一个package在workspace的路径, 就是它被import的路径 被import的package可以是remote的repo, go会自动下载到workspace里面 典型的go workspace布局 bin/ hello # command executable outyet # command executable src/ github.com/golang/example/ .git/ # Git repository metadata hello/ hello.go # command source outyet/ main.go # command source main_test.go # test source stringutil/ reverse.go # package source reverse_test.go # test source golang.org/x/image/ .git/ # Git repository metadata bmp/ reader.go # package source writer.go # package source ... (many more repositories and packages omitted) ... $GOPATH就是这个workspaceexport PATH=$PATH:$(go env GOPATH)/bin export GOPATH=$(go env GOPATH) go install就是把编译好的可执行文件拷贝到$GOPATH/bin 比如可以在任意的地方执行go install github.com/user/hello go会去$GOPATH/src找 对你写的go lib(不是以package main开头的是lib)来说, go build会把编译好的package保存在local build cache里 go的package xxx, 这里的xxx是import路径的base name. 在引用的时候, 用相对src的路径引用 package main import ( \"fmt\" \"github.com/user/stringutil\" ) func main() { fmt.Println(stringutil.Reverse(\"!oG ,olleH\")) } 此时源文件布局如下: bin/ hello # command executable src/ github.com/user/ hello/ hello.go # command source stringutil/ reverse.go # package source 完整布局参考 https://github.com/golang-standards/project-layout go知识点 package main import \"fmt\" func main() { /* 这是我的第一个简单的程序 */ fmt.Println(\"Hello, World!\") } #运行 go run hello.go #只编译 go build hello.go package main package关键词表示这个文件属于哪个包, 一般都是多个源文件属于同一个包. import告诉go编译器要使用的包 func main是程序开始执行的函数, 每个可执行的go程序必须包含main函数. 一般情况下, 程序会第一个执行main. 但如果有init()函数, 会先执行init() {不能占单独的一行 每行不必用;结尾 用+可以连接字符串, 比如fmt.Println(\"Google\" + \"Runoob\"), 得到GoogleRunoob 当标识符（包括常量、变量、类型、函数名、结构字段等等）以一个大写字母开头，如：Group1，那么使用这种形式的标识符的对象就可以被外部包的代码所使用（客户端程序需要先导入这个包），这被称为导出（像面向对象语言中的 public）；标识符如果以小写字母开头，则对包外是不可见的，但是他们在整个包的内部是可见并且可用的（像面向对象语言中的 protected ）。 go的数据类型有bool byte(类似uint8) rune(类似int32) uintptr(无符号整型, 用于存放指针) int(32位或64位, 和体系架构有关) uint8/16/32/64 int8/16/32/64 float float32 float64 string struct channel 指针 数组 func 切片 interface map等 var a string = \"Runoob\" //也可以省略类型, 编译器根据赋值自动推断 var a = \"RUNOOB\" var b, c int = 1, 2 fmt.Println(a, b, c) //没有初始化的变量, 默认为0, 或空字符串\"\", 或nil, 比如: var a *int var a []int var a map[string] int var a chan int var a func(string) int var a error // error 是接口 //也可以省略var, 用v_name := value形式, 但只能在函数体内出现? f := \"Runoob\" //空白标识符_只能写, 不能读, 用于占位, 抛弃不想要的值; 比如下面的值5就被抛弃了 _, b = 5, 7 //取地址&和取值*和C一样 用type_name(expression)做类型转换 Go 没有三目运算符，所以不支持 ?: 形式的条件判断 循环只有forfunc main() { //也可以不要true, 直接一个for就行了 for true { fmt.Printf(\"这是无限循环。\\n\"); } } //一般的for形式都支持, 里面可以有break continue goto for C 函数定义形式, 可以有多个返回值 func function_name( [parameter list] ) [return_types] { 函数体 } /* 函数返回两个数的最大值 */ func max(num1, num2 int) int { /* 声明局部变量 */ var result int if (num1 > num2) { result = num1 } else { result = num2 } return result } 函数可以直接\"赋值\"给变量, 在大部分现代语言中, 函数也可以当作变量 func main(){ /* 声明函数变量 */ getSquareRoot := func(x float64) float64 { return math.Sqrt(x) } /* 使用函数 */ fmt.Println(getSquareRoot(9)) } go的闭包, 在这里就是函数返回值是另一个函数 //Go 语言支持匿名函数，可作为闭包。匿名函数是一个\"内联\"语句或表达式。匿名函数的优越性在于可以直接使用函数内的变量，不必申明。 //以下实例中，我们创建了函数 getSequence() ，返回另外一个函数。该函数的目的是在闭包中递增 i 变量，代码如下： package main import \"fmt\" func getSequence() func() int { i:=0 return func() int { i+=1 return i } } func main(){ /* nextNumber 为一个函数，函数 i 为 0 */ nextNumber := getSequence() /* 调用 nextNumber 函数，i 变量自增 1 并返回 */ fmt.Println(nextNumber()) fmt.Println(nextNumber()) fmt.Println(nextNumber()) /* 创建新的函数 nextNumber1，并查看结果 */ //重新调用getSequence()函数, i是重新申请的变量 nextNumber1 := getSequence() //这里的number会重新开始编号 fmt.Println(nextNumber1()) fmt.Println(nextNumber1()) fmt.Println(nextNumber()) fmt.Println(nextNumber()) } //结果 1 2 3 1 2 //这里是nextNumber继续编号 4 5 //闭包也可以带参数 func main() { add_func := add(1,2) fmt.Println(add_func(1,1)) fmt.Println(add_func(0,0)) fmt.Println(add_func(2,2)) } // 闭包使用方法 //x1 x2是初始化add_func用的, x3 x4是传给add_func的 func add(x1, x2 int) func(x3 int,x4 int)(int,int,int) { i := 0 return func(x3 int,x4 int) (int,int,int){ i++ return i,x1+x2,x3+x4 } } //结果 1 3 2 2 3 0 3 3 4 和C一样, 局部变量在函数内使用; 全局变量在函数体外声明, 在整个包使用, 甚至可以被导出后的外部包使用. 数组变量定义: var variable_name [SIZE] variable_type, 和C一样, 下标从0开始 数组变量赋值var balance = [5]float32{1000.0, 2.0, 3.4, 7.0, 50.0} var balance = [...]float32{1000.0, 2.0, 3.4, 7.0, 50.0} var salary float32 = balance[9] var a = [5][2]int{ {0,0}, {1,2}, {2,4}, {3,6},{4,8}} 指针和引用 func main() { /* 定义局部变量 */ var a int = 100 var b int= 200 fmt.Printf(\"交换前 a 的值 : %d\\n\", a ) fmt.Printf(\"交换前 b 的值 : %d\\n\", b ) /* 调用函数用于交换值 * &a 指向 a 变量的地址 * &b 指向 b 变量的地址 */ swap(&a, &b); fmt.Printf(\"交换后 a 的值 : %d\\n\", a ) fmt.Printf(\"交换后 b 的值 : %d\\n\", b ) } func swap(x *int, y *int) { var temp int temp = *x /* 保存 x 地址的值 */ *x = *y /* 将 y 赋值给 x */ *y = temp /* 将 temp 赋值给 y */ } 结构体, 和c不同的是, 结构体指针访问成员的时候, 也是用. type Books struct { title string author string subject string book_id int } //声明结构体变量并初始化 //variable_name := structure_variable_type {value1, value2...valuen} //或 //variable_name := structure_variable_type { key1: value1, key2: value2..., keyn: valuen} fmt.Println(Books{\"Go 语言\", \"www.runoob.com\", \"Go 语言教程\", 6495407}) Book1.title = \"Go 语言\" Book1.author = \"www.runoob.com\" Book1.subject = \"Go 语言教程\" Book1.book_id = 6495407 切片, 数组的大小是固定的, 而切片大小可以变. \"动态数组\" func main() { /* 创建切片 */ numbers := []int{0,1,2,3,4,5,6,7,8} printSlice(numbers) /* 打印原始切片 */ fmt.Println(\"numbers ==\", numbers) /* 打印子切片从索引1(包含) 到索引4(不包含)*/ fmt.Println(\"numbers[1:4] ==\", numbers[1:4]) /* 默认下限为 0*/ fmt.Println(\"numbers[:3] ==\", numbers[:3]) /* 默认上限为 len(s)*/ fmt.Println(\"numbers[4:] ==\", numbers[4:]) numbers1 := make([]int,0,5) printSlice(numbers1) /* 打印子切片从索引 0(包含) 到索引 2(不包含) */ number2 := numbers[:2] printSlice(number2) /* 打印子切片从索引 2(包含) 到索引 5(不包含) */ number3 := numbers[2:5] printSlice(number3) } func printSlice(x []int){ fmt.Printf(\"len=%d cap=%d slice=%v\\n\",len(x),cap(x),x) } range关键字用于 for 循环中迭代数组(array)、切片(slice)、通道(channel)或集合(map)的元素 func main() { //这是我们使用range去求一个slice的和。使用数组跟这个很类似 nums := []int{2, 3, 4} sum := 0 for _, num := range nums { sum += num } fmt.Println(\"sum:\", sum) //在数组上使用range将传入index和值两个变量。上面那个例子我们不需要使用该元素的序号，所以我们使用空白符\"_\"省略了。有时侯我们确实需要知道它的索引。 for i, num := range nums { if num == 3 { fmt.Println(\"index:\", i) } } //range也可以用在map的键值对上。 kvs := map[string]string{\"a\": \"apple\", \"b\": \"banana\"} for k, v := range kvs { fmt.Printf(\"%s -> %s\\n\", k, v) } //range也可以用来枚举Unicode字符串。第一个参数是字符的索引，第二个是字符（Unicode的值）本身。 for i, c := range \"go\" { fmt.Println(i, c) } } 值传递和指针类型 有人总结: golang中的传参是值传递, 但因为map channel和slice等内置数据结构本身是指针类型, 所以它们作为参数传递时, 相当于传递了指针. struct 形式 type Books struct { title string author string subject string book_id int } 结构体方法 Go 语言中同时有函数和方法。一个方法就是一个包含了接受者的函数，接受者可以是命名类型或者结构体类型的一个值或者是一个指针。所有给定类型的方法属于该类型的方法集 形式为: 注意和函数定义的区别 func(receiver type)methodName([参数列表]) [返回值列表]{ } 普通函数定义为: func function_name( [parameter list] ) [return_types] { 函数体 } package main import ( \"fmt\" ) type Student struct{ Name string Age int } func (stu *Student)Set(name string,age int){ stu.Name = name stu.Age = age } func main(){ var s Student s.Set(\"tome\",23) fmt.Println(s) } //注意:方法的访问控制也是通过大小写控制的 //在上面这个例子中需要注意一个地方 //func (stu *Student)Set(name string,age int) //这里使用的是(stu *Student)而不是(stu Student)这里其实是基于指针对象的方法 //当调用一个函数时，会对其每个参数值进行拷贝，如果一个函数需要更新一个变量，或者函数的其中一个参数是在太大 //我们希望能够避免进行这种默认的拷贝，这种情况下我们就需要用到指针了，所以在上一个代码例子中那样我们需要 //func (stu *Student)Set(name string,age int)来声明一个方法 基于指针对象的方法 package main import ( \"fmt\" ) type Point struct{ X float64 Y float64 } func (p *Point) ScaleBy(factor float64){ p.X *= factor p.Y *= factor } func main(){ //两种方法 //方法1 r := &Point{1,2} r.ScaleBy(2) fmt.Println(*r) //方法2 p := Point{1,2} pptr := &p pptr.ScaleBy(2) fmt.Println(p) //方法3 p2 := Point{1,2} (&p2).ScaleBy(2) fmt.Println(p2) //相对来说方法2和方法3有点笨拙 //方法4,go语言这里会自己判断p是一个Point类型的变量， //并且其方法需要一个Point指针作为指针接收器，直接可以用下面简单的方法 p3 := Point{1,2} p3.ScaleBy(2) fmt.Println(p3) } //上面例子中最后一种方法，编译器会隐式的帮我们用&p的方法去调用ScaleBy这个方法 继承 package main import ( \"fmt\" ) type People struct{ Name string Age int } type Student struct{ People Score int } func main(){ var s Student /* s.People.Name = \"tome\" s.People.Age = 23 */ //上面注释的用法可以简写为下面的方法 s.Name = \"tom\" s.Age = 23 s.Score = 100 fmt.Printf(\"%+v\\n\",s) //注意：关于字段冲突的问题，我们在People中定义了一个Name字段，在Student中再次定义Name,这个时候，我们通过s.Name获取的就是Student定义的Name字段 } 结构体可以比较 如果结构体的所有成员变量都是可比较的，那么结构体就可比较 如果结构体中存在不可比较的成员变量，那么结构体就不能比较 map和切片不能比较 package main import ( \"fmt\" ) type Point struct{ x int y int } func main(){ p1 := Point{1,2} p2 := Point{2,3} p3 := Point{1,2} fmt.Println(p1==p2) //false fmt.Println(p1==p3) //true } new分配内存 package main import ( \"fmt\" ) type Student struct { Id int Name string } func main() { s := new(Student) s.Id = 1 s.Name = \"test\" s1 := Student{Id: 2, Name: \"test1\"} fmt.Println(s, s1) } //输出结果: &{1 test} {2 test1} //s 的类型为指针，s1 为一个Student类型; //说明new的返回值是指针, 而用struct_type{}声明的变量是struct_type实例本身. 工厂模式初始化 go没有构造函数, 所以工厂模式很常用 package main import ( \"fmt\" ) type Student struct{ Name string Age int } func NewStudent(name string,age int) *Student{ return &Student { Name:name, Age:age, } } func main(){ s := NewStudent(\"tom\",23) fmt.Println(s.Name) } 接口 interface Go 语言提供了另外一种数据类型即接口，它把所有的具有共性的方法定义在一起，任何其他类型只要实现了这些方法就是实现了这个接口。 /* 定义接口 */ type interface_name interface { method_name1 [return_type] method_name2 [return_type] method_name3 [return_type] ... method_namen [return_type] } /* 定义结构体 */ type struct_name struct { /* variables */ } /* 实现接口方法 */ func (struct_name_variable struct_name) method_name1() [return_type] { /* 方法实现 */ } ... func (struct_name_variable struct_name) method_namen() [return_type] { /* 方法实现*/ } package main import ( \"fmt\" ) type Phone interface { call() } type NokiaPhone struct { } func (nokiaPhone NokiaPhone) call() { fmt.Println(\"I am Nokia, I can call you!\") } type IPhone struct { } func (iPhone IPhone) call() { fmt.Println(\"I am iPhone, I can call you!\") } func main() { //这是interface类型的变量 var phone Phone //new返回一个指针, 并对其内存清零; //也可以写成phone = NokiaPhone{}, 或phone = &NokiaPhone{} phone = new(NokiaPhone) phone.call() phone = new(IPhone) phone.call() } //在上面的例子中，我们定义了一个接口Phone，接口里面有一个方法call()。 //然后我们在main函数里面定义了一个Phone类型变量，并分别为之赋#值为NokiaPhone和IPhone。 //然后调用call()方法，输出结果如下： I am Nokia, I can call you! I am iPhone, I can call you! 类型断言 类型断言提供了一个机制找出接口底层对应的实际类型: t := i.(T) 这个语句在断言接口i中实际包含了类型T，然后把底层类型T的值赋值给变量t 如果断言失败，i中没有包含T，这条语句会触发panic 为了测试接口是否包含指定的类型，类型断言会返回2个值，底层类型实际对应的值和一个bool值，来报告断言是否成功 t, ok := i.(T) 如果i中包含T，则t是底层类型的实际值，变量ok是真 如果断言失败，ok变量是假，t是一个零值类型T，不会触发panic，这个语法和对map操作类似 只有interface有类型断言, 因为interface可以指向任何东西, 所以要有办法知道它的运行时类型: https://www.jianshu.com/p/6a46fc7b6e5b x.(T) 检查x的动态类型是否是T，其中x必须是接口值。 switch形式: func main() { var x interface{} x = 17 //fmt包看实际类型, 因为Printf的入参就是接口类型 fmt.Printf(\"type x is %T, value x is %d\\n\", x, x) //这是个特殊的type switch, switch后面是个赋值表达式, 但case的对象是类型 switch v := x.(type) { //case的顺序是有意义的，因为可能同时满足多个接口，不可以用fallthrough, default的位置无所谓。 case nil: fmt.Printf(\" x 的类型 :%T\", v) case int: fmt.Printf(\"x 是 int 型, 值为%v\", v) case float64: fmt.Printf(\"x 是 float64 型, 值为%v\", v) case func(int) float64: fmt.Printf(\"x 是 func(int) 型, 值为%p\", v) case bool, string: fmt.Printf(\"x 是 bool 或 string 型, 值为%v\", v) default: fmt.Printf(\"未知型\") } } //结果: type x is int, value x is 17 x 是 int 型, 值为17 类型断言判断对象是否实现了一个接口 判断val是否实现了json.Marshaler需要的接口, 即val是否为json.Marshaler类型. if _, ok := val.(json.Marshaler); ok { fmt.Printf(\"value %v of type %T implements json.Marshaler\\n\", val, val) } goroutine goroutine 是轻量级线程，goroutine 的调度是由 Golang 运行时进行管理的。 //语法 go 函数名(参数列表) //比如 go f(x, y, z) //并行执行, 就像shell的后台执行一样 package main import ( \"fmt\" \"time\" ) func say(s string) { for i := 0; i 通道 通道（channel）是用来传递数据的一个数据结构, 可用于两个 goroutine 之间通过传递一个指定类型的值来同步运行和通讯。操作符 // 声明一个通道很简单，我们使用chan关键字即可，通道在使用前必须先创建： ch := make(chan int) ch // 也可以不用make来声明channel, 用var也可以; 默认初始化为nil var c chan int fmt.Println(c) select语句形式上类似 switch 语句，但实际上和C里面的select差不多: 用于监听和channel有关的IO操作，当 IO 操作发生时，触发相应的动作. 和switch不同, select会对每个case, 都会对channel求值, 如果有可用的IO, 则随机执行其中一个case的后续指令. 如果没有可用的IO, 则执行default. 如果没有可用IO, 也没有default, 则一直阻塞到IO可用. 比如一个loadbalancer func (b *Banlancer) balance(work chan Request) { for { select { case req := package main import \"fmt\" func sum(s []int, c chan int) { sum := 0 for _, v := range s { sum += v } c 带缓冲的通道 // 默认情况下，通道是不带缓冲区的。发送端发送数据，同时必须又接收端相应的接收数据。 // 通道可以设置缓冲区，通过 make 的第二个参数指定缓冲区大小： ch := make(chan int, 100) package main import \"fmt\" func main() { // 这里我们定义了一个可以存储整数类型的带缓冲通道 // 缓冲区大小为2 ch := make(chan int, 2) // 因为 ch 是带缓冲的通道，我们可以同时发送两个数据 // 而不用立刻需要去同步读取数据 ch 通道用close来关闭 //如果通道接收不到数据后 ok 就为 false，这时通道就可以使用 close() 函数来关闭 v, ok := package main import ( \"fmt\" ) func fibonacci(n int, c chan int) { x, y := 0, 1 for i := 0; i 切片 切片是对数组的描述, 一个数组可以有多个描述 用make创建切片的时候, make([]type, length, capacity), 其中, length表示切片的大小, capacity表示切片底层array的大小; capacity容量够的话, append()不会产生新的更大的array 切片的append arr从[0 1 2 3 4 5 6 7]变为[0 1 2 3 4 5 6 10]是因为s3 := append(s2, 10)，s2=[5 6]，再往后添加10的时候，把arr中的7变为了10。而后面再添加11、12时，因为已经超越了arr的cap，所以系统会重新分配更大的底层数组，而不再是对arr进行操作，原来的数组如果有人用就会依旧存在，如果没人用了就会自动垃圾回收 package main import \"fmt\" func main() { arr := [...]int{0, 1, 2, 3, 4, 5, 6, 7} fmt.Println(\"arr: \", arr) // output：arr: [0 1 2 3 4 5 6 7] s1 := arr[2:6] s2 := s1[3:5] //其实s1的index只有0 1 2 3, 这里的3:5是3和4, 引用的是原始arr fmt.Println(\"s1: \", s1) // output：s1: [2 3 4 5] //len(s2)为2 fmt.Println(\"s2: \", s2) // output：s2: [5 6] s3 := append(s2, 10) s4 := append(s3, 11) s5 := append(s4, 12) fmt.Println(\"s3: \", s3) // output：s3: [5 6 10] fmt.Println(\"s4: \", s4) // output：s4: [5 6 10 11] fmt.Println(\"s5: \", s5) // output：s5: [5 6 10 11 12] fmt.Println(\"arr: \", arr) // output：arr: [0 1 2 3 4 5 6 10] } 当我们用append追加元素到切片时，如果容量不够，go就会创建一个新的切片变量, 并进行\"深拷贝\", 即把原来的array的元素值, 都拷贝到新的更大的array里面, 再append. 如果用量够用, 就不用\"深拷贝\"了 func main() { var sa []string //用%p可以打印sa的地址, sa本来就是个 fmt.Printf(\"addr:%p \\t\\tlen:%v content:%v\\n\",sa,len(sa),sa); for i:=0;i map 集合 Map 是一种无序的键值对的集合。Map 最重要的一点是通过 key 来快速检索数据，key 类似于索引，指向数据的值。 Map 是一种集合，所以我们可以像迭代数组和切片那样迭代它。不过，Map 是无序的，我们无法决定它的返回顺序，这是因为 Map 是使用 hash 表来实现的。 /* 声明变量，默认 map 是 nil */ var map_variable map[key_data_type]value_data_type /* 使用 make 函数 */ map_variable := make(map[key_data_type]value_data_type) package main import \"fmt\" func main() { var countryCapitalMap map[string]string /*创建集合 */ countryCapitalMap = make(map[string]string) /* map插入key - value对,各个国家对应的首都 */ countryCapitalMap [ \"France\" ] = \"Paris\" countryCapitalMap [ \"Italy\" ] = \"罗马\" countryCapitalMap [ \"Japan\" ] = \"东京\" countryCapitalMap [ \"India \" ] = \"新德里\" /*使用键输出地图值 */ for country := range countryCapitalMap { fmt.Println(country, \"首都是\", countryCapitalMap [country]) } /*查看元素在集合中是否存在 */ captial, ok := countryCapitalMap [ \"美国\" ] /*如果确定是真实的,则存在,否则不存在 */ /*fmt.Println(captial) */ /*fmt.Println(ok) */ if (ok) { fmt.Println(\"美国的首都是\", captial) } else { fmt.Println(\"美国的首都不存在\") } } //运行结果 France 首都是 Paris Italy 首都是 罗马 Japan 首都是 东京 India 首都是 新德里 美国的首都不存在 delete可以删除元素 range 用range可以对数组(array), 切片(slice), 通道(channel), 集合(map)进行遍历 package main import \"fmt\" func main() { //这是我们使用range去求一个slice的和。使用数组跟这个很类似 nums := []int{2, 3, 4} sum := 0 for _, num := range nums { sum += num } fmt.Println(\"sum:\", sum) //在数组上使用range将传入index和值两个变量。上面那个例子我们不需要使用该元素的序号，所以我们使用空白符\"_\"省略了。有时侯我们确实需要知道它的索引。 for i, num := range nums { if num == 3 { fmt.Println(\"index:\", i) } } //range也可以用在map的键值对上。 kvs := map[string]string{\"a\": \"apple\", \"b\": \"banana\"} for k, v := range kvs { fmt.Printf(\"%s -> %s\\n\", k, v) } //range也可以用来枚举Unicode字符串。第一个参数是字符的索引，第二个是字符（Unicode的值）本身。 for i, c := range \"go\" { fmt.Println(i, c) } } // 运行结果: sum: 9 index: 1 a -> apple b -> banana 0 103 1 111 "},"notes/golang_json性能.html":{"url":"notes/golang_json性能.html","title":"Golang json性能比较","keywords":"","body":" json性能之 C vs Go 代码如下: 编译和运行 C Go 编译后大小 测试用的json文件 jq x86 mips 性能结果分析 补充mips测试结果 go工具链test代码 encoding/json encoding/json benchmarking模式 强制单核模式 json benchmarking流程 test2json运行 test2json_test代码 test2json 返回interface类型 github golang example reverse.go reverse test json性能之 C vs Go 思路: 对一个给定的xxxx.json文件, 调用相应的API把json文件load并解析到内部表达, 再调用相应API把内部表达转换回json文件, 输出到控制台. 测试时传入循环次数, 统计消耗时间. Reborn平台上, 用的是C语言实现的libjansson库, 使用json_load_file()load并解析json文件, 使用json_dumpfd()反解析并输出到文件. 对Go来说, toolchain自带json解析库, 用json.Unmarshal()来解析json, 用json.Marshal()来做反解析. 代码如下: C语言版本(使用libjansson库): Go语言版本: 编译和运行 在x86环境下 C #先安装libjansson库 #x86 ubuntu apt install libjansson-dev #mips gentoo taskset -c 1,2,3 emerge -av dev-libs/jansson #编译, 生成a.out gcc -O2 json_load.c -ljansson #运行 time ./a.out test.json 100 > /dev/null Go #编译 go build json_load.go #运行, 默认多进程 time ./json_load -fileName test.json -loopNum 100 > /dev/null #强制单核 taskset -c 1 time ./json_load -fileName test.json -loopNum 100 > /dev/null #gentoo上time不是独立的程序 taskset -c 1 sh -c \"time ./json_load -fileName test.json -loopNum 100 > /dev/null\" 编译后大小 测试用的json文件 example.json test.json1.9M, 190万个字符, 复杂嵌套 jq x86 mips 性能结果分析 C语言json解析运行时 Go语言json解析运行时, 默认多核运行. 注意: 测试代码和运行时命令都并没有显式开启多线程, 多线程是json库函数的行为.纠正: 多线程是go runtime的行为. 详见goroutine解析 Go语言json解析运行时, 强制单核 结果. 运行多次比较稳定: Go toolchain自带的json解析器默认是多进程解析 纠正: 是go runtime起的多线程, go的json解析只是一个goroutine, 被go的runtime调度器调度. 本质上还是单进程. libjansson是单进程解析. C语言版本有大量内核态操作, 估计是load文件操作, 用户态只有一半多一点时间在真正干活. Go语言load文件优化的很好, 强制单进程时, 用户态进程占比99%, 说明所有时间都在做json解析和反解析. 即使除去内核态读文件的差异, Go的json库也比libjansson快一些. 此性能对比无法做到绝对公平, C语言也有更快的json库, Go也可以继续优化; 但从实际使用角度出发, 此对比还是很有参考意义. 补充mips测试结果 go工具链test代码 encoding/json go tool dist test -list | grep -i json cd ~/repo/gorepo/go/src/encoding/json go test -c -o jsontest #help ./jsontest -h #多进程运行 time ./jsontest #单核运行 time taskset -c 1 ./jsontest #所有测试项: ./jsontest -test.list .* ...很多, Test开头的是功能测试, Benchmark开头的是性能测试 encoding/json是多个test的集合: encoding/json benchmarking模式 go test命令族实现了benchmarking的测试, 详见go help test 和 go doc testing 看起来是要加-bench来使能benchmarking模式 对上面的已经编译好的./jsontest来说, ./jsontest -h说要加./jsontest -test.bench .* 默认是多进程模式 强制单核模式 taskset -c 1 ./jsontest -test.bench .* json benchmarking流程 code.json是个复杂的, 多层嵌套的json, 1.9M大小. 测试代码测试code.json到in-memory 结构体的各种操作: Marshal Unmarshal Encode Decode等待 type codeResponse struct { //指针 Tree *codeNode `json:\"tree\"` Username string `json:\"username\"` } type codeNode struct { Name string `json:\"name\"` //数组, 嵌套的codeNode指针 Kids []*codeNode `json:\"kids\"` CLWeight float64 `json:\"cl_weight\"` Touches int `json:\"touches\"` MinT int64 `json:\"min_t\"` MaxT int64 `json:\"max_t\"` MeanT int64 `json:\"mean_t\"` } var codeJSON []byte var codeStruct codeResponse func codeInit() { //os Open f, err := os.Open(\"testdata/code.json.gz\") if err != nil { panic(err) } //defer是先进后出, 也就是说, 离开这个函数的时候, 最后才调用f.Close() defer f.Close() gz, err := gzip.NewReader(f) if err != nil { panic(err) } //data是文件内容 data, err := ioutil.ReadAll(gz) if err != nil { panic(err) } //赋值会深拷贝吗? -- 好像是的 -- 后记: 应该不拷贝 //data是切片, codeJSON也是切片; codeJSON和data同时指向底层文件的数据. 相当于两个指针都指向底层的真正数据 //func ReadAll(r io.Reader) ([]byte, error); 真正的数据保存在这个函数里申请的内存中; codeJSON = data //先Unmarshal, 即转为结构体 if err := Unmarshal(codeJSON, &codeStruct); err != nil { panic(\"unmarshal code.json: \" + err.Error()) } //在把codeStruct Marshal到data; //这么看data变了, 但codeJSON 不变 -- 后记: data只是相当于指针, 指针变了而已 if data, err = Marshal(&codeStruct); err != nil { panic(\"marshal code.json: \" + err.Error()) } //字节比较 if !bytes.Equal(data, codeJSON) { println(\"different lengths\", len(data), len(codeJSON)) for i := 0; i test2json运行 在go/src/cmd/internal/test2json下面运行go test 或者 还是在这个目录下, 编译出可执行文件再运行go test -c -o test2json 这是个多进程程序, 用strace的-f命令可以看出来: strace -f -o log ./test2json 用taskset -c 1执行, 可强制单核跑: taskset -c 1 ./test2json 列出所有测试项: ./test2json -test.list .* TestGolden TestTrimUTF8 test2json特性: 用户态占比接近100% 操作包括 文件open, read json和go结构体互相转换, 调用标准json库 字符串解析和操作 结构体深度比较, 使用go的反射机制 test2json_test代码 测试的思路是: 先调用test2json的converter方法, 把xxx.test转为json, 和已经保存好的正确的xxx.json比对, 一致则测试通过. 如何证明一致: 用json.Unmarshal()把have和want的json都转为结构体event 用反射包里提供的reflect.DeepEqual来深度比较两个结构体. func TestGolden(t *testing.T) { //func Glob(pattern string) (matches []string, err error) //读*.test文件列表 files, err := filepath.Glob(\"testdata/*.test\") if err != nil { t.Fatal(err) } //对每个文件 for _, file := range files { name := strings.TrimSuffix(filepath.Base(file), \".test\") //用testing的Run方法, 注册每个xxx.test 的sub run //用test2json的converter去把test里面的字段, 转换为json //和保存好的对应的xxx.json文件比对, 一致则测试成功. t.Run(name, func(t *testing.T) {......}) } } t.Run是subtest, 由其parent test控制, 可多进程执行. func (t *T) Run(name string, f func(t *T)) bool Run runs f as a subtest of t called name. It runs f in a separate goroutine and blocks until f returns or calls t.Parallel to become a parallel test. Run reports whether f succeeded (or at least did not fail before calling t.Parallel). Run may be called simultaneously from multiple goroutines, but all such calls must return before the outer test function for t returns. test2json go tool dist test -list go tool dist test -rebuild -run test2json 这个程序的思路是: 根据xxx.test文件, 填event结构体; 用json.Marshal()把结构体转为json, 保存到文件xxx.json //`json:\",omitempty\"`是反射字段, 给encoding/json包看的, json.Marshal()会用到 // event is the JSON struct we emit. type event struct { Time *time.Time `json:\",omitempty\"` Action string Package string `json:\",omitempty\"` } 返回interface类型 // io.WriteCloser是个interface类型 type WriteCloser interface { Writer Closer } // 而Writer又是个interface, 包装了Write方法 type Writer interface { Write(p []byte) (n int, err error) } // 同理是Closer type Closer interface { Close() error } // converter实现了Write方法和Close方法 func (c *converter) Write(b []byte) (int, error) { c.input.write(b) return len(b), nil } //这里new(converter)返回指针, converter实现了io.WriteCloser的方法, 符合io.WriteCloser的interface要求 //之所以要先new一个converter, 而不是直接返回&converter, 是因为在给*c赋值的时候, 要用到c.writeOutputEvent; 这是个先又new的c对象, 在给*c赋值的特殊场景;实际上, 是对*c包含的一个结构体赋值. //如果没有上面的特殊场景, 直接返回converter的实例就行. 比如c= converter{....} return &c //把converter当作io.WriteCloser返回 func NewConverter(w io.Writer, pkg string, mode Mode) io.WriteCloser { c := new(converter) *c = converter{ w: w, pkg: pkg, mode: mode, start: time.Now(), input: lineBuffer{ b: make([]byte, 0, inBuffer), line: c.handleInputLine, part: c.output.write, }, output: lineBuffer{ b: make([]byte, 0, outBuffer), line: c.writeOutputEvent, part: c.writeOutputEvent, }, } return c } github golang example reverse.go https://github.com/golang/example/blob/master/stringutil/reverse.go // Package stringutil contains utility functions for working with strings. package stringutil // Reverse returns its argument string reversed rune-wise left to right. func Reverse(s string) string { //rune是int32的别名; 这里是把s转换为int32的数组 r := []rune(s) for i, j := 0, len(r)-1; i reverse test https://github.com/golang/example/blob/master/stringutil/reverse_test.go //package和reverse.go相同, 同一个文件夹下面的惯例都是同一个package //package不是main, 所以这个是个lib package stringutil import \"testing\" //T是个struct: type T struct{ ... } func TestReverse(t *testing.T) { //range遍历, 返回index和value for _, c := range []struct { //in和want写一行, 都是string类型 in, want string }{ {\"Hello, world\", \"dlrow ,olleH\"}, {\"Hello, 世界\", \"界世 ,olleH\"}, {\"\", \"\"}, } { got := Reverse(c.in) if got != c.want { t.Errorf(\"Reverse(%q) == %q, want %q\", c.in, got, c.want) } } } "},"notes/golang_原理.html":{"url":"notes/golang_原理.html","title":"Golang 原理相关","keywords":"","body":" go的内存模型 例子1 例子2 业务逻辑上避免调用锁 for循环里的变量 变量每次进入循环都会初始化 变量的地址会变吗? 不会? 会! 结论: 理论解释 -- 表示怀疑 怀疑 合理解释 内置new函数也不是一定分配到堆 总结 修正 再修正 reflect.ValueOf ValueOf流程 什么是ifaceIndir Value的Elem()方法 emptyInterface和nonEmptyInterface ValueOf实例 结论 强制escape 问答 为什么看到value的kind是54? interface赋值给interface 直接赋值: interface只有一层 取地址赋值: interface包interface地址 结论 slice能当作\"出参\"传递 结论 slice和gc 具体例子 Remove all elements Keep allocated memory 结论 float32和data race 使用sync/atomic 再说reflect 什么是unaddressable 不能改变a 能改变a 为什么? 更进一步解释 结论 例子 原理 结论 再议interface interface{}回顾 set get性能损失如何? 结论 标准库的time.Now()如何取得系统时间? time.Now()的实现流程 clock_gettime系统调用 逃逸分析和变量分配 逃逸分析使用 逃逸分析实例 go的值和指针 map 初始化 hashmap结构体 make map变量的时候 遍历 hash算法 常用的hash算法 结论 Gc的演进 interface赋值 更正 interface的内部表达 reflect Type和interface reflect method reflect Value timer timer API 单次timer 周期性timer timer实现原理 add timer 触发timer time.NewTimer()注册了sendTime()回调 timer堆的维护 time包的NewTimer方法调用了runtime.startTimer 性能测试和结果 测试结果 结论 go1.14对timer的优化 go1.13的timer问题 1.14解决思路 系统监控 监控循环 检查timer 检查死锁 轮询网络 抢占处理器 垃圾回收 IO多路复用 golang对epoll的封装 数据结构 初始化 goroutine等待事件 调用epoll 截至日期 GC 垃圾收集器 go runtime调度器 相关的结构体表示 go比较快的5点 go的内存模型 对一个goroutine来说, 编译器和CPU可以合理的乱序, 但必须保证程序顺序的正确性. 即无关的指令才能reorder, 比如a = 1; b = 2;, 在另外一个routine观察可以先看到b = 2 多routine对共享变量的access(重点, 包括读和写), 必须用sync方法 对大于machine word(比如32bit)的值的读写, 是多个machine word size的操做, 它们的顺序未定义 init函数是在特殊的初始化gorotine里执行的, 但init函数可以启动新的goroutine. 被import包的init函数一定先于importer完成. main.main一定是最后执行 例子1 var a, b int func f() { a = 1 b = 2 } func g() { print(b) print(a) } func main() { go f() g() } 可能打印2然后是0. 即f()的b = 2先被g()观察到. 例子2 var a string var done bool func setup() { a = \"hello, world\" done = true } func doprint() { if !done { once.Do(setup) } print(a) } func twoprint() { go doprint() go doprint() } 在doprint()里, 观察到done的写入, 因为乱序执行, 不一定a = \"hello, world\"也完成了. 下面的代码也不对: var a string var done bool func setup() { a = \"hello, world\" done = true } func main() { go setup() for !done { } print(a) } 过了for !done之后, a可能依然是空. 更糟糕的是, 因为没有使用sync, done的写入不能保证一定被main观察到, 以至于main永远不退出. 下面的错误代码更有隐蔽性: type T struct { msg string } var g *T func setup() { t := new(T) t.msg = \"hello, world\" g = t } func main() { go setup() for g == nil { } print(g.msg) } 既是main看到g不是nil了, 也不能保证g.msg就有值了. 业务逻辑上避免调用锁 代码1 func (mq *msgQ) putMsg(mm *metaKnownMsg) { if _, ok := mm.msg.(HighPriorityMessage); ok { if mq.ingressHighChan == nil { mq.Lock() if mq.ingressHighChan == nil { mq.ingressHighChan = make(chan *metaKnownMsg, mq.qsize) } mq.Unlock() } mq.ingressHighChan 代码2 func (mq *msgQ) putMsg(mm *metaKnownMsg) { if _, ok := mm.msg.(HighPriorityMessage); ok { mq.Lock() if mq.ingressHighChan == nil { mq.ingressHighChan = make(chan *metaKnownMsg, mq.qsize) } mq.Unlock() mq.ingressHighChan 在并发场景下, 代码1比代码2理论上性能高非常多. 这里在ingressHighChan为空的时候, 需要新建队列. 这个事情只用做一次. 而代码2在每次进入函数的时候, 都要去获取锁, 那么比如说并发100个函数都走到这里, 就只有一个人能够获取到做, 其他人必须等待锁释放. 而接下来的99个人, 都必须串行的完成这个过程. 总结: 在业务逻辑侧尽量减少lock的调用. 比如这里已知队列为空的时候才调用锁. for循环里的变量 变量每次进入循环都会初始化 比如下面的代码: for { var a, b, c int fmt.Println(\"Hello, playground\", a, b, c) a = 9 fmt.Println(\"Hello, playground\", a, b, c) time.Sleep(time.Second) } 输出: Hello, playground 0 0 0 Hello, playground 9 0 0 Hello, playground 0 0 0 Hello, playground 9 0 0 可以看到: a,b,c都是for里面定义的变量, 初始为零值. 没毛病 循环体里面把a赋值为9, 随后打印a为9, 也没毛病. 本次循环体执行完毕后, 下次循环体执行时, a的值又从零值开始. -- 这里不能用C的思路去理解 变量的地址会变吗? 不会? 看下面的代码: for i := 0; i 输出: Hello, playground 0 824634355464 Hello, playground 9 824634355464 Hello, playground 0 824634355464 Hello, playground 9 824634355464 Hello, playground 0 824634355464 Hello, playground 9 824634355464 看起来变量a的地址并没有变化 会! func main() { for i := 0; i 输出: Hello, playground 0 824634388176 Hello, playground 9 824634388176 Hello, playground 0 824634388176 Hello, playground 9 824634388176 Hello, playground 0 824634388176 Hello, playground 9 824634388176 in main 0 824634499152 in go 9 824634499152 in main 0 824634499176 in go 9 824634499176 in main 0 824634499200 in go 9 824634499200 那么更进一步的问题: 这里go中看到的变量a地址, 和main中每次进入循环体时a的地址一样, 是否是因为它们在时间顺序上前者在后, 后者在前? 改成这样: for i := 0; i 在go函数里面, 先延迟一秒钟. 那么main的for循环会先执行完, goroutine都在后面执行. 结果如下, 说明go函数里面取得到的变量a, 就是本次循环体里面的变量a. in main 0 824634499136 in main 0 824634499152 in main 0 824634499168 in main 0 824634499184 in main 0 824634499200 in main 0 824634499216 in main 0 824634499232 in main 0 824634499248 in main 0 824634499264 in main 0 824634499280 in main 0 824634499296 in main 0 824634499312 in main 0 824634499328 in main 0 824634499344 in main 0 824634499360 in main 0 824634499376 in main 0 824634499392 in main 0 824634499408 in main 0 824634499424 in main 0 824634499440 in main 0 824634499456 in main 0 824634499472 in main 0 824634499488 in main 0 824634499504 in main 0 824634499520 in main 0 824634499536 in main 0 824634499552 in main 0 824634499568 in main 0 824634499584 in main 0 824634499600 in go 9 824634499136 in go 9 824634499600 in go 9 824634499584 in go 9 824634499568 in go 9 824634499552 in go 9 824634499536 in go 9 824634499520 in go 9 824634499504 in go 9 824634499488 in go 9 824634499472 in go 9 824634499456 in go 9 824634499184 in go 9 824634499424 in go 9 824634499408 in go 9 824634499392 in go 9 824634499376 in go 9 824634499360 in go 9 824634499344 in go 9 824634499328 in go 9 824634499312 in go 9 824634499296 in go 9 824634499280 in go 9 824634499264 in go 9 824634499248 in go 9 824634499232 in go 9 824634499216 in go 9 824634499200 in go 9 824634499440 in go 9 824634499168 in go 9 824634499152 结论: for循环体里的变量, 但按照下面的理论来说, 每次进入循环体, 都进入了一个新的scope, 变量地址应该会变化. 少数情况下, 循环体比较简单, 可能变量地址碰巧不变. --结论错误!!!!! for循环体里的变量, 被go函数捕获时, 用的是本次循环体里的变量. 即使循环体在main中异步的改变了该变量, 也不影响已经go出去的routine. --表面正确!!!! 正确结论见下面 理论解释 -- 表示怀疑 参考stackoverflow 有人问为什么在循环里可以: func main() { for i := 0; i 但自己手动写就编译不过: func main() { a := 77 fmt.Println(a) a := 77 fmt.Println(a) } 为啥? 专家的解答是: for循环每次进入循环体大括号块{}, 都是一个新的scope The reason is each time you enter a block of curly braces {} you're creating a new nested scope. When you declare the variable x at the top of the loop it is a new variable and it goes out of scope at the end of the loop. When the program comes back around to the top of the loop again it's another new scope. 有人给出了证据: func main() { for i := 0; i output 0x1040e0f8 0x1040e0fc 可以手动加{}来添加scope: func main() { a := 77 fmt.Println(&a) { a := 77 fmt.Println(&a) } } output 0x1040e0f8 0x1040e0fc 上面的例子就可以\"连续\"定义a两次, 但第二次是个新的变量地址 怀疑 证据例子中, 变量x的地址改变, 不是因为重新进入{}scope的原因. 比如把下面的\"证据\" func main() { for i := 0; i 结果 0x1040e0f8 0x1040e0fc 改成: func main() { for i := 0; i 注意第4行, 用了unsafe.Pointer取x的地址. 结果: 824634150736 824634150736 为什么结果不一样? 上面的证据显示x的地址改变了, 而下面的代码中x的地址没变. 合理解释 因为有fmt.Println(&x), 变量x逃逸到了堆中, 自然每次进入循环其地址都会改变. 而fmt.Println(uintptr(unsafe.Pointer(&x)))不会逃逸, x还是在栈上, 自然地址不变. 内置new函数也不是一定分配到堆 比如下面的代码, 不管是x := 77, 还是x := new(int), 连续两次的地址都是一样的 for i := 0; i 总结 for的循环变量, 比如i++和循环体里面的变量是两码事: for循环同一行的变量作用域在for里面没错, 但更像是在进入循环前定义的一样: for循环里面对循环变量的引用都是指向同一个东西 for循环体里面用var v int或vc := vc定义的变量, 并非同一个地址, 每次循环都是\"临时\"生成的. 所以上面在第13行的修改可以解决问题. 以后检查go出去的函数是否有这个问题, 只检查循环变量就行了 -- 结论正确, 但前面推导过程不对. 见下面 修正 变量地址是否改变, 要看 如果变量在栈上没有逃逸到堆, 那每次for循环里的变量地址是不变的 如果变量逃逸到堆, 那每次for循环里的变量地址不一样 fmt.Println类的函数会导致变量逃逸(大概率) go 函数造成的闭包引用会导致变量逃逸(必然) channel的send应该也会必然导致变量逃逸 不清楚的情况下, 请默认变量是同一个地址. 这样你可以更小心的避免\"无意中\"改变了一个你认为是独立的变量但实际是共享的, 因为你一开始就应该假定这个变量就是共享的. 错误代码示例: for { // fill bufMsg from network socket var tm streamTransportMsg dec.Decode(bufMsg, &tm) streamChan 注意这里取tm地址做为decode的\"出参\", 会实际改变tm底层的数据; 而chan的发送是异步的效果, 真正处理的routine可能看到的tm.msg已经改变. 再修正 上面的错误代码示例不准确 for { // fill bufMsg from network socket var tm streamTransportMsg dec.Decode(bufMsg, &tm) streamChan 另外一个goroutine从streamChan中得到streamTransportMsg的引用, 但看到的tm.msg会在for循环里改变, 不是因为tm的地址没变, 实际上tm的地址会变, 因为channel的发送会导致逃逸. 那错误的原因是tm.msg地址没变, 这是gotiny的Decode问题, 是另外一个故事. reflect.ValueOf ValueOf流程 // ValueOf returns a new Value initialized to the concrete value // stored in the interface i. ValueOf(nil) returns the zero Value. func ValueOf(i interface{}) Value { if i == nil { return Value{} } // TODO: Maybe allow contents of a Value to live on the stack. // For now we make the contents always escape to the heap. It // makes life easier in a few places (see chanrecv/mapassign // comment below). escapes(i) return unpackEface(i) } 从这个函数传进来的i, 不管之前是什么类型, 到这里都是eface, 即empty interface. 这里的escapes(i)我理解i这个interface变量的结构体是在栈上的, 但其\"contents\"要强制分配到heap中, 这里的contents就是i的指针域指向的实体. // unpackEface converts the empty interface i to a Value. func unpackEface(i interface{}) Value { e := (*emptyInterface)(unsafe.Pointer(&i)) //明确知道i是个emptyInterface // NOTE: don't read e.word until we know whether it is really a pointer or not. t := e.typ if t == nil { return Value{} } f := flag(t.Kind()) if ifaceIndir(t) { f |= flagIndir } return Value{t, e.word, f} } 我们看到: reflect.Value只是unpackEface这个interface, 组成一个Value的结构体, 这中间并没有真正拷贝\"contents\", 而是把\"contents\"做为word返回. 这个word可以是指针, 也可以是值. 这里的Value是如下结构体: // Value is the reflection interface to a Go value. // // Not all methods apply to all kinds of values. Restrictions, // if any, are noted in the documentation for each method. // Use the Kind method to find out the kind of value before // calling kind-specific methods. Calling a method // inappropriate to the kind of type causes a run time panic. // // The zero Value represents no value. // Its IsValid method returns false, its Kind method returns Invalid, // its String method returns \"\", and all other methods panic. // Most functions and methods never return an invalid value. // If one does, its documentation states the conditions explicitly. // // A Value can be used concurrently by multiple goroutines provided that // the underlying Go value can be used concurrently for the equivalent // direct operations. // // To compare two Values, compare the results of the Interface method. // Using == on two Values does not compare the underlying values // they represent. type Value struct { // typ holds the type of the value represented by a Value. typ *rtype // Pointer-valued data or, if flagIndir is set, pointer to data. // Valid when either flagIndir is set or typ.pointers() is true. ptr unsafe.Pointer // flag holds metadata about the value. // The lowest bits are flag bits: // - flagStickyRO: obtained via unexported not embedded field, so read-only // - flagEmbedRO: obtained via unexported embedded field, so read-only // - flagIndir: val holds a pointer to the data // - flagAddr: v.CanAddr is true (implies flagIndir) // - flagMethod: v is a method value. // The next five bits give the Kind of the value. // This repeats typ.Kind() except for method values. // The remaining 23+ bits give a method number for method values. // If flag.kind() != Func, code can assume that flagMethod is unset. // If ifaceIndir(typ), code can assume that flagIndir is set. flag // A method value represents a curried method invocation // like r.Read for some receiver r. The typ+val+flag bits describe // the receiver r, but the flag's Kind bits say Func (methods are // functions), and the top bits of the flag give the method number // in r's type's method table. } 什么是ifaceIndir 在unpackEface中, 调用了函数ifaceIndir(t)来检查是否eface的data域是个指针(一般都是), 但也有时候这个data域直接存的就是值. // unpackEface converts the empty interface i to a Value. func (i interface{}) Value { e := (*emptyInterface)(unsafe.Pointer(&i)) //明确知道i是个emptyInterface // NOTE: don't read e.word until we know whether it is really a pointer or not. t := e.typ if t == nil { return Value{} } f := flag(t.Kind()) if ifaceIndir(t) { f |= flagIndir } return Value{t, e.word, f} //这里的e.word就是eface的data. } // ifaceIndir reports whether t is stored indirectly in an interface value. func ifaceIndir(t *rtype) bool { return t.kind&kindDirectIface == 0 } //rtype的kind字段, 第5位表示data指针实际存的是值. const ( kindDirectIface = 1 综上, eface和iface的data域不一定都是指针, 还可能是值. 这可能是个优化, 但其实挺费劲的. 都是指针多好. Value的Elem()方法 Elem()方法用于取得interface或者ptr的\"contents\" // Elem returns the value that the interface v contains // or that the pointer v points to. // It panics if v's Kind is not Interface or Ptr. // It returns the zero Value if v is nil. func (v Value) Elem() Value { k := v.kind() switch k { case Interface: var eface interface{} if v.typ.NumMethod() == 0 { eface = *(*interface{})(v.ptr) //ptr指向empty interface } else { eface = (interface{})(*(*interface { M() //临时构造一个带一个M方法的interface类型, 相当于eface = someOtherInterfaceWithMethod })(v.ptr)) //ptr指向带方法的interface } x := unpackEface(eface) if x.flag != 0 { x.flag |= v.flag.ro() //这里就是unsettable的来源? } return x case Ptr: ptr := v.ptr //默认是data, 只不过保存在ptr里面. 比如就是int 5 if v.flag&flagIndir != 0 { //指针 ptr = *(*unsafe.Pointer)(ptr) //解引用得到data } // The returned value's address is v's value. if ptr == nil { return Value{} } tt := (*ptrType)(unsafe.Pointer(v.typ)) typ := tt.elem fl := v.flag&flagRO | flagIndir | flagAddr //这里我有点困惑, 为什么要设置flagIndir | flagAddr? fl |= flag(typ.Kind()) return Value{typ, ptr, fl} } panic(&ValueError{\"reflect.Value.Elem\", v.kind()}) } 看起来Elem()也是操做ptr, 没有明显的值拷贝. emptyInterface和nonEmptyInterface emptyInterface比较简单 // emptyInterface is the header for an interface{} value. type emptyInterface struct { typ *rtype word unsafe.Pointer } 而nonEmptyInterface就复杂多了, 包括静态interface类型, concrete类型, 和方法表. 方法表容量有100000个之多, 但我判断这部分其实不占那么多内存的. // nonEmptyInterface is the header for an interface value with methods. type nonEmptyInterface struct { // see ../runtime/iface.go:/Itab itab *struct { ityp *rtype // static interface type typ *rtype // dynamic concrete type hash uint32 // copy of typ.hash _ [4]byte fun [100000]unsafe.Pointer // method table } word unsafe.Pointer } 上面是reflect的定义, 相应的runtime表达, 在src/runtime/runtime2.go中, 有: type iface struct { tab *itab data unsafe.Pointer } // layout of Itab known to compilers // allocated in non-garbage-collected memory // Needs to be in sync with // ../cmd/compile/internal/gc/reflect.go:/^func.dumptabs. type itab struct { inter *interfacetype _type *_type hash uint32 // copy of _type.hash. Used for type switches. _ [4]byte fun [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter. } type eface struct { _type *_type data unsafe.Pointer } func efaceOf(ep *interface{}) *eface { return (*eface)(unsafe.Pointer(ep)) } 基本上差不多. ValueOf实例 把一个指向interface类型的指针, 解引用后做ValueOf操做: // p是unsafe.Pointer, 已知指向reflect.Interface类型, rt是这个类型的TypeOf()后的reflect.Type if rt.NumMethod() == 0 { // 没有方法是eface ti := *(*interface{})(p) v := reflect.ValueOf(ti) } 此时p指向eface, 见上面eface定义; 在这个例子中, 这个interface的content是个gotiny.baseTyp结构体: // p是unsafe.Pointer, 已知指向reflect.Interface类型, rt是这个类型的TypeOf()后的reflect.Type if rt.NumMethod() > 0 { // 有方法是iface ti := *(*interface { M() })(p) v := reflect.ValueOf(ti) et := v.Type() } 此时p指向iface, 见上面iface定义; 在这个例子中, 这个interface是: // tint是int, 实现了io.ReadWriteCloser type tint int func (tint) Read([]byte) (int, error) { return 0, nil } func (tint) Write([]byte) (int, error) { return 0, nil } func (tint) Close() error { return nil } v1interface io.ReadWriteCloser = tint(2) 这里我们看到, ti在dlv看来, 其data是2. 我们知道一个interface的\"data\"域是个指针, 但这里的2刚好就是v1interface的值, 那么这个data已经不是指针而是值了, 是否是因为dlv\"自动\"解引用了呢? 在ValueOf(ti)的里面的unpackEface()中, ti被\"值拷贝\"(interface的值拷贝)到i: // unpackEface converts the empty interface i to a Value. func unpackEface(i interface{}) Value { e := (*emptyInterface)(unsafe.Pointer(&i)) // NOTE: don't read e.word until we know whether it is really a pointer or not. t := e.typ if t == nil { return Value{} } f := flag(t.Kind()) if ifaceIndir(t) { f |= flagIndir } return Value{t, e.word, f} } 因为i在内存中是emptyInterface类型, 强转成e, 我们能借助dlv看到e的typ和word:注意到t的Kind是2(也就是int), 而word是个指针. 这个e就是i, 也就是ti. 所以我们看到v := reflect.ValueOf(ti)执行后, v就是ti的实际内存表达:但从此丢失了ti的method信息??? 结论 当类型的Kind是Interface的时候, 如果只有指向这个变量的指针p, 那么要区分p指向的到底是eface还是iface, 不能混用, 否则会panic. 所以要这样: // p是unsafe.Pointer, 已知指向reflect.Interface类型, rt是这个类型的TypeOf()后的reflect.Type if rt.NumMethod() > 0 { // 有方法是iface ti := *(*interface { M() })(p) v := reflect.ValueOf(ti) et := v.Type() } else { // 没有方法是eface ti := *(*interface{})(p) v := reflect.ValueOf(ti) et := v.Type() } 强制escape /usr/local/go/src/reflect/value.go 定义一个全局变量: var dummy struct { b bool x interface{} } 如果需要强制escape一个变量, 只需要赋值给dummy的x. 因为一个全局变量持有x的引用, 那x必须在heap里面. // Dummy annotation marking that the value x escapes, // for use in cases where the reflect code is so clever that // the compiler cannot follow. func escapes(x interface{}) { if dummy.b { dummy.x = x } } 问答 为什么看到value的kind是54? 如图?答: 这里的kind不是反射那个Kind, 或者说不完全是. 这里的kind是rtype类型的一个field. 真正的kind是这个field在与上kindMask, 相当于t.kind & 31 func (t *rtype) Kind() Kind { return Kind(t.kind & kindMask) } 所以54&31后, 是22. 22对应的Kind是reflect.Ptr interface赋值给interface 直接赋值: interface只有一层 我们知道interface的内部第二个field是个指针 type eface struct { // 16 bytes _type *_type data unsafe.Pointer } type iface struct { // 16 bytes tab *itab data unsafe.Pointer } 那么如果我把一个interface变量(t1i)赋值给另一个interface变量(t2i), 那么t2i的data是指向t1i的拷贝的吗? 比如 t := 9 var t1i interface{} t1i = t var t2i interface{} t2i = t1i 答: 不是. 首先, interface的赋值也有值拷贝, 前面说过的: 再理解一下 The second word in the interface value points at the actual data, in this case a copy of b. The assignment var s Stringer = b makes a copy of b rather than point at b for the same reason that var c uint64 = b makes a copy: if b later changes, s and c are supposed to have the original value, not the new one. 其次, 这里并不是把t2i这个interface的结构即eface结构拷贝一份, 并用t1i的data域来指向. 而是t2i发现赋值对象也是个interface, 就直接查其concrete类型再赋值. 所以我说interface变量只有一级, 不存在interface里面再包一层interface. 而一定是interface下面就是concrete类型. 注意我说的是运行时, 不是定义时. 定义时可以嵌套interface. 证明: fmt.Println(reflect.TypeOf(t1i).String())结果是int TypeOf(t1i)这步就有t1i赋值给入参的过程, 这个就是interface赋值给interface. 如果允许运行时嵌套interface, 那多层函数传递interface就会嵌套好多层. 用户不会知道里面有多少层interface的. 取地址赋值: interface包interface地址 如果把上面的代码改成 ti := int64(9) var tinterface interface{} tinterface = ti fmt.Println(reflect.TypeOf(tinterface).String()) var t2interface interface{} t2interface = &tinterface ti = 10 fmt.Println(t2interface) 那么t2interface的具体内存表达是什么样的?答: t2interface的data指针是*interface{}类型, 应该就是指向tinterface 结论 ti := int64(9) var tinterface interface{} tinterface = ti fmt.Println(reflect.TypeOf(tinterface).String()) var t2interface interface{} t2interface = tinterface fmt.Println(reflect.TypeOf(t2interface).String()) var t3interface interface{} t3interface = &tinterface fmt.Println(reflect.TypeOf(t3interface).String()) var t4interface interface{} t4interface = &t3interface fmt.Println(reflect.TypeOf(t4interface).String()) 这段代码打印: int64 int64 *interface {} *interface {} 特别的, t4interface有3层嵌套, 包括&t3interface一层, &tinterface一层, 最后的int64(9)一层. 所以: interface i1值赋值给interface i2, 其concrete类型会传递(或者说短接)到\"第一层\".(t2interface的行为) interface i1值赋值给interface i2, 其i1的concrete的值会拷贝给i2. interface取地址赋值给interface, 并非传递, 而是嵌套.(t4interface的行为) slice能当作\"出参\"传递 比如io.Reader type Reader interface { Read(p []byte) (n int, err error) } 这里说的很清楚, Read reads up to len(p) bytes into p. 注意这里 p做为出参, Read函数内对p的修改是能够被调用者看到的. 但注意up to len(p), 因为p是调用者传入slice的\"浅拷贝\", 大小是不能改变的, append()函数等改变len()的不会体现到调用者看到的\"p\"中. 结论 在slice p被当作参数传递的过程中, 发生了slice的\"浅拷贝\", 浅拷贝共享底层数组, 所以对底层数组的修改能够被调用者看到, 其作用类似\"出参\". 但\"浅拷贝\"对slice本身的改变, 比如改变len, 原slice是看不到的. a := []int{1,2,3} a1 := a a1[2]=100 a1 = append(a1, 4) fmt.Println(a1) fmt.Println(a) //输出 [1 2 100 4] [1 2 100] slice和gc 对一个slice进行切片不会导致底层array被gc. 具体见https://stackoverflow.com/questions/28432658/does-go-garbage-collect-parts-of-slices As mentioned earlier, re-slicing a slice doesn't make a copy of the underlying array. The full array will be kept in memory until it is no longer referenced. Occasionally this can cause the program to hold all the data in memory when only a small piece of it is needed. Since the slice references the original array, as long as the slice is kept around the garbage collector can't release the array. 具体例子 https://yourbasic.org/golang/clear-slice/ Remove all elements To remove all elements, simply set the slice to nil. a := []string{\"A\", \"B\", \"C\", \"D\", \"E\"} a = nil fmt.Println(a, len(a), cap(a)) // [] 0 0 This will release the underlying array to the garbage collector (assuming there are no other references). Keep allocated memory To keep the underlying array, slice the slice to zero length. a := []string{\"A\", \"B\", \"C\", \"D\", \"E\"} a = a[:0] fmt.Println(a, len(a), cap(a)) // [] 0 5 If the slice is extended again, the original data reappears. fmt.Println(a[:2]) // [A B] 结论 对slice进行切片不会导致gc 即a = a[:0]不会把底层的array gc掉. float32和data race 在pidinfo.go中, 我使用了float32类型的变量userHz var userHz float32 = 100 我当时认为一个CPU对齐的32bit变量, 它的load和store操作是原子的. -- 好像理论上是的. 但go test -race还是认为这里有问题: 即同时读写这个变量被认为是数据竞争: WARNING: DATA RACE Read at 0x00000071c34c by goroutine 8: gitlabe1.ext.net.nokia.com/godevsig/system/pidinfo.(*TidInfo).CPUpercent() /builds/godevsig/system/pidinfo/pidinfo.go:480 +0x269 ... Previous write at 0x00000071c34c by goroutine 10: gitlabe1.ext.net.nokia.com/godevsig/system/pidinfo.hzUpdater() /builds/godevsig/system/pidinfo/pidinfo.go:331 +0x22c 使用sync/atomic // AtomicLoadFloat64 loads float64 atomically func AtomicLoadFloat64(addr *float64) float64 { return math.Float64frombits(atomic.LoadUint64((*uint64)(unsafe.Pointer(addr)))) } // AtomicStoreFloat64 stores float64 atomically func AtomicStoreFloat64(addr *float64, val float64) { atomic.StoreUint64((*uint64)(unsafe.Pointer(addr)), math.Float64bits(val)) } 再说reflect 什么是unaddressable 不能改变a package main import ( \"fmt\" \"reflect\" ) func main() { a := 55 fmt.Println(a) rv := reflect.ValueOf(a) fmt.Println(rv) rv.SetInt(66) fmt.Println(a) } 上面的代码输出: 代码13行, 说值不能被寻址 55 55 panic: reflect: reflect.Value.SetInt using unaddressable value goroutine 1 [running]: reflect.flag.mustBeAssignableSlow(0x82) /usr/local/go-faketime/src/reflect/value.go:260 +0x138 reflect.flag.mustBeAssignable(...) /usr/local/go-faketime/src/reflect/value.go:247 reflect.Value.SetInt(0x4a4220, 0x54ab98, 0x82, 0x42) /usr/local/go-faketime/src/reflect/value.go:1633 +0x3b main.main() /tmp/sandbox505010953/prog.go:13 +0x1d9 能改变a 但下面的代码就能够修改变量a的值: package main import ( \"fmt\" \"reflect\" ) func main() { a := 55 fmt.Println(a) rptr := reflect.ValueOf(&a) rv := rptr.Elem() fmt.Println(rv) rv.SetInt(66) fmt.Println(a) } 上面代码输出: 55 55 66 关键在于第11和12行. rptr是&a的值, 也就是a的地址; 而rv是rptr的解引用, 也即rv就是a. 对rv的值的改变, 就是对a的改变. 为什么? 在下面代码中, rv := reflect.ValueOf(a)实际上是得到a的副本的值 而如果rv.SetInt(66)能够成立的话, 也只能是set这个副本的值, 且这个修改也不会反应到a上. func main() { a := 55 fmt.Println(a) rv := reflect.ValueOf(a) fmt.Println(rv) rv.SetInt(66) fmt.Println(a) } 那为什么这样可以? func main() { a := 55 fmt.Println(a) rptr := reflect.ValueOf(&a) rv := rptr.Elem() fmt.Println(rv) rv.SetInt(66) fmt.Println(a) } rptr := reflect.ValueOf(&a)也是得到&a的副本的值. 但&a是个指针, 它的解引用rptr.Elem()就是a, 而不是a的副本. 所以可以修改a.副本发生在指针是没问题的. 更进一步解释 rv.SetInt(66)的源码如下: // SetInt sets v's underlying value to x. // It panics if v's Kind is not Int, Int8, Int16, Int32, or Int64, or if CanSet() is false. func (v Value) SetInt(x int64) { v.mustBeAssignable() switch k := v.kind(); k { default: panic(&ValueError{\"reflect.Value.SetInt\", v.kind()}) case Int: *(*int)(v.ptr) = int(x) case Int8: *(*int8)(v.ptr) = int8(x) case Int16: *(*int16)(v.ptr) = int16(x) case Int32: *(*int32)(v.ptr) = int32(x) case Int64: *(*int64)(v.ptr) = x } } 可以看到, 只有v有ptr才能赋值. 结论 其实很简单, 通过反射赋值, 实际上就是两个过程: 先取地址, 再赋值. ptr = &a *ptr = x 例子 var v interface{} a := 55 v = &a rptr := reflect.ValueOf(v) //unaddressable rptr.Set(reflect.New(rptr.Type().Elem())) //可以赋值, 相当于v = &b reflect.ValueOf(&v).Elem().Set(reflect.New(rptr.Type().Elem())) 原理 https://blog.golang.org/laws-of-reflection 结论 每个类型都对应一个_type结构, 描述了该类型的属性和方法 interface也是类型(也是type声明的), 用interfacetype来描述, 后者内部也包括了_type结构 空interface也是类型, 但没有方法 带方法的interface规定了方法集, 也保存在其_type中 interface的表达可以大概认为是(value, type)对, 更具体的说, 是个16字节的结构, 包括一个指针和实际的(concrete)类型// 没方法的interface type eface struct { _type *_type data unsafe.Pointer } // 有方法的interface type iface struct { tab *itab data unsafe.Pointer } interface变量的静态类型是代码中声明的类型, 这个类型会伴随这个interface变量一生, 不会改变 静态类型在编译阶段用来检查是否赋值成立 -- 即对方是否实现了我规定的方法集 静态类型规定了这个interface变量可以直接调用的方法. concrete类型(有时也称动态类型)是给interface变量赋值的时候, 实际的对象类型 interface变量的concrete类型会随着再次赋值而改变 类型断言的意义在于断言这个concrete类型是否满足断言 -- 用类型断言能够突破静态类型的限制, 调用concrete类型的其他方法 有方法的interface的itable是动态计算的 -- runtime通过匹配该interface类型的方法集和concrete对象类型的方法集, 来生成itable. -- 每个interface变量都有个动态生成的itable. 这个和编译时检查能否赋值不同 -- 但我没想明白, 似乎在编译阶段就能确定下来. 相关的说法是: 就是说可以在编译时搞, 但没必要. Go's dynamic type conversions mean that it isn't reasonable for the compiler or linker to precompute all possible itables: there are too many (interface type, concrete type) pairs, and most won't be needed 反射本质上是一种检测interface变量底层的(value, type)对的方法 -- 这里指concrete类型 reflect.Value类型其实就是这个interface变量的内部表达, 它本身既包含了\"值\", 也包含了类型. 所以reflect.Value有Type()方法得到其concrete类型 所以Value类型的Interface()方法能够再次\"组装(pack)\"一个interface变量, 返回一个空的interface{}类型 reflect.TypeOf()方法其实是个shortcut, 和先ValueOf()再Type()效果一样. reflect.Type类型是go内部的_type的表达 能否对Value类型进行Set()操作, 取决于是否这个Value对象是否是另一个对象的引用type Struct1 struct { A int64 } p := Struct1{} V := reflect.ValueOf(&p).Elem() V.FieldByName(\"A\").SetInt(100) 上面代码能工作, 因为V是对p的引用, 就能修改p的内容. 理解起来, 和f(x)不能改变x, 但f(&x)能改变x是一个道理 再议interface 主要是想考察一下, 我曾经用过的map接口 type intMap struct { ks []int // in insertion order mp map[int]interface{} } 这里面的key是int, value是interface{} 这个interface{}的使用会不会有性能问题? 这里的使用是指: set, 对value赋值, 和普通的map相比, 这里多了对interface{}赋值 get, 获取value func (im *intMap) set(k int, v interface{}) { _, has := im.mp[k] if !has { im.ks = append(im.ks, k) } im.mp[k] = v } func (im *intMap) get(k int) (interface{}, bool) { v, has := im.mp[k] return v, has } interface{}回顾 上图的Binary是uint64, 有两个方法 type Binary uint64 func (i Binary) String() string { return strconv.Uitob64(i.Get(), 2) } func (i Binary) Get() uint64 { return uint64(i) } s是个Stringer的interface{}, 是带方法的. 但它的itable表只有String一个方法. Binary的Get方法不是Stringer的关注点, 不在Stringer的itable里面 itable是运行时动态计算的. 虽然在编译的时候, 编译器是可以知道这些信息的:S := Stringer(b)就包含了所有的关键点, 但在编译阶段就写好itable太粗暴了: interface{}和底层concrete类型的配对可以有非常多种, 很多在运行时可能都不真正需要. 动态itable基于 编译器给每个concrete类型都生成了类型描述, 包括它的方法(函数指针形式)列表 -- 实现表 编译器给每个interface类型也生成类型描述, 它也有方法列表. -- 需求表 运行时按照需求表来查实现表, 完成itable的构建 构建好的itable表会被cache, 同一个interface{}和concrete只会构建一次. set get性能损失如何? type intMap struct { ks []int // in insertion order mp map[int]interface{} } 这里的value是个empty的interface{}, go里面有专门的eface来表达: type eface struct { // 16 bytes _type *_type data unsafe.Pointer } type iface struct { // 16 bytes tab *itab data unsafe.Pointer } 那么对空interface赋值, 除了data域的值拷贝, 还有个_type *_type指针的赋值, 这里应该就是指向concrete类型的类型描述. 这样, 赋值完成后, 这个interface变量, 就有所有concrete变量的所有信息. 看起来多出来的_type *_type指针赋值, 并没有多少性能损耗. 在get的时候, 直接获取到interface, 通常需要类型断言才能被业务逻辑使用: 比如 //childpi是个空的interface{}类型 childpi, _ := pi.children.get(pid) //断言成*PidInfo才能使用 children = append(children, childpi.(*PidInfo)) 我猜从原理上, 这个类型断言就是看_type *_type是不是*PidInfo 看起来性能也没有多少损失 结论 我目前倾向没有多少性能损失的结论 标准库的time.Now()如何取得系统时间? time.Now()的实现流程 实际的系统时间是汇编代码, 比如mips64是在src/runtime/sys_linux_mips64x.s // func walltime() (sec int64, nsec int32) TEXT runtime·walltime(SB),NOSPLIT,$16 MOVW $0, R4 // CLOCK_REALTIME MOVV $0(R29), R5 MOVV $SYS_clock_gettime, R2 SYSCALL MOVV 0(R29), R3 // sec MOVV 8(R29), R5 // nsec MOVV R3, sec+0(FP) MOVW R5, nsec+8(FP) RET TEXT runtime·nanotime(SB),NOSPLIT,$16 MOVW $1, R4 // CLOCK_MONOTONIC MOVV $0(R29), R5 MOVV $SYS_clock_gettime, R2 SYSCALL MOVV 0(R29), R3 // sec MOVV 8(R29), R5 // nsec // sec is in R3, nsec in R5 // return nsec in R3 MOVV $1000000000, R4 MULVU R4, R3 MOVV LO, R3 ADDVU R5, R3 MOVV R3, ret+0(FP) RET 这里的walltime和nanotime会被time_now()调用. 在src/runtime/timestub.go中 time_now()是time.now的linkname. 即实际上time.now()就是runtime.time_now() package runtime import _ \"unsafe\" // for go:linkname //go:linkname time_now time.now func time_now() (sec int64, nsec int32, mono int64) { sec, nsec = walltime() return sec, nsec, nanotime() } 这里的问题是, 每次获取系统时间, 都有2次系统调用: 第一次是clock_gettime获取CLOCK_REALTIME 第二次是clock_gettime获取CLOCK_MONOTONIC 同时, 我们也看到, 虽然调用了系统调用, 但这个调用路径上没有埋伏runtime的调度等函数. 最后, 标准库time包的Now()调用了now() // Now returns the current local time. func Now() Time { sec, nsec, mono := now() mono -= startNano sec += unixToInternal - minWall if uint64(sec)>>33 != 0 { return Time{uint64(nsec), sec + minWall, Local} } return Time{hasMonotonic | uint64(sec) clock_gettime系统调用 man clock_gettime中说: #include int clock_gettime(clockid_t clk_id, struct timespec *tp); 这里的clk_id可以是从Epoch(1970年?)算起的绝对时间, 这个时间对所有进程都一样. 也可以是按进程角度看起来的时间 CLOCK_REALTIME: 系统时间, 墙上时间. wall clock CLOCK_REALTIME_COARSE: 系统时间, 精度稍差, 但快速的版本 CLOCK_MONOTONIC: 从开机算起的时间, 不能更改 CLOCK_MONOTONIC_COARSE: 精度稍差但快的版本 CLOCK_MONOTONIC_RAW: 硬件返回的时间, 不受NTP影响 CLOCK_PROCESS_CPUTIME_ID: 按进程算的时间 CLOCK_THREAD_CPUTIME_ID: 按线程算的时间 逃逸分析和变量分配 go的程序在编译的时候, 通过逃逸分析来确定变量是分配在栈上, 还是分配到堆上. 一个变量分配在哪里是编译时决定的 如果编译器通过分析得知, 一个变量可能脱离其声明时所在的函数作用域, 就会把这个变量分配到堆上. 否则, 编译器知道这个变量的所有引用都在此函数的生命周期内, 那这个变量就可以被安全的分配到栈上. 在堆上分配的开销相对很大, 编译器会插入CALL runtime.newobject(SB)的汇编代码来实现堆分配. 而栈分配就是简单的通过栈指针SP+偏移的引用. 一个典型的堆分配如图: 下面我们来简单了解一下编译器如何判断一个变量是否可能逃逸 逃逸分析使用 go build, go run, go test都支持-gcflags '-m -l'选项, 打开逃逸分析的输出. -m: 最多4个-m连用, 打开丰富的编译过程的逃逸分析记录 -l: 禁止inline, 让-m的信息更容易阅读 逃逸分析实例 比如下面的代码 package main import ( \"fmt\" \"unsafe\" ) var gr *int func change(r *int) { *r = *r + 1 //gr = r } func sum(a, b int) int { s := a + b change(&s) fmt.Println(s) //fmt.Println(&s) addr := uintptr(unsafe.Pointer(&s)) fmt.Printf(\"0x%x %v\\n\", addr, *(*int)(unsafe.Pointer(addr))) return s } func main() { a, b := 1, 2 c := sum(a, b) fmt.Println(c) } 使用逃逸分析结果如下, $ go run -gcflags '-m -l' hello.go # command-line-arguments ./hello.go:10:13: change r does not escape ./hello.go:18:13: sum ... argument does not escape ./hello.go:18:13: s escapes to heap ./hello.go:21:12: sum ... argument does not escape ./hello.go:21:13: addr escapes to heap ./hello.go:21:32: *(*int)(unsafe.Pointer(addr)) escapes to heap ./hello.go:28:13: main ... argument does not escape ./hello.go:28:13: c escapes to heap 4 0xc000096eb8 4 4 解释: 先看简单点的main函数, a和b两个int变量, 传给sum得到int c, 然后打印c. 首先, go里面都是值传递, main的a和b, 在传给sum的时候, 值已经分别被拷贝进sum的参数, 所以a和b不可能逃逸. c拷贝了sum函数的返回值, 在传递给Println的时候, 又发生了值拷贝, c只是int, 不可能逃逸. 但./hello.go:28:13: c escapes to heap是说c逃逸到堆了吗? 其实不是, 因为Println()的入参是interface{}, 而interface{}是由类型域和指针域组成的, 它的指针域指向底层的数据. 这里的意思是说, c的值被拷贝进一个堆的int变量(应该还是栈上), 被Println的入参interface变量的指针域指向. 所以并不是变量c本身逃逸到堆. 注: 通过反汇编发现, c的值拷贝也不是分配到堆上的. 如果改成fmt.Println(&c), 则c会逃逸. 因为Pringln持有了c的引用, 而没有什么办法能阻止一个函数再次\"传递\"这个引用到channel或者一个全局变量, 从而c的引用会被更广泛的持有. 所以编译器认为c会逃逸, 要分配到堆里, 由运行时GC来负责变量c的释放. 真正的逃逸会在变量声明的那行, 打印moved to heap: 变量名 比如, 如果第19行没有被注释, 则变量s会逃逸到堆. 逃逸分析会打印: ./hello.go:16:2: moved to heap: s 表示s真正的逃逸到堆了. 一般的, 取地址后赋值给interface{}, 则会更可能被编译器判定为逃逸. 注意这里说的是可能, 不是绝对. 有些情况下, 取地址赋值给interface{}不会导致逃逸. 比如下面代码片段: 测试版本go1.13 func changeInterface(r interface{}) { v := r.(*int) *v = *v + 1 } a, b := 1, 2 s := a + b changeInterface(&s) //同样是interface{}赋值, 这句不会导致s逃逸 fmt.Println(s) //值拷贝, 不会导致s逃逸 fmt.Println(&s) //fmt.Print函数族+取变量地址会导致变量逃逸到堆. 个人认为这个设定不是很合理. 编译器应该确切知道fmt.Println()有没有再\"散发\" `&s` 不是所有取地址都会逃逸. 比如sum里面调用了change(&s), 传递的是s的引用; 那s是有可能逃逸的, 但编译器发现change函数, 在没有赋值给全局变量gr的情况下(注释掉12行), 并没有实际上让s继续逃逸. 所以上面的代码, 逃逸分析得出, s还是分配到栈里. 同样是打印地址, addr := uintptr(unsafe.Pointer(&s))然后打印addr不会让s逃逸; 而fmt.Println(&s)则会让s逃逸. uintptr和unsafe.Pointer()的互相强转组合能阻断这种\"引用扩散\", 这可能是unsafe包名字的由来: 其引用的地址由于没有被记录在案, 可能被gc回收掉而不知道. 判断一个变量是否真正被编译器判定为逃逸, 看变量声明的那行是否有moved to heap: 变量名, 注意, 变量名 escapes to heap发生在使用改变量那一行, 个人认为不是说这个变量逃逸了. 还有一个办法来确认是否逃逸: 用go tool compile -S -m -l查看汇编. 比如本例中, 考察sum函数中的变量s, 只有编译器判定s会逃逸并打印moved to heap: s, 其汇编代码里才有CALL runtime.newobject(SB)表示真的调用运行时函数来给改变量分配内存空间. 而平常的s escapes to heap在调用fmt.Print族函数的时候都会出现, 个人理解并不是变量已经逃逸的意思, 也不是变量的拷贝被放到堆中. 下面的截图是本例代码的反汇编go tool objdump -S hello > hello.objdump在调用CALL fmt.Println(SB)之前, sum函数的所有操作的变量看起来都是基于SP的, 都是栈变量. 看起来传递给fmt.Println()的拷贝也并没有分配到堆上. 注: unsafe.Pointer有如下性质: unsafe.Pointer和任意的指针类型能互相转换 unsafe.Pointer和uintptr能互相转换 指针和uintptr不能直接互转 uintptr用于做\"指针\"计算 查看汇编 go tool compile -S -m -l hello.go 反汇编 go tool objdump -S hello > hello.objdump go的值和指针 代码: func Show(i interface{}) { if i == nil { fmt.Printf(\"type: %T, value: %#v\\n\", i, i) return } t := reflect.TypeOf(i) if t.Kind() != reflect.Ptr { fmt.Printf(\"type: %T, size: %d; value: %#v\\n\", i, t.Size(), i) } else { v := reflect.ValueOf(i) fmt.Printf(\"type: %T, size: %d; value: %#v, value size: %d\\n\", i, t.Size(), v.Elem(), t.Elem().Size()) } } 结论: 所有的指针都占8个字节 x86_64 type: int, size: 8; value: 99 type: *int, size: 8; value: 99, value size: 8 type: **int, size: 8; value: (*int)(0xc0000e01d0), value size: 8 type: *int, size: 8; value: 0, value size: 8 type: **int, size: 8; value: (*int)(0xc0000e0228), value size: 8 type: string, size: 16; value: \"hello world\" type: *string, size: 8; value: \"hello world\", value size: 16 type: , value: type: *os.File, size: 8; value: os.File{file:(*os.file)(0xc0000cc060)}, value size: 8 type: **os.File, size: 8; value: &os.File{file:(*os.file)(0xc0000cc060)}, value size: 8 补充: 逃逸分析命令 go build -gcflags '-m -l' go test -gcflags '-m -l' map golang的map底层是hash表实现的. 初始化 hashmap结构体 // A header for a Go map. type hmap struct { // Note: the format of the hmap is also encoded in cmd/compile/internal/gc/reflect.go. // Make sure this stays in sync with the compiler's definition. count int // # live cells == size of map. Must be first (used by len() builtin) flags uint8 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details hash0 uint32 // hash seed buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing nevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated) extra *mapextra // optional fields } buckets是底层数组的指针, 用unsafe.Pointer来声明的 make map变量的时候 在make(map[k]v, hint)的时候 调用runtime/map.go // makemap implements Go map creation for make(map[k]v, hint). // If the compiler has determined that the map or the first bucket // can be created on the stack, h and/or bucket may be non-nil. // If h != nil, the map can be created directly in h. // If h.buckets != nil, bucket pointed to can be used as the first bucket. func makemap(t *maptype, hint int, h *hmap) *hmap { mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem > maxAlloc { hint = 0 } // initialize Hmap if h == nil { h = new(hmap) } h.hash0 = fastrand() // Find the size parameter B which will hold the requested # of elements. // For hint 这个hash表的底层承载是数组(bukets), 最大容量是2^B即, 而B是uint8, 故数组元素最大2^256个, 非常大 每个数组元素叫buket, 能装载8个元素; 相同key的buket用链表链接(拉链式解决冲突) 如果make不指定capacity, 初始化hash表的时候默认使用B=0, 即空的bukets数组; 当后面第一次加数据的时候会扩容. -- Lazy模式 扩容时, 容量是原来的2倍. 因为是在runtime包里的, 这些都是运行时的行为. 遍历 用迭代器来遍历map, 用mapiterinit(t *maptype, h *hmap, it *hiter)来初始化一个迭代器, 编译器生成代码的时候会插入这个调用 / A hash iteration structure. // If you modify hiter, also change cmd/compile/internal/gc/reflect.go to indicate // the layout of this structure. type hiter struct { key unsafe.Pointer // Must be in first position. Write nil to indicate iteration end (see cmd/internal/gc/range.go). elem unsafe.Pointer // Must be in second position (see cmd/internal/gc/range.go). t *maptype h *hmap buckets unsafe.Pointer // bucket ptr at hash_iter initialization time bptr *bmap // current bucket overflow *[]*bmap // keeps overflow buckets of hmap.buckets alive oldoverflow *[]*bmap // keeps overflow buckets of hmap.oldbuckets alive startBucket uintptr // bucket iteration started at offset uint8 // intra-bucket offset to start from during iteration (should be big enough to hold bucketCnt-1) wrapped bool // already wrapped around from end of bucket array to beginning B uint8 i uint8 bucket uintptr checkBucket uintptr } func mapiterinit(t *maptype, h *hmap, it *hiter) { it.t = t it.h = h // grab snapshot of bucket state it.B = h.B it.buckets = h.buckets //起始点随机, 这是为什么range map出来的结果顺序不确定. r := uintptr(fastrand()) if h.B > 31-bucketCntBits { r += uintptr(fastrand()) > h.B & (bucketCnt - 1)) //it.bucket是当前的bucket指针, 指向底层buckets数组的元素, 即key对应的table中的index的元素 it.bucket = it.startBucket mapiternext(it) } mapiternext()是遍历map的执行主体, 编译器会在range语句里面反复调用它, 来得到下一个key, value对 func mapiternext(it *hiter) { h := it.h //不能同时写, 否则直接panic if h.flags&hashWriting != 0 { throw(\"concurrent map iteration and map write\") } //it.t.key里面包括了hash算法 t := it.t alg := t.key.alg //bucket是当前的bucket指针 bucket := it.bucket //b是当前的bucket指向的bmap, bmap是8个元素的结构 b := it.bptr //i是拉链8个元素的编号 i := it.i next: 如果当前bucket为空 如果又回到起始的bucket, 说明遍历结束了 it.key = nil it.elem = nil return //根据是否map在增长中, 来算下个pmap拉链元素的地址, 类似这样 b = (*bmap)(add(it.buckets, bucket*uintptr(t.bucketsize))) //指针++, 在it.buckets数组里往下移动一个单元 bucket++ //下一个bucket了, i为0 i = 0 //看当前的bucket链表, 一个bucket最多有8个元素 for ; i hash算法 hash算法是key的类型决定的, 详见:src/runtime/alg.go // typeAlg is also copied/used in reflect/type.go. // keep them in sync. type typeAlg struct { // function for hashing objects of this type // (ptr to object, seed) -> hash hash func(unsafe.Pointer, uintptr) uintptr // function for comparing objects of this type // (ptr to object A, ptr to object B) -> ==? equal func(unsafe.Pointer, unsafe.Pointer) bool } 常用的hash算法 实现hash算法的文件在src/runtime/hash64.go和src/runtime/hash32.go 注: 这两个文件有编译限制, 针对CPU类型的. var algarray = [alg_max]typeAlg{ alg_NOEQ: {nil, nil}, alg_MEM0: {memhash0, memequal0}, alg_MEM8: {memhash8, memequal8}, alg_MEM16: {memhash16, memequal16}, alg_MEM32: {memhash32, memequal32}, alg_MEM64: {memhash64, memequal64}, alg_MEM128: {memhash128, memequal128}, alg_STRING: {strhash, strequal}, //这个是interface的hash方法, 内部调用这个interface的hash方法 alg_INTER: {interhash, interequal}, alg_NILINTER: {nilinterhash, nilinterequal}, alg_FLOAT32: {f32hash, f32equal}, alg_FLOAT64: {f64hash, f64equal}, alg_CPLX64: {c64hash, c64equal}, alg_CPLX128: {c128hash, c128equal}, } //内存hash最普遍, 根据指针p的内容和一些素数常量来做操作 const ( // Constants for multiplication: four random odd 64-bit numbers. m1 = 16877499708836156737 m2 = 2820277070424839065 m3 = 9497967016996688599 m4 = 15839092249703872147 ) func memhash64(p unsafe.Pointer, seed uintptr) uintptr { h := uint64(seed + 8*hashkey[0]) h ^= uint64(readUnaligned32(p)) | uint64(readUnaligned32(add(p, 4)))> 29 h *= m3 h ^= h >> 32 return uintptr(h) } func strhash(a unsafe.Pointer, h uintptr) uintptr { x := (*stringStruct)(a) return memhash(x.str, h, uintptr(x.len)) } 有的架构硬件支持aeshash, 就使用这些硬件算法 func memhash(p unsafe.Pointer, seed, s uintptr) uintptr { if (GOARCH == \"amd64\" || GOARCH == \"arm64\") && GOOS != \"nacl\" && useAeshash { return aeshash(p, seed, s) } ... } 结论 golang的map实现是hash表, 拉链式处理冲突 golang在编译时的make(map[k]v, hint), 会被编译器当作makemap()的函数调用插入到目标代码里, 在runtime真正创建map时调用. 对map的range遍历, 会被编译器当作迭代器的函数调用, 供在runtime时调用 对map的遍历, 没有magic, 还是老老实实的对底层数组从头到尾遍历 根据key的类型不同, 用不同的hash函数, 最后一般会调到memhash(), 做内存hash 如果make时不指定capacity, 默认创建底层数组为0的map. 底层数组会在以后put元素时才创建, 而且刚开始不大; 要put的元素越来越多时, 这个底层数组会以2倍的速率随之扩容, 老的元素会被一个个的拷贝到新的2倍大小的数组里. hash函数虽然不变, 但hash出来算index的算法会根据底层数组大小改变. // A map is just a hash table. The data is arranged // into an array of buckets. Each bucket contains up to // 8 key/elem pairs. The low-order bits of the hash are // used to select a bucket. Each bucket contains a few // high-order bits of each hash to distinguish the entries // within a single bucket. // // If more than 8 keys hash to a bucket, we chain on // extra buckets. // // When the hashtable grows, we allocate a new array // of buckets twice as big. Buckets are incrementally // copied from the old bucket array to the new bucket array. // // Map iterators walk through the array of buckets and // return the keys in walk order (bucket #, then overflow // chain order, then bucket index). To maintain iteration // semantics, we never move keys within their bucket (if // we did, keys might be returned 0 or 2 times). When // growing the table, iterators remain iterating through the // old table and must check the new table if the bucket // they are iterating through has been moved (\"evacuated\") // to the new table. Gc的演进 v1.0 — 完全串行的标记和清除过程，需要暂停整个程序； v1.1 — 在多核主机并行执行垃圾收集的标记和清除阶段11； v1.3 — 运行时基于只有指针类型的值包含指针的假设增加了对栈内存的精确扫描支持，实现了真正精确的垃圾收集12； 将 unsafe.Pointer 类型转换成整数类型的值认定为不合法的，可能会造成悬挂指针等严重问题； v1.5 — 实现了基于三色标记清扫的并发垃圾收集器13； 大幅度降低垃圾收集的延迟从几百 ms 降低至 10ms 以下； 计算垃圾收集启动的合适时间并通过并发加速垃圾收集的过程； v1.6 — 实现了去中心化的垃圾收集协调器； 基于显式的状态机使得任意 Goroutine 都能触发垃圾收集的状态迁移； 使用密集的位图替代空闲链表表示的堆内存，降低清除阶段的 CPU 占用14； v1.7 — 通过并行栈收缩将垃圾收集的时间缩短至 2ms 以内15； v1.8 — 使用混合写屏障将垃圾收集的时间缩短至 0.5ms 以内16； v1.9 — 彻底移除暂停程序的重新扫描栈的过程17； v1.10 — 更新了垃圾收集调频器（Pacer）的实现，分离软硬堆大小的目标18； v1.12 — 使用新的标记终止算法简化垃圾收集器的几个阶段19； v1.13 — 通过新的 Scavenger 解决瞬时内存占用过高的应用程序向操作系统归还内存的问题20； v1.14 — 使用全新的页分配器优化内存分配的速度21； interface赋值 接口有两个字段, 一个是类型, 一个是指针. 那对一个空接口赋值, 是传值还是传地址? type Person struct{ name string } type IPerson interface{} func main() { var person Person = Person{\"John\"} var iPerson IPerson fmt.Println(person) // => John fmt.Println(iPerson) // => ...so looks like a pointer iPerson = person // ...this seems to be making a copy fmt.Println(iPerson) // => John person.name = \"Mike\" fmt.Println(person) // => Mike //这里说明, 对数据源的改变, 不会体现到interface里 //说明interface是值拷贝 fmt.Println(iPerson) // => John ...so looks like it wasn't a pointer, // or at least something was definitely copied } The second word in the interface value points at the actual data, in this case a copy of b. The assignment var s Stringer = b makes a copy of b rather than point at b for the same reason that var c uint64 = b makes a copy: if b later changes, s and c are supposed to have the original value, not the new one. 也就是说, 对空接口的赋值, 发生了值拷贝, 空接口的指针字段, 指向新的拷贝. 更正 又过了一段时间, 觉得上面的解释不对对interface的赋值是值拷贝没错. 但对string的值拷贝不拷贝其底层buffer.上面例子第11行, 对空接口iPerson的赋值, 发生了string的\"值拷贝\", 即只拷贝string的结构体, 不拷贝buffer.关键是第14行, 对原始变量person.name的赋值, 也只是把\"Mike\"这个string的结构体赋值给person.name, 并不是把\"Mike\"拷贝到person.name的buffer里. person.name的底层指针, 改为指向\"Mike\"iPerson的name结构体没有变化, 还是指向原\"John\" interface的内部表达 golang中的interface, 有两种表达 iface是有方法的 eface没有方法, 纯空接口 它们都有指向底层数据的指针. type eface struct { // 16 bytes _type *_type data unsafe.Pointer } type iface struct { // 16 bytes tab *itab data unsafe.Pointer } 参考: https://blog.gopheracademy.com/advent-2018/interfaces-and-reflect/ reflect Type和interface reflect method reflect Value timer 标准库time提供了go语言对时间和定时器的使用接口 timer API 单次timer Timer对象持有channel C. timer超时后, 会发送当前时间到channel C. func NewTimer(d Duration) *Timer 返回一个Timer对象 典型的应用场景是, 从一个管道读数据, 不想永远等下去, 而是设个超时时间. func WaitChannel(conn 周期性timer go里面叫Ticker, 和Timer类似, 但Ticker周期性的往channel发送数据 func NewTicker(d Duration) *Ticker 使用Ticker需要注意的是, 不用的Ticker要调用Stop方法来终止, 否则系统一直会执行这个Ticker. package main import ( \"fmt\" \"time\" ) func main() { ticker := time.NewTicker(time.Second) //不用时stop Ticker defer ticker.Stop() done := make(chan bool) go func() { time.Sleep(10 * time.Second) done timer实现原理 timer的底层实现在runtime里. 在系统监控的循环中，我们通过 runtime.nanotime 和 runtime.timeSleepUntil 获取当前时间和计时器下一次需要唤醒的时间. 其中runtime.timeSleepUntil()函数遍历所有的timer bucket, 返回最小的. func timeSleepUntil() int64 { next := int64(1 timers是个数组, 元素是{timersBucket堆, 和cacheline对齐的pad} const timersLen = 64 // timers contains \"per-P\" timer heaps. // // Timers are queued into timersBucket associated with the current P, // so each P may work with its own timers independently of other P instances. // // Each timersBucket may be associated with multiple P // if GOMAXPROCS > timersLen. var timers [timersLen]struct { timersBucket // The padding should eliminate false sharing // between timersBucket values. pad [cpu.CacheLinePadSize - unsafe.Sizeof(timersBucket{})%cpu.CacheLinePadSize]byte } go1.13里面, 这个数组的大小是64, 是固定的. 按理说应该是per CPU的, 但为了避免动态分配内存, 考虑到主流的CPU核数, 这里就固定了64. 注: 用unsafe.Sizeof可以得到结构体大小 timersBucket是个堆结构, 因为timer的操作都会涉及到排序, 堆的排序和查找性能都不错. type timersBucket struct { lock mutex //goroutine的指针 gp *g created bool sleeping bool rescheduling bool //因为所有timer都是排序的, 这是最小的sleep时间 sleepUntil int64 waitnote note //timer桶下面的所有timer t []*timer } timer结构体, 在timer到时后在timer桶的timerproc协程里执行f type timer struct { tb *timersBucket // the bucket the timer lives in i int // heap index // Timer wakes up at when, and then at when+period, ... (period > 0 only) // each time calling f(arg, now) in the timer goroutine, so f must be // a well-behaved function and not block. when int64 period int64 f func(interface{}, uintptr) arg interface{} seq uintptr } add timer 每个timer的桶都有一个goroutine; 如果add timer的时候, 排好序后该timer是桶里的第一个timer, 会唤醒阻塞的gotoutine; 如果是第一次建立timer桶, 会起goroutine timerproc来执行timer桶里的事件, 因为桶里的timer都是按时间排好序的, 一个goroutine就够了. func addtimer(t *timer) { tb := t.assignBucket() lock(&tb.lock) ok := tb.addtimerLocked(t) unlock(&tb.lock) if !ok { badTimer() } } timer桶的分配 func (t *timer) assignBucket() *timersBucket { //当前g的m的p的id号, 翻译过来就是CPU号 id := uint8(getg().m.p.ptr().id) % timersLen t.tb = &timers[id].timersBucket return t.tb } 真正实现addtimer的函数 func (tb *timersBucket) addtimerLocked(t *timer) bool { // when must never be negative; otherwise timerproc will overflow // during its delta calculation and never expire other runtime timers. if t.when t.when { tb.sleeping = false //这个waitnote是个uintptr, 底层是futex或者是semaphore //每个timer桶一个 notewakeup(&tb.waitnote) } if tb.rescheduling { tb.rescheduling = false goready(tb.gp, 0) } if !tb.created { tb.created = true //起个goroutine, go timerproc(tb) } } return true } 触发timer 按前面所述, add timer时会给每个timer 桶起一个\"守护\"协程timerproc, timer的触发就在这个协程中. 它负责检查桶内的timer, 执行超时后的回调函数, 然后休眠到下个timer到期. //每个timer桶都有一个这个goroutine, 用于到时后执行timer的回调; //大部分回调是time.sendTime()往channel写. func timerproc(tb *timersBucket) { tb.gp = getg() for { lock(&tb.lock) tb.sleeping = false now := nanotime() delta := int64(-1) for { if len(tb.t) == 0 { delta = -1 break } //第一个元素是超时时间最短的 t := tb.t[0] delta = t.when - now //超时时间没到 if delta > 0 { break } //时间到了 ok := true //是周期的timer if t.period > 0 { // leave in heap but adjust next time to fire //这里delta只能是0或负数, delta的偏差越大, 说明系统繁忙来不及响应? 越增大下次的超时时间 t.when += t.period * (1 + -delta/t.period) //重新加入堆排序 if !siftdownTimer(tb.t, 0) { ok = false } } else { //是一次性的timer // remove from heap last := len(tb.t) - 1 if last > 0 { tb.t[0] = tb.t[last] tb.t[0].i = 0 } tb.t[last] = nil tb.t = tb.t[:last] if last > 0 { if !siftdownTimer(tb.t, 0) { ok = false } } t.i = -1 // mark as removed } //f是这个timer的回调 f := t.f arg := t.arg seq := t.seq unlock(&tb.lock) if !ok { badTimer() } if raceenabled { raceacquire(unsafe.Pointer(t)) } //调用这个timer的回调函数, 这个函数必须不能阻塞, 因为这里持有timer桶的锁. //time.NewTimer方法, 传入的是sleep.go里的私有函数sendTime f(arg, seq) lock(&tb.lock) } if delta 0 { // No timers left - put goroutine to sleep. tb.rescheduling = true goparkunlock(&tb.lock, waitReasonTimerGoroutineIdle, traceEvGoBlock, 1) continue } // At least one timer pending. Sleep until then. tb.sleeping = true tb.sleepUntil = now + delta noteclear(&tb.waitnote) unlock(&tb.lock) //这个for循环把那个不是忙等, 而是根据delta时间来sleep. notetsleepg(&tb.waitnote, delta) //底层是futexsleep, 是带超时时间的futex系统调用 //每个timer桶都有个waitnote, 这个应该是futex锁 futexsleep(key32(&n.key), 0, ns) } } time.NewTimer()注册了sendTime()回调 time包的NewTimer()方法, 底层调用的是startTimer(), 向runtime添加timer func NewTimer(d Duration) *Timer { c := make(chan Time, 1) // 创建一个管道 t := &Timer{ // 构造Timer数据结构 C: c, // 新创建的管道 r: runtimeTimer{ when: when(d), // 触发时间 f: sendTime, // 触发后执行函数sendTime arg: c, // 触发后执行函数sendTime时附带的参数 }, } startTimer(&t.r) // 此处启动定时器，只是把runtimeTimer放到系统协程的堆中，由系统协程维护 return t } sendTime()只是向channel发送当前时间 //sleep.go func sendTime(c interface{}, seq uintptr) { // Non-blocking send of time on c. // Used in NewTimer, it cannot block anyway (buffer). // Used in NewTicker, dropping sends on the floor is // the desired behavior when the reader gets behind, // because the sends are periodic. select { //有default是非阻塞发送 case c.(chan Time) timer堆的维护 timer桶里面的所有timer, golang使用了4叉数的顺序存储结构([]*timer切片)来管理. 每次新增 删除 修改timer或者是timer到期, 都会对timer堆重新排序.上图展示的是二叉堆，实际上Go实现时使用的是四叉堆，使用四叉堆的好处是堆的高度降低，堆调整时更快。 具体算法见: time.go siftupTimer()和siftdownTimer()函数 time包的NewTimer方法调用了runtime.startTimer 在src/time/sleep.go里面, 创建Timer实例的API NewTimer(), 调用了内部函数startTimer() // NewTimer creates a new Timer that will send // the current time on its channel after at least duration d. func NewTimer(d Duration) *Timer { c := make(chan Time, 1) t := &Timer{ C: c, r: runtimeTimer{ when: when(d), f: sendTime, arg: c, }, } startTimer(&t.r) return t } //而startTimer函数竟然在这个文件是个空函数 func startTimer(*runtimeTimer) 但在src/runtime/time.go里面, 有这样的声明: // startTimer adds t to the timer heap. //go:linkname startTimer time.startTimer func startTimer(t *timer) { if raceenabled { racerelease(unsafe.Pointer(t)) } addtimer(t) } 注意同名函数startTimer上面的注释, 似乎是某种链接黑科技: 把本地函数即runtime.startTimer函数, 当作time.startTimer来对待. go:linkname是给编译器看的指示. 详见官方说明 //go:linkname localname [importpath.name] The //go:linkname directive instructs the compiler to use “importpath.name” as the object file symbol name for the variable or function declared as “localname” in the source code. 性能测试和结果 在性能测试程序中, 我们起了N个go routine, 每个go routine起一个周期为1秒的timer, 即有N个timer同时周期性运行. timer超时后的动作非常简单(i++), 所以此测试几乎完全是测试golang timer机制的效率. MIPS板子为4核的CFNT-B golang的版本是1.13 测试程序使用taskset强制跑在单核上. 测试结果 Timer Number Sequential delay time Single core CPU load(MIPS) Single core CPU load(X86) 2000 0 ms 0 ~ 2.6% 0 ~ 0.7% 2000 20 ms 0 ~ 9.9% @ stable ~7.3% 0 ~ 5.9% @ stable ~4.6% 2000 Random in 30 ms Similar as 20 ms Similar as 20 ms 512 20 ms 0 ~ 2.6% @ stable ~2.0% 0 ~ 2.0% @ stable ~1.3% 20000 20 ms 0 ~ 50% @ stable ~43.7% 0 ~ 13.2% @ stable ~12.5% 结论 golang里每个CPU一个timer桶 桶内使用4叉树排序 每个桶有个单独的timer守护routine, 负责睡眠到下次timer到期, 执行timer的回调 timer的回调是time包注册的函数, 负责写channel, 对外不可见. go1.14对timer的优化 go1.13的timer问题 如上面所述, 每个P都有一个timer堆, 每个堆都有一个timerproc goroutine, 在这个routine中, 调用futexsleep是要休眠的; 注意这里的休眠是系统线程M直接休眠了, 这就要求M要和P解绑定, 意味着一次线程级别的上下文切换的开销: G5里面等待timerG5: , 系统把G5放在channel的reveive队列里, 超时时间到期时系统的timer机制会发送数据到这个channel来唤醒G5, 大致可以理解为G5在本地P的timer堆里休眠; 现在假设timer到期, timerproc(TP)被排到下一个被运行的goroutine TP开始执行, 但TP要求M休眠, 即TP\"带走\"了M, 和P分离 独立的P会触发wakep, 新建或寻找新的M并与之绑定; 接下来比如是从别的P里偷取G来运行 timer到期后, M被kernel唤醒, 会acquirep, 这里可能是抢占了当前的M TP里面通过channel唤醒G5, G5被放到本地P队列; 当然这次可能有多个timer同时超时, 它们对应的G都被放到运行队列 TP完成对timer的唤醒, 调用futex继续睡眠, 导致下一轮的M和P的分离. 综上, 1.13的问题在于: timperproc使用了阻塞的方式睡眠等待timer堆到期, 导致其所在的线程阻塞, 导致线程级别的上下文切换. 1.14解决思路 1.14中, 不使用timerproc来触发timer到期, 而是复用调度器的调度时机来检查timer是否超时. 调度器是没有单独goroutine的, 这样复用了以后, timer桶的检查点也没有单独的goroutine了. 少个gotouine不算什么, 关键是少了阻塞式的系统调用, 避免了timerporc里面的M因为要阻塞而与P分离操作. 系统监控 统监控是 Go 语言运行时的重要组成部分，它会每隔一段时间检查 Go 语言运行时，确保程序没有进入异常状态。 Go 语言的系统监控也起到了很重要的作用，它在内部启动了一个不会中止的循环，在循环的内部会轮询网络、抢占长期运行或者处于系统调用的 Goroutine 以及触发垃圾回收，通过这些行为，它能够让系统的运行状态变得更健康。 运行时通过系统监控来触发线程的抢占、网络的轮询和垃圾回收，保证 Go 语言运行时的可用性。系统监控能够很好地解决尾延迟的问题，减少调度器调度 Goroutine 的饥饿问题并保证计时器在尽可能准确的时间触发。 sysmon在独立的M(系统线程)中运行 监控循环 当 Go 语言程序启动时，运行时会在第一个 Goroutine 中调用 runtime.main 启动主程序，该函数会在系统栈中创建新的线程： func main() { ... if GOARCH != \"wasm\" { systemstack(func() { newm(sysmon, nil) }) } ... } runtime.newm 会创建一个存储待执行函数和处理器的新结构体 runtime.m。运行时执行系统监控不需要处理器，系统监控的 Goroutine 会直接在创建的线程上运行： func newm(fn func(), _p_ *p) { mp := allocm(_p_, fn) mp.nextp.set(_p_) mp.sigmask = initSigmask ... newm1(mp) } runtime.newm1 会调用特定平台的 runtime.newsproc 通过系统调用 clone 创建一个新的线程并在新的线程中执行 runtime.mstart： func newosproc(mp *m) { stk := unsafe.Pointer(mp.g0.stack.hi) var oset sigset sigprocmask(_SIG_SETMASK, &sigset_all, &oset) ret := clone(cloneFlags, stk, unsafe.Pointer(mp), unsafe.Pointer(mp.g0), unsafe.Pointer(funcPC(mstart))) sigprocmask(_SIG_SETMASK, &oset, nil) ... } 在新创建的线程中，我们会执行存储在 runtime.m 结构体中的 runtime.sysmon 函数启动系统监控： func sysmon() { sched.nmsys++ checkdead() lasttrace := int64(0) idle := 0 delay := uint32(0) for { if idle == 0 { delay = 20 } else if idle > 50 { delay *= 2 } if delay > 10*1000 { delay = 10 * 1000 } usleep(delay) ... } } 当运行时刚刚调用上述函数时，会先通过 runtime.checkdead 检查是否存在死锁，然后进入核心的监控循环；系统监控在每次循环开始时都会通过 usleep 挂起当前线程，该函数的参数是微秒，运行时会遵循以下的规则决定休眠时间： 初始的休眠时间是 20μs； 最长的休眠时间是 10ms； 当系统监控在 50 个循环中都没有唤醒 Goroutine 时，休眠时间在每个循环都会倍增； 当程序趋于稳定之后，系统监控的触发时间就会稳定在 10ms。它除了会检查死锁之外，还会在循环中完成以下的工作： 运行计时器 — 获取下一个需要被触发的计时器； 轮询网络 — 获取需要处理的到期文件描述符; 非阻塞地调用 runtime.netpoll 检查待执行的文件描述符并通过 runtime.injectglist 将所有处于就绪状态的 Goroutine 加入全局运行队列中 抢占处理器 — 抢占运行时间较长的或者处于系统调用的 Goroutine； 垃圾回收 — 在满足条件时触发垃圾收集回收内存； 检查timer 在系统监控的循环中，我们通过 runtime.nanotime 和 runtime.timeSleepUntil 获取当前时间和计时器下一次需要唤醒的时间. 在runtime/proc.go func sysmon() { for { //这个delay大概是10ms usleep(delay) //处理timer的优先级比较低, 比如只有在idle的时候才检查 if 各种条件 next := timeSleepUntil() now := nanotime() //没到超时时间, 需要Relax一下 osRelax(true) } } 检查死锁 计算系统中正在运行的线程个数, 如果为0, 则可能发生了死锁.进一步需要判断如果goroutine有可运行状态的, 则证明发生了死锁. 轮询网络 如果上一次轮询网络已经过去了 10ms，那么系统监控还会在循环中轮询网络，检查是否有待执行的文件描述符： func sysmon() { ... for { ... lastpoll := int64(atomic.Load64(&sched.lastpoll)) if netpollinited() && lastpoll != 0 && lastpoll+10*1000*1000 调用netpoll(0)非阻塞地检查待执行的文件描述符并通过 runtime.injectglist 将所有处于就绪状态的 Goroutine 加入全局运行队列中;该函数会将所有 Goroutine 的状态从 _Gwaiting 切换至 _Grunnable 并加入全局运行队列等待运行，如果当前程序中存在空闲的处理器，就会通过 runtime.startm 函数启动线程来执行这些任务。 抢占处理器 系统监控通过在循环中抢占处理器来避免同一个 Goroutine 占用线程太长时间造成饥饿问题。 垃圾回收 在最后，系统监控还会决定是否需要触发强制垃圾回收，runtime.sysmon 会构建 runtime.gcTrigger 结构体并调用 runtime.gcTrigger.test 函数判断是否需要触发垃圾回收： func sysmon() { ... for { ... if t := (gcTrigger{kind: gcTriggerTime, now: now}); t.test() && atomic.Load(&forcegc.idle) != 0 { lock(&forcegc.lock) forcegc.idle = 0 var list gList list.push(forcegc.g) injectglist(&list) unlock(&forcegc.lock) } ... } } 如果需要触发垃圾回收，我们会将用于垃圾回收的 Goroutine 加入全局队列，让调度器选择合适的处理器去执行。 参考: https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-sysmon/ IO多路复用 golang中的go routine在发生IO调用时(read/write), 不是直接阻塞, 而是使用IO多路复用. goroutine调用read/write时, runtime会把fd加到epoll里等待ready, 同时调用runtime.gopark让出当前线程. Go 语言的运行时会在调度或者系统监控中调用 runtime.netpoll 轮询网络. 网络轮询器并不是由运行时中的某一个线程独立运行的，运行时中的调度和系统调用会通过 runtime.netpoll 与网络轮询器交换消息，获取待执行的 Goroutine 列表，并将待执行的 Goroutine 加入运行队列等待处理。 所有的文件 I/O、网络 I/O 和计时器都是由网络轮询器管理的，它是 Go 语言运行时重要的组成部分。 golang使用多路IO复用, 但没用select, 而是用的效率更高的epoll(在linux平台上), 见src/runtime/netpoll_epoll.go 不同平台使用的系统调用不同 参考: https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-netpoller/ golang对epoll的封装 golang要同时支持epoll, kqueue, windows上的接口, 它们都使用一组接口 //— 初始化网络轮询器，通过 sync.Once 和 netpollInited 变量保证函数只会调用一次； func netpollinit() //监听文件描述符上的边缘触发事件，创建事件并加入监听； func netpollopen(fd uintptr, pd *pollDesc) int32 //轮询网络并返回一组已经准备就绪的 Goroutine，传入的参数会决定它的行为； func netpoll(delta int64) gList //唤醒网络轮询器，例如：计时器向前修改时间时会通过该函数中断网络轮询器； func netpollBreak() // 判断文件描述符是否被轮询器使用； func netpollIsPollDescriptor(fd uintptr) bool 数据结构 type pollDesc struct { //描述符链表 link *pollDesc lock mutex fd uintptr ... rseq uintptr rg uintptr rt timer rd int64 wseq uintptr wg uintptr wt timer wd int64 } 初始化 因为文件 I/O、网络 I/O 以及计时器都依赖网络轮询器，所以 Go 语言会通过以下两条不同路径初始化网络轮询器： internal/poll.pollDesc.init — 通过 net.netFD.init 和 os.newFile 初始化网络 I/O 和文件 I/O 的轮询信息时； runtime.doaddtimer — 向处理器中增加新的计时器时； runtime.netpollinit()做了下面的几个事 调用 epollcreate1 创建一个新的 epoll 文件描述符，这个文件描述符会在整个程序的生命周期中使用； 通过 runtime.nonblockingPipe 创建一个用于通信的管道； 使用 epollctl 将用于读取数据的文件描述符打包成 epollevent 事件加入监听； goroutine等待事件 当我们在文件描述符上执行读写操作时，如果文件描述符不可读或者不可写，当前 Goroutine 就会执行 runtime.poll_runtime_pollWait 检查 runtime.pollDesc 的状态并调用 runtime.netpollblock 等待文件描述符的可读或者可写： func poll_runtime_pollWait(pd *pollDesc, mode int) int { ... for !netpollblock(pd, int32(mode), false) { ... } return 0 } //runtime.netpollblock 是 Goroutine 等待 I/O 事件的关键函数， //它会使用运行时提供的 runtime.gopark 让出当前线程，将 Goroutine 转换到休眠状态并等待运行时的唤醒。 func netpollblock(pd *pollDesc, mode int32, waitio bool) bool { gpp := &pd.rg if mode == 'w' { gpp = &pd.wg } ... if waitio || netpollcheckerr(pd, mode) == 0 { gopark(netpollblockcommit, unsafe.Pointer(gpp), waitReasonIOWait, traceEvGoBlockNet, 5) } ... } 调用epoll 网络轮询器并不是由运行时中的某一个线程独立运行的，运行时中的调度和系统调用会通过 runtime.netpoll 与网络轮询器交换消息，获取待执行的 Goroutine 列表，并将待执行的 Goroutine 加入运行队列等待处理。 Go 语言的运行时会在调度或者系统监控中调用func netpoll(delay int64) gList轮询网络，该函数的执行过程可以分成以下几个部分： 根据传入的 delay 计算 epoll 系统调用需要等待的时间； 调用系统调用epollwait 等待可读或者可写事件的发生； 在循环中依次处理 epollevent 事件； 返回可读写的的goroutine列表, runtime会将列表中的全部 Goroutine 加入运行队列并等待调度器的调度 截至日期 网络轮询器和计时器的关系非常紧密，这不仅仅是因为网络轮询器负责计时器的唤醒，还因为文件和网络 I/O 的截止日期也由网络轮询器负责处理。截止日期在 I/O 操作中，尤其是网络调用中很关键，网络请求存在很高的不确定因素，我们需要设置一个截止日期保证程序的正常运行，这时就需要用到网络轮询器中的 runtime.poll_runtime_pollSetDeadline 函数 如果截至日期到了, 直接唤醒goroutine;Goroutine 在被唤醒之后就会意识到当前的 I/O 操作已经超时，可以根据需要选择重试请求或者中止调用。 GC 垃圾收集器 https://blog.golang.org/ismmkeynote go runtime调度器 相关的结构体表示 The G Struct : This represents a single go routine with it’s properties such as stack pointer, base of stack, it’s ID, it’s cache and it’s status The M Struct : This represents an OS thread. It also contains a pointer to the global queue of runnable goroutines, the current running goroutine and the reference to the scheduler The Sched Struct : It is a global struct and contains the queues free and waiting goroutines as well as threads. go比较快的5点 参考: https://dave.cheney.net/2014/06/07/five-things-that-make-go-fast go变量占的空间更小, 比如一个int, 和c一样, 占4个字节; 而python等动态语言要占24个字节 -- 他们把int当作一个object go编译器会做自动inline和死代码消除 C有stack变量, 也有malloc到堆上的变量; 而go有escape分析, 用来分析一个变量是否有函数外的引用, 没有的话, 分配在栈里; 此时就不需要GC 有的话, 分配在堆里, 需要GC.上面的代码, c分配在栈里, 因为它没有逃离CenterCursor()函数. goroutine, 更加轻量级的协作每个系统调用都会走这里, entersyscall通知runtime这个线程要阻塞了, runtime就把其他goroutine调度到新线程里去执行. 所以go程序会起几个系统线程, 然后go runtime来管理. goroutine的栈管理 普通进程的栈向下生长, 堆向上生长, 中间用guard page来隔离, guard page是只读的, 在之前调试时遇到过. 如果一个进程有多个线程, 那就有多个栈, 需要多个guard pagegoroutine不用guard page, 栈不够时自动分配go1.2的时候, 如果栈不够, 调用新的函数时, 会给新函数在堆里分配一个栈, 该函数返回就销毁这个栈. go1.3改了方法, 栈不够时, 分配一个新栈, 大小为2倍. 把之前所有的栈拷贝过来运行, 从此都用这个更大的新栈. 空间换时间. "},"notes/golang_interface原理.html":{"url":"notes/golang_interface原理.html","title":"Golang interface原理(网摘)","keywords":"","body":"Go Data Structures: Interfaces Posted on Tuesday, December 1, 2009. Go's interfaces—static, checked at compile time, dynamic when asked for—are, for me, the most exciting part of Go from a language design point of view. If I could export one feature of Go into other languages, it would be interfaces. This post is my take on the implementation of interface values in the “gc” compilers: 6g, 8g, and 5g. Over at Airs, Ian Lance Taylor has written two posts about the implementation of interface values in gccgo. The implementations are more alike than different: the biggest difference is that this post has pictures. Before looking at the implementation, let's get a sense of what it must support. Usage Go's interfaces let you use duck typing like you would in a purely dynamic language like Python but still have the compiler catch obvious mistakes like passing an int where an object with a Read method was expected, or like calling the Read method with the wrong number of arguments. To use interfaces, first define the interface type (say, ReadCloser): type ReadCloser interface { Read(b []byte) (n int, err os.Error) Close() } and then define your new function as taking a ReadCloser. For example, this function calls Read repeatedly to get all the data that was requested and then calls Close: func ReadAndClose(r ReadCloser, buf []byte) (n int, err os.Error) { for len(buf) > 0 && err == nil { var nr int nr, err = r.Read(buf) n += nr buf = buf[nr:] } r.Close() return } The code that calls ReadAndClose can pass a value of any type as long as it has Read and Close methods with the right signatures. And, unlike in languages like Python, if you pass a value with the wrong type, you get an error at compile time, not run time. Interfaces aren't restricted to static checking, though. You can check dynamically whether a particular interface value has an additional method. For example: type Stringer interface { String() string } func ToString(any interface{}) string { if v, ok := any.(Stringer); ok { return v.String() } switch v := any.(type) { case int: return strconv.Itoa(v) case float: return strconv.Ftoa(v, 'g', -1) } return \"???\" } The value any has static type interface{}, meaning no guarantee of any methods at all: it could contain any type. The “comma ok” assignment inside the if statement asks whether it is possible to convert any to an interface value of type Stringer, which has the method String. If so, the body of that statement calls the method to obtain a string to return. Otherwise, the switch picks off a few basic types before giving up. This is basically a stripped down version of what the fmt package does. (The if could be replaced by adding case Stringer: at the top of the switch, but I used a separate statement to draw attention to the check.) As a simple example, let's consider a 64-bit integer type with a String method that prints the value in binary and a trivial Get method: type Binary uint64 func (i Binary) String() string { return strconv.Uitob64(i.Get(), 2) } func (i Binary) Get() uint64 { return uint64(i) } A value of type Binary can be passed to ToString, which will format it using the String method, even though the program never says that Binary intends to implement Stringer. There's no need: the runtime can see that Binary has a String method, so it implements Stringer, even if the author of Binary has never heard of Stringer. These examples show that even though all the implicit conversions are checked at compile time, explicit interface-to-interface conversions can inquire about method sets at run time. “Effective Go” has more details about and examples of how interface values can be used. Interface Values Languages with methods typically fall into one of two camps: prepare tables for all the method calls statically (as in C++ and Java), or do a method lookup at each call (as in Smalltalk and its many imitators, JavaScript and Python included) and add fancy caching to make that call efficient. Go sits halfway between the two: it has method tables but computes them at run time. I don't know whether Go is the first language to use this technique, but it's certainly not a common one. (I'd be interested to hear about earlier examples; leave a comment below.) As a warmup, a value of type Binary is just a 64-bit integer made up of two 32-bit words (like in the last post, we'll assume a 32-bit machine; this time memory grows down instead of to the right): Interface values are represented as a two-word pair giving a pointer to information about the type stored in the interface and a pointer to the associated data. Assigning b to an interface value of type Stringer sets both words of the interface value. (The pointers contained in the interface value are gray to emphasize that they are implicit, not directly exposed to Go programs.) The first word in the interface value points at what I call an interface table or itable (pronounced i-table; in the runtime sources, the C implementation name is Itab). The itable begins with some metadata about the types involved and then becomes a list of function pointers. Note that the itable corresponds to the interface type, not the dynamic type. In terms of our example, the itable for Stringer holding type Binary lists the methods used to satisfy Stringer, which is just String: Binary's other methods (Get) make no appearance in the itable. The second word in the interface value points at the actual data, in this case a copy of b. The assignment var s Stringer = b makes a copy of b rather than point at b for the same reason that var c uint64 = b makes a copy: if b later changes, s and c are supposed to have the original value, not the new one. Values stored in interfaces might be arbitrarily large, but only one word is dedicated to holding the value in the interface structure, so the assignment allocates a chunk of memory on the heap and records the pointer in the one-word slot. (There's an obvious optimization when the value does fit in the slot; we'll get to that later.) To check whether an interface value holds a particular type, as in the type switch above, the Go compiler generates code equivalent to the C expression s.tab->type to obtain the type pointer and check it against the desired type. If the types match, the value can be copied by by dereferencing s.data. To call s.String(), the Go compiler generates code that does the equivalent of the C expression s.tab->fun[0](s.data): it calls the appropriate function pointer from the itable, passing the interface value's data word as the function's first (in this example, only) argument. You can see this code if you run 8g -S x.go (details at the bottom of this post). Note that the function in the itable is being passed the 32-bit pointer from the second word of the interface value, not the 64-bit value it points at. In general, the interface call site doesn't know the meaning of this word nor how much data it points at. Instead, the interface code arranges that the function pointers in the itable expect the 32-bit representation stored in the interface values. Thus the function pointer in this example is (*Binary).String not Binary.String. The example we're considering is an interface with just one method. An interface with more methods would have more entries in the fun list at the bottom of the itable. Computing the Itable Now we know what the itables look like, but where do they come from? Go's dynamic type conversions mean that it isn't reasonable for the compiler or linker to precompute all possible itables: there are too many (interface type, concrete type) pairs, and most won't be needed. Instead, the compiler generates a type description structure for each concrete type like Binary or int or func(map[int]string). Among other metadata, the type description structure contains a list of the methods implemented by that type. Similarly, the compiler generates a (different) type description structure for each interface type like Stringer; it too contains a method list. The interface runtime computes the itable by looking for each method listed in the interface type's method table in the concrete type's method table. The runtime caches the itable after generating it, so that this correspondence need only be computed once. In our simple example, the method table for Stringer has one method, while the table for Binary has two methods. In general there might be ni methods for the interface type and nt methods for the concrete type. The obvious search to find the mapping from interface methods to concrete methods would take O(ni × nt) time, but we can do better. By sorting the two method tables and walking them simultaneously, we can build the mapping in O(ni + nt) time instead. Memory Optimizations The space used by the implementation described above can be optimized in two complementary ways. First, if the interface type involved is empty—it has no methods—then the itable serves no purpose except to hold the pointer to the original type. In this case, the itable can be dropped and the value can point at the type directly: Whether an interface type has methods is a static property—either the type in the source code says interface{} or it says interace{ methods... }—so the compiler knows which representation is in use at each point in the program. Second, if the value associated with the interface value can fit in a single machine word, there's no need to introduce the indirection or the heap allocation. If we define Binary32 to be like Binary but implemented as a uint32, it could be stored in an interface value by keeping the actual value in the second word: Whether the actual value is being pointed at or inlined depends on the size of the type. The compiler arranges for the functions listed in the type's method table (which get copied into the itables) to do the right thing with the word that gets passed in. If the receiver type fits in a word, it is used directly; if not, it is dereferenced. The diagrams show this: in the Binary version far above, the method in the itable is (*Binary).String, while in the Binary32 example, the method in the itable is Binary32.String not (*Binary32).String. Of course, empty interfaces holding word-sized (or smaller) values can take advantage of both optimizations: Method Lookup Performance Smalltalk and the many dynamic systems that have followed it perform a method lookup every time a method gets called. For speed, many implementations use a simple one-entry cache at each call site, often in the instruction stream itself. In a multithreaded program, these caches must be managed carefully, since multiple threads could be at the same call site simultaneously. Even once the races have been avoided, the caches would end up being a source of memory contention. Because Go has the hint of static typing to go along with the dynamic method lookups, it can move the lookups back from the call sites to the point when the value is stored in the interface. For example, consider this code snippet: var any interface{} // initialized elsewhere s := any.(Stringer) // dynamic conversion for i := 0; i In Go, the itable gets computed (or found in a cache) during the assignment on line 2; the dispatch for the s.String() call executed on line 4 is a couple of memory fetches and a single indirect call instruction. In contrast, the implementation of this program in a dynamic language like Smalltalk (or JavaScript, or Python, or ...) would do the method lookup at line 4, which in a loop repeats needless work. The cache mentioned earlier makes this less expensive than it might be, but it's still more expensive than a single indirect call instruction. Of course, this being a blog post, I don't have any numbers to back up this discussion, but it certainly seems like the lack of memory contention would be a big win in a heavily parallel program, as is being able to move the method lookup out of tight loops. Also, I'm talking about the general architecture, not the specifics o the implementation: the latter probably has a few constant factor optimizations still available. More Information The interface runtime support is in [$GOROOT/src/pkg/runtime/iface.c](http://code.google.com/p/go/source/browse/src/pkg/runtime/iface.c). There's much more to say about interfaces (we haven't even seen an example of a pointer receiver yet) and the type descriptors (they power reflection in addition to the interface runtime) but those will have to wait for future posts. Code Supporting code (x.go): package main import ( \"fmt\" \"strconv\" ) type Stringer interface { String() string } type Binary uint64 func (i Binary) String() string { return strconv.Uitob64(i.Get(), 2) } func (i Binary) Get() uint64 { return uint64(i) } func main() { b := Binary(200) s := Stringer(b) fmt.Println(s.String()) } Selected output of 8g -S x.go: 0045 (x.go:25) LEAL s+-24(SP),BX 0046 (x.go:25) MOVL 4(BX),BP 0047 (x.go:25) MOVL BP,(SP) 0048 (x.go:25) MOVL (BX),BX 0049 (x.go:25) MOVL 20(BX),BX 0050 (x.go:25) CALL ,BX The LEAL loads the address of s into the register BX. (The notation _n_(SP) describes the word in memory at SP+_n_. 0(SP) can be shortened to (SP).) The next two MOVL instructions fetch the value from the second word in the interface and store it as the first function call argument, 0(SP). The final two MOVL instructions fetch the itable and then the function pointer from the itable, in preparation for calling that function. "},"notes/golang_内存分配.html":{"url":"notes/golang_内存分配.html","title":"Golang 内存分配(网摘)","keywords":"","body":" Go: Memory Management and Allocation Allocation on the heap Small allocation Large allocation Big picture Inspiration go先从本M的buddy系统里分配内存, 再从全局的buddy里面分对于大于32k的大内存, 直接跟os要. 原文链接: https://medium.com/a-journey-with-go/go-memory-management-and-allocation-a7396d430f44 Go: Memory Management and Allocation ℹ️ This article is based on Go 1.13. Go memory management is automatically done by the standard library from the allocation of the memory to its collection when it is not used anymore. Although the developer does not have to deal with it, the underlying management done by Go is well optimized and full of interesting concepts. Allocation on the heap The memory management is designed to be fast in a concurrent environment and integrated with the garbage collector. Let’s start with a simple example: package main type smallStruct **struct** { a, b int64 c, d float64 } func main() { smallAllocation() } //go:noinline func smallAllocation() *smallStruct { return &smallStruct{} } The annotation //go:noinline will disable in-lining that would optimize the code by removing the function and, therefore, end up with no allocation. Running the escape analysis command with go tool compile \"-m\" main.go will confirm the allocation made by Go: Dumping the assembly code for this program, thanks to go tool compile -S main.go, would also explicitly show us the allocation: 0x001d 00029 (main.go:14) LEAQ type.\"\".smallStruct(SB), AX 0x0024 00036 (main.go:14) PCDATA $0, $0 0x0024 00036 (main.go:14) MOVQ AX, (SP) 0x0028 00040 (main.go:14) CALL runtime.newobject(SB) The function newobject is the built-in function for new allocations and proxy mallocgc, a function that manages them on the heap. There are two strategies in Go, one for the small allocations and one for larger ones. Small allocation For the small allocations, under 32kb, Go will try to get the memory from a local cache called mcache. This cache handles a list of span (memory chunk of 32kb), called mspan, that contains the memory available for allocation: Each thread M is assigned to a processor P and handles at most one goroutine at a time. While allocating memory, our current goroutine will use the local cache of its current P to find the first free object available in the span list. Using this local cache does not require lock and makes the allocation more efficient. The span list is divided into ~70 size classes, from 8 bytes to 32k bytes, that can store different object sizes: Each span exists twice: one list for objects that do not contain pointer and another one that contains pointer. This distinction will make the life of the garbage collector easier since it will not have to scan the spans that do not contain any pointer. In our previous example, the size of the structure is 32 bytes and will fit in the 32 bytes span: Now, we may wonder what would happen if the span does not have a free slot during the allocation. Go maintains central lists of spans per size classes, called mcentral, with the spans that contain free objects and the ones that do not: mcentral maintains a double linked list of spans; each of them has a reference to the previous span and next span. A span in the non-empty list — “non-empty” means that at least one slot is free in the list for allocation — could contain some memory in-use already. Indeed, when the garbage collector sweeps the memory, it could clean a part of the span — the part marked as not used anymore — and would put it back in the non-empty list. Our program can now request a span from the central list if it runs out of slots: Go needs a way to get new spans to the central list if none are available in the empty list. New spans will now be allocated from the heap and linked to the central list: The heap pulls the memory from the OS when needed. If it needs more memory, the heap will allocate a large chunk of memory, called arena, of 64Mb for the 64bits architectures and 4Mb for most of the other architectures. The arena also maps the memory page with the spans: Large allocation Go does not manage the large allocations with a local cache. Those allocations, greater than 32kb, are rounded up to the page size and the pages are allocated directly to the heap. Big picture We now have a good view of what is happening at a high level during the memory allocation. Let’s draw all the components together to get the full picture: Inspiration The memory allocator is originally based on TCMalloc, a memory allocator optimized for the concurrent environment created by Google. The documentation of TCMalloc is worth reading; you will also find the concepts explained previously. "},"notes/golang_标准库.html":{"url":"notes/golang_标准库.html","title":"Golang 标准库","keywords":"","body":" text/template Execute原型 actions arguments Pipelines Variables 举例 builtin函数 rand unsafe unsafe.Pointer类型 float64转uint64 strconv sync Cond Map Mutex RWMutex Once Pool 重点WaitGroup net ip tcp和udp IP datagrams UDP TCP 代码 TCP UDP raw socket 名字解析 api path filepath encoding 二进制 UTF8 encoding/gob api 简单例子 自定义encode和decode 传输interface对象 encoding/binary varint time Time 时间格式和单位 Duration api Ticker Timer context api 整数发生器 generator 带超时的Context syscall runtime runtime相关的环境变量 api runtime/pprof runtime/debug runtime/trace os os/exec os/signal cgo go作为so 方法 os/user builtin 函数 类型 strings bytes Buffer log/syslog log type logger logger支持多种格式, 用flag来配置 fmt 方法 Stringer类型 格式化标志符 通用 布尔值 整型 浮点 字符串 切片 指针 默认格式化 宽度 精度 对齐 举例 scanf bufio error对象 Reader的方法 Writer的方法 Scanner的方法 Scanner的方法 按行读 统计字符个数 也可以自定义分割函数 io/ioutil io 变量 以Reader为例, 看golang的派生 PipeReader, 派生类 ReadWriter WriterTo ReaderAt 其他基类 Seeker io包的方法 Copy Pipe TeeReader container/ring api 例子: 连接两个ring container/list api 使用举例 container/heap 树的数组表达 min-int树 可以用heap来实现优先级队列 sort 支持对基本类型的排序 对struct排序 通用排序 对外接口 text/template 注: gitbook对模板关键字{{ }}有特殊处理. 下文中为了规避这个问题, 在两个大括号中间都加了空格. { {.var} }: 要被替换的模板变量. \"{ {23 -} } : -用来去掉空格 \"23 传入模板的数据一般是个结构体, 或者map. Templates are executed by applying them to a data structure. Annotations in the template refer to elements of the data structure (typically a field of a struct or a key in a map) to control execution and derive values to be displayed. Execution of the template walks the structure and sets the cursor, represented by a period '.' and called \"dot\", to the value at the current location in the structure as execution proceeds. Execute原型 入参data可以是结构体, map等. 把结果写入io.Writer(这个设计很好, 结果写入接口, 代码的组合性就很强) func (t *Template) Execute(wr io.Writer, data interface{}) error actions { { } }包起来的叫action. { {/* a comment */} } { {- /* a comment with white space trimmed from preceding and following text */ -} } A comment; discarded. May contain newlines. Comments do not nest and must start and end at the delimiters, as shown here. { {pipeline} } The default textual representation (the same as would be printed by fmt.Print) of the value of the pipeline is copied to the output. { {if pipeline} } T1 { {end} } If the value of the pipeline is empty, no output is generated; otherwise, T1 is executed. The empty values are false, 0, any nil pointer or interface value, and any array, slice, map, or string of length zero. Dot is unaffected. { {if pipeline} } T1 { {else} } T0 { {end} } If the value of the pipeline is empty, T0 is executed; otherwise, T1 is executed. Dot is unaffected. { {if pipeline} } T1 { {else if pipeline} } T0 { {end} } To simplify the appearance of if-else chains, the else action of an if may include another if directly; the effect is exactly the same as writing { {if pipeline} } T1 { {else} }{ {if pipeline} } T0 { {end} }{ {end} } { {range pipeline} } T1 { {end} } The value of the pipeline must be an array, slice, map, or channel. If the value of the pipeline has length zero, nothing is output; otherwise, dot is set to the successive elements of the array, slice, or map and T1 is executed. If the value is a map and the keys are of basic type with a defined order (\"comparable\"), the elements will be visited in sorted key order. { {range pipeline} } T1 { {else} } T0 { {end} } The value of the pipeline must be an array, slice, map, or channel. If the value of the pipeline has length zero, dot is unaffected and T0 is executed; otherwise, dot is set to the successive elements of the array, slice, or map and T1 is executed. { {block \"name\" pipeline} } T1 { {end} } A block is shorthand for defining a template { {define \"name\"} } T1 { {end} } and then executing it in place { {template \"name\" pipeline} } The typical use is to define a set of root templates that are then customized by redefining the block templates within. { {with pipeline} } T1 { {end} } If the value of the pipeline is empty, no output is generated; otherwise, dot is set to the value of the pipeline and T1 is executed. { {with pipeline} } T1 { {else} } T0 { {end} } If the value of the pipeline is empty, dot is unaffected and T0 is executed; otherwise, dot is set to the value of the pipeline and T1 is executed. 特别的, 模板可以引用其他命名模板: New模板的时候会给每个模板起个名字, 用这个名字来引用它. -- 关联模板 { {template \"name\"} } The template with the specified name is executed with nil data. { {template \"name\" pipeline} } The template with the specified name is executed with dot set to the value of the pipeline. arguments action里面的概念: argument 模板支持 struct的.Field引用 map的.Key引用 对象的.Method方法引用 -- 即调用dot.Method()函数 也可以是普通函数func - A boolean, string, character, integer, floating-point, imaginary or complex constant in Go syntax. These behave like Go's untyped constants. Note that, as in Go, whether a large integer constant overflows when assigned or passed to a function can depend on whether the host machine's ints are 32 or 64 bits. - The keyword nil, representing an untyped Go nil. - The character '.' (period): . The result is the value of dot. - A variable name, which is a (possibly empty) alphanumeric string preceded by a dollar sign, such as $piOver2 or $ The result is the value of the variable. Variables are described below. - The name of a field of the data, which must be a struct, preceded by a period, such as .Field The result is the value of the field. Field invocations may be chained: .Field1.Field2 Fields can also be evaluated on variables, including chaining: $x.Field1.Field2 - The name of a key of the data, which must be a map, preceded by a period, such as .Key The result is the map element value indexed by the key. Key invocations may be chained and combined with fields to any depth: .Field1.Key1.Field2.Key2 Although the key must be an alphanumeric identifier, unlike with field names they do not need to start with an upper case letter. Keys can also be evaluated on variables, including chaining: $x.key1.key2 - The name of a niladic method of the data, preceded by a period, such as .Method The result is the value of invoking the method with dot as the receiver, dot.Method(). Such a method must have one return value (of any type) or two return values, the second of which is an error. If it has two and the returned error is non-nil, execution terminates and an error is returned to the caller as the value of Execute. Method invocations may be chained and combined with fields and keys to any depth: .Field1.Key1.Method1.Field2.Key2.Method2 Methods can also be evaluated on variables, including chaining: $x.Method1.Field - The name of a niladic function, such as fun The result is the value of invoking the function, fun(). The return types and values behave as in methods. Functions and function names are described below. - A parenthesized instance of one the above, for grouping. The result may be accessed by a field or map key invocation. print (.F1 arg1) (.F2 arg2) (.StructValuedMethod \"arg\").Field Pipelines 顾名思义, 模板支持pipe操作.即多个commands可以用|连接, 最后命令的值是pipeline的值 Argument The result is the value of evaluating the argument. .Method [Argument...] The method can be alone or the last element of a chain but, unlike methods in the middle of a chain, it can take arguments. The result is the value of calling the method with the arguments: dot.Method(Argument1, etc.) functionName [Argument...] The result is the value of calling the function associated with the name: function(Argument1, etc.) Functions and function names are described below. Variables Action里面可以声明变量: #用variable来获取pipeline的输出; 和shell里面一样, pipeline输出到变量里, 而不是展开 $variable := pipeline range $index, $element := pipeline 特别的, $的默认值为传入Execute的值. When execution begins, $ is set to the data argument passed to Execute, that is, to the starting value of dot. 举例 下面所有例子都是一行的模板, 全部生成\"output\"输出. { {\"\\\"output\\\"\"} } A string constant. { {`\"output\"`} } A raw string constant. { {printf \"%q\" \"output\"} } A function call. { {\"output\" | printf \"%q\"} } A function call whose final argument comes from the previous command. { {printf \"%q\" (print \"out\" \"put\")} } A parenthesized argument. { {\"put\" | printf \"%s%s\" \"out\" | printf \"%q\"} } A more elaborate call. { {\"output\" | printf \"%s\" | printf \"%q\"} } A longer chain. { {with \"output\"} }{ {printf \"%q\" .} }{ {end} } A with action using dot. { {with $x := \"output\" | printf \"%q\"} }{ {$x} }{ {end} } A with action that creates and uses a variable. { {with $x := \"output\"} }{ {printf \"%q\" $x} }{ {end} } A with action that uses the variable in another action. { {with $x := \"output\"} }{ {$x | printf \"%q\"} }{ {end} } The same, but pipelined. builtin函数 模板提供了一些预定义的函数: and Returns the boolean AND of its arguments by returning the first empty argument or the last argument, that is, \"and x y\" behaves as \"if x then y else x\". All the arguments are evaluated. call Returns the result of calling the first argument, which must be a function, with the remaining arguments as parameters. Thus \"call .X.Y 1 2\" is, in Go notation, dot.X.Y(1, 2) where Y is a func-valued field, map entry, or the like. The first argument must be the result of an evaluation that yields a value of function type (as distinct from a predefined function such as print). The function must return either one or two result values, the second of which is of type error. If the arguments don't match the function or the returned error value is non-nil, execution stops. html Returns the escaped HTML equivalent of the textual representation of its arguments. This function is unavailable in html/template, with a few exceptions. index Returns the result of indexing its first argument by the following arguments. Thus \"index x 1 2 3\" is, in Go syntax, x[1][2][3]. Each indexed item must be a map, slice, or array. slice slice returns the result of slicing its first argument by the remaining arguments. Thus \"slice x 1 2\" is, in Go syntax, x[1:2], while \"slice x\" is x[:], \"slice x 1\" is x[1:], and \"slice x 1 2 3\" is x[1:2:3]. The first argument must be a string, slice, or array. js Returns the escaped JavaScript equivalent of the textual representation of its arguments. len Returns the integer length of its argument. not Returns the boolean negation of its single argument. or Returns the boolean OR of its arguments by returning the first non-empty argument or the last argument, that is, \"or x y\" behaves as \"if x then x else y\". All the arguments are evaluated. print An alias for fmt.Sprint printf An alias for fmt.Sprintf println An alias for fmt.Sprintln urlquery Returns the escaped value of the textual representation of its arguments in a form suitable for embedding in a URL query. This function is unavailable in html/template, with a few exceptions. eq Returns the boolean truth of arg1 == arg2 ne Returns the boolean truth of arg1 != arg2 lt Returns the boolean truth of arg1 arg2 ge Returns the boolean truth of arg1 >= arg2 bool值是任何类型的0值. 调用函数不用加括号, 举例: { {printf \"%q\" \"output\"} } A function call. { {printf \"%q\" (print \"out\" \"put\")} } A parenthesized argument. rand import \"math/rand\" func main() { rand.Seed(time.Now().UnixNano()) min := 10 max := 30 fmt.Println(rand.Intn(max - min + 1) + min) } unsafe 如包名所指, unsafe不怎么safe, 但它提供了更底层的操作能力 比如timer的实现里, 调用了unsafe.Sizeof获取一个结构体的大小. //返回任意类型的x的字节对齐要求 func Alignof(x ArbitraryType) uintptr //返回x在结构体里的offset常量, x必须是structValue.field格式 func Offsetof(x ArbitraryType) uintptr //比如 dataOffset = unsafe.Offsetof(struct { b bmap v int64 }{}.v) //返回类型的字节数常量 func Sizeof(x ArbitraryType) uintptr unsafe.Pointer类型 unsafe.Pointer uintptr 和任意类型的指针都可以互相转换. 用了Pointer类型, 可以绕过类型系统, 直接读写任意内存. 用Pointer可以实现C的类型强转 几个例子: float64转uint64 package main import ( \"fmt\" \"unsafe\" ) func main() { var v float64 = 1.1 //类型强转 fmt.Println(uint64(v)) //通过指针强转, 结果是不一样的. fmt.Println(*(*uint64)(unsafe.Pointer(&v))) } //结果 1 4607632778762754458 strconv 提供字符串到其他基本类型的转换 比如Atoi, Itoa sync 一般的同步用channel就好. sync包同时也提供了更底层的同步方法. Once和WaitGroup类型应该是用的比较多的. 注意, 包含这个包里定义的类型的数据不能拷贝. Cond Cond是用来保护并发条件下的条件变量x的. 使用时候需要 变量x来表示业务条件 mutex保护这个变量的修改. 并不是用来休眠的 前面两个是程序已经有了的. 那么用Cond的目的是用来等待和通知的. 比如程序要等待x变成比如说100, 就用cond.wait; 到达条件的routine负责通知或者广播等待者. 设计思路和pthread_cond一毛一样 另见并发 任务 事件 和锁.md type Cond struct { // L is held while observing or changing the condition L Locker // contains filtered or unexported fields } // 其中, Locker是个接口 type Locker interface { Lock() Unlock() } //返回一个Cond结构的变量, 需要传入一个Locker func NewCond(l Locker) *Cond //唤醒所有等在这个Cond上的routine, 不需要持有锁 func (c *Cond) Broadcast() //唤醒在c上等待的一个go routine, 不需要持有锁 func (c *Cond) Signal() //调用Wait时, Wait自动unlock c.L, 然后挂起该go routine //恢复的时候, Wait自动lock c.L, 然后return //除非Broadcast或者Signal, Wait不会返回 func (c *Cond) Wait() 没看懂: Because c.L is not locked when Wait first resumes, the caller typically cannot assume that the condition is true when Wait returns. Instead, the caller should Wait in a loop: c.L.Lock() for !condition() { c.Wait() } ... make use of condition ... c.L.Unlock() Map 内置的带锁保护的map. 普通的map不是并发安全的, 需要自己保护临界区. type Map struct { // contains filtered or unexported fields } func (m *Map) Delete(key interface{}) func (m *Map) Load(key interface{}) (value interface{}, ok bool) func (m *Map) LoadOrStore(key, value interface{}) (actual interface{}, loaded bool) func (m *Map) Range(f func(key, value interface{}) bool) func (m *Map) Store(key, value interface{}) Mutex 初始状态时unlock态 type Mutex struct { // contains filtered or unexported fields } func (m *Mutex) Lock() func (m *Mutex) Unlock() RWMutex type RWMutex struct { // contains filtered or unexported fields } func (rw *RWMutex) Lock() func (rw *RWMutex) RLock() func (rw *RWMutex) RLocker() Locker func (rw *RWMutex) RUnlock() func (rw *RWMutex) Unlock() Once Once只执行一次 type Once struct { // contains filtered or unexported fields } func (o *Once) Do(f func()) //因为f没有参数, 下面的形式更常见 config.once.Do(func() { config.init(filename) }) Pool Pool是个cache, 是个并发安全的free list. 被put进pool的对象可能没有任何通知的被移除. type Pool struct { // New optionally specifies a function to generate // a value when Get would otherwise return nil. // It may not be changed concurrently with calls to Get. New func() interface{} // contains filtered or unexported fields } //随机从pool里选一个对象, 并把该对象从pool里移除. 不保证被put过的对象能够被get //没有对象的时候, 会调用p.New方法. func (p *Pool) Get() interface{} //add对象到pool func (p *Pool) Put(x interface{}) 重点WaitGroup 给main routine来等待所有其他go routine结束用的. 和shell的wait差不多 type WaitGroup struct { // contains filtered or unexported fields } //给counter加delta, delta可以是负值. func (wg *WaitGroup) Add(delta int) //子routine调用done表示自己干完了, counter减一 func (wg *WaitGroup) Done() //main routine调用Wait等待里面的counter减到0 func (wg *WaitGroup) Wait() 例子: package main import ( \"sync\" ) type httpPkg struct{} func (httpPkg) Get(url string) {} var http httpPkg func main() { var wg sync.WaitGroup var urls = []string{ \"http://www.golang.org/\", \"http://www.google.com/\", \"http://www.somestupidname.com/\", } for _, url := range urls { // Increment the WaitGroup counter. wg.Add(1) // Launch a goroutine to fetch the URL. go func(url string) { // Decrement the counter when the goroutine completes. defer wg.Done() // Fetch the URL. http.Get(url) }(url) } // Wait for all HTTP fetches to complete. wg.Wait() } net net提供tcp/ip和unix socket接口 net提供了原始的socket接口, 也提供了一层更简便的接口. 对client来说, 是Dial conn, err := net.Dial(\"tcp\", \"golang.org:80\") if err != nil { // handle error } fmt.Fprintf(conn, \"GET / HTTP/1.0\\r\\n\\r\\n\") status, err := bufio.NewReader(conn).ReadString('\\n') // ... 对server来说, 是Listen ln, err := net.Listen(\"tcp\", \":8080\") if err != nil { // handle error } for { conn, err := ln.Accept() if err != nil { // handle error } go handleConnection(conn) } ip tcp和udp 总的来说, ip是面向报文的, udp也是面向报文的, 只是比ip多加了port.tcp是面向流的.udp有广播. 都有api可以set os的发送接收buffer IP datagrams The IP layer provides a connectionless and unreliable delivery system. It considers each datagram independently of the others. Any association between datagrams must be supplied by the higher layers. The IP layer supplies a checksum that includes its own header. The header includes the source and destination addresses. The IP layer handles routing through an Internet. It is also responsible for breaking up large datagrams into smaller ones for transmission and reassembling them at the other end. UDP UDP is also connectionless and unreliable. What it adds to IP is a checksum for the contents of the datagram and port numbers. These are used to give a client/server model - see later. TCP TCP supplies logic to give a reliable connection-oriented protocol above IP. It provides a virtual circuit that two processes can use to communicate. It also uses port numbers to identify services on a host. 代码 TCP func DialTCP(net string, laddr, raddr *TCPAddr) (c *TCPConn, err os.Error) func (c *TCPConn) Write(b []byte) (n int, err os.Error) func (c *TCPConn) Read(b []byte) (n int, err os.Error) func ListenTCP(net string, laddr *TCPAddr) (l *TCPListener, err os.Error) func (l *TCPListener) Accept() (c Conn, err os.Error) func (c *TCPConn) SetTimeout(nsec int64) os.Error func (c *TCPConn) SetKeepAlive(keepalive bool) os.Error UDP func ResolveUDPAddr(net, addr string) (*UDPAddr, os.Error) func DialUDP(net string, laddr, raddr *UDPAddr) (c *UDPConn, err os.Error) //对client来说, 还是read write: func (c *UDPConn) Read(b []byte) (int, error) func (c *UDPConn) Write(b []byte) (int, error) //对server来说, 有点不一样: 因为UDP的server没有\"连接\"的概念, 只能从每个报文里看到对端的地址. func ListenUDP(net string, laddr *UDPAddr) (c *UDPConn, err os.Error) func (c *UDPConn) ReadFromUDP(b []byte) (n int, addr *UDPAddr, err os.Error func (c *UDPConn) WriteToUDP(b []byte, addr *UDPAddr) (n int, err os.Error) 比如server端的读写是: func handleClient(conn *net.UDPConn) { var buf [512]byte _, addr, err := conn.ReadFromUDP(buf[0:]) if err != nil { return } daytime := time.Now().String() conn.WriteToUDP([]byte(daytime), addr) } raw socket IPConn就是raw socket 比如要写个ping addr, err := net.ResolveIPAddr(\"ip\", os.Args[1]) conn, err := net.DialIP(\"ip4:icmp\", addr, addr) _, err = conn.Write(msg[0:len]) _, err = conn.Read(msg[0:]) 名字解析 名字解析有两个方式: 纯go方式和cgo方式 默认是纯go. 使能了cgo就会用cgo. 环境变量可以选择用哪种方式: export GODEBUG=netdns=go # force pure Go resolver export GODEBUG=netdns=cgo # force cgo resolver api type Buffers func (v *Buffers) Read(p []byte) (n int, err error) func (v *Buffers) WriteTo(w io.Writer) (n int64, err error) //Conn是个interface, 面向流的. 有Read和Write方法. Conn对多个goroutine并发安全 type Conn func Dial(network, address string) (Conn, error) func DialTimeout(network, address string, timeout time.Duration) (Conn, error) //Listener是个接口 type Listener func FileListener(f *os.File) (ln Listener, err error) func Listen(network, address string) (Listener, error) //PacketConn是个接口, 面向datagram的, 有ReadFrom和WriteTo方法. type PacketConn func FilePacketConn(f *os.File) (c PacketConn, err error) func ListenPacket(network, address string) (PacketConn, error) //IPConn is the implementation of the Conn and PacketConn interfaces for IP network connections. //IPConn是Conn的一种实现 type IPConn func DialIP(network string, laddr, raddr *IPAddr) (*IPConn, error) func ListenIP(network string, laddr *IPAddr) (*IPConn, error) func (c *IPConn) Close() error func (c *IPConn) File() (f *os.File, err error) func (c *IPConn) LocalAddr() Addr func (c *IPConn) Read(b []byte) (int, error) func (c *IPConn) ReadFrom(b []byte) (int, Addr, error) func (c *IPConn) ReadFromIP(b []byte) (int, *IPAddr, error) func (c *IPConn) ReadMsgIP(b, oob []byte) (n, oobn, flags int, addr *IPAddr, err error) func (c *IPConn) RemoteAddr() Addr func (c *IPConn) SetDeadline(t time.Time) error func (c *IPConn) SetReadBuffer(bytes int) error func (c *IPConn) SetReadDeadline(t time.Time) error func (c *IPConn) SetWriteBuffer(bytes int) error func (c *IPConn) SetWriteDeadline(t time.Time) error func (c *IPConn) SyscallConn() (syscall.RawConn, error) func (c *IPConn) Write(b []byte) (int, error) func (c *IPConn) WriteMsgIP(b, oob []byte, addr *IPAddr) (n, oobn int, err error) func (c *IPConn) WriteTo(b []byte, addr Addr) (int, error) func (c *IPConn) WriteToIP(b []byte, addr *IPAddr) (int, error) type TCPConn func DialTCP(network string, laddr, raddr *TCPAddr) (*TCPConn, error) func (c *TCPConn) Close() error func (c *TCPConn) CloseRead() error func (c *TCPConn) CloseWrite() error func (c *TCPConn) File() (f *os.File, err error) func (c *TCPConn) LocalAddr() Addr func (c *TCPConn) Read(b []byte) (int, error) func (c *TCPConn) ReadFrom(r io.Reader) (int64, error) func (c *TCPConn) RemoteAddr() Addr func (c *TCPConn) SetDeadline(t time.Time) error func (c *TCPConn) SetKeepAlive(keepalive bool) error func (c *TCPConn) SetKeepAlivePeriod(d time.Duration) error func (c *TCPConn) SetLinger(sec int) error func (c *TCPConn) SetNoDelay(noDelay bool) error func (c *TCPConn) SetReadBuffer(bytes int) error func (c *TCPConn) SetReadDeadline(t time.Time) error func (c *TCPConn) SetWriteBuffer(bytes int) error func (c *TCPConn) SetWriteDeadline(t time.Time) error func (c *TCPConn) SyscallConn() (syscall.RawConn, error) func (c *TCPConn) Write(b []byte) (int, error) type TCPListener func ListenTCP(network string, laddr *TCPAddr) (*TCPListener, error) func (l *TCPListener) Accept() (Conn, error) func (l *TCPListener) AcceptTCP() (*TCPConn, error) func (l *TCPListener) Addr() Addr func (l *TCPListener) Close() error func (l *TCPListener) File() (f *os.File, err error) func (l *TCPListener) SetDeadline(t time.Time) error func (l *TCPListener) SyscallConn() (syscall.RawConn, error) type UDPConn func DialUDP(network string, laddr, raddr *UDPAddr) (*UDPConn, error) func ListenMulticastUDP(network string, ifi *Interface, gaddr *UDPAddr) (*UDPConn, error) func ListenUDP(network string, laddr *UDPAddr) (*UDPConn, error) func (c *UDPConn) Close() error func (c *UDPConn) File() (f *os.File, err error) func (c *UDPConn) LocalAddr() Addr func (c *UDPConn) Read(b []byte) (int, error) func (c *UDPConn) ReadFrom(b []byte) (int, Addr, error) func (c *UDPConn) ReadFromUDP(b []byte) (int, *UDPAddr, error) func (c *UDPConn) ReadMsgUDP(b, oob []byte) (n, oobn, flags int, addr *UDPAddr, err error) func (c *UDPConn) RemoteAddr() Addr func (c *UDPConn) SetDeadline(t time.Time) error func (c *UDPConn) SetReadBuffer(bytes int) error func (c *UDPConn) SetReadDeadline(t time.Time) error func (c *UDPConn) SetWriteBuffer(bytes int) error func (c *UDPConn) SetWriteDeadline(t time.Time) error func (c *UDPConn) SyscallConn() (syscall.RawConn, error) func (c *UDPConn) Write(b []byte) (int, error) func (c *UDPConn) WriteMsgUDP(b, oob []byte, addr *UDPAddr) (n, oobn int, err error) func (c *UDPConn) WriteTo(b []byte, addr Addr) (int, error) func (c *UDPConn) WriteToUDP(b []byte, addr *UDPAddr) (int, error) type UnixConn func DialUnix(network string, laddr, raddr *UnixAddr) (*UnixConn, error) func ListenUnixgram(network string, laddr *UnixAddr) (*UnixConn, error) func (c *UnixConn) Close() error func (c *UnixConn) CloseRead() error func (c *UnixConn) CloseWrite() error func (c *UnixConn) File() (f *os.File, err error) func (c *UnixConn) LocalAddr() Addr func (c *UnixConn) Read(b []byte) (int, error) func (c *UnixConn) ReadFrom(b []byte) (int, Addr, error) func (c *UnixConn) ReadFromUnix(b []byte) (int, *UnixAddr, error) func (c *UnixConn) ReadMsgUnix(b, oob []byte) (n, oobn, flags int, addr *UnixAddr, err error) func (c *UnixConn) RemoteAddr() Addr func (c *UnixConn) SetDeadline(t time.Time) error func (c *UnixConn) SetReadBuffer(bytes int) error func (c *UnixConn) SetReadDeadline(t time.Time) error func (c *UnixConn) SetWriteBuffer(bytes int) error func (c *UnixConn) SetWriteDeadline(t time.Time) error func (c *UnixConn) SyscallConn() (syscall.RawConn, error) func (c *UnixConn) Write(b []byte) (int, error) func (c *UnixConn) WriteMsgUnix(b, oob []byte, addr *UnixAddr) (n, oobn int, err error) func (c *UnixConn) WriteTo(b []byte, addr Addr) (int, error) func (c *UnixConn) WriteToUnix(b []byte, addr *UnixAddr) (int, error) type UnixListener func ListenUnix(network string, laddr *UnixAddr) (*UnixListener, error) func (l *UnixListener) Accept() (Conn, error) func (l *UnixListener) AcceptUnix() (*UnixConn, error) func (l *UnixListener) Addr() Addr func (l *UnixListener) Close() error func (l *UnixListener) File() (f *os.File, err error) func (l *UnixListener) SetDeadline(t time.Time) error func (l *UnixListener) SetUnlinkOnClose(unlink bool) func (l *UnixListener) SyscallConn() (syscall.RawConn, error) path path是个辅助包, 用于解析路径的, 基本上带斜杠的路径常用的方法都有. 针对URL地址的, 对unix路径也有效. windows路径无效. func Base(path string) string func Clean(path string) string func Dir(path string) string func Ext(path string) string func IsAbs(path string) bool func Join(elem ...string) string func Match(pattern, name string) (matched bool, err error) func Split(path string) (dir, file string) filepath filepath提供OS无关的路径, 正斜杠和反斜杠都支持. filepath提供基本的path方法, 还提供了一个很好的目录遍历方法: Walk, 对目录下的所有文件调用WalkFunc func Abs(path string) (string, error) func Base(path string) string func Clean(path string) string func Dir(path string) string func EvalSymlinks(path string) (string, error) func Ext(path string) string func FromSlash(path string) string func Glob(pattern string) (matches []string, err error) func HasPrefix(p, prefix string) bool func IsAbs(path string) bool func Join(elem ...string) string func Match(pattern, name string) (matched bool, err error) func Rel(basepath, targpath string) (string, error) func Split(path string) (dir, file string) func SplitList(path string) []string func ToSlash(path string) string func VolumeName(path string) string func Walk(root string, walkFn WalkFunc) error type WalkFunc encoding encoding下面有好几种encode方法, 但都能用到下面两对接口. 二进制 type BinaryMarshaler interface { MarshalBinary() (data []byte, err error) } type BinaryUnmarshaler interface { UnmarshalBinary(data []byte) error } UTF8 type TextMarshaler interface { MarshalText() (text []byte, err error) } type TextUnmarshaler interface { UnmarshalText(text []byte) error } encoding/gob gob提供在发送和接收双方传输二进制流的方法. 比较常用于rpc gob流是自解释的. 每个gob数据前面都有个类型, 是预定义好的. 在gob流中, 指针会被变成其指向的内容. 使用gob的时候, Encoder把本地变量变成gob流,Decoder把流数据还原到本地变量. 接收端和发送端不一定要完全一样. 结构体的成员按名字来匹配. 比如发送一个结构体: struct { A, B int } 那下面的形式都可以: struct { A, B int } // the same *struct { A, B int } // extra indirection of the struct struct { *A, **B int } // extra indirection of the fields struct { A, B int64 } // different concrete value type; see below 接受方可以是: struct { A, B int } // the same *struct { A, B int } // extra indirection of the struct struct { *A, **B int } // extra indirection of the fields struct { A, B int64 } // different concrete value type; see below 接收下面的形式是错误的: struct { A int; B uint } // change of signedness for B struct { A int; B float } // change of type for B struct { } // no field names in common struct { C, D int } // no field names in common 传输的过程不是很简单, 比如对整形来说, int是以变长方式传输的, 并不区分int32, int64等. string和byte切片, 数组和map, 都可以被Encode然后send. 也可以自定义Encode方法来发送自定义数据. api func Register(value interface{}) func RegisterName(name string, value interface{}) type CommonType type Decoder func NewDecoder(r io.Reader) *Decoder func (dec *Decoder) Decode(e interface{}) error func (dec *Decoder) DecodeValue(v reflect.Value) error type Encoder func NewEncoder(w io.Writer) *Encoder func (enc *Encoder) Encode(e interface{}) error func (enc *Encoder) EncodeValue(value reflect.Value) error type GobDecoder type GobEncoder 注意: type Decoder和type Encoder都是struct, 而不是interface 返回struct, 是要调用这个对象的方法. 而实现interface, 是实现一个派生类 入参都是e interface{}, 这是个万能对象. 可以是指针, 也可以是值. 看怎么用. 简单例子 package main import ( \"bytes\" \"encoding/gob\" \"fmt\" \"log\" ) type P struct { X, Y, Z int Name string } type Q struct { X, Y *int32 Name string } // This example shows the basic usage of the package: Create an encoder, // transmit some values, receive them with a decoder. func main() { // Initialize the encoder and decoder. Normally enc and dec would be // bound to network connections and the encoder and decoder would // run in different processes. var network bytes.Buffer // Stand-in for a network connection enc := gob.NewEncoder(&network) // Will write to network. dec := gob.NewDecoder(&network) // Will read from network. // Encode (send) some values. err := enc.Encode(P{3, 4, 5, \"Pythagoras\"}) if err != nil { log.Fatal(\"encode error:\", err) } err = enc.Encode(P{1782, 1841, 1922, \"Treehouse\"}) if err != nil { log.Fatal(\"encode error:\", err) } // Decode (receive) and print the values. var q Q err = dec.Decode(&q) if err != nil { log.Fatal(\"decode error 1:\", err) } fmt.Printf(\"%q: {%d, %d}\\n\", q.Name, *q.X, *q.Y) err = dec.Decode(&q) if err != nil { log.Fatal(\"decode error 2:\", err) } fmt.Printf(\"%q: {%d, %d}\\n\", q.Name, *q.X, *q.Y) } 结果 \"Pythagoras\": {3, 4} \"Treehouse\": {1782, 1841} 自定义encode和decode Vector里面有私有成员, gob无法直接访问, 必须通过Vector自己的方法. gob会调用encoding的MarshalBinary和UnmarshalBinary来完成这个任务. package main import ( \"bytes\" \"encoding/gob\" \"fmt\" \"log\" ) // The Vector type has unexported fields, which the package cannot access. // We therefore write a BinaryMarshal/BinaryUnmarshal method pair to allow us // to send and receive the type with the gob package. These interfaces are // defined in the \"encoding\" package. // We could equivalently use the locally defined GobEncode/GobDecoder // interfaces. type Vector struct { x, y, z int } func (v Vector) MarshalBinary() ([]byte, error) { // A simple encoding: plain text. var b bytes.Buffer fmt.Fprintln(&b, v.x, v.y, v.z) return b.Bytes(), nil } // UnmarshalBinary modifies the receiver so it must take a pointer receiver. //因为下面Decode()入参是Vector的地址, 所以这里要引用方式. func (v *Vector) UnmarshalBinary(data []byte) error { // A simple encoding: plain text. b := bytes.NewBuffer(data) _, err := fmt.Fscanln(b, &v.x, &v.y, &v.z) return err } // This example transmits a value that implements the custom encoding and decoding methods. func main() { var network bytes.Buffer // Stand-in for the network. // Create an encoder and send a value. enc := gob.NewEncoder(&network) err := enc.Encode(Vector{3, 4, 5}) if err != nil { log.Fatal(\"encode:\", err) } // Create a decoder and receive a value. dec := gob.NewDecoder(&network) var v Vector err = dec.Decode(&v) if err != nil { log.Fatal(\"decode:\", err) } fmt.Println(v) } 结果: {3 4 5} 传输interface对象 interface, channel等默认不能传输. 但可以注册interface的concrete 类型来传输. 比如: Point是个struct, 可以直接gob; 但Pythagoras是interface, 要传输它必须要注册它对应的concrete类型: gob.Register(Point{}) package main import ( \"bytes\" \"encoding/gob\" \"fmt\" \"log\" \"math\" ) type Point struct { X, Y int } func (p Point) Hypotenuse() float64 { return math.Hypot(float64(p.X), float64(p.Y)) } type Pythagoras interface { Hypotenuse() float64 } // This example shows how to encode an interface value. The key // distinction from regular types is to register the concrete type that // implements the interface. func main() { var network bytes.Buffer // Stand-in for the network. // We must register the concrete type for the encoder and decoder (which would // normally be on a separate machine from the encoder). On each end, this tells the // engine which concrete type is being sent that implements the interface. gob.Register(Point{}) // Create an encoder and send some values. enc := gob.NewEncoder(&network) for i := 1; i encoding/binary binary提供对字节序列到任意格式的转换. 比如下面的例子, 把一个字节序列, \"读\"到一个结构体里面. func main() { b := []byte{0x18, 0x2d, 0x44, 0x54, 0xfb, 0x21, 0x09, 0x40, 0xff, 0x01, 0x02, 0x03, 0xbe, 0xef} r := bytes.NewReader(b) var data struct { PI float64 Uate uint8 Mine [3]byte Too uint16 } //对字符数组来说, 大小端没关系. 但对uint16来说, 要知道大小端才知道如何解读. if err := binary.Read(r, binary.LittleEndian, &data); err != nil { fmt.Println(\"binary.Read failed:\", err) } fmt.Println(data.PI) fmt.Println(data.Uate) fmt.Printf(\"% x\\n\", data.Mine) fmt.Println(data.Too) } //输出. 3.141592653589793 255 01 02 03 61374 下面是个写的例子: func main() { buf := new(bytes.Buffer) var data = []interface{}{ uint16(61374), int8(-54), uint8(254), } for _, v := range data { err := binary.Write(buf, binary.LittleEndian, v) if err != nil { fmt.Println(\"binary.Write failed:\", err) } } fmt.Printf(\"%x\", buf.Bytes()) } varint varint是种对int的变长存储策略, 越小的数, 用越少的字节存储. time 底层OS一般提供 墙上(wall clock)时间: 用来报告时间, 表示绝对时间; 墙上时间可能由于设定时间等操作, 造成跳变. 单一(monotonic)时间: 用来测量时间, 表示相对时间 time.Now()返回的就是Time类型的值, 这个值既包括墙上时间, 又包括单一时间. 下面的代码保证即使墙上时间跳变了, 比如时间被校准了, 但相对时间elapsed是实际的delta时间. start := time.Now() ... operation that takes 20 milliseconds ... t := time.Now() elapsed := t.Sub(start) 在api中, 应该说有相对概念的api, 比如Time.sub, Time.before等等, 都用的是单一时间. 相对时间只对当前的进程有效. Time 首先, Time是个结构体; 但其内部结构对外是不可见的. 我们知道的, 只是Time是对绝对时间的表达, 精度是ns Time是值, 不是指针. Time的0值是1年1月1号0点 比较符号==可以比较两个Time, 但推荐用t1.Equal(t2), 后者更精确. type Time struct { // contains filtered or unexported fields } Time有如下方法: func Date(year int, month Month, day, hour, min, sec, nsec int, loc *Location) Time 比如: t := time.Date(2009, time.November, 10, 23, 0, 0, 0, time.UTC), 这样初始化一个time 有Now(), 有从字符串解析时间的Parse()方法. 有Add(), After(), Before() 还有给定Time t, 有解析到Date, Day的方法 比如: d := time.Date(2000, 2, 1, 12, 30, 0, 0, time.UTC) year, month, day := d.Date() fmt.Printf(\"year = %v\\n\", year) fmt.Printf(\"month = %v\\n\", month) fmt.Printf(\"day = %v\\n\", day) 还有把Time转为json格式的方法. 时间格式和单位 const ( ANSIC = \"Mon Jan _2 15:04:05 2006\" UnixDate = \"Mon Jan _2 15:04:05 MST 2006\" RubyDate = \"Mon Jan 02 15:04:05 -0700 2006\" RFC822 = \"02 Jan 06 15:04 MST\" RFC822Z = \"02 Jan 06 15:04 -0700\" // RFC822 with numeric zone RFC850 = \"Monday, 02-Jan-06 15:04:05 MST\" RFC1123 = \"Mon, 02 Jan 2006 15:04:05 MST\" RFC1123Z = \"Mon, 02 Jan 2006 15:04:05 -0700\" // RFC1123 with numeric zone RFC3339 = \"2006-01-02T15:04:05Z07:00\" RFC3339Nano = \"2006-01-02T15:04:05.999999999Z07:00\" Kitchen = \"3:04PM\" // Handy time stamps. Stamp = \"Jan _2 15:04:05\" StampMilli = \"Jan _2 15:04:05.000\" StampMicro = \"Jan _2 15:04:05.000000\" StampNano = \"Jan _2 15:04:05.000000000\" ) //这些时间都是以ns为单位的 const ( Nanosecond Duration = 1 Microsecond = 1000 * Nanosecond Millisecond = 1000 * Microsecond Second = 1000 * Millisecond Minute = 60 * Second Hour = 60 * Minute ) Duration Duration是个时间delta, type Duration int64, 时间单位是ns, 是64位的int, 最大能表示290年. 比如 t0 := time.Now() expensiveCall() t1 := time.Now() fmt.Printf(\"The call took %v to run.\\n\", t1.Sub(t0)) Duration的值有几个方法, 比如 d Duration, d.Hours, d.Senconds等, 把Duration转换为相应的单位.其String方法返回类似的字符串: \"72h3m0.5s\" api func Now() Time : 返回当前local时间 func After(d Duration) : 提供一个简单的超时机制, 它返回一个channel, 超时后可读. 超时后可以被垃圾回收. var c chan int func handle(int) {} func main() { select { case m := time.Sleep(100 * time.Millisecond) : sleep time.Tick 返回一个channel, 这个channel不会close, 一直能读到Time, 它是对NewTicker的封装 比如下面的代码一直会打印, 每5秒一次. 这个是不是和while 1里面sleep差不多. func main() { c := time.Tick(5 * time.Second) for now := range c { fmt.Printf(\"%v %s\\n\", now, statusUpdate()) } } func Since(t Time) Duration : 和time.Now().Sub(t)一样. 从t到现在的Duration Ticker type Ticker struct { //C是给外部用的channel C 使用func NewTicker(d Duration) *Ticker来获得一个Ticker实例 Ticker是周期的tick, 有NewTicker(), Stop()等方法. Timer type Timer struct { //外部可以用C这个channel C 使用func NewTimer(d Duration) *Timer先生成一个Timer对象 和Ticker不同的是, Timer是一次性的事件. 有NewTimer() Stop等方法. context //Context是个interface type Context interface { Deadline() (deadline time.Time, ok bool) //这是个空结构体的channel Done() 顾名思义, 上下文. 提供通用的deadline, cancel等API, 用于多个go routine之间协作的. context不应该被包含在任何struct里, 而是应该直接传入函数, 通常应该是第一个参数, 叫ctx. 一个context可能会被多个go routine使用, context保证并发安全. context对象都有个Done()方法, 返回一个channel. 读这个channel就知道是否有人或有事件通知我们该结束了. 通常在服务端收到一个request时, 会新开一个goroutine来响应这个请求, 我们称其为这个请求的主routine;通常这个主routine会再开其他的goroutine用来访问数据库或者其他RPC服务, 这些子routine都会用到request的相关user的token等身份数据. 当主routine认为超时的时候, 这些子routine应该尽快结束. 所以是主routine创建Context, 设置超时时间, 把Context传入每个子routine; 主routine检测到超时, 会调用Context的Cancel函数, 触发Done的通道关闭; 子routine需要在代码里检测Done通道, 如果关闭了就马上退出. 这是一种主routine\"通知\"子routine退出的方法. api func Background() Context : 返回一个空的ctx变量 整数发生器 generator func WithCancel(parent Context) (ctx Context, cancel CancelFunc) 返回一个Context变量ctx, ctx有个Done的channel 返回的第二个值是CancelFunc函数, 用于关闭ctx.Done 举例: 下面的例子, main调用gen(), 后者返回个channel, 并起个 go routine不断的产生整数, 写入这个channel. 这个go routine还监视ctx.Done(), 这是ctx提供的channel, 能读到东西说明别人告诉他事情做完了. package main import ( \"context\" \"fmt\" ) func main() { // gen generates integers in a separate goroutine and // sends them to the returned channel. // The callers of gen need to cancel the context once // they are done consuming generated integers not to leak // the internal goroutine started by gen. gen := func(ctx context.Context) Context适合用于main通知子routine结束; 而WaitGroup机制是子routine通知main完成, main可以结束等待. 带超时的Context func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) 和WithCancel类似, 返回一个以time.Time为deadline的ctx. 如果时间到了, 或者ctx的CancelFunc被调用了, 或者这个Done channel本身被close了, 那这个ctx的Done channel会close. 举例: 下面的例子设置超时时间d为现在开始后50ms, 并以d初始化ctx. 然后用defer关键词执行cancel(). 注意, 不管哪种情况, 在函数退出的时候执行cancel都是没坏处的. 然后select会阻塞, 等1秒或者是等超时, 很明显, 是超时. package main import ( \"context\" \"fmt\" \"time\" ) func main() { d := time.Now().Add(50 * time.Millisecond) ctx, cancel := context.WithDeadline(context.Background(), d) // Even though ctx will be expired, it is good practice to call its // cancellation function in any case. Failure to do so may keep the // context and its parent alive longer than necessary. defer cancel() select { case 思考: 如果是用c写一个带超时的等待, 该怎么写呢? WithTimeout方法是WithDeadline的简化版: 即WithTimeout = WithDeadline(parent, time.Now().Add(timeout)) syscall syscall包装了很多底层OS系统调用 但只有一部分? 比如没有ioctl? runtime runtime包提供对go的runtime系统访问的功能 runtime相关的环境变量 GOGC: 新分配数据和老数据的比率. 默认是100. 即新老数据一样多的时候, 触发一次垃圾回收.这个比率越大, gc被触发次数越少, 比如说调高到500 SetGCPercent函数和GOGC环境变量作用一样, 但可以在运行时调用. GODEBUG: 里面有控制相关模块的debug的开关, 比如 gc, cgo, 调度, 内存分析等.见笔记go调试 GODEBUG=schedtrace=10000,scheddetail=1 GOMAXPROCS: 控制多少个goroutine可以并行跑 另外还有关于调用栈的, 关于竞争的环境变量 api runtime很强大, 它暴露了很多go的细节. 比如 #执行断点 func Breakpoint() #调用goroutine退出, 其他不受影响 func Goexit() #触发一次调度 func Gosched() #绑定goroutine到底层线程 func LockOSThread() #有MemProfile和CPUProfile等检查性能的函数, 但推荐普通用户使用 runtime/pprof #可用cpu数 func NumCPU() int #还有api可以获得goroutine的个数, 调用cgo的次数等 //...省略一些高端api #返回版本号 func Version() string #调用栈 Callers CallersFrames runtime/pprof 管性能分析的. runtime/debug 顾名思义, 有打印调用栈的, 有调节gc的等等. func FreeOSMemory() func PrintStack() func ReadGCStats(stats *GCStats) func SetGCPercent(percent int) int func SetMaxStack(bytes int) int func SetMaxThreads(threads int) int func SetPanicOnFault(enabled bool) bool func SetTraceback(level string) func Stack() []byte func WriteHeapDump(fd uintptr) type BuildInfo func ReadBuildInfo() (info *BuildInfo, ok bool) type GCStats type Module runtime/trace 和ftrace概念类似, 在关键点上打桩. os os包提供平台无关的接口. 它的错误处理方式是go的经典方式: file, err := os.Open(\"file.go\") // For read access. if err != nil { log.Fatal(err) } 提供的接口包括 Chdir Chmod Link Mkdir Pipe Remove TempDir Rename Expand环境变量等 Getpid, ppid等 IsExist IsTimeout等 文件类 type File Create NewFile Open 用于创建File对象 注意, Open()返回只读的File对象, OpenFile()可以指定打开模式, 和C一样. 读 写 Seek Sync Close 进程类 type Process 按pid查找进程 开始进程 Kill Release Signal Wait等 定义了错误类型 PathError LinkError SyscallError package main import ( \"log\" \"os\" ) func main() { f, err := os.OpenFile(\"notes.txt\", os.O_RDWR|os.O_CREATE, 0755) if err != nil { log.Fatal(err) } if err := f.Close(); err != nil { log.Fatal(err) } } os/exec 用于执行外部程序; 只能在linux上跑 和c里面的system不太一样, exec和c的exec更像, 比system简单. Run和Start的区别是: Run要等待命令返回, 而Start只管开始, 不等待返回, 要和Wait连用. package main import ( \"log\" \"os/exec\" ) func main() { //一般的模式是先生成一个cmd对象. cmd := exec.Command(\"sleep\", \"1\") log.Printf(\"Running command and waiting for it to finish...\") //后面都用这个cmd对象来操作 err := cmd.Run() log.Printf(\"Command finished with error: %v\", err) } os/signal 用来处理go的signal SIGKILL and SIGSTOP不能被捕获, 所以这里的东西都管不了它们 同步的signal, 一般是SIGBUS, SIGFPE, and SIGSEGV, 是由正在执行的go程序引起的 在go里, 这些signal被转换为运行时的panic 剩下的signal, 是其他进程异步通知的signal, 用这里的接口处理 默认行为: By default, a synchronous signal is converted into a run-time panic. A SIGHUP, SIGINT, or SIGTERM signal causes the program to exit. A SIGQUIT, SIGILL, SIGTRAP, SIGABRT, SIGSTKFLT, SIGEMT, or SIGSYS signal causes the program to exit with a stack dump. A SIGTSTP, SIGTTIN, or SIGTTOU signal gets the system default behavior (these signals are used by the shell for job control). The SIGPROF signal is handled directly by the Go runtime to implement runtime.CPUProfile. Other signals will be caught but no action will be taken. cgo 一般go程序在启动的时候, runtime会先安装默认的sighandler, cgo如果自己有处理signal的需求, 要用SA_ONSTACK标记. 否则程序会crash 这种情况要当心, 详见: https://golang.google.cn/pkg/os/signal/#hdr-Go_programs_that_use_cgo_or_SWIG go作为so 当go编译为-buildmode=c-shared时, 一般调用这个so的非go程序, 已经安装了sighandler, 那go runtime有相关处理. https://golang.google.cn/pkg/os/signal/#hdr-Non_Go_programs_that_call_Go_code 方法 func Ignore(sig ...os.Signal) func Ignored(sig os.Signal) bool func Notify(c chan 比如 package main import ( \"fmt\" \"os\" \"os/signal\" ) func main() { // Set up channel on which to send signal notifications. // We must use a buffered channel or risk missing the signal // if we're not ready to receive when the signal is sent. c := make(chan os.Signal, 1) // 在os包中, var Interrupt Signal = syscall.SIGINT signal.Notify(c, os.Interrupt) // Block until a signal is received. s := os/user 提供用户name id等查询的功能. builtin builtin里预定义了golang的常用函数和类型 比如 函数 append cap close copy delete len make new panic recover print 类型 bool byte error int u/int8/16/32/64 float32/64 rune string uintptr strings strings和bytes的方法差不多, 不同在于 strings是UTF8编码, 变长的 bytes是C里的字符数组 举例: Builder是strings包提供的一个struct, 实现了Write方法. 它尽量少copy package main import ( \"fmt\" \"strings\" ) func main() { var b strings.Builder for i := 3; i >= 1; i-- { fmt.Fprintf(&b, \"%d...\", i) } b.WriteString(\"ignition\") fmt.Println(b.String()) } //输出 3...2...1...ignition bytes bytes可以有 两个byte切片可以比较: func Compare(a, b []byte) int 可以判断是否有子串: Contains 子串计数: Count 字串位置: Index 相等: Equal 前后缀: HasPrefix HasSuffix 字符串逐个map: Map 重复N次字符串: Repeat 替换: Replace 分割: Split 大小写转换: ToUpper ToLower 去掉空白: Trim 连接: Join Buffer Buffer是个struct, 是动态大小的byte切片. 有Read和Write方法, 符合io.Reaer和io.Writer NewBuffer返回一个Buffer对象, 但通常new(Buffer)或者var b bytes.Buffer就够实例化一个Buffer了. Buffer的所有方法, 都是引用方式. 比如: func (b *Buffer) Read(p []byte) (n int, err error) func (b *Buffer) Write(p []byte) (n int, err error) log/syslog 虽然这个包已经停止维护, 但它提供了一个参考设计 这个包通过unix socket, 发送记录到syslog守护进程; 只需要调用一次Dial方法来连接到syslog守护进程, 如果有问题, 会自动重连. package main import ( \"fmt\" \"log\" \"log/syslog\" ) func main() { sysLog, err := syslog.Dial(\"tcp\", \"localhost:1234\", syslog.LOG_WARNING|syslog.LOG_DAEMON, \"demotag\") if err != nil { log.Fatal(err) } fmt.Fprintf(sysLog, \"This is a daemon warning with demotag.\") sysLog.Emerg(\"And this is a daemon emergency with demotag.\") } 新的第三方的syslog更好用 log log包实现了一个logger类型, 并提供基础的log功能. log还提供了一个\"标准\"的logger, 用来记录log到stdout log给每个message都加换行. Fatal()会调用os.Exit() Panic()会调用内置函数panic() Print()家族函数用来打印到log type logger logger是个struct, 提供在io.Writer基础上的log功能. 它不是个interface, 并不抽象, 它是实实在在的一个对象. 用func New(out io.Writer, prefix string, flag int) *Logger来实例化一个logger 这个New很精髓, 实例化一个对象来对外提供功能. package main import ( \"bytes\" \"fmt\" \"log\" ) func main() { var ( buf bytes.Buffer logger = log.New(&buf, \"logger: \", log.Lshortfile) ) logger.Print(\"Hello, log file!\") fmt.Print(&buf) } logger支持多种格式, 用flag来配置 比如Ldate | Ltime | Lmicroseconds | Llongfile产生这样的打印 2009/01/23 01:23:23.123123 /a/b/c/d.go:23: message 我们注意到, logger的一个功能就是打印当前的文件名和行数 有个Output()函数, 地一个参数是skip多少级的意思, 因为打印当前行, 也就是打印语句本身所在的行, 并没有什么实际意义. 这和callstack函数会skip几个层级道理一样. fmt 与python不同的是, 字符串格式化不是语言级别的. 在go里, 用fmt包提供这类功能. 方法 比较常用的有 //接受格式化字符串, 返回error对象 func Errorf(format string, a ...interface{}) error //Fprintf/Fprintln家族, 输出到io.Writer func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) //格式化输入, 输入源是io.Reader func Fscanf(r io.Reader, format string, a ...interface{}) (n int, err error) //去掉F的是 标准输入输出 Printf/Scanf Stringer类型 Stringer类型的String方法是默认的打印方法, 有String方法的类型都能被Print家族打印, 不用显式给出格式化串. type Stringer interface { String() string } package main import ( \"fmt\" ) // Animal has a Name and an Age to represent an animal. type Animal struct { Name string Age uint } // String makes Animal satisfy the Stringer interface. func (a Animal) String() string { return fmt.Sprintf(\"%v (%d)\", a.Name, a.Age) } func main() { a := Animal{ Name: \"Gopher\", Age: 2, } fmt.Println(a) } 格式化标志符 通用 %v the value in a default format when printing structs, the plus flag (%+v) adds field names %#v a Go-syntax representation of the value %T a Go-syntax representation of the type of the value %% a literal percent sign; consumes no value 布尔值 %t the word true or false 整型 %b base 2 %c the character represented by the corresponding Unicode code point %d base 10 %o base 8 %O base 8 with 0o prefix %q a single-quoted character literal safely escaped with Go syntax. %x base 16, with lower-case letters for a-f %X base 16, with upper-case letters for A-F %U Unicode format: U+1234; same as \"U+%04X\" 浮点 %b decimalless scientific notation with exponent a power of two, in the manner of strconv.FormatFloat with the 'b' format, e.g. -123456p-78 %e scientific notation, e.g. -1.234456e+78 %E scientific notation, e.g. -1.234456E+78 %f decimal point but no exponent, e.g. 123.456 %F synonym for %f %g %e for large exponents, %f otherwise. Precision is discussed below. %G %E for large exponents, %F otherwise %x hexadecimal notation (with decimal power of two exponent), e.g. -0x1.23abcp+20 %X upper-case hexadecimal notation, e.g. -0X1.23ABCP+20 字符串 %s the uninterpreted bytes of the string or slice %q a double-quoted string safely escaped with Go syntax %x base 16, lower-case, two characters per byte %X base 16, upper-case, two characters per byte 切片 %p address of 0th element in base 16 notation, with leading 0x 指针 %p base 16 notation, with leading 0x The %b, %d, %o, %x and %X verbs also work with pointers, formatting the value exactly as if it were an integer. 默认格式化 bool: %t int, int8 etc.: %d uint, uint8 etc.: %d, %#x if printed with %#v float32, complex64, etc: %g string: %s chan: %p pointer: %p struct: {field0 field1 ...} array, slice: [elem0 elem1 ...] maps: map[key1:value1 key2:value2 ...] pointer to above: &{}, &[], &map[] 宽度 精度 对齐 %f default width, default precision %9f width 9, default precision %.2f default width, precision 2 %9.2f width 9, precision 2 %9.f width 9, precision 0 + always print a sign for numeric values; guarantee ASCII-only output for %q (%+q) - pad with spaces on the right rather than the left (left-justify the field) # alternate format: add leading 0b for binary (%#b), 0 for octal (%#o), 0x or 0X for hex (%#x or %#X); suppress 0x for %p (%#p); for %q, print a raw (backquoted) string if strconv.CanBackquote returns true; always print a decimal point for %e, %E, %f, %F, %g and %G; do not remove trailing zeros for %g and %G; write e.g. U+0078 'x' if the character is printable for %U (%#U). ' ' (space) leave a space for elided sign in numbers (% d); put spaces between bytes printing strings or slices in hex (% x, % X) 0 pad with leading zeros rather than spaces; for numbers, this moves the padding after the sign 举例 fmt格式的详细例子, 在golang的doc上都有: https://golang.org/pkg/fmt/#example__formats scanf package main import ( \"fmt\" \"os\" \"strings\" ) func main() { var ( i int b bool s string ) r := strings.NewReader(\"5 true gophers\") n, err := fmt.Fscanf(r, \"%d %t %s\", &i, &b, &s) if err != nil { fmt.Fprintf(os.Stderr, \"Fscanf: %v\\n\", err) } fmt.Println(i, b, s) fmt.Println(n) } bufio bufio是在io层之上的, 提供带buffer的io New家族的方法有 使用io.Reader初始化一个bufio.Reader func NewReader(rd io.Reader) *Reader 使用io.Writer初始化一个bufio.Writer func NewWriter(w io.Writer) *Writer 使用bufio的Reader和Writer创建一个ReadWriter func NewReadWriter(r *Reader, w *Writer) *ReadWriter error对象 这个模式在go里很常见: 把错误定义为变量, 错误用error.New创建, 可以被当作error返回. var ( ErrInvalidUnreadByte = errors.New(\"bufio: invalid use of UnreadByte\") ErrInvalidUnreadRune = errors.New(\"bufio: invalid use of UnreadRune\") ErrBufferFull = errors.New(\"bufio: buffer full\") ErrNegativeCount = errors.New(\"bufio: negative count\") ) Reader的方法 bufio的Reader, 可以 看当前buffer的可读取byte数: Buffered 可以丢弃当前buffer的n个字节: Discard 临时读n个byte: Peek 调用底层io.Reader, 读一次: Read 读一个字节: ReadByte 读n个字节, 直到分隔符: func (b *Reader) ReadBytes(delim byte) ([]byte, error) 读一行: ReadLine 读一个符文: ReadRune 读出一个切片, 直到某个字符: ReadSlice 读出一个字符串, 直到某个字符: ReadString 重置buffer: Reset 大小: Size 取消上次读: UnreadByte 写到别处: WriteTo Writer的方法 有多少个字节在buffer里未被使用: Available 已经用了多少: Buffered 从一个Reader里读: ReadFrom 重置buffer: Reset 写字符切片, 写单个字符, 写单个符文, 写utf8字符串 Scanner的方法 bufio提供了对文件遍历的方案: 按行(默认行为), 按空白字符, 按指定的rune, 按制定的byte等方式. Scanner的方法 这里的Scanner是个很好的迭代器的参考 New: func NewScanner(r io.Reader) *Scanner 常用的方法: Scan: 根据split的定义, 内部保存本次token, 返回是否结束. Text或者Bytes : 获取最后一次Scan到的token 我觉得其实Scan可以返回两个值, 第一个是token, 第二个才是表示结束的bool值. 那么for scanner.Scan() {...} 就可以写成for token, finish := scanner.Scan(); finish != ture; token, finish := scanner.Scan(); {...}, 不是省掉了scanner.Text()调用? 似乎人家这么设计是有原因的, 可能和go没有do while结构有关. SplitFunc方法 用来定义分割符的 type SplitFunc func(data []byte, atEOF bool) (advance int, token []byte, err error) 被Scan调用. data是Scan传给SplitFunc的数据, 是剩下未被扫描的字符数组, 和一个底层io.Reader是否已经是EOF的标记. 返回的avance是本次在buffer里前进了多少个字节, token是本次扫描到的token. 按行读 package main import ( \"bufio\" \"fmt\" \"os\" ) func main() { scanner := bufio.NewScanner(os.Stdin) //这里像个迭代器, 实际是每次for都调用一次Scan, Scan返回这次的token, 即被分割后的字符 //用Text或者Bytes方法返回token. for scanner.Scan() { fmt.Println(scanner.Text()) // Println will add back the final '\\n' } if err := scanner.Err(); err != nil { fmt.Fprintln(os.Stderr, \"reading standard input:\", err) } } 统计字符个数 package main import ( \"bufio\" \"fmt\" \"os\" \"strings\" ) func main() { // An artificial input source. const input = \"Now is the winter of our discontent,\\nMade glorious summer by this sun of York.\\n\" scanner := bufio.NewScanner(strings.NewReader(input)) // Set the split function for the scanning operation. scanner.Split(bufio.ScanWords) // Count the words. count := 0 for scanner.Scan() { count++ } if err := scanner.Err(); err != nil { fmt.Fprintln(os.Stderr, \"reading input:\", err) } fmt.Printf(\"%d\\n\", count) } 也可以自定义分割函数 package main import ( \"bufio\" \"fmt\" \"strconv\" \"strings\" ) func main() { // An artificial input source. const input = \"1234 5678 1234567901234567890\" scanner := bufio.NewScanner(strings.NewReader(input)) // Create a custom split function by wrapping the existing ScanWords function. split := func(data []byte, atEOF bool) (advance int, token []byte, err error) { advance, token, err = bufio.ScanWords(data, atEOF) if err == nil && token != nil { _, err = strconv.ParseInt(string(token), 10, 32) } return } // Set the split function for the scanning operation. scanner.Split(split) // Validate the input for scanner.Scan() { fmt.Printf(\"%s\\n\", scanner.Text()) } if err := scanner.Err(); err != nil { fmt.Printf(\"Invalid input: %s\", err) } } io/ioutil 包括了ReadAll ReadDir ReadFile WriteFile等方法. 还包括一个/dev/null对象: var Discard io.Writer = devNull(0) func NopCloser(r io.Reader) io.ReadCloser func ReadAll(r io.Reader) ([]byte, error) func ReadDir(dirname string) ([]os.FileInfo, error) func ReadFile(filename string) ([]byte, error) func TempDir(dir, prefix string) (name string, err error) func TempFile(dir, pattern string) (f *os.File, err error) func WriteFile(filename string, data []byte, perm os.FileMode) error io io包提供底层的io原语, 主要的工作是对os包的一层封装, 提供接口的抽象 变量 io包里, 定义了一些变量. 是io包持有的一些对象. 主要是一些错误的对象, 其他程序可以直接用. 比如: var EOF = errors.New(\"EOF\") var ErrClosedPipe = errors.New(\"io: read/write on closed pipe\") ... 以Reader为例, 看golang的派生 io里面的方法和结构, 充分展示了golang实现基类和派生类的方式. C++的派生是靠血缘方式, 家族式的 Go的派生是党派方式, 只要你的方法和我的一样, 就是自己人. Reader, 可以认为是个基类 比如, 只要实现了Read方法的类型, 都是Reader: //read到p里, 最大len(p) type Reader interface { Read(p []byte) (n int, err error) } 注: Read, 就是从什么地方, 把data读出来, 放到一个buf里. 和C的read一样, 用户要传入一个buf, 在这里是个切片p []byte, go的切片自带大小, 所以不用传size Read不用等到到最大size, 即len(p)才返回. 只要这次的data到位了, 就返回. Read的实现中, 不能持有p, 即不能对p增加引用. PipeReader, 派生类 Reader的派生类, PipeReader实现了Read方法. 所以虽然形式上PipeReader是个struct, 而Reader是interface, 但因为interface可以是任何东西, 有点类似C语言的void * 所以, PipeReader也是Reader. 所有Reader能适用的地方, PipeReader一样适用. type PipeReader struct { // contains filtered or unexported fields } func (r *PipeReader) Read(data []byte) (n int, err error) Writer和Reader类似, 它有派生类: PipeWriter StringWriter ByteWriter等等 ReadWriter ReadWriter是Reader和Writer的组合 type ReadWriter interface { Reader Writer } WriterTo 一个实现了Reader的类型, 也可以实现WriteTo方法, 从而也是WriterTo 定义这个WriterTo类型, 类型断言会用到. type WriterTo interface { WriteTo(w Writer) (n int64, err error) } ReaderAt 带offset的Reader, 是个接口. 其他基类 Closer: 包装了Close方法的interfacetype Closer interface { Close() error } 从C的概念来的, 指定下次的读写从哪里开始 type Seeker interface { Seek(offset int64, whence int) (int64, error) } 等等 Seeker Seeker决定下次读写的offset. whence表示offset的相对位置, 可以相对文件开头, 当前, 和结尾. type Seeker interface { Seek(offset int64, whence int) (int64, error) } io包的方法 Copy func Copy(dst Writer, src Reader) (written int64, err error) 举例, 这里只是演示, 把一段字符串, copy到标准输出. 因为标准输出也是一个Writer package main import ( \"io\" \"log\" \"os\" \"strings\" ) func main() { r := strings.NewReader(\"some io.Reader stream to be read\\n\") if _, err := io.Copy(os.Stdout, r); err != nil { log.Fatal(err) } } Copy的实现: 上面提到过, WriterTo和Reader可以是同一个类型, 在Copy的实现中, 有这样的类型断言 // copyBuffer is the actual implementation of Copy and CopyBuffer. // if buf is nil, one is allocated. func copyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error) { // If the reader has a WriteTo method, use it to do the copy. // Avoids an allocation and a copy. if wt, ok := src.(WriterTo); ok { return wt.WriteTo(dst) } // Similarly, if the writer has a ReadFrom method, use it to do the copy. if rt, ok := dst.(ReaderFrom); ok { return rt.ReadFrom(src) } if buf == nil { //默认32K的buf buf = make([]byte, 32*1024) } for { nr, er := src.Read(buf) if nr > 0 { //这里直接传入切片 buf[0:nr] nw, ew := dst.Write(buf[0:nr]) if nw > 0 { written += int64(nw) } if ew != nil { err = ew break } if nr != nw { err = ErrShortWrite break } } if er == EOF { break } if er != nil { err = er break } } return written, err } Pipe Pipe返回一个管道对, 一个用来读, 一个用来写 func Pipe() (*PipeReader, *PipeWriter) 举例, 创建一对读写pipe, 一个go routine写, 主程序读. package main import ( \"bytes\" \"fmt\" \"io\" ) func main() { r, w := io.Pipe() go func() { fmt.Fprint(w, \"some text to be read\\n\") w.Close() }() buf := new(bytes.Buffer) buf.ReadFrom(r) fmt.Print(buf.String()) } TeeReader package main import ( \"bytes\" \"fmt\" \"io\" \"io/ioutil\" \"log\" \"strings\" ) func main() { r := strings.NewReader(\"some io.Reader stream to be read\\n\") var buf bytes.Buffer tee := io.TeeReader(r, &buf) printall := func(r io.Reader) { b, err := ioutil.ReadAll(r) if err != nil { log.Fatal(err) } fmt.Printf(\"%s\", b) } printall(tee) printall(&buf) } //输出: some io.Reader stream to be read some io.Reader stream to be read container/ring ring是个循环链表. 里面的任何一个元素都可以被用来引用这个ring. 所以不像链表有元素和list两个概念, ring只有一个类型: Ring api type Ring struct { Value interface{} // for use by client; untouched by this library // contains filtered or unexported fields } type Ring func New(n int) *Ring //对每个元素执行f, f不能改变ring本身 func (r *Ring) Do(f func(interface{})) func (r *Ring) Len() int //链接连个ring func (r *Ring) Link(s *Ring) *Ring func (r *Ring) Move(n int) *Ring func (r *Ring) Next() *Ring func (r *Ring) Prev() *Ring func (r *Ring) Unlink(n int) *Ring 例子: 连接两个ring package main import ( \"container/ring\" \"fmt\" ) func main() { // Create two rings, r and s, of size 2 r := ring.New(2) s := ring.New(2) // Get the length of the ring lr := r.Len() ls := s.Len() // Initialize r with 0s for i := 0; i container/list 实现了一个双向链表 对这个链表的遍历: for e := l.Front(); e != nil; e = e.Next() { // do something with e.Value } api //这是个struct, Value并不关心具体的数据类型, 只是以万能类型interface存储. type Element struct { // The value stored with this element. Value interface{} // contains filtered or unexported fields } type Element func (e *Element) Next() *Element func (e *Element) Prev() *Element //也是个struct type List func New() *List func (l *List) Back() *Element func (l *List) Front() *Element func (l *List) Init() *List func (l *List) InsertAfter(v interface{}, mark *Element) *Element func (l *List) InsertBefore(v interface{}, mark *Element) *Element func (l *List) Len() int func (l *List) MoveAfter(e, mark *Element) func (l *List) MoveBefore(e, mark *Element) func (l *List) MoveToBack(e *Element) func (l *List) MoveToFront(e *Element) func (l *List) PushBack(v interface{}) *Element func (l *List) PushBackList(other *List) func (l *List) PushFront(v interface{}) *Element func (l *List) PushFrontList(other *List) func (l *List) Remove(e *Element) interface{} 使用举例 package main import ( \"container/list\" \"fmt\" ) func main() { // Create a new list and put some numbers in it. l := list.New() e4 := l.PushBack(4) e1 := l.PushFront(1) l.InsertBefore(3, e4) l.InsertAfter(2, e1) // Iterate through list and print its contents. for e := l.Front(); e != nil; e = e.Next() { fmt.Println(e.Value) } } container/heap 参考: https://golang.org/pkg/container/heap/ heap是个树, 每个node的值都是它的子树的\"最小值\". 所以根节点是最小的. import \"container/heap\" type Interface interface { sort.Interface Push(x interface{}) // add x as element Len() Pop() interface{} // remove and return element Len() } 注意, 这里的\"最小值\"的英文表述为\"minimum-valued\" node, 何为最小值? 由heap.Interface里面的sort.Interface说了算. 就是说, heap包的逻辑是, 按照sort的方法, 做成一个最小树. 实现了heap.Interface的任何type都可以是heap Push把一个元素排序后放入树中. Pop把树中的一个元素返回. Less()方法决定返回哪个元素. 树的数组表达 节点在数组中的位置对应它在树中的位置,下标为0 的节点为根节点,下标为1是根的左节点,2为根节点的右节点,依次类推,从左到右的顺序存储树的每一层,包括空节点. min-int树 // This example demonstrates an integer heap built using the heap interface. package main import ( \"container/heap\" \"fmt\" ) // An IntHeap is a min-heap of ints. type IntHeap []int func (h IntHeap) Len() int { return len(h) } func (h IntHeap) Less(i, j int) bool { return h[i] 0 { fmt.Printf(\"%d \", heap.Pop(h)) } } //结果 minimum: 1 1 2 3 5 可以用heap来实现优先级队列 // This example demonstrates a priority queue built using the heap interface. package main import ( \"container/heap\" \"fmt\" ) // An Item is something we manage in a priority queue. type Item struct { value string // The value of the item; arbitrary. priority int // The priority of the item in the queue. // The index is needed by update and is maintained by the heap.Interface methods. index int // The index of the item in the heap. } //这个heap来存储上是建立在slice上的? // A PriorityQueue implements heap.Interface and holds Items. type PriorityQueue []*Item func (pq PriorityQueue) Len() int { return len(pq) } //原本head是node的值比其子树要小, 这里\"重载\"了Less方法, 返回大的值, 从而实现先pop大的 func (pq PriorityQueue) Less(i, j int) bool { // We want Pop to give us the highest, not lowest, priority so we use greater than here. return pq[i].priority > pq[j].priority } //为什么不是指针传递? 因为pq是slice吗? func (pq PriorityQueue) Swap(i, j int) { pq[i], pq[j] = pq[j], pq[i] pq[i].index = i pq[j].index = j } //实现了Push和Pop就是heap func (pq *PriorityQueue) Push(x interface{}) { n := len(*pq) //类型断言 item := x.(*Item) item.index = n //用append就能push进树结构? *pq = append(*pq, item) } //实现了Push和Pop就是heap func (pq *PriorityQueue) Pop() interface{} { //怎么隐约感觉这里是deep copy old := *pq n := len(old) //这样就行了? item := old[n-1] item.index = -1 // for safety //返回一个切片? *pq = old[0 : n-1] return item } // update modifies the priority and value of an Item in the queue. func (pq *PriorityQueue) update(item *Item, value string, priority int) { item.value = value item.priority = priority heap.Fix(pq, item.index) } // This example creates a PriorityQueue with some items, adds and manipulates an item, // and then removes the items in priority order. func main() { // Some items and their priorities. items := map[string]int{ \"banana\": 3, \"apple\": 2, \"pear\": 4, } // Create a priority queue, put the items in it, and // establish the priority queue (heap) invariants. pq := make(PriorityQueue, len(items)) i := 0 for value, priority := range items { pq[i] = &Item{ value: value, priority: priority, index: i, } i++ } heap.Init(&pq) // Insert a new item and then modify its priority. item := &Item{ value: \"orange\", priority: 1, } heap.Push(&pq, item) pq.update(item, item.value, 5) // Take the items out; they arrive in decreasing priority order. for pq.Len() > 0 { item := heap.Pop(&pq).(*Item) fmt.Printf(\"%.2d:%s \", item.priority, item.value) } } 对上面几个问题的解释: 这里的head用数组slice做底层存储 heap.Push()先调用具体实例的Push()实现, 再调用up()排序 heap.Pop()先调用具体实例的Swap()方法, 再用down方法排序, 再调用具体实例的Pop() up()和down()是src/container/heap/heap.go的排序实现 // Push pushes the element x onto the heap. // The complexity is O(log n) where n = h.Len(). func Push(h Interface, x interface{}) { h.Push(x) up(h, h.Len()-1) } // Pop removes and returns the minimum element (according to Less) from the heap. // The complexity is O(log n) where n = h.Len(). // Pop is equivalent to Remove(h, 0). func Pop(h Interface) interface{} { n := h.Len() - 1 //这里把0号node和len()-1互换了, 正好对应上面的item := old[n-1]直接取 h.Swap(0, n) //头节点没了, 肯定要重新排序 down(h, 0, n) return h.Pop() } sort 提供对切片和自定义的结构的排序. 支持对基本类型的排序 排序是原地排序, 即直接修改底层的数组 func Float64s(a []float64) func Ints(a []int) func IsSorted(data Interface) bool //返回满足条件的下标 func Search(n int, f func(int) bool) int //在已经排好序的slice里search func SearchInts(a []int, x int) int func SearchStrings(a []string, x string) int func Strings(a []string) 比如: package main import ( \"fmt\" \"math\" \"sort\" ) func main() { s := []float64{5.2, -1.3, 0.7, -3.8, 2.6} // unsorted //对s排序, 升序排序 sort.Float64s(s) //排好序后还是s fmt.Println(s) s = []float64{math.Inf(1), math.NaN(), math.Inf(-1), 0.0} // unsorted sort.Float64s(s) fmt.Println(s) } [-3.8 -1.3 0.7 2.6 5.2] [NaN -Inf 0 +Inf] 对struct排序 //传入一个slice, 按照less函数排序 func Slice(slice interface{}, less func(i, j int) bool) //带stable字眼的表示排序是稳定的 func SliceStable(slice interface{}, less func(i, j int) bool) 比如: package main import ( \"fmt\" \"sort\" ) func main() { people := []struct { Name string Age int }{ {\"Gopher\", 7}, {\"Alice\", 55}, {\"Vera\", 24}, {\"Bob\", 75}, } sort.Slice(people, func(i, j int) bool { return people[i].Name 通用排序 //通过调用data.Len, data.Less, data.Swap来进行排序 func Sort(data Interface) func Stable(data Interface) 对外接口 一个用户自定义的数据表达, 要满足sort.Interface要求, 就能被排序. 要求底层承载数据的东西, 是按整数index寻址的. 也就是说, 必须是数组. type Interface interface { // Len is the number of elements in the collection. Len() int // Less reports whether the element with // index i should sort before the element with index j. Less(i, j int) bool // Swap swaps the elements with indexes i and j. Swap(i, j int) } "},"notes/golang_我的反射代码.html":{"url":"notes/golang_我的反射代码.html","title":"Golang 我的反射代码","keywords":"","body":" 背景 ToObject FromObject 测试程序 背景 在早期的gshellos的实现中, 我们用了tengo解释器来在gshell框架下解释运行.tengo代码. tengo使用tengo object来表示对象, 比如int在tengo中是tengo.Int. ToObject ToObject函数的作用是把一个原生的go对象转换为tengo对象, 支持简单的int byte bool等基础value, 以及map slice等复合value的组合. ToObject函数是递归的, 并且设计了fast path和slow path来做value的转换. fast path用类型断言 slow path用reflect 下面我把代码贴出来, 用作后面参考. // ToObject traverses the value v recursively and converts the value to tengo object. // Pointer values encode as the value pointed to. // A nil pointer/interface/slice/map encodes as the tengo.UndefinedValue value. // Struct values encode as tengo map. Only exported field can be encoded with filed names as map keys but with its first letter turned into lower case. // e.g. struct{Field1: 123, AnotherField: 456} will be converted to tengo map{field: 123, anotherField: 456} // int, string, float, bool, Time.time, error encodes as their corresponding tengo object. // slices encode as tengo Array, maps with key as string encode as tengo Map, returns ErrInvalidType if key type in map is not string. // Returns ErrInvalidType on unsupported value type. // Note as ToObject follows pointers, be careful with cyclic pointer references which results in infinite loop. func ToObject(v interface{}) (tengo.Object, error) { // fast path switch v := v.(type) { case nil: return tengo.UndefinedValue, nil case string: if len(v) > tengo.MaxStringLen { return nil, tengo.ErrStringLimit } return &tengo.String{Value: v}, nil case int64: return &tengo.Int{Value: v}, nil case int: return &tengo.Int{Value: int64(v)}, nil case bool: if v { return tengo.TrueValue, nil } return tengo.FalseValue, nil case rune: return &tengo.Char{Value: v}, nil case byte: return &tengo.Char{Value: rune(v)}, nil case float64: return &tengo.Float{Value: v}, nil case *UserFunction: return v, nil case *tengo.UserFunction: return v, nil case tengo.Object: return v, nil case tengo.CallableFunc: if v == nil { return tengo.UndefinedValue, nil } return &tengo.UserFunction{Value: v}, nil case []byte: if v == nil { return tengo.UndefinedValue, nil } if len(v) > tengo.MaxBytesLen { return nil, tengo.ErrBytesLimit } return &tengo.Bytes{Value: v}, nil case error: if v == nil { return tengo.UndefinedValue, nil } return &tengo.Error{Value: &tengo.String{Value: v.Error()}}, nil case map[string]tengo.Object: if v == nil { return tengo.UndefinedValue, nil } return &tengo.Map{Value: v}, nil case map[string]int: if v == nil { return tengo.UndefinedValue, nil } kv := make(map[string]tengo.Object, len(v)) for vk, vv := range v { vo, err := ToObject(vv) if err != nil { return nil, err } kv[vk] = vo } return &tengo.Map{Value: kv}, nil case map[string]int64: if v == nil { return tengo.UndefinedValue, nil } kv := make(map[string]tengo.Object, len(v)) for vk, vv := range v { vo, err := ToObject(vv) if err != nil { return nil, err } kv[vk] = vo } return &tengo.Map{Value: kv}, nil case map[string]float64: if v == nil { return tengo.UndefinedValue, nil } kv := make(map[string]tengo.Object, len(v)) for vk, vv := range v { vo, err := ToObject(vv) if err != nil { return nil, err } kv[vk] = vo } return &tengo.Map{Value: kv}, nil case map[string]string: if v == nil { return tengo.UndefinedValue, nil } kv := make(map[string]tengo.Object, len(v)) for vk, vv := range v { vo, err := ToObject(vv) if err != nil { return nil, err } kv[vk] = vo } return &tengo.Map{Value: kv}, nil case map[string]interface{}: if v == nil { return tengo.UndefinedValue, nil } kv := make(map[string]tengo.Object, len(v)) for vk, vv := range v { vo, err := ToObject(vv) if err != nil { return nil, err } kv[vk] = vo } return &tengo.Map{Value: kv}, nil case []tengo.Object: if v == nil { return tengo.UndefinedValue, nil } return &tengo.Array{Value: v}, nil case []int: if v == nil { return tengo.UndefinedValue, nil } arr := make([]tengo.Object, len(v)) for i, e := range v { vo, err := ToObject(e) if err != nil { return nil, err } arr[i] = vo } return &tengo.Array{Value: arr}, nil case []int64: if v == nil { return tengo.UndefinedValue, nil } arr := make([]tengo.Object, len(v)) for i, e := range v { vo, err := ToObject(e) if err != nil { return nil, err } arr[i] = vo } return &tengo.Array{Value: arr}, nil case []float64: if v == nil { return tengo.UndefinedValue, nil } arr := make([]tengo.Object, len(v)) for i, e := range v { vo, err := ToObject(e) if err != nil { return nil, err } arr[i] = vo } return &tengo.Array{Value: arr}, nil case []string: if v == nil { return tengo.UndefinedValue, nil } arr := make([]tengo.Object, len(v)) for i, e := range v { vo, err := ToObject(e) if err != nil { return nil, err } arr[i] = vo } return &tengo.Array{Value: arr}, nil case []interface{}: if v == nil { return tengo.UndefinedValue, nil } arr := make([]tengo.Object, len(v)) for i, e := range v { vo, err := ToObject(e) if err != nil { return nil, err } arr[i] = vo } return &tengo.Array{Value: arr}, nil case time.Time: return &tengo.Time{Value: v}, nil } // slow path rv := reflect.ValueOf(v) switch rv.Kind() { case reflect.Ptr, reflect.Interface, reflect.Map, reflect.Slice: if rv.IsNil() { return tengo.UndefinedValue, nil } } rv = reflect.Indirect(rv) switch rv.Kind() { case reflect.Bool: if rv.Bool() { return tengo.TrueValue, nil } return tengo.FalseValue, nil case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: return &tengo.Int{Value: rv.Int()}, nil case reflect.Uint, reflect.Uintptr, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64: return &tengo.Int{Value: int64(rv.Uint())}, nil case reflect.Float32, reflect.Float64: return &tengo.Float{Value: rv.Float()}, nil case reflect.Array, reflect.Slice: arr := make([]tengo.Object, rv.Len()) for i := 0; i FromObject FromObject是ToObject的反过程. // FromObject parses the tengo object and stores the result in the value pointed to by v. // FromObject uses the inverse of the encodings that ToObject uses, allocating maps, slices, and pointers as necessary. // // FromObject converts tengo Map object into a struct by map look up with field names as keys. // Filed name and tengo map key are matched in a way that the first letter is insensitive. // e.g. both tengo map{name: \"san\"} and map{Name: \"san\"} can be converted to struct{Name: \"san\"} // If v is nil or not a pointer, ObjectToValue returns an ErrInvalidPtr error. // If o is already a tengo object, it is copied to the value that v points to. // If v represents a *tengo.CallableFunc, and o is a tengo UserFunction object, the CallableFunc f will be copied to where v points. // Returns ErrNotConvertibleType if o can not be converted to v, e.g. you are trying to get a map vale from tengo Array object. // Not supported value types: // interface, chan, complex, func // In particular, interface error is not convertible. func FromObject(v interface{}, o tengo.Object) error { if o == tengo.UndefinedValue { return nil // ignore undefined value } // fast path switch ptr := v.(type) { case *int: if ptr == nil { return ErrInvalidPtr } if v, ok := tengo.ToInt(o); ok { *ptr = v return nil } case *int64: if ptr == nil { return ErrInvalidPtr } if v, ok := tengo.ToInt64(o); ok { *ptr = v return nil } case *string: if ptr == nil { return ErrInvalidPtr } if v, ok := tengo.ToString(o); ok { *ptr = v return nil } case *float64: if ptr == nil { return ErrInvalidPtr } if v, ok := tengo.ToFloat64(o); ok { *ptr = v return nil } case *bool: if ptr == nil { return ErrInvalidPtr } if v, ok := tengo.ToBool(o); ok { *ptr = v return nil } case *rune: if ptr == nil { return ErrInvalidPtr } if v, ok := tengo.ToRune(o); ok { *ptr = v return nil } case *[]byte: if ptr == nil { return ErrInvalidPtr } if v, ok := tengo.ToByteSlice(o); ok { *ptr = v return nil } case *time.Time: if ptr == nil { return ErrInvalidPtr } if v, ok := tengo.ToTime(o); ok { *ptr = v return nil } case *[]int: if ptr == nil { return ErrInvalidPtr } toA := func(objArray []tengo.Object) bool { array := make([]int, len(objArray)) for i, o := range objArray { v, ok := tengo.ToInt(o) if !ok { return false } array[i] = v } *ptr = array return true } switch o := o.(type) { case *tengo.Array: if toA(o.Value) { return nil } case *tengo.ImmutableArray: if toA(o.Value) { return nil } } case *[]int64: if ptr == nil { return ErrInvalidPtr } toA := func(objArray []tengo.Object) bool { array := make([]int64, len(objArray)) for i, o := range objArray { v, ok := tengo.ToInt64(o) if !ok { return false } array[i] = v } *ptr = array return true } switch o := o.(type) { case *tengo.Array: if toA(o.Value) { return nil } case *tengo.ImmutableArray: if toA(o.Value) { return nil } } case *[]float64: if ptr == nil { return ErrInvalidPtr } toA := func(objArray []tengo.Object) bool { array := make([]float64, len(objArray)) for i, o := range objArray { v, ok := tengo.ToFloat64(o) if !ok { return false } array[i] = v } *ptr = array return true } switch o := o.(type) { case *tengo.Array: if toA(o.Value) { return nil } case *tengo.ImmutableArray: if toA(o.Value) { return nil } } case *[]string: if ptr == nil { return ErrInvalidPtr } toA := func(objArray []tengo.Object) bool { array := make([]string, len(objArray)) for i, o := range objArray { v, ok := tengo.ToString(o) if !ok { return false } array[i] = v } *ptr = array return true } switch o := o.(type) { case *tengo.Array: if toA(o.Value) { return nil } case *tengo.ImmutableArray: if toA(o.Value) { return nil } } case *map[string]int: if ptr == nil { return ErrInvalidPtr } toM := func(objMap map[string]tengo.Object) bool { mp := make(map[string]int, len(objMap)) for k, o := range objMap { v, ok := tengo.ToInt(o) if !ok { return false } mp[k] = v } *ptr = mp return true } switch o := o.(type) { case *tengo.Map: if toM(o.Value) { return nil } case *tengo.ImmutableMap: if toM(o.Value) { return nil } } case *map[string]int64: if ptr == nil { return ErrInvalidPtr } toM := func(objMap map[string]tengo.Object) bool { mp := make(map[string]int64, len(objMap)) for k, o := range objMap { v, ok := tengo.ToInt64(o) if !ok { return false } mp[k] = v } *ptr = mp return true } switch o := o.(type) { case *tengo.Map: if toM(o.Value) { return nil } case *tengo.ImmutableMap: if toM(o.Value) { return nil } } case *map[string]float64: if ptr == nil { return ErrInvalidPtr } toM := func(objMap map[string]tengo.Object) bool { mp := make(map[string]float64, len(objMap)) for k, o := range objMap { v, ok := tengo.ToFloat64(o) if !ok { return false } mp[k] = v } *ptr = mp return true } switch o := o.(type) { case *tengo.Map: if toM(o.Value) { return nil } case *tengo.ImmutableMap: if toM(o.Value) { return nil } } case *map[string]string: if ptr == nil { return ErrInvalidPtr } toM := func(objMap map[string]tengo.Object) bool { mp := make(map[string]string, len(objMap)) for k, o := range objMap { v, ok := tengo.ToString(o) if !ok { return false } mp[k] = v } *ptr = mp return true } switch o := o.(type) { case *tengo.Map: if toM(o.Value) { return nil } case *tengo.ImmutableMap: if toM(o.Value) { return nil } } case *tengo.Object: if ptr == nil { return ErrInvalidPtr } *ptr = o return nil case *tengo.CallableFunc: if ptr == nil { return ErrInvalidPtr } if f, ok := o.(*tengo.UserFunction); ok { *ptr = f.Value return nil } default: // slow path rptr := reflect.ValueOf(v) if rptr.Kind() != reflect.Ptr || rptr.IsNil() { return ErrInvalidPtr } rv := rptr.Elem() switch rv.Kind() { case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: if v, ok := tengo.ToInt64(o); ok { rv.SetInt(v) return nil } case reflect.Uint, reflect.Uintptr, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64: if v, ok := tengo.ToInt64(o); ok { rv.SetUint(uint64(v)) return nil } case reflect.Float32, reflect.Float64: if v, ok := tengo.ToFloat64(o); ok { rv.SetFloat(v) return nil } case reflect.Ptr: if rv.IsNil() { rv.Set(reflect.New(rv.Type().Elem())) } if err := FromObject(rv.Interface(), o); err == nil { return nil } case reflect.Array, reflect.Slice: toA := func(objArray []tengo.Object) bool { array := reflect.MakeSlice(rv.Type(), len(objArray), len(objArray)) for i, o := range objArray { if o == tengo.UndefinedValue { continue } elem := array.Index(i) if err := FromObject(elem.Addr().Interface(), o); err != nil { return false } } rv.Set(array) return true } switch o := o.(type) { case *tengo.Array: if toA(o.Value) { return nil } case *tengo.ImmutableArray: if toA(o.Value) { return nil } } case reflect.Map: toM := func(objMap map[string]tengo.Object) bool { typ := rv.Type() if typ.Key().Kind() != reflect.String { return false } mp := reflect.MakeMapWithSize(typ, len(objMap)) elemPtr := reflect.New(typ.Elem()) for k, o := range objMap { if o == tengo.UndefinedValue { continue } if err := FromObject(elemPtr.Interface(), o); err != nil { return false } mp.SetMapIndex(reflect.ValueOf(k), elemPtr.Elem()) } rv.Set(mp) return true } switch o := o.(type) { case *tengo.Map: if toM(o.Value) { return nil } case *tengo.ImmutableMap: if toM(o.Value) { return nil } } case reflect.Struct: toStruct := func(objMap map[string]tengo.Object) bool { typ := rv.Type() for i := 0; i 测试程序 package gshellos import ( \"errors\" \"fmt\" \"reflect\" \"regexp\" \"strings\" \"testing\" \"time\" \"github.com/d5/tengo/v2\" ) func TestToObject(t *testing.T) { testb := true var testi uint32 = 88 testf := 33.33 var itf interface{} itf = testf type student struct { Name string Age int Scores map[string]float32 Friends []student } empty := struct { A string B int C float64 D []int E []int64 F []float64 G []tengo.Object H []string I []byte J []interface{} K tengo.CallableFunc L error M map[string]int N map[string]int64 O map[string]float64 P map[string]string Q map[string]interface{} R map[string]tengo.Object S *int }{} cases := []struct { v interface{} want string }{ {&tengo.Int{Value: 123}, `&tengo.Int{ObjectImpl:tengo.ObjectImpl{}, Value:123}`}, {nil, `&tengo.Undefined{ObjectImpl:tengo.ObjectImpl{}}`}, {1, `&tengo.Int{ObjectImpl:tengo.ObjectImpl{}, Value:1}`}, {\"hello world\", `&tengo.String{ObjectImpl:tengo.ObjectImpl{}, Value:\"hello world\", runeStr:[]int32(nil)}`}, {99.99, `&tengo.Float{ObjectImpl:tengo.ObjectImpl{}, Value:99.99}`}, {false, `&tengo.Bool{ObjectImpl:tengo.ObjectImpl{}, value:false}`}, {'@', `&tengo.Char{ObjectImpl:tengo.ObjectImpl{}, Value:64}`}, {byte(56), `&tengo.Char{ObjectImpl:tengo.ObjectImpl{}, Value:56}`}, {[]byte(\"567\"), `&tengo.Bytes{ObjectImpl:tengo.ObjectImpl{}, Value:[]uint8{0x35, 0x36, 0x37}}`}, {errors.New(\"err\"), `&tengo.Error{ObjectImpl:tengo.ObjectImpl{}, Value:\\(\\*tengo.String\\)\\(0x[0-9a-f]+\\)}`}, {map[string]int{\"zhangsan\": 30, \"lisi\": 35}, `^&tengo.Map{ObjectImpl:tengo.ObjectImpl{}, Value:map\\[string\\]tengo.Object{(\"((zhangsan)|(lisi))\":\\(\\*tengo.Int\\)\\(0x[0-9a-f]+\\),? ?){2}}}$`}, {map[string]int64{\"zhangsan\": 30, \"lisi\": 35}, `^&tengo.Map{ObjectImpl:tengo.ObjectImpl{}, Value:map\\[string\\]tengo.Object{(\"((zhangsan)|(lisi))\":\\(\\*tengo.Int\\)\\(0x[0-9a-f]+\\),? ?){2}}}$`}, {map[string]string{\"zhangsan\": \"teacher\", \"lisi\": \"student\"}, `^&tengo.Map{ObjectImpl:tengo.ObjectImpl{}, Value:map\\[string\\]tengo.Object{(\"((zhangsan)|(lisi))\":\\(\\*tengo.String\\)\\(0x[0-9a-f]+\\),? ?){2}}}$`}, {map[string]interface{}{\"zhangsan\": 30, \"lisi\": \"student\"}, `^&tengo.Map{ObjectImpl:tengo.ObjectImpl{}, Value:map\\[string\\]tengo.Object{(\"((zhangsan)|(lisi))\":\\(\\*tengo.((String)|(Int))\\)\\(0x[0-9a-f]+\\),? ?){2}}}$`}, {map[string]float64{\"zhangsan\": 30.1, \"lisi\": 35.2}, `^&tengo.Map{ObjectImpl:tengo.ObjectImpl{}, Value:map\\[string\\]tengo.Object{(\"((zhangsan)|(lisi))\":\\(\\*tengo.Float\\)\\(0x[0-9a-f]+\\),? ?){2}}}$`}, {[2]int{11, 13}, `^&tengo.Array{ObjectImpl:tengo.ObjectImpl{}, Value:\\[\\]tengo.Object{(\\(\\*tengo.Int\\)\\(0x[0-9a-f]+\\),? ?){2}}}$`}, {[]int{101, 103, 105}, `^&tengo.Array{ObjectImpl:tengo.ObjectImpl{}, Value:\\[\\]tengo.Object{(\\(\\*tengo.Int\\)\\(0x[0-9a-f]+\\),? ?){3}}}$`}, {[]int64{101, 103, 105}, `^&tengo.Array{ObjectImpl:tengo.ObjectImpl{}, Value:\\[\\]tengo.Object{(\\(\\*tengo.Int\\)\\(0x[0-9a-f]+\\),? ?){3}}}$`}, {[]float64{101.1, 103.1, 105.1}, `^&tengo.Array{ObjectImpl:tengo.ObjectImpl{}, Value:\\[\\]tengo.Object{(\\(\\*tengo.Float\\)\\(0x[0-9a-f]+\\),? ?){3}}}$`}, {[]string{\"ni\", \"hao\", \"ma\"}, `^&tengo.Array{ObjectImpl:tengo.ObjectImpl{}, Value:\\[\\]tengo.Object{(\\(\\*tengo.String\\)\\(0x[0-9a-f]+\\),? ?){3}}}$`}, {[]interface{}{\"ni\", \"hao\", 123}, `^&tengo.Array{ObjectImpl:tengo.ObjectImpl{}, Value:\\[\\]tengo.Object{(\\(\\*tengo.((String)|(Int))\\)\\(0x[0-9a-f]+\\),? ?){3}}}$`}, {time.Now(), `^&tengo.Time{ObjectImpl:tengo.ObjectImpl{}, Value:time.Time{.*}}$`}, {&testb, `&tengo.Bool{ObjectImpl:tengo.ObjectImpl{}, value:true}`}, {int16(55), `&tengo.Int{ObjectImpl:tengo.ObjectImpl{}, Value:55}`}, {&testi, `&tengo.Int{ObjectImpl:tengo.ObjectImpl{}, Value:88}`}, {&testf, `&tengo.Float{ObjectImpl:tengo.ObjectImpl{}, Value:33.33}`}, {itf, `&tengo.Float{ObjectImpl:tengo.ObjectImpl{}, Value:33.33}`}, {student{\"lisi\", 20, map[string]float32{\"yuwen\": 86.5, \"shuxue\": 83.1}, []student{{Name: \"zhangsan\"}, {Name: \"wangwu\"}}}, `^&tengo.Map{ObjectImpl:tengo.ObjectImpl{}, Value:map\\[string\\]tengo.Object{(\"((age)|(friends)|(name)|(scores))\":\\(\\*tengo.((Int)|Array|String|Map)\\)\\(0x[0-9a-f]+\\),? ?){4}}}$`}, {map[string]student{\"zhangsan\": {Name: \"zhangsan\"}, \"lisi\": {Name: \"lisi\"}}, `^&tengo.Map{ObjectImpl:tengo.ObjectImpl{}, Value:map\\[string\\]tengo.Object{(\"((lisi)|(zhangsan))\":\\(\\*tengo.Map\\)\\(0x[0-9a-f]+\\),? ?){2}}}$`}, {empty, `^&tengo.Map{ObjectImpl:tengo.ObjectImpl{}, Value:map\\[string\\]tengo.Object{\"a\":\\(\\*tengo.String\\)\\(0x[0-9a-f]+\\), \"b\":\\(\\*tengo.Int\\)\\(0x[0-9a-f]+\\), \"c\":\\(\\*tengo.Float\\)\\(0x[0-9a-f]+\\), (\"[d-s]{1}\":\\(\\*tengo.Undefined\\)\\(0x[0-9a-f]+\\),? ?){16}}}$`}, } for _, c := range cases { if len(c.want) == 0 { t.Error(\"empty want\") } obj, err := ToObject(c.v) if err != nil { t.Error(err) continue } got := fmt.Sprintf(\"%#v\", obj) //t.Logf(\"%v\\n\", obj) if got == c.want { continue } re := regexp.MustCompile(c.want) if !re.MatchString(got) { t.Errorf(\"want: %s, got: %s\", c.want, got) } } _, err := ToObject([]complex64{complex(1, -2), complex(1.0, -1.4)}) if err != ErrInvalidType { t.Error(\"complex supported?\") } _, err = ToObject(map[string]interface{}{\"a\": complex(1, -2), \"b\": complex(1.0, -1.4)}) if err != ErrInvalidType { t.Error(\"complex supported?\") } } func TestFromObject(t *testing.T) { obj, _ := ToObject(55) emptyCases := []interface{}{ (*int)(nil), (*int64)(nil), (*string)(nil), (*float64)(nil), (*bool)(nil), (*rune)(nil), (*[]byte)(nil), (*time.Time)(nil), (*[]int)(nil), (*[]int64)(nil), (*[]float64)(nil), (*[]string)(nil), (*map[string]int)(nil), (*map[string]int64)(nil), (*map[string]float64)(nil), (*map[string]string)(nil), (*tengo.Object)(nil), (*tengo.CallableFunc)(nil), (*int32)(nil), nil, } for _, c := range emptyCases { err := FromObject(c, obj) if err != ErrInvalidPtr { t.Fatal(\"empty ptr error expected\") } } var got tengo.Object err := FromObject(&got, obj) if err != nil { t.Error(err) } if !reflect.DeepEqual(got, obj) { t.Errorf(\"want: %#v, got: %#v\", obj, got) } testf := func(args ...tengo.Object) (tengo.Object, error) { return nil, nil } var gotf func(args ...tengo.Object) (tengo.Object, error) fobj, err := ToObject(testf) if err != nil { t.Error(err) } err = FromObject(&gotf, fobj) if err != nil { t.Error(err) } var itf interface{} = gotf gotstring := fmt.Sprintf(\"%#v\", itf) wantstring := `(func(...tengo.Object) (tengo.Object, error))` if !strings.Contains(gotstring, wantstring) { t.Errorf(\"want: %s, got: %s\", wantstring, gotstring) } err = FromObject(&gotf, obj) if !errors.As(err, &ErrNotConvertibleType{}) { t.Error(err) } type student struct { Name string Age int Scores map[string]float32 Classmates []student Deskmate *student Friends map[string]*student } studentA := student{ \"lisi\", 20, map[string]float32{\"yuwen\": 86.5, \"shuxue\": 83.1}, []student{{Name: \"zhangsan\"}, {Name: \"wangwu\"}}, nil, nil, } studentB := student{ \"zhangsan\", 21, map[string]float32{\"yuwen\": 78.5, \"shuxue\": 96.1}, []student{{Name: \"lisi\"}, {Name: \"wangwu\"}}, &studentA, map[string]*student{\"si\": &studentA}, } cases := []interface{}{ \"hello world\", 55, int64(33), 55.77, true, 'U', []byte{1, 2, 3, 4, 5}, time.Now(), []int{22, 33, 44}, []int64{22, 33, 44}, []float64{22.1, 33.2, 44.9}, []string{\"ni\", \"hao\", \"ma\"}, map[string]int{\"A\": 1, \"b\": 15}, map[string]int64{\"A\": 1, \"b\": 15}, map[string]float64{\"a\": 1.54, \"U\": 3.14}, map[string]string{\"a\": \"12345\", \"U\": \"hello world\"}, int16(12), uint16(12), float32(1.2345), studentB, studentA, } for _, c := range cases { t.Logf(\"c: %#v\", c) obj, err := ToObject(c) if err != nil { t.Fatal(err) } t.Logf(\"obj: %#v\", obj) ptr := reflect.New(reflect.TypeOf(c)) err = FromObject(ptr.Interface(), obj) if err != nil { t.Error(err) continue } v := ptr.Elem().Interface() t.Logf(\"v: %#v\", v) if !reflect.DeepEqual(c, v) { t.Errorf(\"want: %#v, got: %#v\", c, v) } } //t.Error(\"err\") } "},"notes/golang_问答.html":{"url":"notes/golang_问答.html","title":"Golang 问答","keywords":"","body":" 用range迭代一个map的时候, 删除或者新增key安全吗? net/Conn可以多个goroutine同时读写吗? fmt.Println并发安全吗? write系统调用是原子的吗? System call atomicity 用range迭代一个map的时候, 删除或者新增key安全吗? 比如下面的代码: for key := range m { if key.expired() { delete(m, key) } } 答: 安全. 删除一个还没有被loop到的key, range保证不会这个key不会被loop到; 新增一个key, 则可能会也可能不会被loop到. 因为map是hash桶, loop随机选择一个index然后依次看桶里的元素. The iteration order over maps is not specified and is not guaranteed to be the same from one iteration to the next. If map entries that have not yet been reached are removed during iteration, the corresponding iteration values will not be produced. If map entries are created during iteration, that entry may be produced during the iteration or may be skipped. The choice may vary for each entry created and from one iteration to the next. If the map is nil, the number of iterations is 0. 参考: stackoverflow net/Conn可以多个goroutine同时读写吗? 可以, Conn是并发安全的 Multiple goroutines may invoke methods on a Conn simultaneously. 参考笔记: 网络杂记2.md, 搜多线程能不能同时写同一个socket fmt.Println并发安全吗? 不安全. 见讨论: https://stackoverflow.com/questions/14694088/is-it-safe-for-more-than-one-goroutine-to-print-to-stdout go文档种, 并发安全的api都会说的; 没说的都是并发不安全的. This is an instance of a more universal Go documentation rule: Things are not safe for concurrent access unless specified otherwise or where obvious from context. Everything fmt does falls back to w.Write() as can be seen here. Because there's no locking around it, everything falls back to the implementation of Write(). As there is still no locking (for Stdout at least), there is no guarantee your output will not be mixed. I'd recommend using a global log routine. Furthermore, if you simply want to log data, use the log package, which locks access to the output properly. See the implementation for reference. write系统调用是原子的吗? 答: 好像应该是, 但实际上并不是; 但对于append模式下的文件来说, 实际上也是 https://cs61.seas.harvard.edu/site/2018/Storage4/#:~:text=System%20call%20atomicity,write%20%2C%20should%20have%20atomic%20effect. System call atomicity Unix file system system calls, such as read and write, should have atomic effect. Atomicity is a correctness property that concerns concurrency—the behavior of a system when multiple computations are happening at the same time. (For example, multiple programs have the file open at the same time.) An operation is atomic, or has atomic effect, if it always behaves as if it executes without interruption, at one precise moment in time, with no other operation happening. Atomicity is good because it makes complex behavior much easier to understand. The standards that govern Unix say reads and writes should have atomic effect. It is the operating system kernel’s job to ensure this atomic effect, perhaps by preventing different programs from read or writing to the same file at the same time. Unfortunately for our narrative, experiment shows that on Linux many write system calls do not have atomic effect, meaning Linux has bugs. But writes made in “append mode” (open(… O_APPEND) or fopen(…, \"a\")) do have atomic effect. In this mode, which is frequently used for log files, writes are always placed at the end of the open file, after all other data. "},"notes/golang_高效go.html":{"url":"notes/golang_高效go.html","title":"Golang 高效go","keywords":"","body":" 高效go web服务器例子 代码 运行 panic流程和普通执行流程 panic recover recover里面可以改变量值 总结 错误处理 error定义和使用 errors包 自己实现error接口 error和类型断言的例子 进一步看error如何返回的 错误处理化简 channel和并发 channel channel用于同步 channel用于semaphore 一个channel有多个goroutine接收 函数和channel都是first class值: channel in channel 通道in通道的另一个例子 分割和并发 用channel管理message buffer 接口嵌套 结构体嵌套 例子 logger 接口和方法 即使一个int类型也可以带方法 chan带方法 函数也可以带方法!!! 多态 接口和类型断言 方法 接口 gofmt 注释即文档 if可以有初始化语句 for是三种C循环的统一 switch接受非常量 多返回值 defer 用defer做函数trace 在panic场景下, defer的最大好处是panic链上的defer都会被调用到 new和make分配数据 new 数组和切片的区别 make 数组 切片 map 打印 append和...扩展 全局变量初始化和init 高效go https://golang.org/doc/effective_go.html#concurrency web服务器例子 这个web服务器, 利用了chart.apis.google.com提供的api, 把文本转化成二维码. 但你需要把data都放到URL中去做query. 代码 下面的代码, 可以把文本的输入, 通过google的api, 转换成一个QR code. 然后你就可以用手机扫描这个文本类型的QR码, 就能看到对应的文本 package main //这几个库都是标准库 import ( \"flag\" \"html/template\" \"log\" \"net/http\" ) //这里设置默认的http port是1718 var addr = flag.String(\"addr\", \":1718\", \"http service address\") // Q=17, R=18 //根据下面的描述生成模板, 这个模板html被server执行来显示这个页面 var templ = template.Must(template.New(\"qr\").Parse(templateStr)) func main() { flag.Parse() //把函数QR挂到http的根目录 http.Handle(\"/\", http.HandlerFunc(QR)) //开始运行这个server err := http.ListenAndServe(*addr, nil) //QR函数接收到http的request, 里面包含了data if err != nil { log.Fatal(\"ListenAndServe:\", err) } } func QR(w http.ResponseWriter, req *http.Request) { //data从一个叫s的表格而来 templ.Execute(w, req.FormValue(\"s\")) } // html/template很强大, 这里只用了一点点. // 它把req.FormValue(\"s\")返回来的data, 写入下面的模板中 const templateStr = ` QR Link Generator {{if .}} {{.}} {{end}} 具体的template用法在此 运行 #把上面的代码保存为goweb.go #代码格式化 gofmt -w goweb.go #编译 go build goweb.go #直接运行 ./goweb 浏览器打开http://192.168.56.101:1718/, 这个ip就是运行goweb的机器的ip. 会有个很简单的输入框, 输入一些文本后点Show QR就能显示二维码 手机扫描二维码就能还原文本. panic流程和普通执行流程 比如下面的流程: a{ b{ defer c{ recover() }() d{ e{ } f{ } } } g{ } } 比如d()里面panic了 正常的执行流程, 是有上有下的: 先深度执行到f(), 然后return路径沿途返回 panic流程一定是一直向上的, 如果沿途的defer里面没有recover, panic流程向上回溯到这个goroutine的顶层函数停止. 比如在e()里面panic, 它后面的函数就不执行了, 直接向上回溯, 而且只有沿途的defer函数会被执行: While executing a function F, an explicit call to panic or a run-time panic terminates the execution of F. Any functions deferred by F are then executed as usual. Next, any deferred functions run by F's caller are run, and so on up to any deferred by the top-level function in the executing goroutine. At that point, the program is terminated and the error condition is reported, including the value of the argument to panic. This termination sequence is called panicking. recover()阻止了panic的向上的流程, 还是比如e()中panic了, 但在b()的defer列表里, 要执行的c()里面recover, 那么b()正常返回到a(), 接着正常向下执行g() panic panic是go的内建函数, 用于在程序无法运行下去的时候退出 var user = os.Getenv(\"USER\") func init() { if user == \"\" { panic(\"no value for $USER\") } } 那么panic的时候, 执行了什么? panic立即终止执行当前的函数, 向上回溯当前goroutine的调用栈, 依次执行沿途的deferred函数, 然后die. 什么时候隐含有panic? 比如: 类型断言失败, 且没有用ok捕捉第二个返回值 slice越界 recover 可以在panic之后, 用recover恢复go的控制权. 这要求recover要在defer的函数里执行, 因为只有沿途的deferred函数会被panic执行. 在这个例子中: server的一个worker挂了, 调用了panic, 在它的defer函数里, 用recover重新获取执行权. recover函数停止panic发起的unwinding调用栈, 返回当时传给panic的参数. 这里recover停止panic, 效果是干净的关闭失败的goroutine, 其他的goroutien不受影响. recover只在defer的函数里调用才有用. 不是defer的函数, recover什么都不做, 直接返回nil. func server(workChan 注意, 上面的表述中, recover只有在defer的函数里面被调用, 才能生效. 比如下面的代码, recover()什么都不做. 因为它本身就是defer的函数, 而不是被defer的函数调用. package main func main() { defer recover() panic(\"panic\") } 正确的写法是 package main func main() { defer func() { recover() }() panic(\"panic\") } If recover is called outside the deferred function it will not stop a panicking sequence. 所以go的panic加recover的效果, 和C的longjump有点像. 这里补充一下, 如果没有这个recover, 很可能因为这个goroutine调用了panic, 导致整个程序退出. 因为panic会依次回溯defer的函数, 遇到这里的recover, 就停止回溯. 效果就是在safelyDo这一层级停止panic, 程序从此返回, 这个goroutine终结, 但整个程序继续运行. recover里面可以改变量值 // Error is the type of a parse error; it satisfies the error interface. type Error string func (e Error) Error() string { return string(e) } // error is a method of *Regexp that reports parsing errors by // panicking with an Error. func (regexp *Regexp) error(err string) { panic(Error(err)) } // re模块调用error方法时, 就会调用panic // error方法时小写的, 它是个私有方法.和builtin的error重名了, 但没影响? if pos == 0 { re.error(\"'*' illegal at start of expression\") } // Compile returns a parsed representation of the regular expression. func Compile(str string) (regexp *Regexp, err error) { regexp = new(Regexp) // doParse will panic if there is a parse error. defer func() { if e := recover(); e != nil { //recover里面, 依然可以改变量值 regexp = nil // Clear return value. //如果断言失败, 会再次触发panic //这里的作用是, 其他错误情况下, 会继续panic err = e.(Error) // Will re-panic if not a parse error. } }() return regexp.doParse(str), nil } 这里的第二次panic, 和第一次panic一起, 会被crash report记录, 但他们的值不通. 总结 panic和recover常组合用来错误处理, 而不是真正的停止程序运行. 错误处理 先看看C语言版本的open: 成功返回fd, 失败返回-1; 需要单独查errno才能知道失败的原因 Linux Mint 19.1 Tessa $ man 2 open OPEN(2) Linux Programmer's Manual OPEN(2) NAME open, openat, creat - open and possibly create a file SYNOPSIS #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); RETURN VALUE open(), openat(), and creat() return the new file descriptor, or -1 if an error occurred (in which case, errno is set appropriately). go的函数支持多返回值, 在错误处理时能返回更多的信息. 比如go的os.Open方法: 失败的时候不仅返回nil, 还返回一个error值 Linux Mint 19.1 Tessa $ go doc os.Open func Open(name string) (*File, error) Open opens the named file for reading. If successful, methods on the returned file can be used for reading; the associated file descriptor has mode O_RDONLY. If there is an error, it will be of type *PathError. error定义和使用 在go传统中, error是个builtin的interface, 即任何实现了Error()方法的类型, 都可以被看作是error对象. src/builtin/builtin.go, 里面还有close(), len()等内建函数. type error interface { Error() string } 在使用时, 一种方式是: 如果open失败, log.Fatal(err)记下log然后程序退出. f, err := os.Open(\"filename.ext\") if err != nil { log.Fatal(err) } // do something with the open *File f errors包 errors包提供了对error简单的封装: 注意errorString是个私有结构, 对外不可见 // errorString is a trivial implementation of error. type errorString struct { s string } func (e *errorString) Error() string { return e.s } 使用errors.New函数, 可以返回errorString, 但是以error类型返回的. 外部不知道有errorString // New returns an error that formats as the given text. func New(text string) error { return &errorString{text} } 在你的函数里, 你不用自己实现Error()方法, 用errors.New就可以了: func Sqrt(f float64) (float64, error) { if f 在后面, 我们会用fmt.Errorf来代替errors.New 你的函数最终被别人调用, 出错时, 可以调用err.Error()方法返回字符串, 也可以直接print: f, err := Sqrt(-1) if err != nil { fmt.Println(err) } err是个error类型的接口变量, 根据上文, 它又为什么能直接print呢? 见下面, 类型断言. 动态判断类型, 如果是error类型, 调用它的Error()方法. 比如这样: 这段代码在go/src/fmt/print.go具体来说, fmt.Println会先看类型, 先是基础类型, 不是基础类型default是走上面的代码. 前面说过, 用errors.New返回一个error对象, 但New()只接受一个字符串. 用fmt.Errorf可以接受一个带格式化的字符串, 按照Printf的格式打印成字符串, 内部再调用errors.New()返回error. 所以可以这样写: 能带更多的信息 func Sqrt(f float64) (float64, error) { if f 自己实现error接口 通常, 用fmt.Errorf就足够好了, 但还有更高级的写法. 实现了error的Error()方法, 就是error类型 比如json包里, 定义了一个SyntaxError类型, 如果解析json文件出错时, 就把它作为error类型返回. 注意, SyntaxError带一个叫Offset的数据, 后面要用到. type SyntaxError struct { msg string // description of error Offset int64 // error occurred after reading Offset bytes } func (e *SyntaxError) Error() string { return e.msg } 当调用json.Decode的人, 发现出错的时候, 他可以检查这个Offset: 这要用到类型断言 if err := dec.Decode(&val); err != nil { if serr, ok := err.(*json.SyntaxError); ok { line, col := findLine(f, serr.Offset) return fmt.Errorf(\"%s:%d:%d: %v\", f.Name(), line, col, err) } return err } 为啥要在调用函数里搞这些呢? 在SyntaxError的Error()直接写好不就完了吗? error和类型断言的例子 通过类型断言, 可以从err中提取更多的信息: for try := 0; try 进一步看error如何返回的 前面说了, os.Open返回的error会是os.PathError的指针. Linux Mint 19.1 Tessa $ go doc os.PathError type PathError struct { Op string Path string Err error } PathError records an error and the operation and file path that caused it. func (e *PathError) Error() string func (e *PathError) Timeout() bool 解释如下: // PathError records an error and the operation and // file path that caused it. type PathError struct { Op string // \"open\", \"unlink\", etc. Path string // The associated file. Err error // Returned by the system call. } func (e *PathError) Error() string { return e.Op + \" \" + e.Path + \": \" + e.Err.Error() } open失败的meesage像这样: open /etc/passwx: no such file or directory os.Open()最终会调用私有函数:func openFileNolog(name string, flag int, perm FileMode) (*File, error) open失败的时候, 返回return nil, &PathError{\"open\", name, e} 前面说了, PathError实现了Error()方法, 就可以被当做error类型使用. 注: 为什么要对PathError{\"open\", name, e}取地址呢? 答: 对error类型来说, 它是interface, 既可以接受值, 也可以是取地址后的引用. 取决于对应方法的实现方式. 这里PathError实现Error原型是: func (e *PathError) Error() string 它的receiver是pointer receiver, 是指针, 这里要返回PathError取地址; 否则编译不过. 如果receiver的声明是非指针方式, 那么取不取地址都行. type Phone interface { call() } type NokiaPhone struct { } func (nokiaPhone NokiaPhone) call() { fmt.Println(\"I am Nokia, I can call you!\") } //这是interface类型的变量 var phone Phone //可以写成phone = NokiaPhone{}, 或phone = &NokiaPhone{} //结果是一样的 phone.call() 参考: https://blog.golang.org/error-handling-and-go 错误处理化简 go的错误处理是类似C的, 在出错时记录, 调用者来检查. 这样的好处是, 错误能被及时的处理; 但相比于python等语言的try catch机制, go的代码会繁琐. 比如http的处理函数里面, 第10行和第14行, 有一样的错误打印. 代码逻辑按步骤, 调用不同的处理函数, 出错时都要给user返回http错误码: 500 (\"Internal Server Error\") 如果后面处理的步骤增多, 会有大量的重复代码. func init() { http.HandleFunc(\"/view\", viewRecord) } func viewRecord(w http.ResponseWriter, r *http.Request) { c := appengine.NewContext(r) key := datastore.NewKey(c, \"Record\", r.FormValue(\"id\"), 0, nil) record := new(Record) if err := datastore.Get(c, key, record); err != nil { http.Error(w, err.Error(), 500) return } if err := viewTemplate.Execute(w, record); err != nil { http.Error(w, err.Error(), 500) } } 在C里面, 可以定义宏函数, 或者wrapper函数, 把要调用的函数包装一下, 统一返回错误码. 在go里, 用函数的方法可以解决这个问题: http的ServeHTTP方法的格式没有返回值 type Handler interface { ServeHTTP(ResponseWriter, *Request) } 通过函数的方法, 可以实际上带上返回值: //定义一个带返回error的函数类型 type appHandler func(http.ResponseWriter, *http.Request) error //实现ServeHTTP方法, 满足http.Handler func (fn appHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { if err := fn(w, r); err != nil { http.Error(w, err.Error(), 500) } } //实现具体的viewRecord函数, 从形式上, 它是有error返回值的. func viewRecord(w http.ResponseWriter, r *http.Request) error { c := appengine.NewContext(r) key := datastore.NewKey(c, \"Record\", r.FormValue(\"id\"), 0, nil) record := new(Record) if err := datastore.Get(c, key, record); err != nil { return err } return viewTemplate.Execute(w, record) } 上面实现了带error返回的viewRecord, 它符合appHandler类型的形式, 所以有ServeHTTP方法: 即函数的方法(方法的receiver是函数) 这里的ServeHTTP方法调用了它的receiver函数, 统一处理错误, 通过http发送给user错误码500; ServeHTTP会被http框架调用, 进而receiver函数被调用. 那么如何注册viewRecord函数为http.Handler呢? package http // import \"net/http\" func Handle(pattern string, handler Handler) Handle registers the handler for the given pattern in the DefaultServeMux. The documentation for ServeMux explains how patterns are matched. //按照上面的说明, 注册viewRecord: 强制转换为appHandler即可 func init() { http.Handle(\"/view\", appHandler(viewRecord)) } 注: 此例子充分说明了: go中的函数是一等公民 上面的例子可以更进一步, 不只返回error, 还返回更多的信息: //返回一个专用结构: 包含error接口 type appError struct { Error error Message string Code int } //现在appHandler类型返回appError对象指针 type appHandler func(http.ResponseWriter, *http.Request) *appError func (fn appHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { if e := fn(w, r); e != nil { // e is *appError, not os.Error. c := appengine.NewContext(r) c.Errorf(\"%v\", e.Error) http.Error(w, e.Message, e.Code) } } //现在返回的错误信息更丰富了 func viewRecord(w http.ResponseWriter, r *http.Request) *appError { c := appengine.NewContext(r) key := datastore.NewKey(c, \"Record\", r.FormValue(\"id\"), 0, nil) record := new(Record) if err := datastore.Get(c, key, record); err != nil { return &appError{err, \"Record not found\", 404} } if err := viewTemplate.Execute(w, record); err != nil { return &appError{err, \"Can't display record\", 500} } return nil } channel和并发 go处理并发的哲学很简单: 不共享内存, 所有共享走channel 这个哲学能从设计上, 就避免了竞争. Do not communicate by sharing memory; instead, share memory by communicating. 比如: go list.Sort() // run list.Sort concurrently; don't wait for it. 这有点像shell的后台运行& 单独这样除了能在后台跑sort, 没有特别的好处. 一般还要配合某种同步机制, 通知其他相关线程. go里面用channel来通知. 一个func里面, 也可以用go来运行goroutine func Announce(message string, delay time.Duration) { go func() { time.Sleep(delay) fmt.Println(message) }() // Note the parentheses - must call the function. } go的函数, 实际上都是闭包, 它需要的变量, 比如上面的message, 只要在用就会存在. channel 默认的size是0, 表示unbuffered通道 ci := make(chan int) // unbuffered channel of integers cj := make(chan int, 0) // unbuffered channel of integers cs := make(chan *os.File, 100) // buffered channel of pointers to Files unbuffered通道是同步安全的. channel用于同步 接上面sort的例子, sort完成后, 通过channel发送完成的\"通知\". 主程序在某个地方等待这个\"通知\". c := make(chan int) // Allocate a channel. // Start the sort in a goroutine; when it completes, signal on the channel. go func() { list.Sort() c 如果是unbufferd的通道, 发送者阻塞, 直到接收方接收到这个值. 如果是buffered模式, 则发送方只阻塞到值被拷贝进通道. 如果这个buffer满了, 则发送方一直阻塞到接收方收到值. channel用于semaphore 下面的例子中, serve函数把进入的请求, 分发给handle执行. handle是并发执行的. 它们都先写channel, 直到到达最大并发数:MaxOutstanding 这演示了sem这个channel的容量, 决定了这个生产-消费模型的最大并发数. 在到达并发数之前, handle都是并发的; 到达之后, 只有等有的handle退出, 才能执行新的handle. var sem = make(chan int, MaxOutstanding) func handle(r *Request) { //这里并不是和自己互斥; 而是可能会有最多MaxOutstanding个handle同时运行, 它们之间互斥. sem 这个design有个问题, Serve为每个请求起个goroutine来处理, 虽然最大并发被限制为MaxOutstanding, 但新的goroutine还是在被创建. 如果进来的请求太快, 资源消耗会很快. 可以在Serve里面限制goroutine的生成速度: 用range.range遍历通道queue, 如果通道不关闭, 那么range不会结束; 只在没有数据时阻塞. 但这个修改有个bug, 见下文 func Serve(queue chan *Request) { for req := range queue { //并不是range来控制go routine的生成速度 //而是说把sem 这里的bug在于, 作为循环变量, for里的req只有一个地址, 会被所有goroutine共享. 导致最后这些\"handle\"都在处理同一个req. 那么需要把req复制一份, 传给handle 这个req是个Request指针, 每次range得到一个新的req指针. 闭包函数增加个参数, 然后把req传进去: go是值传递, 所以req指针的值对每个goroutine都是unique的. func Serve(queue chan *Request) { for req := range queue { sem 还有一个写法: 在循环体里面: req := req形成一个新的副本. 名字都一样, 作用域不同. 看着有点怪, 但很合法, 也是go的一种常见写法. func Serve(queue chan *Request) { for req := range queue { req := req // Create new instance of req for the goroutine. sem 一个channel有多个goroutine接收 channel应该是天然支持多对多的模型. 对channel做range也支持多个goroutine对同一个channel做range 其实range就是个iterator? 上面的Serve例子, 用一个更省资源的方法来实现: 起固定个数的goroutine, 每个goroutine都直接从req通道读请求. quit是bool类型的通道, 主线程等待这个信号退出. func handle(queue chan *Request) { //每个handle都会对这个queue做range, 如果都等待, 那唤醒谁呢? for r := range queue { process(r) } } func Serve(clientRequests chan *Request, quit chan bool) { // Start handlers for i := 0; i 函数和channel都是first class值: channel in channel first class意思是, 函数和channel级别和int是一样的, 可以在任何地方传递.比如: channels of channels这里的f是个函数变量, resultChan是个chan int类型的变量这个Request被client用来发送请求, 它发一个切片, 一个函数, 和通过chennal传递的结果 type Request struct { args []int f func([]int) int resultChan chan int } 为什么不直接用个int放结果呢? 比如result int 因为client要用通道来等待server返回结果. client提供一个sum方法, 当做f, make(chan int)当做resultChan func sum(a []int) (s int) { for _, v := range a { s += v } return } request := &Request{[]int{3, 4, 5}, sum, make(chan int)} // Send request //这里是go重要的特征: request里面包括了函数变量和channel变量, 它们通过clientRequests 这个channel传递 clientRequests 在server端: 每个handle按照request里面的f方法, 计算出结果, 传递给request内部的通道. 这个结果会\"直达\"给对应的client. 不需要锁. func handle(queue chan *Request) { for req := range queue { req.resultChan 虽然只是演示代码, 但这个是个支持rate限制, 并发, 无锁的RPC框架. 通道in通道的另一个例子 package main import ( \"fmt\" \"math/rand\" \"sync\" \"time\" ) func main() { reqs := []int{1, 2, 3, 4, 5, 6, 7, 8, 9} // 存放结果的channel的channel outs := make(chan chan int, len(reqs)) var wg sync.WaitGroup wg.Add(len(reqs)) for _, x := range reqs { o := handle(&wg, x) outs 分割和并发 比如一个切片, 如果对每个元素的操作都是独立的, 那么这个结构就是很理想的可以多核并行的结构. type Vector []float64 // Apply the operation to v[i], v[i+1] ... up to v[n-1]. func (v Vector) DoSome(i, n int, u Vector, c chan int) { for ; i 这时可以用一个buffered channel来实现并发, channel的size为CPU个数. const numCPU = 4 // number of CPU cores func (v Vector) DoAll(u Vector) { c := make(chan int, numCPU) // Buffering optional but sensible. for i := 0; i 这里的CPU个数是hardcode, 可以动态获取: var numCPU = runtime.NumCPU() //传入0是查询, 传入个正数是设置 var numCPU = runtime.GOMAXPROCS(0) 用channel管理message buffer 虽然go有垃圾回收, 但有时候还是希望能维护一个机制, 可以不用一直alloc buffer, 尽可能的reuse buffer. 用buffered channel可以实现, 把它当做一个free list. 比如: client端, 从freeList取buffer, freeList为空时申请新的buffer. 然后从网络读消息填充buffer, 再通过和server的通道serverChan 传递. //100个room的freeList var freeList = make(chan *Buffer, 100) var serverChan = make(chan *Buffer) func client() { for { var b *Buffer // Grab a buffer if available; allocate if not. //有default的select不会阻塞 select { //从freeList取buffer, 如果有, 就直接用 case b = 结合下面的server代码, server从serverChan 读buffer, 处理完还到freeList里面. func server() { for { b := 仔细想想, 这段代码有几个问题: serverChan 是个unbuffered的通道, 如果只有一个client和一个server, 实际是用不到100个元素的freeList通道的. 因为client和server会串行在通道上.答: 是的. 一个client和一个server是的. 我理解这里可以有多个client和多个server, 虽然通过serverChan 是串行的, 但通过channel很快, 多个server都能得到buffer同时进行处理. 多个client也基本不用等待serverChan, 有请求丢过去就行, 会有server马上响应. 通道的两端是多对多的情况下, 通道本身永远不会是瓶颈; 极端情况下, 100个server都在处理buffer, 这时freeList为空, 此时client需要重新alloc buffer. 这样buffer数会多于100, 这多出来的buffer 在哪里被丢弃? 内存泄漏了吗?答: 在server的select的default分支里丢弃; 内存不会泄漏, 因为go有垃圾回收 有垃圾回收为啥还要这样搞? 全部新申请buffer不行吗?答: 可以. 但这样垃圾回收任务变繁重, 性能差点. 对高吞吐的网络buffer来说, 一般都要reuse buffer. 接口嵌套 比如下面的ReadWriter 就是Reader和Writer的组合. 只有接口才能被嵌套进接口. type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } // ReadWriter is the interface that combines the Reader and Writer interfaces. type ReadWriter interface { Reader Writer } 结构体嵌套 比如bufio的ReadWriter, 包括一个Reader类型的指针, 和一个Writer类型的指针. 但没有名字, 只有类型. 这就叫嵌入. go编译器会默认把类型名当做变量名, 所以可以用ReadWriter.Reader来访问它包含的Reader成员. // ReadWriter stores pointers to a Reader and a Writer. // It implements io.ReadWriter. type ReadWriter struct { *Reader // *bufio.Reader *Writer // *bufio.Writer } 嵌入的好处是, ReadWriter 直接就有了Reader和writer的方法. 注意是, 直接, 即ReadWriter.Read和ReadWriter.Write 这里的Reader和Writer结构体分别实现的Read和Write方法, 是符合io.Reader和io.Writer接口的. 所以ReadWriter符合所有3个接口: io.Reader, io.Writer, 和 io.ReadWriter 需要注意的是, 比如var rw ReadWriter, 虽然调用方法的形式是rw.Read, 但实际的receive对象是 rw.Reader.Read, 也就是说, outer类型(ReadWriter)直接\"拥有\"Read方法, 但实际传入的还是inner类型(Reader) 一个啰嗦的写法是: named方式包含对象, 然后定义一个\"forward\"方法, 比如这样: 效果是一样的. type ReadWriter struct { reader *Reader writer *Writer } func (rw *ReadWriter) Read(p []byte) (n int, err error) { return rw.reader.Read(p) } 例子 logger type Job struct { Command string *log.Logger } 因为嵌入了log.Logger, Job可以直接使用Print, Printf, Println等log.Logger方法 在初始化后, 就可以使用了: job.Println(\"starting now...\") 初始化Job就像一般的初始化一样: 定义一个\"构造\"函数, go没有构造函数机制, 一般都是一个初始化函数. func NewJob(command string, logger *log.Logger) *Job { return &Job{command, logger} } 或者这样初始化: job := &Job{command, log.New(os.Stderr, \"Job: \", log.Ldate)} 如果需要引用其内嵌的域, 前面说过, 直接用job.Logger 注意这里, Job实现了自己的Printf, 所以引用Logger的Printf就要通过Logger func (job *Job) Printf(format string, args ...interface{}) { job.Logger.Printf(\"%q: %s\", job.Command, fmt.Sprintf(format, args...)) } 这里的Job和它的嵌入对象都有Printf方法, 有类似冲突的时候, 看嵌套层级: 层级少的方法会被使用.但有时候同一个层级的嵌套可能有同名的方法, 如果最外层不用这个同名的方法, 也没问题;如果调用, 那通常是错误. 接口和方法 几乎任何东西都可以满足一个interface定义的接口, 比如, 任何实现了Handler接口的对象, 都可以处理http请求. 在http包里: type Handler interface { ServeHTTP(ResponseWriter, *Request) } //调用http.Handle来使用这个interface func Handle(pattern string, handler Handler) ResponseWriter被http handler用来构建http 响应, go doc http.ResponseWriter看到: type ResponseWriter interface { Header() Header Write([]byte) (int, error) WriteHeader(statusCode int) ResponseWriter实现了Write方法, 这个方法满足io.Writer要求, 可以在任何io.Writer可以使用的地方使用. Request是对client的http请求的抽象. 下面是个很简单但完整的http handler实现, 可以统计http请求的次数: // Simple counter server. type Counter struct { n int } func (ctr *Counter) ServeHTTP(w http.ResponseWriter, req *http.Request) { ctr.n++ //注意, 这里Fprintf是向w里面打印, 即直接打印到http的response fmt.Fprintf(w, \"counter = %d\\n\", ctr.n) } 如何attach这个ctr到一个url地址. import \"net/http\" ... ctr := new(Counter) http.Handle(\"/counter\", ctr) 即使一个int类型也可以带方法 // Simpler counter server. type Counter int func (ctr *Counter) ServeHTTP(w http.ResponseWriter, req *http.Request) { *ctr++ fmt.Fprintf(w, \"counter = %d\\n\", *ctr) } chan带方法 有时你希望在url被访问的时候, 得到通知, 可以在chan上挂这个http的handler // A channel that sends a notification on each visit. // (Probably want the channel to be buffered.) type Chan chan *http.Request func (ch Chan) ServeHTTP(w http.ResponseWriter, req *http.Request) { ch 函数也可以带方法!!! 比如想访问/args来得到运行这个http server时的参数, 在http包里, 是这样写的: 把func(ResponseWriter, *Request)作为一个type, 可以带方法 // The HandlerFunc type is an adapter to allow the use of // ordinary functions as HTTP handlers. If f is a function // with the appropriate signature, HandlerFunc(f) is a // Handler object that calls f. type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, req). func (f HandlerFunc) ServeHTTP(w ResponseWriter, req *Request) { f(w, req) } 这可能开起来有点奇怪, 在f的方法中, 调用了f本身. 但这和chan作为receiver本质上差不多. 所以, 这么写的好处是, 符合func(ResponseWriter, *Request)形式的函数, 都可以使用ServeHTTP方法 // Argument server. func ArgServer(w http.ResponseWriter, req *http.Request) { fmt.Fprintln(w, os.Args) } //把ArgServer转换为http.HandlerFunc类型, 就有ServeHTTP方法. http.Handle(\"/args\", http.HandlerFunc(ArgServer)) 注: 在使用时, 要把ArgServer转换成HandlerFunc类型才能使用这个类型带的方法. 这是否也可以理解成修饰器? 用type关键字定义一个函数类的类型, 这个类型可以实现别的接口规定的方法, 在这个方法里, 再调用函数自身. 函数还是这个函数, 但可以被不同的人(接口), 用不同的形式(接口方法)来调用. 多态 多态的意思是, 一个通用方法, 在各个子类里面实现, 但对外接口统一, 通常是父类定义好的(虚函数). 在基类中定义了一个虚拟函数，然后在派生类中又定义一个同名，同参数表的函数，这就是多态。多态是这3种情况中唯一采用动态绑定技术的一种情况。也就是说，通过一个基类指针来操作对象，如果对象是基类对象，就会调用基类中的那个函数，如果对象实际是派生类对象，就会调用派声类中的那个函数，调用哪个函数并不由函数的参数表决定，而是由函数的实际类型决定. 一个操作随着所传递或捆绑的对象类型的不同能够做出不同的反应，其行为模式称为多态。 在go里, 天然就是多态的: 只要实现了interface定义的方法, 就隐含了是该interface类型. 比如: NewCTR是counter mode (CTR) stream, 功能是把block cipher转换成stream cipher. // NewCTR returns a Stream that encrypts/decrypts using the given Block in // counter mode. The length of iv must be the same as the Block's block size. func NewCTR(block Block, iv []byte) Stream 这两个cipher都是通用格式, 比如只要实现了Block接口的三个函数, 都可以作为NewCTR的输入. 只要实现了Stream 接口, 就能作为输出. type Block interface { BlockSize() int Encrypt(dst, src []byte) Decrypt(dst, src []byte) } type Stream interface { XORKeyStream(dst, src []byte) } 接口和类型断言 fmt.Printf接受各种类型的入参, 它怎么知道怎么打印呢? Linux Mint 19.1 Tessa $ go doc fmt.Printf func Printf(format string, a ...interface{}) (n int, err error) Printf formats according to a format specifier and writes to standard output. It returns the number of bytes written and any write error encountered. 答案是类型断言 类型断言是对interface{}来说的, 对fmt.Printf来说, 它判断如果入参是string, 就直接打印. 如果有String方法, 那就是Stringer类型, 就调用它的String方法. type Stringer interface { String() string } var value interface{} // Value provided by caller. switch str := value.(type) { case string: return str case Stringer: return str.String() } 方法 方法的接收Type不限于结构体 在前面, 有个append方法: type ByteSlice []byte func (slice ByteSlice) Append(data []byte) []byte { // Body exactly the same as the Append function defined above. } 这个方法需要用return返回一个新的slice, 有点笨 用*Type, 即指针形式, 可以直接改caller的切片; 前面说过, 切片结构体有三个filed, 是对其底层数组的描述. func (p *ByteSlice) Append(data []byte) { slice := *p // Body as above, without the return. *p = slice } 更进一步, 可以写成Write的标准格式: func (p *ByteSlice) Write(data []byte) (n int, err error) { slice := *p // Again as above. *p = slice return len(data), nil } 有了这个方法, 就符合io.Writer接口, 就可以用标准的接口调用: var b ByteSlice //这里一定要用&对b取地址,因为只有*ByteSlice才符合io.Writer的要求 fmt.Fprintf(&b, \"This hour has %d days\\n\", 7) //go doc fmt 看Fprintf原型如下: func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) //继续用go doc io.Writer看详细定义 type Writer interface { Write(p []byte) (n int, err error) } The rule about pointers vs. values for receivers is that value methods can be invoked on pointers and values, but pointer methods can only be invoked on pointers. 这里如果直接传b, 传值会发生slice的拷贝(浅拷贝, 只拷贝slice结构体本身), 那对这个拷贝的修改就没有任何意义了. python是共享传参(不变的拷贝, 可变的相当于传指针), 而go的参数传递都是传值: 其实都是拷贝值 对int string来说, 就是拷贝值 对slice, map, channel来说, 拷贝\"描述\", 但底层数据不拷贝. -- 浅拷贝 有个特殊的地方, 虽然方法被声明为指针形式, func (p *ByteSlice) Write(data []byte) (n int, err error) 但b.Write和(&b).Write效果是一样的, 因为为了更好看, 编译器会把前者转换为后者. 接口 上面说了方法, 这里说接口 比如sort, 任何实现了sort.Interface的东东, 都能被sort包里的函数排序 //go doc sort type Interface interface{ ... } //go doc sort.Interface type Interface interface { Len() int Less(i, j int) bool Swap(i, j int) } 下面这个例子就实现了sort的Interface //这个Sequence是切片, 定义时[]里面没东西的是切片 type Sequence []int // Methods required by sort.Interface. func (s Sequence) Len() int { return len(s) } func (s Sequence) Less(i, j int) bool { return s[i] 0 { str += \" \" } str += fmt.Sprint(elem) } return str + \"]\" } 在上面Sequence的方法String中, 对每个元素(range s)调用fmt.Sprint, 是挺啰嗦的操作, 效率低. 实际上, fmt.Sprint支持直接传入slice, 在调用之前Sequence强转成slice func (s Sequence) String() string { s = s.Copy() sort.Sort(s) return fmt.Sprint([]int(s)) } 强制转换[]int(s)并没有创建新的值, 只是把s当做int切片使用; 在有些情况下, 强制转换会创建新的值, 比如把int转成float sort包提供了对int切片的排序: IntSlice 除了实现了sort的Interface方法, IntSlice 还包装了自己的Sort方法, 这样可以用 p.Sort() 的形式调用. Linux Mint 19.1 Tessa $ go doc -src sort.IntSlice // IntSlice attaches the methods of Interface to []int, sorting in increasing order. type IntSlice []int func (p IntSlice) Len() int func (p IntSlice) Less(i, j int) bool func (p IntSlice) Search(x int) int func (p IntSlice) Sort() func (p IntSlice) Swap(i, j int) yingjieb@yingjieb-VirtualBox ~ Linux Mint 19.1 Tessa $ go doc -src sort.IntSlice.Sort // Sort is a convenience method. func (p IntSlice) Sort() { Sort(p) } 最后我们的Sequence可以简化成这样: type Sequence []int // Method for printing - sorts the elements before printing func (s Sequence) String() string { s = s.Copy() //这里是把s强转成IntSlice sort.IntSlice(s).Sort() return fmt.Sprint([]int(s)) } 一个东西可以实现多个接口, 比如这个Sequence类型的东东, 既实现了sort的接口, 又实现了fmt包里的Stringer接口 type Stringer interface { String() string } 使用时 package main import \"fmt\" type Person struct { Name string Age int } func (p Person) String() string { return fmt.Sprintf(\"%v (%v years)\", p.Name, p.Age) } func main() { a := Person{\"Arthur Dent\", 42} z := Person{\"Zaphod Beeblebrox\", 9001} fmt.Println(a, z) } gofmt gofmt是编译器自带的格式化代码工具 写代码的时候不需要手动对齐, 用gofmt会自动对齐 不需要关心行宽, go没有限制; gofmt全部搞定 注释即文档 godoc会自动提取下面的注释为package的文档 top level的注释是doc commet if可以有初始化语句 go的if允许和for类似的语法, 有个初始化语句 if err := file.Chmod(0664); err != nil { log.Print(err) return err } for是三种C循环的统一 // Like a C for for init; condition; post { } // Like a C while for condition { } // Like a C for(;;) for { } // Reverse a for i, j := 0, len(a)-1; i switch接受非常量 从上到下依次对case求值, 直到为ture. 空的case是ture. func unhex(c byte) byte { switch { case '0' switch和类型断言联用: var t interface{} t = functionOfSomeType() switch t := t.(type) { default: fmt.Printf(\"unexpected type %T\\n\", t) // %T prints whatever type t has case bool: fmt.Printf(\"boolean %t\\n\", t) // t has type bool case int: fmt.Printf(\"integer %d\\n\", t) // t has type int case *bool: fmt.Printf(\"pointer to boolean %t\\n\", *t) // t has type *bool case *int: fmt.Printf(\"pointer to integer %d\\n\", *t) // t has type *int } 多返回值 C语言只有一个返回值, 有的时候只能用指针作为参数获取 //C语言版本, 出错时, 返回-1, 然后还要用errno来看具体错误. ssize_t write(int fd, const void *buf, size_t count); //go语言版本, 同时返回已经写入的个数和错误, func (file *File) Write(b []byte) (n int, err error) 正如上面的例子, go的返回值可以有名字, 初始值为\"零值\"; 不带参数的return, 就返回它们的当前值. 这样程序可读性更好. defer defer关键词, 声明一个函数, 在defer return时调用(我理解就是函数返回前). 比如函数return someValue, defer的函数在someValue被计算之后, 在return之前被调用. // Contents returns the file's contents as a string. func Contents(filename string) (string, error) { f, err := os.Open(filename) if err != nil { return \"\", err } defer f.Close() // f.Close will run when we're finished. var result []byte buf := make([]byte, 100) for { n, err := f.Read(buf[0:]) result = append(result, buf[0:n]...) // append is discussed later. if err != nil { if err == io.EOF { break } return \"\", err // f will be closed if we return here. } } return string(result), nil // f will be closed if we return here. } defer声明时对参数求值, 而不是在退出时再求值; 执行是LIFO顺序的 for i := 0; i 用defer做函数trace func trace(s string) { fmt.Println(\"entering:\", s) } func untrace(s string) { fmt.Println(\"leaving:\", s) } // Use them like this: func a() { trace(\"a\") defer untrace(\"a\") // do something.... } 因为defer语句是在声明的时候求值的, 所以先求值\"trace\"函数, 打印\"entering\"; 然后在defer return的时候, 执行\"un\"函数 func trace(s string) string { fmt.Println(\"entering:\", s) return s } func un(s string) { fmt.Println(\"leaving:\", s) } func a() { defer un(trace(\"a\")) fmt.Println(\"in a\") } func b() { defer un(trace(\"b\")) fmt.Println(\"in b\") a() } func main() { b() } 结果: entering: b in b entering: a in a leaving: a leaving: b 在panic场景下, defer的最大好处是panic链上的defer都会被调用到 比如RunCompiled中的错误处理, 如果v.run()中出现panic, 这个defer的函数还是会被调用到, 做错误处理. // RunCompiled run the VM with user supplied function fn. func (v *VM) RunCompiled(fn *CompiledFunction, args ...Object) (val Object, err error) { ... defer func() { v.childCtl.Wait() // waits for all child VMs to exit if err = v.postRun(); err != nil { fmt.Println(err) return } if fn != nil && atomic.LoadInt64(&v.aborting) == 0 { val = v.stack[v.sp-1] } }() val = UndefinedValue v.run() return } new和make分配数据 new new的数据会被初始化为0, 返回指针. 这个方式虽然没有调用构造函数或这init之类的函数来初始化结构体, 但new保证这块数据都是0. 很多时候, 0可以直接使用. 比如下面的sync.Mutex初值为0, 就表示是unlock状态, 可以直接用; bytes.Buffer也是全0, 是空的buffer, 可以直接用. type SyncedBuffer struct { lock sync.Mutex buffer bytes.Buffer } p := new(SyncedBuffer) // type *SyncedBuffer var v SyncedBuffer // type SyncedBuffer 有时候结构体初始化需要非0值, 可以new一个结构体, 然后对每个filed单独赋值 func NewFile(fd int, name string) *File { if fd 但这样显得很啰嗦, \"There's a lot of boilerplate in there\", 有很多样板 此时可以用 func NewFile(fd int, name string) *File { if fd 数组, 切片, map都可以这样赋值 //这里Enone Eio Einval应该是int类型 a := [...]string {Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"} s := []string {Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"} m := map[int]string{Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"} 数组和切片的区别 定义不一样 //数组定义 var a1 [3]int var a2 [...]int{1,2,3} //切片定义 var b1 []int b2 := make([]int, 3, 5) 初始化不同 //数组 a1 := [...]int{1,2,3} a2 := [5]int{1,2,3} //切片 b1 := make([]int,3,5) make make用于slice map 和channel, 返回初始化后的type T, 而非*T; 这里的初始化不是指0值, 而是初始化其内部表达. 比如slice的内部结构包括一个指针(指向内部数组), 还有长度和容量; 这些就不是填0值. 比如 make([]int, 10, 100) 创建一个100大小的int数组, 然后创建一个切片结构体, 指向int数组, 长度为10, 容量为100. new([]int) 创建一个0值切片, 也就是nil切片, 这样一个空切片没啥实际作用. var p *[]int = new([]int) // allocates slice structure; *p == nil; rarely useful var v []int = make([]int, 100) // the slice v now refers to a new array of 100 ints // Unnecessarily complex: var p *[]int = new([]int) *p = make([]int, 100, 100) // Idiomatic: v := make([]int, 100) 数组 和C的数组相比, go的array有如下特点: array是值, 数组赋值会拷贝数组所有元素; 所以, 如果数组作为函数参数传参, 会造成数组拷贝 数组的大小也是其类型的一部分; 所以[10]int和[20]int是不同的类型 在go里, 数组是切片的底层承载; 能用slice就不要用数组 切片 切片是对数组的包装和引用 对切片的赋值不会复制数组, 只会引用同一个数组 传入切片到函数, 是传引用. 对数组的改变会被caller看到; 和传数组的指针效果差不多. 因为切片本身就有大小, 所以这个Read原型不需要C里面的nbyte参数. //go版本 func (f *File) Read(buf []byte) (n int, err error) //只readbuf的前32字节, 数组的切片buf[i:j]从0开始编号, 表示从i到(j-1)的元素, 不包括j; 所以这里是0到31的元素 n, err := f.Read(buf[0:32]) //C版本 ssize_t read(int fildes, void *buf, size_t nbyte); map map里面没有的key, 返回0值. attended := map[string]bool{ \"Ann\": true, \"Joe\": true, ... } if attended[person] { // will be false if person is not in the map fmt.Println(person, \"was at the meeting\") } 因为没有的元素也返回0值, 但有的时候map里存在的元素的值就是0值, 那怎么区分? 可以这样, if支持初始化语句: if seconds, ok := timeZone[tz]; ok 打印 fmt.Printf(\"Hello %d\\n\", 23) fmt.Fprint(os.Stdout, \"Hello \", 23, \"\\n\") fmt.Println(\"Hello\", 23) fmt.Println(fmt.Sprint(\"Hello \", 23)) append和...扩展 //原型 func append(slice []T, elements ...T) []T //append接受变长参数 x := []int{1,2,3} x = append(x, 4, 5, 6) fmt.Println(x) //append另外一个slice, 用...扩展 x := []int{1,2,3} y := []int{4,5,6} //没有...的话, y的类型错误 x = append(x, y...) fmt.Println(x) 全局变量初始化和init const变量是编译时初始化的, 只能是常量 而普通变量在运行时初始化, 比C更方便的是, 可以在声明的时候就调用函数来初始化. var ( home = os.Getenv(\"HOME\") user = os.Getenv(\"USER\") gopath = os.Getenv(\"GOPATH\") ) 每个文件都可以有一个或多个init函数, 它在全局变量初始化后被调用 一个常见的场景是, 在main执行之前, 用init来检查运行环境: func init() { if user == \"\" { log.Fatal(\"$USER not set\") } if home == \"\" { home = \"/home/\" + user } if gopath == \"\" { gopath = home + \"/go\" } // gopath may be overridden by --gopath flag on command line. flag.StringVar(&gopath, \"gopath\", gopath, \"override default GOPATH\") } "},"notes/golang_进阶.html":{"url":"notes/golang_进阶.html","title":"Golang 进阶","keywords":"","body":" 继承, 匿名包含2 实例 StatsConn 总结 继承, 匿名包含 总结 interface的赋值 第一种情况: 如果方法的receiver类型不是引用方式 第二种情况: 方法的receiver是引用方式声明的 总结 go FAQ go1.3是c写的, 后面的编译器是go写的 工具自动把C转成go的 interface的签名必须完全一样 例子1 例子2 nil error不是nil map的元素不能取地址 内置println 空结构体 迭代器样板 scanner样板 range样板 关于copy 函数级并发 顺序版本 goroutine并发版本 channel的广播 close广播 交换机复制式广播 改进 代码心得 再看testing 例子 再议reflect 打印变量信息 测试代码 包初始化 再说切片和数组 数组是值 GO的数组表达和C一样 切片 动态链接 支持动态链接的平台 动态链接支持的架构 交叉编译go 支持交叉编译的平台 一次运行时异常打印 goroutine和共享变量 理论 实践 调试环境变量 举例 GODEBUG GOTRACEBACK 名字冲突和type别名 type定义新类型 type别名 goroutine vs thread 默认的thread数 goroutine介绍 json和万能interface{} 已知结构体 不知道结构体情况下 encoder和decoder 继承, 匿名包含2 我们都知道, 一个结构体匿名包含另一个结构体, 就能继承其方法. 其实, 匿名包含一个interface, 也能继承. 比如: //Fooer是个interface type Fooer interface { Foo() string } //Container匿名包含了Fooer type Container struct { Fooer } // TheRealFoo is a type that implements the Fooer interface. type TheRealFoo struct { } // TheRealFoo真正实现了Fooer func (trf TheRealFoo) Foo() string { return \"TheRealFoo Foo\" } // sink takes a value implementing the Fooer interface. func sink(f Fooer) { fmt.Println(\"sink:\", f.Foo()) } //shilhuaContainer的时候, 初始化Fooer域 co := Container{Fooer: TheRealFoo{}} //下面申请的一幕来了, co也是Fooer //co因为是Container的实例, 后者匿名包含了Fooer(底层实现是TheRealFoo{}) sink(co) //输出: sink: TheRealFoo Foo 由以上得知, 匿名包含interface的效果和匿名包含结构体类似, 但interface变量要正确的初始化. 如果没有正确初始化, 编译是能通过的, 但会运行时错误, 比如下面的错误实例: co := Container{} sink(co) 会打印runtime error: invalid memory address or nil pointer dereference 实例 StatsConn 这里例子里, StatsConn匿名包含了net.Conn, 从而继承了net.Conn的所有函数, 但它重载了Read方法, 目的是增加字节数统计 type StatsConn struct { net.Conn //注意这里的net.Conn是个interface BytesRead uint64 } func (sc *StatsConn) Read(p []byte) (int, error) { n, err := sc.Conn.Read(p) sc.BytesRead += uint64(n) return n, err } 实例化StatsConn的时候, 用tcp的net.Conn实例来初始化. conn, err := net.Dial(\"tcp\", u.Host+\":80\") if err != nil { log.Fatal(err) } sconn := &StatsConn{conn, 0} 从此以后, sconn拥有net.Conn的所有属性, 所有net.Conn能用的地方, 都可以用sconn, 比如: resp, err := ioutil.ReadAll(sconn) if err != nil { log.Fatal(err) } 总结 匿名包含interface和匿名包含struct都能继承 需要注意被包含的interface域应该正确的实例化 参考: https://eli.thegreenplace.net/2020/embedding-in-go-part-3-interfaces-in-structs/ 继承, 匿名包含 package main import \"fmt\" /* 继承 一个结构体嵌到另一个结构体，称作组合 匿名和组合的区别 如果一个struct嵌套了另一个匿名结构体，那么这个结构可以直接访问匿名结构体的方法，从而实现继承 如果一个struct嵌套了另一个【有名】的结构体，那么这个模式叫做组合 如果一个struct嵌套了多个匿名结构体，那么这个结构可以直接访问多个匿名结构体的方法，从而实现多重继承 */ type Car struct { weight int name string } func (p *Car) Run() { fmt.Println(\"running\") } type Bike struct { Car //注意这里, 虽然Car的receiver是引用方式, 而这里Car做为\"值\"被包含, 但是Bike依然继承了Car的方法. lunzi int } type Train struct { Car } func (p *Train) String() string { str := fmt.Sprintf(\"name=[%s] weight=[%d]\", p.name, p.weight) return str } func main() { var a Bike a.weight = 100 a.name = \"bike\" a.lunzi = 2 fmt.Println(a) a.Run() var b Train b.weight = 100 b.name = \"train\" b.Run() fmt.Printf(\"%s\", &b) } 总结 不管一个type是用receiver方式实现的方法, 还是值方式. 我们都说这个type实现了这个方法, 从而满足了某类interface的要求 对引用方式的receiver来说, 只要这个对象是addressable的, 编译器就会自动取地址. 换个说法是, 只要这个\"值\"对象, 在传递的过程中(比如赋值)不被赋值, 对其取地址不会取到其\"拷贝对象\"上去, 就可以搞. interface的赋值 比如最简单的interface type Phone interface { call() } 只要实现了call()方法的类型, 都可以是Phone 声明一个Phone类型的变量 //这是interface类型的变量 var phone Phone 对interface类型的变量赋值, 要看方法的实现情况 第一种情况: 如果方法的receiver类型不是引用方式 type NokiaPhone struct { } func (nokiaPhone NokiaPhone) call() { fmt.Println(\"I am Nokia, I can call you!\") } 那么对phone变量的赋值, 地址和值都可以 //new返回一个指针, 并对其内存清零; //以下都对 phone = new(NokiaPhone) phone = &NokiaPhone{} phone = NokiaPhone{} phone.call() 更进一步, 其实取不取引用, 是不一样的. 但都能直接用phone.call(), 因为go对指针也是用点方式调用其方法. func main() { //这是interface类型的变量 var phone Phone var nokiaPhone NokiaPhone fmt.Printf(\"addr: %p, value: %v, type: %T \\n\", &nokiaPhone, nokiaPhone, nokiaPhone) phone = &nokiaPhone fmt.Printf(\"\\naddr: %p, value: %v, type: %T \\n\", phone, phone, phone) phone.call() phone = nokiaPhone fmt.Printf(\"\\naddr: %p, value: %v, type: %T \\n\", phone, phone, phone) phone.call() } //输出 addr: 0x1e529c, value: {}, type: main.NokiaPhone addr: 0x1e529c, value: &{}, type: *main.NokiaPhone I am Nokia, I can call you! addr: %!p(main.NokiaPhone={}), value: {}, type: main.NokiaPhone I am Nokia, I can call you! 第二种情况: 方法的receiver是引用方式声明的 type NokiaPhone struct { } func (nokiaPhone *NokiaPhone) call() { fmt.Println(\"I am Nokia, I can call you!\") } 这种情况下, phone = &nokiaPhone是合法的. 而phone = nokiaPhone不行 错误是: NokiaPhone 没有实现Phone方法. ./prog.go:30:8: cannot use nokiaPhone (type NokiaPhone) as type Phone in assignment: NokiaPhone does not implement Phone (call method has pointer receiver) 注意, 虽然当receiver是引用方式时, 不能phone = nokiaPhone, 但是可以nokiaPhone可以调用call()函数, 即这样写没有任何问题: type NokiaPhone struct { } func (nokiaPhone *NokiaPhone) call() { fmt.Println(\"I am Nokia, I can call you!\") } func main() { var nokiaPhone NokiaPhone //即使receiver是引用方式, 也能直接用\"值\"来调用 nokiaPhone.call() } 不是说NokiaPhone does not implement Phone (call method has pointer receiver)吗? 怎么矛盾了? -- 不矛盾. 编译器做了语法糖, 实际调用的是: (&nokiaPhone).call() 在call()函数看来, receiver还是地址. 那为什么上面的interface赋值phone = nokiaPhone不行呢? -- 因为interface赋值也是值拷贝. 如果这里可以的话, 后面对phone.call()的调用, 实际上最终是(&拷贝后的nokiaPhone).call(), 对拷贝后的对象取地址没有任何意义, 所以编译器禁止这样搞. 总结 方法的receiver是pointer receiver类型时, interface变量只接受引用方式赋值. 方法的receiver是非指针方式, interface变量的赋值既可以是值, 也可以是引用. 即使是receiver方式, \"值\"对象也可以直接调用其方法. go FAQ 官方faq 101FAQ go1.3是c写的, 后面的编译器是go写的 工具自动把C转成go的 go编译器实现背景 工具把gc编译器从C转成go interface的签名必须完全一样 例子1 type Equaler interface { Equal(Equaler) bool } type T int func (t T) Equal(u T) bool { return t == u } // does not satisfy Equaler type T2 int func (t T2) Equal(u Equaler) bool { return t == u.(T2) } // satisfies Equaler 例子2 type Opener interface { Open() Reader } func (t T3) Open() *os.File //T3 does not satisfy Opener nil error不是nil error是个interface{}, 而interface{}是{类型 值}的表达, 两个都是nil的时候, interface才是nil //这里return的error永远不是nil, 因为其类型是*MyError func returnsError() error { var p *MyError = nil if bad() { p = ErrBad } return p // Will always return a non-nil error. } map的元素不能取地址 因为map是hash表, 随着map大小的动态增减, 里面的元素可能会重新算hash, 改变存储位置, 那地址就变了. In Go, a map is designed as a container which can contain unlimited number of entries if memory is available. And, in the official Go runtime implementation, to ensure good map element indexing efficiency, each map value only maintains one continuous memory segment for the entire entries stored in that map. Therefor, Go runtime needs to allocate larger memory segments for a map from time to time when there are more and more entries being put into the map. In the process, the entries stored on older memory segments will be moved to newer memory segments. There might be also some other reasons caausing entry memory movements. In other words, the addresses of map elements will change from time to time on need. If map elements are allowed to be taken addresses, then when some map entries are moved, Go runtime must update all pointers which are storing the addresses of the moved elements, which brings many difficulties in implemnting Go compilers and runtimes and decreases Go program running performance much. So, currently, map elements are disallowed to be taken addresses. 内置println 内置的println是debug时候用的, 输出到stderr 另外, println不会导致变量逃逸 Currently (Go Toolchain 1.15), for the standard Go compiler, calls to the built-in print/println functions will not make the values referenced by the arguments of the calls escape to heap, whereas the print functions in the fmt and log standard packages will. 参考: print-builtin-fmt-log 空结构体 一般来说, 空结构体struct{}是0字节大小的. 但放在另外一个结构体的最后, 编译器会pad一些字节. func main() { type T1 struct { a struct{} x int64 } fmt.Println(unsafe.Sizeof(T1{})) // 8 type T2 struct { x int64 a struct{} } fmt.Println(unsafe.Sizeof(T2{})) // 16 } 解释是: 一个结构体如果可以取地址, 那里面的每个field都应该能被取地址. 而如果T2不pad8个字节, 那对T2的a取地址就是其他对象的地址了, 就超过了T2结构体的范围了. 所以后面要加8字节. 而T1不会有这个越结构体问题. 参考: 空结构体在最后 迭代器样板 scanner样板 bufio的scanner是个很好的迭代器样板. 下面的代码使用scanner扫描一个文件, 比对每一行, 如果不是msg就判定错误. file := \"test/testc.log\" f, err := os.Open(file) if err != nil { t.Fatal(err) } defer f.Close() scanner := bufio.NewScanner(f) var i int pos := len(\"[2020/08/09 14:47:41.769979][testfile][INFO] \") for i = 0; scanner.Scan(); i++ { if scanner.Text()[pos:pos+len(msg)] != msg { break } } if err := scanner.Err(); err != nil { t.Fatal(err) } if i != routineNum*lineNum { t.Fatalf(\"Unexpected line number: got %d, expect %d\", i, routineNum*lineNum) } 关于scanner迭代器: scanner.Scan()返回bool, 利于for判断, 为true继续, 为false则退出for scanner本身是个NewScanner()出来的对象, 在for循环里scanner.xxx()来反应本次迭代的更新的内容 for循环退出, 判断scanner.Err()就能知道是异常终止还是for循环自然结束. range样板 对于自己定义的结构体, 不能用内置的range关键词来迭代; 但可以自己模拟一个, 比如sync/map包, 提供了对map的同步访问API type Map func (m *Map) Delete(key interface{}) func (m *Map) Load(key interface{}) (value interface{}, ok bool) func (m *Map) LoadAndDelete(key interface{}) (value interface{}, loaded bool) func (m *Map) LoadOrStore(key, value interface{}) (actual interface{}, loaded bool) func (m *Map) Range(f func(key, value interface{}) bool) func (m *Map) Store(key, value interface{}) 这里的Range()函数就是一个迭代器. 它接受一个函数f, f返回false则停止迭代. 关于copy 比如对一个slice做copy, 这样的代码能work copy(l[i+1:], l[i:]) 为了把位置i后面的元素都挪一个位置, 代价是把这后面的所有元素都copy一遍 这里的copy实际上是overlap的, copy的时候, 源为l[i:], 目的是l[i+1:], 目的比源向后移动一个位置, 而且目的的元素个数也少了一个. golang的copy允许overlap $ go doc builtin.copy package builtin // import \"builtin\" func copy(dst, src []Type) int The copy built-in function copies elements from a source slice into a destination slice. (As a special case, it also will copy bytes from a string to a slice of bytes.) The source and destination may overlap. Copy returns the number of elements copied, which will be the minimum of len(src) and len(dst). 注意, 如果按照普通的for循环式的copy思路, src和dst重叠时不能正常工作的. 有人讨论说golang的copy语义类似memmove memcpy: 当src和dst重叠时, 结果不确定 memmove: src和dst可以重叠, 结果是正确的拷贝; 可以理解成有个临时缓冲做中转. 实际上并不需要中间buffer, 只需要在开始的时候判断是从前往后拷贝还是从后往前拷贝就行了. 结论: golang的copy支持overlap, 可以正确的拷贝 举例: func main() { l := []int{0,1,2,3,4,5,6,7,8,9} l = append(l, 100) copy(l[4+1:], l[4:]) fmt.Println(l) } 输出: [0 1 2 3 4 4 5 6 7 8 9] 可以看到从i=4开始, 整个slice向右平移了一个位置, 是符合预期的. 函数级并发 顺序版本 piMap := make(map[int]*pidinfo.PidInfo, len(tgtPids)) collect := func() { for _, pid := range tgtPids { pi := piMap[pid] if pi != nil { if err := pi.Update(); err != nil { //fmt.Printf(\"main: %v\\n\", err) piMap[pid] = nil continue } } } } piMap是个全局的map. 现在想把第6行pi.Update()routine化调用, 达到并发执行这个函数的效果. 要解决的问题: piMap全局map的修改不是并发安全的 主程序怎么等待多个pi.Update()完成 参考下面的并发化实现 goroutine并发版本 使用unbuffered channel传递结果 使用计数器控制等待次数 程序的效果是第10行的for代码块可以并发执行, 第22行的for代码块等待上面的routine执行完 并发的版本比顺序版本代码逻辑更复杂点, goroutine的创建 调度和channel的同步都有开销, 但如果并发的收益很大的化还是值得的. piMap := make(map[int]*pidinfo.PidInfo, len(tgtPids)) collect := func() { //可以在函数内部type定义类型 type result struct { pid int err error } resultCh := make(chan result) //计数器, 记录go了多少次 cnt := 0 for _, pid := range tgtPids { pid := pid pi := piMap[pid] if pi != nil { cnt++ go func() { err := pi.Update() //结果写回channel resultCh channel的广播 close广播 close()一个channel, 所有read这个channel的routine都会被唤醒. 注意, 对同一个channel的操作都会有加锁操作, 因而在多核环境下, 锁竞争的开销会变得非常大. 除了这个锁的缺点, close()广播的实现的好处是比较简洁. //代码片段, \"监听\"的goroutine //这样的goroutine可以有很多 //goroutine n for { //update from rootCheck pointer //ti.rootCheck是个指向channel的指针 //关键是用临时变量保存这个channel的一个拷贝 check := *ti.rootCheck select { case 交换机复制式广播 如何让channel做到1对多的广播? 我们知道close一个channel, 那么它的所有reader都会返回一个零值, 这就是广播. 还有别的方法吗? 下面的例子使用订阅模式模仿了广播的api. 详见:stackoverflow github A more elegant solution is a \"broker\", where clients may subscribe and unsubscibe to messages. To also handle subscribing and unsubscribing elegantly, we may utilize channels for this, so the main loop of the broker which receives and distributes the messages can incorporate all these using a single select statement, and synchronization is given from the solution's nature. Another trick is to store the subscribers in a map, mapping from the channel we use to distribute messages to them. So use the channel as the key in the map, and then adding and removing the clients is \"dead\" simple. This is made possible because channel values are comparable, and their comparison is very efficient as channel values are simple pointers to channel descriptors. Without further ado, here's a simple broker implementation: type Broker struct { stopCh chan struct{} publishCh chan interface{} subCh chan chan interface{} unsubCh chan chan interface{} } func NewBroker() *Broker { return &Broker{ stopCh: make(chan struct{}), publishCh: make(chan interface{}, 1), subCh: make(chan chan interface{}, 1), unsubCh: make(chan chan interface{}, 1), } } func (b *Broker) Start() { subs := map[chan interface{}]struct{}{} for { select { case Example using it: func main() { // Create and start a broker: b := NewBroker() go b.Start() // Create and subscribe 3 clients: clientFunc := func(id int) { msgCh := b.Subscribe() for { fmt.Printf(\"Client %d got message: %v\\n\", id, Output of the above will be Client 2 got message: msg#0 Client 0 got message: msg#0 Client 1 got message: msg#0 Client 2 got message: msg#1 Client 0 got message: msg#1 Client 1 got message: msg#1 Client 1 got message: msg#2 Client 2 got message: msg#2 Client 0 got message: msg#2 Client 2 got message: msg#3 Client 0 got message: msg#3 Client 1 got message: msg#3 改进 You may consider the following improvements. These may or may not be useful depending on how / to what you use the broker. Broker.Unsubscribe() may close the message channel, signalling that no more messages will be sent on it: func (b *Broker) Unsubscribe(msgCh chan interface{}) { b.unsubCh This would allow clients to range over the message channel, like this: msgCh := b.Subscribe() for msg := range msgCh { fmt.Printf(\"Client %d got message: %v\\n\", id, msg) } Then if someone unsubscribes this msgCh like this: b.Unsubscribe(msgCh) The above range loop will terminate after processing all messages that were sent before the call to Unsubscribe(). If you want your clients to rely on the message channel being closed, and the broker's lifetime is narrower than your app's lifetime, then you could also close all subscribed clients when the broker is stopped, in the Start() method like this: case 代码心得 go b.Start()起了单独的routine来维护b(即中转线程), 主要是subscribe和unsubscribe的map维护, 和msg的分发; 用单独的goroutine可以避免锁. 订阅和取消订阅实际上是对内部map的添加和删除key, subs[msgCh] = struct{}{}和delete(subs, msgCh). 这里把新make的channel当作map的key, 因为channel是可比较的, 而且比较效率高.关键是插入和删除key非常简单. 中转线程是关键, 类似个交换机; 为了避免在select里面等待, msg都是bufferred模式.而且写都带default, 是非阻塞的. 再看testing 格式go test [build/test flags] [packages] [build/test flags & test binary flags] 帮助文档go help test和go doc testing testdata目录会被go tool忽略, 留给测试case使用 测试case的输出全部走stdout, 即使case里输出到stderr; stderr是留给testing框架用的 默认跑test, 但不跑benchamrk go test除了支持全部go build参数外, 还支持 #基础选项 -args 传个测试case的 -c 只编译不运行, 生成pkg.test -exec xprog 用外部程序xprog来运行test二进制 -i 只安装不运行 -o 输出test二进制 #测试过程控制 -bench regexp 运行正则匹配到的benchmark; -bench .除了会运行test项, 还会运行所有benchmark项 -run regexp 只运行正则的case, 只匹配test项和example项: -benchtime t 默认是1s, 可以指定其他的time.Duration比如1h30s -count n 重复执行所有case n次 -cover 使能coverage 分析 -cpu 1,2,4 指定CPU列表 -parallel n 对调用t.Parallel的case, 指定并发为n; 默认为GOMAXPROCS -short 减小测试时间, 自己在case里判断是否if testing.Short() -timeout d 默认超时时间是10m, 可以改 -list regexp 列出正则匹配的项 #profiling相关 -benchmem: 打印内存使用情况 -blockprofile block.out: 打印goroutine的阻塞情况 -coverprofile cover.out: 打印coverage -memprofile mem.out: mem profile -cpuprofile cpu.out: cpu profile -trace trace.out: 输出trace profile # 命令举例 //不跑test项, 跑所有的benchmark项: go test -run 任意匹配不到的正则 -bench . -benchtime 10s 例子 func TestAbs(t *testing.T) { got := Abs(-1) if got != 1 { t.Errorf(\"Abs(-1) = %d; want 1\", got) } } func BenchmarkHello(b *testing.B) { for i := 0; i t.Run(\"A=1\", func(t *testing.T) { ... }) t.Run(\"A=2\", func(t *testing.T) { ... }) t.Run(\"B=1\", func(t *testing.T) { ... }) // } go test -run '' # Run all tests. go test -run Foo # Run top-level tests matching \"Foo\", such as \"TestFooBar\". go test -run Foo/A= # For top-level tests matching \"Foo\", run subtests matching \"A=\". go test -run /A=1 # For all top-level tests, run subtests matching \"A=1\". // 用TestMain来接管test执行, 必须先做一些复杂的初始化; 然后调用m.Run func TestMain(m *testing.M) { // call flag.Parse() here if TestMain uses flags os.Exit(m.Run()) } 再议reflect reflect.Type是个interface, 而reflect.Value是个结构体. Type更抽象, 而Value是实例. 打印变量信息 func desc(i interface{}) { t := reflect.TypeOf(i) if t.Kind() != reflect.Ptr { //加下面的代码会panic, 提示 panic: reflect.Value.Addr of unaddressable value //我理解是这样的: 传参的时候, 把对象赋值给interface{}, 也是值传递; //reflect.ValueOf(i)其实是有地址的, 但它是个\"临时的拷贝\", 它的地址没有意义 //所以下面的语句不让获取这个临时拷贝的地址; 这个临时的拷贝被认为是unaddressable //fmt.Println(reflect.ValueOf(i).Addr()) fmt.Printf(\"(%v, %T)@\\n\", i, i, t.Size()) } else { //这个case对应desc(&a), 此时i实际上是个指针. //那么v.Elem()和t.Elem()就是解引用 //用v.Elem().UnsafeAddr()代替i, 也能正确打印地址(需换成%x打印) v := reflect.ValueOf(i) fmt.Printf(\"(%v, %v)@[%p]\\n\", v.Elem(), t.Elem(), i, t.Elem().Size()) } } 其中 t是Type类型的对象, t本身是interface类型 t.Kind()是底层实例的类型, 与reflect包里的常量类型可比较 t.Size()返回类型t的大小 v是Value类型的对象, 代表了实际的value v.Elem()指v是指针的时候, 其指向的对象的值 t.Elem()是t是指针的时候, 其指向对象的类型 t.Elem().Size是其指向对象类型的大小 测试代码 func main() { a := 55 fmt.Printf(\"(%v, %T)@(%p, %d)\\n\", a, a, &a, unsafe.Sizeof(a)) desc(a) desc(&a) m := struct { int string }{6, \"hello\"} fmt.Printf(\"(%v, %T)@[%p, %d]\\n\", m, m, &m, unsafe.Sizeof(m)) desc(m) desc(&m) } //结果: (55, int)@(0xc0000c0020, 8) (55, int)@ (55, int)@[0xc0000c0020] ({6 hello}, struct { int; string })@[0xc0000b8040, 24] ({6 hello}, struct { int; string })@ ({6 hello}, struct { int; string })@[0xc0000b8040] 包初始化 再main执行之前, 被依赖的包会递归的包含, 并执行里面的全局变量, init()函数等. 再说切片和数组 数组是值 数组是值类型, 参数里传数组会复制整个数组. Go语言中数组是值语义。一个数组变量即表示整个数组，它并不是隐式的指向第一个元素的指针（比如C语言的数组），而是一个完整的值。当一个数组变量被赋值或者被传递的时候，实际上会复制整个数组。如果数组较大的话，数组的赋值也会有较大的开销。为了避免复制数组带来的开销，可以传递一个指向数组的指针，但是数组指针并不是数组。 var a = [...]int{1, 2, 3} // a 是一个数组 var b = &a // b 是指向数组的指针 fmt.Println(a[0], a[1]) // 打印数组的前2个元素 fmt.Println(b[0], b[1]) // 通过数组指针访问数组元素的方式和数组类似 for i, v := range b { // 通过数组指针迭代数组的元素 fmt.Println(i, v) } GO的数组表达和C一样 注意, go里面数组是数组, 切片是切片; 上面说到切片的表达是个结构体, 包括了数组的指针和大小, 但数组还是和C一样的\"原始\"样子: 指向首元素的指针可以代表这个数组. func main() { // s是个切片 s := []int{1,2,3,4,5,6,7} sp := &s fmt.Println(sp) //&s[0]已经是对底层数组的元素了, 前面说过, go的底层数组和c表达一样 //所以可以强转成*[3]int, 注意这个是个数组; 只能转成数组 sp3 := (*[3]int)(unsafe.Pointer(&s[0])) fmt.Println(sp3) } //结果 &[1 2 3 4 5 6 7] &[1 2 3] 切片 切片是对数组的表达, 带有个header; header里面有底层数组的指针和数组的大小;字符串是只读的int数组, 有个和切片类似的头, 行为和切片有点像. func main() { type MyInt int var a = []int{7, 8, 9} //b也是切片, 但和a共享底层数组 var b = *(*[]MyInt)(unsafe.Pointer(&a)) b[0]= 123 fmt.Println(a) // [123 8 9] fmt.Println(b) // [123 8 9] fmt.Printf(\"%T \\n\", a) // []int fmt.Printf(\"%T \\n\", b) // []main.MyInt } 动态链接 应该是从go1.8左右开始, go支持动态链接 This is possible now using -linkshared flag What you need to do is to first run this command: go install -buildmode=shared -linkshared std (Above code makes all common packages shareable!) then 这个命令在mint下面, 会生成一个36M的libstd.so; 用xz压缩后是8M左右. Linux Mint 19.1 Tessa $ llh ./go/pkg/linux_amd64_dynlink/libstd.so -rw-rw-r-- 1 yingjieb yingjieb 36M Oct 9 09:15 ./go/pkg/linux_amd64_dynlink/libstd.so go install -buildmode=shared -linkshared userownpackage finally when compiling your code you need to run: go build -linkshared yourprogram What the above those is now it rather than statically linking everything only dynamically links them and you will end up with much smaller compiled files. Just to give you an idea my \"hello.go\" file with static linking is 2.3MB while the same code using dynamic linking is just 12KB! 我在mint上, 用共享模式, hello的可执行大小是20K. 静态模式是2M. Linux Mint 19.1 Tessa $ ldd hello linux-vdso.so.1 (0x00007ffea79b8000) libstd.so => /home/yingjieb/repo/gorepo/go/pkg/linux_amd64_dynlink/libstd.so (0x00007f8189201000) libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f8188e10000) libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f8188c0c000) libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f81889ed000) /lib64/ld-linux-x86-64.so.2 (0x00007f818bdda000) Linux Mint 19.1 Tessa $ ldd /home/yingjieb/repo/gorepo/go/pkg/linux_amd64_dynlink/libstd.so linux-vdso.so.1 (0x00007fff4b3cb000) libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f065b410000) libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f065b1f1000) libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f065ae00000) /lib64/ld-linux-x86-64.so.2 (0x00007f065dfe9000) 说明: go程序的静态编译时按需编译. 共享编译std, 会把go的库全部编译, 引入全部依赖, 比如libc 看起来hello依赖libc, 但其实是go的共享库libstd.so依赖libc等动态库. 如果说MIPS上n32不支持cgo的话, 那静态编译用MIPS64的libc.a行吗? 这个东西编译的时候有. buildmode详细说明 go help buildmode -buildmode=shared Combine all the listed non-main packages into a single shared library that will be used when building with the -linkshared option. Packages named main are ignored. 支持动态链接的平台 go1.12只有下面的平台支持: \"linux/386\", \"linux/amd64\", \"linux/arm\", \"linux/arm64\", \"linux/ppc64le\", \"linux/s390x\" 动态链接支持的架构 src/cmd/go/internal/work/init.go 只有case \"linux/386\", \"linux/amd64\", \"linux/arm\", \"linux/arm64\", \"linux/ppc64le\", \"linux/s390x\": MIPS和PPC大端都不支持 交叉编译go go的交叉编译很简单, 只要配置两个环境变量 GOOS GOARCH 比如: //就用x86的gc go编译器 //默认编译出native的go程序, 那这里就是x86-64 go build goweb.go //加环境变量, 就能交叉编译 GOOS=linux GOARCH=mips64 go build goweb.go //编译很顺利, 直接生成mips64的可执行文件 Linux Mint 19.1 Tessa $ file goweb goweb: ELF 64-bit MSB executable, MIPS, MIPS-III version 1 (SYSV), statically linked, not stripped 网上说用gcc go编译器不行:最后一个回复 支持交叉编译的平台 Linux Mint 19.1 Tessa # go tool dist list | grep linux linux/386 linux/amd64 linux/arm linux/arm64 linux/mips linux/mips64 linux/mips64le linux/mipsle linux/ppc64 linux/ppc64le linux/s390x 一次运行时异常打印 func main() { flag.Parse() http.Handle(\"/\", http.HandlerFunc(QR)) err := http.ListenAndServe(*addr, nil) if err != nil { log.Fatal(\"ListenAndServe:\", err) } } 在err := http.ListenAndServe(*addr, nil), 打印出死锁异常 能看出来: go的crash报告很清楚: 有调用链, 行数, 参数 这个http的调用, 大体上经过了net/http, net, poll, syscall, sync等模块的调用, 分层很清楚 fatal error: all goroutines are asleep - deadlock! goroutine 1 [sync.Cond.Wait]: runtime.goparkunlock(...) /usr/local/go/src/runtime/proc.go:310 sync.runtime_notifyListWait(0x8407f0, 0x0) /usr/local/go/src/runtime/sema.go:510 +0x120 sync.(*Cond).Wait(0x8407e8, 0x8407e0) /usr/local/go/src/sync/cond.go:56 +0xe0 syscall.(*queue).waitRead(0x8407e0, 0x1, 0x0, 0x0, 0x8001e0, 0x4b6bb0, 0x864c8c, 0x0) /usr/local/go/src/syscall/net_nacl.go:292 +0xe0 syscall.(*msgq).read(0x8407e0, 0x4296e0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0) /usr/local/go/src/syscall/net_nacl.go:409 +0xc0 syscall.(*netFile).accept(0x832240, 0x0, 0x832240, 0x0, 0x0, 0x4c49, 0x70, 0x4382e0) /usr/local/go/src/syscall/net_nacl.go:571 +0x40 syscall.Accept(0x3, 0x0, 0xff800000, 0x7ff, 0x56f40, 0x4c49, 0xf8ea81ab, 0x850848) /usr/local/go/src/syscall/net_nacl.go:799 +0xa0 internal/poll.accept(0x3, 0x4c01, 0x1, 0x83f000, 0x2, 0x2, 0x8, 0x743800, 0x2, 0x0) /usr/local/go/src/internal/poll/sys_cloexec.go:24 +0x40 internal/poll.(*FD).Accept(0x850840, 0x4c49, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0) /usr/local/go/src/internal/poll/fd_unix.go:377 +0x140 net.(*netFD).accept(0x850840, 0x864e20, 0x7321c0, 0x4c49, 0xfefc0008, 0x0) /usr/local/go/src/net/fd_unix.go:238 +0x40 net.(*TCPListener).accept(0x83efa0, 0x4c49, 0x20, 0x44c9a0, 0x83c201, 0x0) /usr/local/go/src/net/tcpsock_posix.go:139 +0x40 net.(*TCPListener).Accept(0x83efa0, 0x0, 0x83f040, 0x4c49, 0x4242e0, 0x725e08) /usr/local/go/src/net/tcpsock.go:261 +0x40 net/http.(*Server).Serve(0x8526c0, 0x5194d0, 0x83efa0, 0x5, 0x0, 0x0) /usr/local/go/src/net/http/server.go:2896 +0x2e0 net/http.(*Server).ListenAndServe(0x8526c0, 0x469211, 0x8526c0, 0x516bb0) /usr/local/go/src/net/http/server.go:2825 +0xe0 net/http.ListenAndServe(...) /usr/local/go/src/net/http/server.go:3080 main.main() /tmp/sandbox101442177/prog.go:17 +0x160 goroutine和共享变量 理论 在loop里使用goroutine时, 比如 for _, val := range values { go func() { fmt.Println(val) }() } 首先, goroutine是能使用变量val的; 因为goroutine底层也是运行在线程上的, 每个go进程有自己的\"worker\"线程, 一般在启动时和CPU核数相等, 它们的地址空间是一样的. 但goroutine会等待调度执行, 可能在主线程的loop结束后才开始执行. 此时的val被\"捕获\"进goroutine(有时也叫闭包), 但val只有一个值, 在for里改变, 那很可能总是捕获到最后一次的值. 后面的实验也验证了这一点. 一般不这样直接用loop变量, 而是把loop变量当做参数传入goroutine, 这样这个参数在传递的时候, 就保存在goroutine的栈里了, 不用等goroutine被调度执行才evaluate这个值. for _, val := range values { go func(val interface{}) { fmt.Println(val) }(val) } 实践 对比左右两个代码, 左边在循环结束后sleep了, 右边没有. 输出就不一样, 为什么呢? 注: 这个网站提供的环境, 只有一个核.fmt.Println(\"CPU num = \", runtime.NumCPU())打出来是1. 所以goroutine\"并行\"x++, 不存在并发的问题, 结果是对的. x++并不是原子的, 一般C里面要用编译器提供的原子操作宏: __sync_fetch_and_add( &x, 1 );或atomic系列宏 为什么sleep和不sleep结果不同? sleep时, goroutine们先运行 不sleep时, 大概率主线程先运行, 此时goroutine们还没有开始 后面加了打印x的版本看的更清楚: 不sleep时, goroutine都没有开始执行. 我们改一下代码: 注: 在最后是不能打印i的值的, 因为i的作用域只在循环里面, 出了循环就没有了. 这里首先有个警告: 是go vet打印出来的, 大意是说循环变量i被goroutine函数func捕获. 一般这样用容易出问题. 这也说明go提供里vet机制, 能做代码静态检查 每个goroutine的x, 都是同一个x, 没有私有拷贝. 在x++的时候, 从同一个地址取值, 加一, 再放回同一个内存地址. 所以现象上是正常的, 但前提是单核运行才正常. 每个goroutine都打印循环变量i, 但都是5; 因为i在for里改变, 等到goroutine被调度执行时, 这个变量已经是5了. 把i在循环里赋值给ii, 结果又不一样: 注: 区别在于变量ii的位置: 在循环外定义, 则ii只是一个地址. 在循环里面定义, 则ii每次进循环都要重新创建, 地址不一样. 地址不变, 则goroutine取值的时候, 就只能拿到一个值. 地址变了, 则每个goroutine都能取到对应的值. goroutine知道自己要\"捕获\"哪个ii 把17行的go关键词去掉, 只留闭包函数func, 则这个func会在loop里依次被调用, 打印正常的0到4. 调试环境变量 举例 #10s钟打印一次调度信息 GODEBUG=schedtrace=10000,scheddetail=1 GOTRACEBACK=system ./json_load -fileName test.json -loopNum 100000 > /dev/null GODEBUG GODEBUG环境变量包括子变量, 用name=val, name=val的形式, 详见https://golang.org/pkg/runtime/ #GC相关的 allocfreetrace=1 gctrace=1 #cgo cgocheck=1 #调度, 每X ms打印信息 schedtrace=X and scheddetail=1 GOTRACEBACK 默认在程序出错时打印调用栈, #依次加强 GOTRACEBACK=none GOTRACEBACK=single 默认 GOTRACEBACK=all GOTRACEBACK=system GOTRACEBACK=crash 触发coredump 名字冲突和type别名 在匿名继承过程中, 我遇到了一个问题: tengo.String类型, 有String()方法. 如果我想继承这个类型, 通常是这样写: package extension type String struct { tengo.String } 但编译不通过, 大意是String是个field name, 但我们这里需要String()方法. 原因是extension.String, 有String属性, 也有String()方法, 重名了. go编译器不允许field name和method name重名, 否则会冲突. type定义新类型 下面的代码也是不行的, 甚至更糟: package extension type tString tengo.String type String struct { tString } 因为tString是个新的类型, 没有任何方法; 匿名包含tString继承不到任何东西 type别名 这里要用type的别名机制: 定义type的时候用=号 package extension type tString = tengo.String type String struct { tString } 别名保留原名的所有东西. 所以这里tSting就是tengo.String, 只是拼写不一样了. 原始解释 An alias declaration doesn’t create a new distinct type different from the type it’s created from. It just introduces an alias name T1, an alternate spelling, for the type denoted by T2. Type aliases are not meant for everyday use. They were introduced to support gradual code repair while moving a type between packages during large-scale refactoring. Codebase Refactoring (with help from Go) covers this in detail. goroutine vs thread 默认的thread数 一个go进程会起GOMAXPROCS 个线程, 从go1.5之后, 这个值默认是CPU核数. 可以在环境变量改, 也可以用runtime.GOMAXPROCS来在运行时改. GOMAXPROCS=4 goapp 注意GOMAXPROCS 是指同时running的线程数, 不包括被block的线程. go进程对block的线程数没有限制. goroutine介绍 参考 https://codeburst.io/why-goroutines-are-not-lightweight-threads-7c460c1f155f https://medium.com/@riteeksrivastava/a-complete-journey-with-goroutines-8472630c7f5c goroutine并不是linux thread, 但建立在thread上, 受go runtime scheduler调度的任务单位. 对比linux thread, 它更轻量, 表现在如下几个方面: thread的stack比较大, > 1M, 所以如果有1000个thread, 要占1G的内存; goroutine默认stack是2K, 如果有goroutine需要更大的stack, 会分配新的内存区域. goroutine不受OS调度, 而是受go runtime scheduler调度, 所以不需要比如100Hz的中断来调度 基本上, 调度的策略类似linux的非抢占模式, 主动让出或阻塞时让出CPU. goroutine阻塞是阻塞在当前运行它的线程, go scheduler会运行其他thread来运行其他goroutine. 不需要等线程阻塞才调度, 比如goroutine用channel和另一个goroutine通信, 本身并不会导致goroutine所在线程阻塞, 但go scheduler会调度其他goroutine来在当前线程替代之前的goroutine运行. 一个goroutine被调度运行, 只发生在当前的goroutine 调用了channel send, 并阻塞 用go关键词启动一个goroutine 在file access或network相关系统调用上阻塞 被Gc停住后 thread上下文切换的代价偏大, 要保存很多寄存器, 除了常见的通用寄存器, 还包括AVX, SSE, 浮点等. goroutine则不需要保存这么多的寄存器, 因为调度都在特定的点上, 那保存的寄存器也是按需的. json和万能interface{} 普通的结构体和[]byte的转换, 用json.Marshal()和json.Unmarshal() 已知结构体 b := []byte(`{\"Name\":\"Wednesday\",\"Age\":6,\"Parents\":[\"Gomez\",\"Morticia\"]}`) type FamilyMember struct { Name string Age int Parents []string } var m FamilyMember err := json.Unmarshal(b, &m) 注意: 用var声明FamilyMember变量m的时候, Parents还只是个nil 而Unmarshal会再后台申请新的切片来放Parents 同样的, 比如下面, 如果结构体中包括指针, 则Unmarshal会申请Bar的空间.type Foo struct { Bar *Bar } 不知道结构体情况下 那不知道结构体的json, 怎么解析呢? 这就要用到万能的interface{}接口了. The json package uses map[string]interface{} and []interface{} values to store arbitrary JSON objects and arrays; it will happily unmarshal any valid JSON blob into a plain interface{} value. //比如b是json, 但不知道对应的结构体 b := []byte(`{\"Name\":\"Wednesday\",\"Age\":6,\"Parents\":[\"Gomez\",\"Morticia\"]}`) var f interface{} err := json.Unmarshal(b, &f) fmt.Printf(\"%#v\\n\", f) //输出, 可以看到, 这个map的key是string, value是interface{}, 因为只有interface{}可以是任何东西. map[string]interface {}{\"Age\":6, \"Name\":\"Wednesday\", \"Parents\":[]interface {}{\"Gomez\", \"Morticia\"}} //此时用类型断言得到这个类型的\"值\", 也就是这个map m := f.(map[string]interface{}) //然后就可以用range来遍历了 for k, v := range m { switch vv := v.(type) { case string: fmt.Println(k, \"is string\", vv) case float64: fmt.Println(k, \"is float64\", vv) case []interface{}: fmt.Println(k, \"is an array:\") for i, u := range vv { fmt.Println(i, u) } default: fmt.Println(k, \"is of a type I don't know how to handle\") } } 常见的json类型对照: bool for JSON booleans, float64 for JSON numbers, string for JSON strings, and nil for JSON null. encoder和decoder func NewDecoder(r io.Reader) *Decoder func NewEncoder(w io.Writer) *Encoder package main import ( \"encoding/json\" \"log\" \"os\" ) func main() { dec := json.NewDecoder(os.Stdin) enc := json.NewEncoder(os.Stdout) for { var v map[string]interface{} if err := dec.Decode(&v); err != nil { log.Println(err) return } for k := range v { if k != \"Name\" { delete(v, k) } } if err := enc.Encode(&v); err != nil { log.Println(err) } } } "},"notes/as_title_golang2.html":{"url":"notes/as_title_golang2.html","title":"Golang 杂记","keywords":"","body":"如题 "},"notes/golang_杂记1.html":{"url":"notes/golang_杂记1.html","title":"Golang 杂记1","keywords":"","body":" 类型断言很慢吗? 结果 zlib压缩 gob编码 gob会缓存 结论 gob的编码规则 decode时使用指针方式避免interface的值拷贝 msg的值拷贝 优化成指针 interface也可以序列化, 但需要Register 输出 如果不Register会怎样? 顶层是interface{}的情况 能直接encode interface{} 使用interface的地址来encode Register()函数 非要Register()吗? 没有特列, append也是值拷贝 要用interface抽象行为, 就不要多一层struct马甲. interface{}变量可以直接和concrete类型的变量比较 Read不能保证全读 用io.ReadFull 没有io.WriteFull string强转 先return再defer, defer里面能看到return的值 包的初始化只执行一次 goroutine与channel 使用channel时一定要判断peer的goroutine是否还在1 问题场景 解决 使用channel时一定要判断peer的goroutine是否还在2 解决 写空的channel不会panic 简单的程序可以检测死锁 复杂的程序检测不出来, 直接卡住 patherror 永久阻塞 selectgo源码杂记 强转成切片指针 突破数组大小限制 切片截取 子切片共享底层数组 go test 测试对象方法 子项 性能测试 不推荐用self或者this指代receiver 范式 在链接阶段对全局变量赋值 使用场景 如何做到的? 链接选项 编译限制Build Constraints 使用-tags参数指定用户自定义constraints 无表达式的switch 无缓冲和缓冲为1的通道不一样 书 go内存模型 解决1: 用channel 解决2: 用sync sync的once 类型断言很慢吗? 答: 不慢, 甚至比直接函数调用还快... 黑科技 package main import ( \"testing\" ) type myint int64 type Inccer interface { inc() } func (i *myint) inc() { *i = *i + 1 } func BenchmarkIntmethod(b *testing.B) { i := new(myint) incnIntmethod(i, b.N) } func BenchmarkInterface(b *testing.B) { i := new(myint) incnInterface(i, b.N) } func BenchmarkTypeSwitch(b *testing.B) { i := new(myint) incnSwitch(i, b.N) } func BenchmarkTypeAssertion(b *testing.B) { i := new(myint) incnAssertion(i, b.N) } func incnIntmethod(i *myint, n int) { for k := 0; k 结果 yingjieb@3a9f377eee5d /repo/yingjieb/godev/practice/src/benchmarks/typeassertion $ go test -bench . goos: linux goarch: amd64 BenchmarkIntmethod-23 465990427 2.55 ns/op BenchmarkInterface-23 269690563 4.46 ns/op BenchmarkTypeSwitch-23 590743738 2.06 ns/op BenchmarkTypeAssertion-23 577222344 2.10 ns/op PASS ok _/repo/yingjieb/godev/practice/src/benchmarks/typeassertion 5.949s BenchmarkInterface: 通过interface直接调用最慢 BenchmarkIntmethod: 直接函数调用做为基准 BenchmarkTypeSwitch/BenchmarkTypeAssertion: 类型断言比直接函数调用还快!!!!! zlib压缩 zlib提供的压缩接口是io.Writer. 即z := zlib.NewWriter(s)是个io.Writer, 往里面写就是压缩写. 但要调用z.Close()接口做flush操作, close后数据才写入底层的io.Writer. 不想close的话, 调用Flush()接口也行. gob编码 json的编码体积偏大, 改用gob的性能和json差不多, 但体积能减小一半. gob专用于go程序之间的数据编码方法, 借鉴并改进了了很多GPB的设计, 应该说是go世界的首选序列化反序列化方法. gob会缓存 比如发送方连续两次发送 conn.enc.Encode(A) conn.enc.Encode(B) 在接收方看起来, 连续两次Decode, 能还原A和B的值 conn.enc.Decode(&A) conn.enc.Decode(&B) 如果两次Encode间隔很短, 比如连续的2次Encode, 在对端Decode的时候, 第一把decode A的时候, 可能已经缓存了部分B的字节, 第二把decode可以用这个缓存. 但如果A的后面是对原始io.Reader的直接操做, 比如: conn.enc.Decode(&A) io.Copy(os.Stdout, conn) 那么可能os.Stdout不会看到B, B缓存在第一把的Decode里面. 比如在发送方发送A和B之间加个sleep 1秒, 实验结果是对A的decode就不缓存. 结论 如果发送方encode间隔很短, gob会预取socket里面的紧跟着上次encode的内容, 那么: 接收方一直用gob去decode的话, 是没问题的. 但如果decode中途去直接操做io读, 是可能读不到数据的. gob的编码规则 以int为基础, size变长 第一次传输一个新的结构体的时候, 先传输这个结构体的定义, 即layout 后面传输的时候, 带结构体标识就可以了 即先描述这个东西长什么样子, 取个名字, 后面直接用名字指代. decode时使用指针方式避免interface的值拷贝 之前我的代码里定义了isMessageOut和messageIn两个接口 // messageOut represents a message to be sent type messageOut interface { isMessageOut() } // messageIn represents an incoming message, a handler is needed to process the message type messageIn interface { // reply(if not nil) is the data that will be sent back to the connection, // the message may be marshaled or compressed. // remember in golang assignment to interface is also value copy, // so return reply as &someStruct whenever possible in your handler implementation. handle() (reply messageOut, err error) } 我有个record类型的结构体要传输. 在接收端, 我定义了handle方法, 接收到messageIn类型的结构体就可以直接handle()了 type record struct { Timestamp int64 Payload recordPayload } func (rcd record) handle() (reply messageOut, err error) { fmt.Println(rcd) return nil, nil } func fakeServer() { ... var msg messageIn for { decoder.Decode(&msg) reply, err := msg.handle() } } 注意decoder.Decode(&msg), 要求msg必须是messageIn, decoder会自动分配concrete类型实例并赋值给msg. 如果对端发过来的消息concrete类型不是messageIn, Decode会返回错误, 类似这样: gob: local interface type *main.messageIn can only be decoded from remote interface type; received concrete type string 意思是对端发过来的是string类型, 我已经收好了; 但是你不是messageIn, 所以不符合用户要求. msg的值拷贝 上面的代码可以工作, 但有个性能问题. 注意到record的handle()接收record的值, 而不是指针. 所以第16行reply, err := msg.handle()时, 对msg发生了一次值拷贝. 在go里面, 值拷贝是浅拷贝, 一般性能开销不大. 因为浅拷贝遇到切片, 字符串, map等等\"引用\"属性的对象, 浅拷贝只拷贝\"指针\", 不拷贝内容. 但这里为了进一步避免浅拷贝, 需要想办法把record的实现改成下面:注意只多了个* func (rcd *record) handle() (reply messageOut, err error) { fmt.Println(rcd) return nil, nil } 编译没问题, 但运行时gob报错: gob: main.record is not assignable to type main.messageIn 熟悉interface的同学应该知道这里的意思其实是: decoder.Decode(&msg) decode出来的\"值\", 不是messageIn, 不能赋值给msg. 那decode出来的\"值\"是什么呢? 这就要提到gob要求interface的具体类型要注册, 我是这样注册的: //在初始化路径上调用一次 gob.Register(record{}) 那么decode出来的\"值\"就是record{}, 而record不是messageIn, *record才是 优化成指针 那么这样改就可以: 把*record注册给gob gob.Register(&record{}) interface也可以序列化, 但需要Register 比如下面的代码中, 要marshal的是record结构体. type record struct { Timestamp int64 // time.Now().Unix() Payload interface{} } 但它的Payload部分是个interface, 可以是 string []processInfo{}type processInfo struct { Pid int Ucpu string //%.2f Scpu string //%.2f Mem uint64 Name string } 一个是内置的字符串, 一个是自定义的结构体数组. 对record类型来说, 这两个类型都是叫Payload 前面说过, 每个新东西都要描述一番, 取个名字. 但对interface来说, 它有很多面孔. 一个描述是不够的. 所以gob规定, interface所指代的具体类型, 要先注册. 下面的Register()调用就注册了这个结构体数组 //把nil强转成目标类型的实例, 因为Register接收实例 //gob.Register([]processInfo{})也是可以的, 只要能得到实例 gob.Register([]processInfo(nil)) var tmp bytes.Buffer enc := gob.NewEncoder(&tmp) dec := gob.NewDecoder(&tmp) //模拟processInfo切片 err := enc.Encode(record{time.Now().Unix(), []processInfo{}}) fmt.Println(\"encoded:\", string(tmp.Bytes())) //模拟一个hello字符串 err := enc.Encode(record{time.Now().Unix(), \"hello\"}) fmt.Println(\"encoded:\", string(tmp.Bytes())) err = dec.Decode(&data) switch v := data.Payload.(type) { case []processInfo: fmt.Println(data.Timestamp, \"====processinfo \", v) case string: fmt.Println(data.Timestamp, \"====string \", v) } 输出 这里用string强转了bytes, 有些不能打印字符. $ ./topid -p 2 -snapshot //从这里可以看出来, 第一次描述了processInfo的layout encoded: .record TimestampPayload)[]main.processInfoD processInfoPidUcpu Scpu MemName .000.0kthreadd //能被decode还原 1590418131 ====processinfo [{2 0.00 0.00 0 kthreadd}] //Payload的string方式第一次出现 描述一下. 有些byte没打印, 但应该第一次的信息是全的. encoded: string hello 1590418131 ====string hello //后面的打印就不带layout信息了 encoded: ;[]main.processInfo.000.0kthreadd 1590418132 ====processinfo [{2 0.00 0.00 0 kthreadd}] encoded: ;[]main.processInfo.000.0kthreadd 1590418133 ====processinfo [{2 0.00 0.00 0 kthreadd}] ... //不知为何, 原始string还是每次有带. encoded: string hello 1590418161 ====string hello 如果不Register会怎样? 不管encode还是decode, 都会打印提示: gob: type not registered for interface: []main.processInfo 顶层是interface{}的情况 上面的例子中, bog可以编码结构体中间的field是interface{}的情况. 那么如果直接encode一个interface{}可以吗? 能decode吗? 先回答: 能; 能, 但需要技巧. 能直接encode interface{} encode的入参就是interface{}类型, 即任何类型都可以被encode. 一个interface{}变量在被赋值的时候, runtime知道它的concrete类型. 参考 神作: interface的运行时的lookup Go Data Structures: Interfaces gob会按照concreate类型传输. 所以这样的代码是可以被encode的. var msg interface{} // msg = anyvariable enc.Encode(msg) 但不能被decode. var msg interface{} dec.Decode(&msg) 报错误: gob: local interface type *interface {} can only be decoded from remote interface type; received concrete type sessionReq = struct { SessionTag string; SysInfo sysInfo = struct { BoardName string; CPUInfo string; KernelInfo string; PackageInfo packageInfo = struct { BuildVersion string; SwID string; BuildServer string; BuildDate string; Repo string; Branch string; }; }; } 这个错误说明两点: gob知道对方传输过来的结构体, 并且能精确解析 但gob不能把它decode给*interface {}, 即&msg; gob还提示, decode给interface必须对端也是interface类型. 使用interface的地址来encode gob支持interface的传输, 比如在再上面的record类型中的interface就可以被传输和decode. 但顶层的interface需要些技巧. 在encode的时候, 传入interface的地址就行. 这样: // encode var msg interface{} // msg = anyvariable enc.Encode(&msg) // decode var msg interface{} dec.Decode(&msg) 很对称, 挺好的. gob会对指针解引用, encode时&msg被赋值给内部interface{}时, gob发现这是个指针类型, 指向interface类型. 所以gob按照interface类型来传输 根据interface的传输规则, encode端和decode端都要提前注册具体类型到gob Register()函数 使用了Type的String方法获得类型的名称 func Register(value interface{}) { rt := reflect.TypeOf(value) name := rt.String() //处理指针的情况, 指针前面加* RegisterName(name, value) gob包默认注册了基础类型 func registerBasics() { Register(int(0)) Register(int8(0)) Register(int16(0)) Register(int32(0)) Register(int64(0)) Register(uint(0)) Register(uint8(0)) Register(uint16(0)) Register(uint32(0)) Register(uint64(0)) Register(float32(0)) Register(float64(0)) Register(complex64(0i)) Register(complex128(0i)) Register(uintptr(0)) Register(false) Register(\"\") Register([]byte(nil)) Register([]int(nil)) Register([]int8(nil)) Register([]int16(nil)) Register([]int32(nil)) Register([]int64(nil)) Register([]uint(nil)) Register([]uint8(nil)) Register([]uint16(nil)) Register([]uint32(nil)) Register([]uint64(nil)) Register([]float32(nil)) Register([]float64(nil)) Register([]complex64(nil)) Register([]complex128(nil)) Register([]uintptr(nil)) Register([]bool(nil)) Register([]string(nil)) } 非要Register()吗? 既然有实例就能注册, 为什么不在encode/decode时自动注册了, 非要搞一个Register()?答: 可能是因为用了反射比较慢的缘故. 注册一次就够了, 每次都\"注册\"反射开销大. 没有特列, append也是值拷贝 那问题是, 做为入参传入append的时候, 是否已经发生了一次值拷贝, 然后再拷贝到[]slice里面去? 要用interface抽象行为, 就不要多一层struct马甲. 少用通用的interface然后再在里面搞类型断言; 而是用具体的interface, 这样在编译阶段就能\"断言\"类型. 比如下面的第45行. package main import ( \"bytes\" \"encoding/json\" \"fmt\" //\"os\" \"bufio\" \"io\" ) type record interface { tag() string doPrint() } type teacher struct { Name string } type student struct { Id int Name string Class int Email string Message string } func (stdt *student) tag() string { return \"student\" } func (stdt *student) doPrint() { fmt.Println(\"do student\", stdt) } func (tc *teacher) tag() string { return \"teacher\" } func (tc *teacher) doPrint() { fmt.Println(\"do teacher\", tc) } func marshalRecord(w io.Writer, rcd record) error { jsn, err := json.Marshal(rcd) if err != nil { return err } w.Write([]byte(rcd.tag() + \": \")) w.Write(jsn) w.Write([]byte{'\\n'}) return nil } func main() { var b bytes.Buffer stdt := student{Id: 9527, Name: \"sam\", Class: 3, Email: \"sam@godev.com\", Message: \"hello\\n world\\n\"} err := marshalRecord(&b, &stdt) if err != nil { fmt.Println(err) } tc := teacher{Name: \"shuxue\"} err = marshalRecord(&b, &tc) if err != nil { fmt.Println(err) } //b.WriteTo(os.Stdout) r := bufio.NewReader(&b) for { line, err := r.ReadBytes('\\n') if err != nil { return } fmt.Printf(\"%s\", line) sep := bytes.Index(line, []byte{':'}) key := string(line[:sep]) value := line[sep+2:] fmt.Printf(\"key: %s\\n\", key) fmt.Printf(\"value: %s\\n\", value) var rcd record switch key { case \"student\": rcd = &student{} case \"teacher\": rcd = &teacher{} } err = json.Unmarshal(value, rcd) if err != nil { fmt.Println(err) return } rcd.doPrint() } } interface{}变量可以直接和concrete类型的变量比较 我实现了一个map, 提供set和get函数. get出来的value是个万能interface{}, func (im *intMap) set(k int, v interface{}) { _, has := im.mp[k] if !has { im.ks = append(im.ks, k) } im.mp[k] = v } func (im *intMap) get(k int) interface{} { v, has := im.mp[k] if has { return v } return nil } func main() { im := newIntMap(10) im.set(1, \"1234\") # v的类型是interface v := im.get(1) show(v) //interface变量可以直接和具体类型的值比较 if v == \"1234\" { fmt.Println(\"interface{} can be compared directly with string\") } im.set(2, &[]int{1, 2, 3}) v = im.get(2) //这里可以比较, 但地址是不一样的 if v == &[]int{1, 2, 3} { fmt.Println(\"should not be\") } show(v) im.set(3, struct{ a, b, c int }{1, 2, 3}) v = im.get(3) //结构体也可以比较, 但前提是结构体里面的元素都可以比较 if v == struct{ a, b, c int }{1, 2, 3} { fmt.Println(\"interface{} can be compared directly with comparable struct\") } show(v) im.set(4, 155) v = im.get(4) show(v) //可以直接和整型比较 if v == 155 { fmt.Println(\"interface{} can be compared directly with int\") } v = im.get(100) // 100不存在, v是nil // nil可以比较, 不会panic; 只是从来不一致. if v == 10086 { fmt.Println(\"nil interface{} can be compared, but never succeed\") } show(v) //切片不能比较 /* if v == []int{5,6,7} { fmt.Println(\"error! operator == not defined on slice\") } */ } //结果: string:(\"1234\") interface{} can be compared directly with string *[]int:([]int{1, 2, 3})@[0xc0000ac040] interface{} can be compared directly with comparable struct struct { a int; b int; c int }:(struct { a int; b int; c int }{a:1, b:2, c:3}) int:(155) interface{} can be compared directly with int :() Read不能保证全读 golang的Reader不保证能read完整的len(buf), 即使没有到EOF, Read也不保证完整的Read. 所以Read会返回已经读的字节数n type Reader interface { Read(p []byte) (n int, err error) } 在C里面, 系统调用read()可能被信号打断而提前返回, 俗称短读. 一般的做法是自己写个包装, 用while一直读, 直到读完为止. 用io.ReadFull 在go里, io包已经提供了这个包装, 就是 func ReadFull(r Reader, buf []byte) (n int, err error) ReadFull保证填满buf, 除非EOF时buf还没填满, 此时返回ErrUnexpectedEOF ReadFull实际上是调用ReadAtLeast func ReadAtLeast(r Reader, buf []byte, min int) (n int, err error) { if len(buf) = min { err = nil } else if n > 0 && err == EOF { err = ErrUnexpectedEOF } return } ReadAtLeast()用一个循环反复Read() 没有io.WriteFull 标准库里面有ReadFull, 但没有WriteFull. 为什么呢? 有人还真实现了WriteFull, 有必要吗? 没必要: 因为read的语义允许short read而不返回error; 但write的语义是要写就都写完, 除非有错误. I would really rather not. ReadFull exists because Read is allowed to return less than was asked without an error. Write is not. I don't want to encourage people to think that buggy Writers are okay by introducing a buggy Writer fixer. string强转 结论: byte强转成string, 会去掉其中的0 buf := make([]byte, 32) buf = append(buf, '1', 0, '2') fmt.Println(buf) fmt.Println(string(buf)) //结果: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 49 0 50] 12 说明byte切片转成string, 每个byte都被扫描, 去掉其中的0. 也说明go里面的类型转换, 是有不小开销的. 先return再defer, defer里面能看到return的值 func checkHierarchy(p int) (err error) { //defer里面可以直接使用returned变量 defer func() { if err != nil { fmt.Println(err) } else { fmt.Println(\"no err returned\") } }() //对返回变量直接赋值是可以的 err = errors.New(\"default err\") if p == 1 { return nil } else if p == 2 || p == 3{ //相当于对err赋值. return fmt.Errorf(\"return: %d\", p) } //空的return会默认返回有名的return变量, 即上面的err return } func main() { checkHierarchy(1) checkHierarchy(2) checkHierarchy(3) checkHierarchy(4) } //输出 no err returned return: 2 return: 3 default err defer里面可以直接使用return变量 空的return会默认返回有名的return变量 程序里可以提前对有名的return变量赋值, 然后用空return返回该值. 带值的return实际上会给有名的return变量赋值, defer的时候看到的是return的指定值. defer在return之后执行 -- 先执行return那一行. 下面的代码会返回\"Change World\"而不是\"Hello World\" func foo() (result string) { defer func() { result = \"Change World\" // change value at the very last moment }() return \"Hello World\" } 包的初始化只执行一次 Package initialization is done only once even if package is imported many times. goroutine与channel goroutine的生命周期和channel要配合, routine的产生与消亡要考虑对channel的影响 使用channel时一定要判断peer的goroutine是否还在1 这个例子中,newPidInfo()函数中, 起了goroutine pi.checkThreads() func newPidInfo(pid int, ppi *PidInfo) (*PidInfo, error) { pi.hierarchyDone = make(chan int) go func() { pi.err = pi.checkThreads() }() pi.triggerAndWaitHierarchy() } func (pi *PidInfo) checkThreads() error { defer func() { pi.hierarchyDone pi.checkThreads()虽然做了异常分支的channel处理, 比如defer那句. 但这里还是出了channel问题. 问题场景 某次进程A要更新hierarchy, A发hierarchyCheck A的checkThreads守护routine收到hierarchyCheck信号, 开始doWork() A的doWork()里, 触发子进程B的hierarchyCheck 子进程B的checkThreads守护routine开始doWork() B的doWork()出错, B的hierarchyDone从异常分支写1, 通知A的doWork()往下走. 对应代码第20行 OK. 到这里还是正常的 下次A要更新hierarchy, A发hierarchyCheck A的checkThreads守护routine收到hierarchyCheck信号, 开始doWork() A的doWork()里, 不知什么原因, A还是认为自己有子进程B. 触发子进程B的hierarchyCheck 子进程B已经没有checkThreads守护routine A永远阻塞在第42行 本质上, 出现问题的原因是: 子进程B的goroutine的异常分支已经完善, 但goroutine异常退出后, 业务逻辑不应该再去用channel和子进程B交互. 即永远判断要交互的goroutine是否存在. 解决 triggerAndWaitHierarchy()加异常判断 func (pi *PidInfo) triggerAndWaitHierarchy() { //for some reason there is no checkThreads routine, thus nowhere we can send to and receive from //this can happen if the children file has the child, but actually the child is dying if pi.err != nil { //fmt.Println(pi.err) return } //trigger once, none blocking pi.hierarchyCheck 使用channel时一定要判断peer的goroutine是否还在2 比如下面的代码, 倒数第二行在写channel的时候, 对应的checkChild协程不一定能到达37行select. 实际上, 只有一个case的select可以只保留channel读部分. for _, tid := range pi.threadIds { tid := tid if pi.childCheckers[tid] == nil { s := childChecker{make(chan int), make(chan []int, 1)} checkChild := func() { f, err := os.Open(\"/proc/\" + pi.pidstr + \"/task/\" + strconv.Itoa(tid) + \"/children\") if err != nil { return } defer f.Close() doWork := func() []int { if _, err := f.Seek(0, 0); err != nil { return nil } buf, err := ioutil.ReadAll(f) if err != nil { return nil } //ToDo: return nil if buf is empty strs := strings.Fields(string(buf)) childrenIds := make([]int, len(strs)) for i := 0; i 实际上, 这里还有一个错误. 在后面的处理里, 下面代码倒数第二行, 会阻塞的从childCheckers[tid].children读出数据, 但如果上面的goroutine异常退出了, //use new refilled map pi.childCheckers = childCheckers //with initial capacity of the previous run pi.childrenIds = make([]int, 0, len(pi.childrenIds)) for _, tid := range pi.threadIds { //concatenate the children slices retrieved from channel to a single one pi.childrenIds = append(pi.childrenIds, 解决 在goroutine刚开始, 加defer函数, 默认退出时读/写channel 这里用的非阻塞读写. checkChild := func() { defer func() { select { case 写空的channel不会panic 简单的程序可以检测死锁 func main() { var c chan int c 复杂的程序检测不出来, 直接卡住 //pi.hierarchyDone是个空的channel for { select { case patherror // requires go1.13 and later func pathError(err error) bool { var perr *os.PathError if errors.As(err, &perr) { return true } return false } 永久阻塞 空的select永远等待 select {} selectgo源码杂记 selectgo()函数是select语句的runtime实现, 由编译器在编译时把select语句块转换为runtime的selectgo()的调用. 强转成切片指针 下面的代码是把一个指针, 强转成指向数组的指针; go里面强转不是万能的, 指针必须转成指针 cas1 := (*[1 和下面的效果类似 func main() { s := []int{1,2,3,4,5,6,7} sp := &s fmt.Println(sp) sp3 := (*[3]int)(unsafe.Pointer(&s[0])) fmt.Println(sp3) } //结果 &[1 2 3 4 5 6 7] &[1 2 3] 突破数组大小限制 func main() { s := [7]int{1,2,3,4,5,6,7} //数组的指针可以当数组用 sp := &s //如果s是切片, 则下面语句报错:(type *[]int does not support indexing) sp[1] = 999 fmt.Println(sp) //强转成指向数组的指针可以突破数组的length限制 //但要注意, 你明确知道在干什么. 否则, segment fault snp := (*[20]int)(unsafe.Pointer(&s[0])) snp[15] = 995 fmt.Println(snp) } 切片截取 比如slice是个切片, 那么slice[ i : j : k ]是截取slice并限制capacity的切片: Length: j - i Capacity: k - i 第三个参数的用法不常见, 但加了可以限制新切片的capacity的能力, 好处是防止越过capacity访问. 下面的代码取自selectgo, 就使用了第二个冒号, scases := cas1[:ncases:ncases] pollorder := order1[:ncases:ncases] lockorder := order1[ncases:][:ncases:ncases] 子切片共享底层数组 子切片的capacity和其主切片的capacity有关, 因为他们都共享底层的数组. 比如下面的例子证明了, 子切片的修改会影响到母切片. func main() { s := []int{1,2,3,4,5,6,7} s1 := s[0:3] fmt.Println(s) fmt.Println(s1) //修改切片s1会影响原切片s s1[1] = 999 fmt.Println(s) fmt.Println(s1) //非法访问, slice越界 //panic: runtime error: index out of range [5] with length 3 s1[5] = 888 } 结果: [1 2 3 4 5 6 7] [1 2 3] [1 999 3 4 5 6 7] [1 999 3] go test 测试对象方法 用Test对象名_方法名. 比如tengo代码中的 func TestScript_Run(t *testing.T) { s := tengo.NewScript([]byte(`a := b`)) err := s.Add(\"b\", 5) require.NoError(t, err) c, err := s.Run() require.NoError(t, err) require.NotNil(t, c) compiledGet(t, c, \"a\", int64(5)) } 子项 Test 函数可以调用t.Run func TestFoo(t *testing.T) { // t.Run(\"A=1\", func(t *testing.T) { ... }) t.Run(\"A=2\", func(t *testing.T) { ... }) t.Run(\"B=1\", func(t *testing.T) { ... }) // } go test可以指定子项: go test -run Foo # Run top-level tests matching \"Foo\", such as \"TestFooBar\" go test -run Foo/A= # For top-level tests matching \"Foo\", run subtests matching \"A=\" go test -run /A=1 # For all top-level tests, run subtests matching \"A=1\" 性能测试 用go test -bench func BenchmarkHello(b *testing.B) { big := NewBig() //开始循环测试之前先reset时间 b.ResetTimer() for i := 0; i 用b.RunParallel()来并发执行, 和-cpu 1,2,4配合, 可以测一个核, 两个核, 四个核的并发性能 go help testflag查看详细的选项 不推荐用self或者this指代receiver https://stackoverflow.com/questions/23482068/in-go-is-naming-the-receiver-variable-self-misleading-or-good-practice receiver有两个作用, 第一, 声明了类型和方法的绑定关系. 第二, 在运行时给方法额外传了这个类型的参数. 对v.Method()的调用时编译器的语法糖, 本质上是(T).Method(v). 参考这里 C++和Python的方法基于具体对象, 所以this和self隐含代指这个具体对象的内存地址, 和方法是紧耦合关系. 而go的方法基于类型, 和具体对象是松耦合关系, 具体的对象只是做为额外的参数传给方法. 以下代码可以运行, 没有错误 package main import \"fmt\" type T struct{} func (t T) Method(msg string) { fmt.Println(msg) } func main() { t := T{} t.Method(\"hello\") // this is valid //使用类型调用方法, 第一个参数是实例 (T).Method(t, \"world\") // this too } 范式 所以实例只是其中的一个参数, 要给个合适的名字, go惯例使用类型的第一个字母小写, 或者更贴切的名字. type MyStruct struct { Name string } //这个更符合go的范式 func (m *MyStruct) MyMethod() error { // do something useful } //self的语义并不贴切 func (self *MyStruct) MyMethod() error { // do something useful } 在链接阶段对全局变量赋值 使用场景 在vonu里面, 编译的时候传入了git commit id和编译时间; 在go build的时候用-ldflags传入 LDFLAGS=-ldflags '-X env.VOnuRevCommitId=$(VONUMGMT_REV_COMMIT_ID) -X \"env.VOnuBuildDate=$(VONUMGMT_BUILD_DATE)\"' go build $(LDFLAGS) -tags static $(APP_MAIN) 这个env.VOnuRevCommitId是env包里的一个全局变量 //定义的时候是nil var VOnuRevCommitId string //直接使用 func VOnuRevCommitIdInfo() string { return VOnuRevCommitId } 如何做到的? go build可以传入的选项有: -a 强制全部重编 -work 不删除临时目录 -race 打开竞争检查 -buildmode 比如共享库方式的选择 -compiler 选gccgo或者gc -gccgoflags -gcflags -ldflags arguments to pass on each go tool link invocation 这是本节的重点 -linkshared 链接共享库 -tags 自定义build constraints -trimpath 不保存绝对路径, 这个功能很好! 链接选项 其中-ldflags里面说到go tool link, 那就要看它的help go tool link -h //go tool link控制很底层的链接行为, 比如链接地址, 共享库路径 -T address 代码段起始地址 -X importpath.name=value 这就是本节用到的点, 定义package.name的变量未value, value是字符串 链接的时候\"初始化\"这个变量 -cpuprofile 写profiling信息到文件 -dumpdep -linkmode -buildmode // 对减小size有好处 -s disable symbol table -w disable DWARF generation 所以这里用了-X选项, 在链接的时候\"初始化\"变量值. 编译限制Build Constraints 详细说明: go doc build 编译限制用来指明一个文件是否要参与编译, 形式上要在文件开始的时候, \"注释\"编译限制, 比如: 只在linux并且使用cgo, 或者OS x并且使用cgo情况下编译该文件 // +build linux,cgo darwin,cgo 只在ignore情况下编译, 即不参与编译. 因为没有东西会匹配ignore. 用其他怪异的tag也行, 但ignore的意思更贴切 // +build ignore 这个注释必须在package语句之前 内置的tag有: During a particular build, the following words are satisfied: - the target operating system, as spelled by runtime.GOOS - the target architecture, as spelled by runtime.GOARCH - the compiler being used, either \"gc\" or \"gccgo\" - \"cgo\", if ctxt.CgoEnabled is true - \"go1.1\", from Go version 1.1 onward - \"go1.2\", from Go version 1.2 onward - \"go1.3\", from Go version 1.3 onward - \"go1.4\", from Go version 1.4 onward - \"go1.5\", from Go version 1.5 onward - \"go1.6\", from Go version 1.6 onward - \"go1.7\", from Go version 1.7 onward - \"go1.8\", from Go version 1.8 onward - \"go1.9\", from Go version 1.9 onward - \"go1.10\", from Go version 1.10 onward - \"go1.11\", from Go version 1.11 onward - \"go1.12\", from Go version 1.12 onward - \"go1.13\", from Go version 1.13 onward - \"go1.14\", from Go version 1.14 onward - any additional words listed in ctxt.BuildTags 另外, 如果文件名有如下形式, 则会被go build认为是隐含了对应tag的build constraint *_GOOS *_GOARCH *_GOOS_GOARCH 使用-tags参数指定用户自定义constraints 比如在kafka.go最开始添加: // +build !device 这里的device就是自定义的tag, 这里的意思是不带device的tag时, kafka.go才参与编译. 或者说有device的tag, kafka.go不编. 下面是测试结果: 带了device tag, $ RUNMODE=cloud go test -tags device --- FAIL: TestMsgCall (0.00s) msgchan_test.go:20: No message channel for kafka FAIL exit status 1 FAIL msgchan 0.002s 无表达式的switch switch 中的表达式是可选的，可以省略。如果省略表达式，则相当于 switch true，这种情况下会将每一个 case 的表达式的求值结果与 true 做比较，如果相等，则执行相应的代码。 func main() { num := 75 switch { // expression is omitted case num >= 0 && num = 51 && num = 101: fmt.Println(\"num is greater than 100\") } } 无缓冲和缓冲为1的通道不一样 无缓冲的通道，写会阻塞，直到有人读。 缓冲为1的通道，写第一个不会阻塞，而写第二个会。 书 https://www.cntofu.com/book/73/readme.html go内存模型 简单来说, 是和C语系一样: 可以编译时乱序, 执行时乱序(CPU特性) 那么, 下面的写法是不对的: 不能保证done在a的赋值之后执行. var a string var done bool func setup() { a = \"hello, world\" done = true } func main() { go setup() for !done { } print(a) } 下面的例子更有隐蔽性: 即使在main看来, g不是nil了, 也不能保证g.msg有值. type T struct { msg string } var g *T func setup() { t := new(T) t.msg = \"hello, world\" g = t } func main() { go setup() for g == nil { } print(g.msg) } 解决1: 用channel var c = make(chan int, 10) var a string func f() { a = \"hello, world\" c 解决2: 用sync var l sync.Mutex var a string func f() { a = \"hello, world\" l.Unlock() } func main() { l.Lock() go f() l.Lock() print(a) } sync的once once 保证之执行一次. var a string var once sync.Once func setup() { a = \"hello, world\" } func doprint() { once.Do(setup) print(a) } func twoprint() { go doprint() go doprint() } "},"notes/golang_杂记2.html":{"url":"notes/golang_杂记2.html","title":"Golang 杂记2","keywords":"","body":" 使用reflect的MethodByName调用方法 gob decode零值不赋值 现象 原始数据 代码逻辑 问题现象 解释 验证 结论 setuid 调用小写函数 使用了nohup命令启动gshelld, 还是收到signal退出 SIGHUP Types of signals ¶ 推测 解决 gshell的内存使用 recover不能捕获SIGSEGV类型的panic sigint会发给所有前台进程组 写已经closed的conn 场景 答案 结论 读写nil channel会永远阻塞 流式接口和go 流式函数链的go执行顺序 解读 运行结果和结论 go get和go mod go get go list go mod tidy 总结 go clean 清理编译cache 又犯了经典的goroutine引用闭包变量错误!!!!! 原因 解决 理论解释 总结 if else if 共享一个变量scope 经典的append地址错误 打印指针slice 解决: 给结构体加String方法 关于wait group 反例 从栈上获取变量地址 参考代码 运行时获取函数名 方法1 方法2 continue能返回N层for interface的理解 interface{}是青出于蓝 interface{}是带上下文的方法集合 runtime.Caller获取当前运行目录 An interface holding nil value is not nil net/http导致包变大 现象 解决 原因 结论 Options初始化 总结 空白import不会增加binary size reflect高阶用法 动态构建一个struct byName API -- 神奇的重名API reflect.Value的MethodByName方法 reflect.Type的MethodByName方法 go调用外部程序 text/template代码生成举例 map的重要属性 unaddressable 方法1 整体给map赋值 方法2 让value的类型为指针 结论 go的树状表达 存储data 改进存储 如果用key来存储data会怎样? golang的SIGABRT 关于syscall 标准syscall库的问题 解决 更好的syscall库 使用 ioctl c的ioctl接口 cmd argp go的ioctl接口 ioctl的宏定义在哪里? asm files mmap mmap返回的byte切片是哪里来的? fnctl RWLock死锁 float强转为int go generate go generate常使用的一些工具 godoc安装 godoc使用 package通配符和导入路径 package通配符... 导入路径 obj.function()中obj可以是nil 如何处理? 切片的reslicing 再议目录结构 具体error判断 函数赋值给变量 gob encode和网络io的结合 单步decoder.Decode(&msg) protobuf里面oneof转成go结构体 proto定义 转成的结构体 使用reflect的MethodByName调用方法 下面的代码在Handle()这个方法里面, 调用了同对象的DoHandle()方法. // Handle handles SessionRequest. func (msg *SessionRequest) Handle(stream as.ContextStream) (reply interface{}) { // ONLY use reflect when clients do not have full server functions built-in(aka lite build) handle := reflect.ValueOf(msg).MethodByName(\"DoHandle\") if handle.IsValid() && !handle.IsZero() { ret := handle.Call([]reflect.Value{reflect.ValueOf(stream)})[0] if !ret.IsValid() { return nil } return ret.Interface() } return nil } 被调用的方法必须大写开头. \"DoHandle\"可以, 但是如果这个函数是\"handle\", 就不行 返回值要先检查是否valid, 再检查是否是zero 使用Call()方法调用函数, 传参要自己知道该传什么, 返回值也要知道. 参数和返回值都是reflect.Value类型. gob decode零值不赋值 现象 我有一些cpu使用率的统计数据, 使用gob编码保存在文件中. 但decode出来的数据, 和原始数据不一样. 最典型的特征是, 似乎原值为0的时候, decode出来的数据的值可能\"很随机\" 原始数据 原始数据是如下结构体的切片 type procStat struct { Pid int Name string Ucpu float64 Scpu float64 Mem uint64 } type record struct { Time int64 Procs []procStat } 代码逻辑 在一个routine里, 随机生成record数据, 不断的写入文件; 在另一个routine里, 读这个文件decode. func main() { rand.Seed(time.Now().Unix()) flt := 0.01 fmt.Println(flt) fmt.Printf(\"%x\\n\", *(*uint64)(unsafe.Pointer(&flt))) file := \"gob.data\" records := make(map[int64][]procStat, 1000) go func() { f, _ := os.Create(file) defer f.Close() enc := gob.NewEncoder(f) for i := 0; i 小知识: float在内存中和整形的存储方式很不一样, 比如0.01在内存中是3f847ae147ae147b 问题现象 运行这段程序, 会走到reflect.DeepEqual为假的分支, 说明encode的数据和decode的数据不一样. 但还是很有规律的: $ go run gob.go 0.01 3f847ae147ae147b 2021-10-13 02:44:30 +0000 UTC 2021-10-13 02:44:32 +0000 UTC 2021-10-13 02:44:34 +0000 UTC 2021-10-13 02:44:36 +0000 UTC 2021-10-13 02:44:38 +0000 UTC enc done ---- {1634093070 [{34730 worker 0 0.86 11985} {63668 worker 0.65 0 11985} {54333 worker 0.93 0 11985} {45231 worker 0 0.76 11985} {19332 worker 0.82 0 11985} {51576 worker 0.59 0 11985} {16966 worker 0 0 11985} {17849 worker 0.84 0 11985} {18887 worker 0 0 11985} {36216 worker 0 0.87 11985}]} ---- {1634093072 [{45159 worker 0.79 0.86 11985} {22705 worker 0.94 0 11985} {15938 worker 0. 93 0 11985} {45296 worker 0.71 0.63 11985} {8740 worker 0.82 0.93 11985} {36634 worker 0.94 0 .76 11985} {26329 worker 0 0.99 11985} {1891 worker 0.94 0 11985} {33214 worker 0.7 0 11985} {33629 worker 0.75 0.64 11985}]} 2021-10-13 02:44:32 +0000 UTC Should: [{45159 worker 0.79 0 11985} {22705 worker 0.94 0 11985} {15938 worker 0 0 11985} {45296 wor ker 0.71 0.63 11985} {8740 worker 0 0.93 11985} {36634 worker 0.94 0.76 11985} {26329 worker 0 0.99 11985} {1891 worker 0.94 0 11985} {33214 worker 0.7 0 11985} {33629 worker 0.75 0.64 1 1985}] Got: [{45159 worker 0.79 0.86 11985} {22705 worker 0.94 0 11985} {15938 worker 0.93 0 11985} {452 96 worker 0.71 0.63 11985} {8740 worker 0.82 0.93 11985} {36634 worker 0.94 0.76 11985} {2632 9 worker 0 0.99 11985} {1891 worker 0.94 0 11985} {33214 worker 0.7 0 11985} {33629 worker 0. 75 0.64 11985}] ---- {1634093074 [{1940 worker 0.87 0.62 11985} {59400 worker 0.94 0.57 11985} {10964 worker 0.93 0 11985} {40707 worker 0.67 0.91 11985} {51810 worker 0.74 0.84 11985} {26919 worker 0.5 5 0.53 11985} {62442 worker 0 0.99 11985} {25 worker 0.97 0.78 11985} {4644 worker 0.79 0 119 85} {39752 worker 0.75 0.64 11985}]} 规律: 第一次时间戳时, enc和dec的数据是一致的, 此时还没有错误. 从第二次时间戳开始, dec的数据就开始发生\"跳变\", 表现是结构体中float64类型的两个field(Ucpu和Scpu), 应该是0值的地方, 不是零值, 比如:Should: [{45159 worker 0.79 0 11985} {22705 worker 0.94 0 11985} ... ] Got: [{45159 worker 0.79 0.86 11985} {22705 worker 0.94 0 11985} ...] 上面第一个procStat结构体的Scpu, 应该是0, 却变成了0.86 0.86似乎并不是\"随机\"的值, 经过发现, 正好是上一次recorde的同位置的值. 解释 在结论之前, 先排除几点: 和同时读写文件没有关系. 开始我们怀疑是文件写的同时, 又去文件读, 是否文件的内核态buffer没有\"及时\"写进去, 造成读文件的时候\"部分\"读, 造成数据异常. 但其实不是, 开始的时候我们让写的routine一直写, 后面改成写完close, 然后再读; 问题依旧 和float64的编解码有关吗? 似乎有关, 但如果编解码出错, 应该是全部float64的编解码都有问题. 但这里的现象是\"个别\"数据\"跳变\" 结合以上两点, 特别是第二点, 数据跳变似乎是跳变成了以前出现过的值. 那么这个0.86是哪里来的呢? 正好是上一次record的同位置的值: {34730 worker 0 0.86 11985} 那么现在现象比较明确了: 零值可能跳变 跳变的值是上一次同位置的值. 第49行, 在for之前定义了var rcd record, for里面的dec.Decode(&rcd)都是一直往这个地址decode. 第一次rcd全部是零值的时候, decode没问题; 但第二次rcd已经有了第一次的值了, 又如果gob在decode时候遇到零值, 比如下面Ucpu和Scpu是0, gob并不会给rcd的对应field赋值, 导致rcd的这部分值还是零值. type procStat struct { Pid int Name string Ucpu float64 Scpu float64 Mem uint64 } 验证 把49行换成51行, 即在for内部定义rcd, 结果就ok了. 我猜想是因为dec.Decode(&rcd)因为入参是interface{}, 导致rcd逃逸到堆. 因为是for内部定义的, 每次运行到var rcd record, 都会在堆里新分配rcd, 这样就不受前值影响. 把float64改成int, 按照上面的理论, 0值也会\"跳变\" type procStat struct { Pid int Name string Ucpu int Scpu int Mem uint64 } 经过验证, 确实零值也会跳变. 说明和float64编码没有关系. 结论 dec.Decode(&rcd)gob遇到零值, 不会给rcd相应的field赋值 所以, 需要每一次在使用rcd之前, 都要保证它为零值 要么在for里面定义rcd, 还要观察rcd是否真的逃逸到堆 要么手动给rcd赋值为0 所以, 在go里面, 涉及到解码, 或者对变量指针操做, 要特别注意一个变量在内存里的表达: 这个变量是同一个内存地址时, 通常都有bug 补充: 冒号定义变量var v := ...和new变量都不能保证分配新的变量地址 补充, 改成json编解码不会有这个问题, 估计json在遇到零值依然有赋值动作. setuid https://dustinspecker.com/posts/setuid-elevating-privileges/ 一个文件可以有setuid属性: Setuid, which stands for set user ID on execution, is a special type of file permission in Unix and Unix-like operating systems such as Linux and BSD. It is a security tool that permits users to run certain programs with escalated privileges. When an executable file's setuid permission is set, users may execute that program with a level of access that matches the user who owns the file. For instance, when a user wants to change their password, they run the passwd command. The passwd program is owned by the root account and marked as setuid, so the user is temporarily granted root access for that limited purpose. 命令: chmod u+s myfile chmod u+x myfile chmod g+s myfile2 一般都是谁执行myfile, 这个进程算谁的. 但如果由setuid属性的文件, 执行算这个文件的owner的. 比如这个文件的owner是root, 那普通用户执行这个带s属性的文件, 也有root权限. 调用小写函数 一般的, package的小写函数是没法直接调用的, 但有个办法可以绕过这个限制. 用go:linkname 比如标准库里 package reflect //go:linkname call runtime.reflectcall func call(argtype *rtype, fn, arg unsafe.Pointer, n uint32, retoffset uint32) reflect的call被link成了runtime.reflectcall, 也就是说, 调用reflect.call就是调用小写的runtime.reflectcall. 使用了nohup命令启动gshelld, 还是收到signal退出 nohup bin/gshell -wd rootregistry -loglevel debug daemon -registry 10.182.105.179:11985 -bcast 9923 -root -repo gitlabe1.ext.net.nokia.com/godevsig/grepo/master & 但gshell还是会退出 [2021/09/09 12:23:16.705595][daemon][WARN](adaptiveservice.go:215) signal: hangup [2021/09/09 12:23:16.705762][daemon][INFO](server.go:350) server closing [2021/09/09 12:23:16.705827][daemon][DEBUG](scalamsgq.go:49) msgq closed SIGHUP Types of signals ¶ The signals SIGKILL and SIGSTOP may not be caught by a program, and therefore cannot be affected by this package. Synchronous signals are signals triggered by errors in program execution: SIGBUS, SIGFPE, and SIGSEGV. These are only considered synchronous when caused by program execution, not when sent using os.Process.Kill or the kill program or some similar mechanism. In general, except as discussed below, Go programs will convert a synchronous signal into a run-time panic. The remaining signals are asynchronous signals. They are not triggered by program errors, but are instead sent from the kernel or from some other program. Of the asynchronous signals, the SIGHUP signal is sent when a program loses its controlling terminal. The SIGINT signal is sent when the user at the controlling terminal presses the interrupt character, which by default is ^C (Control-C). The SIGQUIT signal is sent when the user at the controlling terminal presses the quit character, which by default is ^\\ (Control-Backslash). In general you can cause a program to simply exit by pressing ^C, and you can cause it to exit with a stack dump by pressing ^. 注意上面的解释, 当主控制台丢失的时候, 会发SIGHUP给程序. SIGHUP的默认行为是程序退出: man 7 signal 搜索SIGHUP 推测 nohup命令ignore了SIGHUP, 从而其子进程也默认继承了这个irgnore的行为. 而gshell代码里, 显式捕捉了syscall.SIGHUP: func initSigCleaner(lg Logger) { sigOnce.Do(func() { // handle signal sigChan := make(chan os.Signal, 1) signal.Notify(sigChan, syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM) go func() { sig := 所以明明应该ignore的SIGHUP, 又发挥了作用... 解决 应该去掉这个syscall.SIGHUP的捕捉就好了, 还是用nohup启动. 更好的办法, 是用Ignore API: signal.Ignore(syscall.SIGHUP) gshell的内存使用 用到的可执行文件是bin/gshell, 命令: cat /proc/29961/smaps readelf -a 注: 内存\\场景 单daemon KB daemon + master gre .text RSS 2388 2388 + 2516 .text PSS 2388 1194 + 1322 .rodata RSS 2024 2088 + 2152 .rodata PSS 2024 1044 + 1108 .go.buildinfo RSS 144 144 + 144 .go.buildinfo PSS 144 96 + 96 .bss RSS 84 84 + 72 .bss PSS 84 84 + 72 栈? RSS 2244 2352 + 2772 栈? PSS 2244 2352 + 2772 堆? RSS 1504 1576 + 320 堆? PSS 1504 1576 + 320 n个routine栈 RSS 0/4 0/4 + 0/4 n个routine栈 PSS 0/4 0/4 + 0/4 系统栈1? RSS 68 68 + 68 系统栈1? PSS 68 68 + 68 系统栈2? RSS 12 12 + 12 系统栈2? PSS 12 12 + 12 vdso RSS 4 4 + 4 vdso PSS 0 0 + 0 结论: 同一个go的binary, 多次单独启动的情况下: 代码段.text是共享的 只读数据段.rodata是共享的 vdso等kernel so是共享的 其他好像都不共享 recover不能捕获SIGSEGV类型的panic 比如下面的代码: func() { defer func() { if err := recover(); err != nil { lg.Errorf(\"broken stream chan: %v\", err) } }() streamChan := *(*chan *streamTransportMsg)(unsafe.Pointer(uintptr(tm.dstChan))) // swap src and dst streamChan defer的函数中, recover并不能捕获下面的panic: fatal error: unexpected signal during runtime execution [signal SIGSEGV: segmentation violation code=0x80 addr=0x0 pc=0x40bbee] ... sigint会发给所有前台进程组 gshell daemon起了子进程gre-master, 在前台ctrl+c daemon, gre-master也会收到sigint, 两个进程都会消失. 但如果daemon进程自己panic挂掉, gre-master还会继续运行. 值得注意的是, 如果用kill命令指定向daemon进程发送signit, 那就指定pid才会收到信号, 也不会导致gre-master挂掉. 见system 原理杂记 写已经closed的conn 场景 机器A的进程a和机器B的进程b建立了TCP的conn a一直读, b一直写. a被杀掉, b会怎么样? 会写失败吗? 写会一直阻塞吗? 答案 b会写失败, error信息为: writev tcp 172.17.0.1:40757->172.17.0.4:34458: use of closed network connection 结论 不管是socket读, 还是socket写, 都能感知到connection已经关闭. 但前提是机器A不是突然断电. 在机器A还在但进程a被杀掉的情况下, A的kernel会关闭和B的连接, B的kernel和进程b都是知道对方(也就是A)连接中断了. 读写nil channel会永远阻塞 并且调用栈里面, 会明显的标识: [chan send (nil chan)] 比如: goroutine 34 [chan send (nil chan)]: github.com/godevsig/adaptiveservice.(*streamTransport).receiver.func1(0x5e7c48, 0xc0000c2ba0, 0xc0000a7140) /repo/yingjieb/github/godevsig/adaptiveservice/streamtransport.go:213 +0x49 created by github.com/godevsig/adaptiveservice.(*streamTransport).receiver /repo/yingjieb/github/godevsig/adaptiveservice/streamtransport.go:206 +0x8a 参考go规范: Receiving from a nil channel blocks forever A send on a nil channel blocks forever. 流式接口和go 我们知道流式接口就是说返回其自身的方法. 比如: // NewClient creates a client which discovers services. func NewClient() *Client { return &Client{ base: newBase(), discoverTimeout: -1, } } // WithinScopeOS sets the discover scope in same OS. func (c *Client) WithinScopeOS() *Client { c.withinScopeOS() return c } 这样就可以\"流式\"的初始化: c := NewClient().WithinScopeOS().WithDiscoverTimeout(0) 那么如果我向go这个流式函数链呢? 比如 go NewClient().WithinScopeOS().WithDiscoverTimeout(0) 其执行顺序是怎么样的? 流式函数链的go执行顺序 先看代码: package main import ( \"fmt\" \"time\" ) type test struct { name string } func NewTest(f func() string) *test { fmt.Println(\"in NewTest\") return &test{f()} } func (t *test) fa() *test { fmt.Println(t.name, \"in fa\") return t } func (t *test) fb(f func()) *test { fmt.Println(\"fb\") f() return t } func main() { fmt.Println(\"Hello, playground\") go NewTest( func() func() string { fmt.Println(\"before go? -- YES\") return func() string { return \"San\" } }()). fa(). fb( func() func() { fmt.Println(\"before go? -- NO\") return func() { fmt.Println(\"in go? -- YES\") } }()) time.Sleep(time.Second) } 解读 NewTest函数入参是个函数, 传入的时候返回闭包函数传给它 fa函数是普通的函数链上的一个 fb函数也是函数链上的, 但传入一个闭包函数给它. 大的调用关系是go NewTest(入参).fa().fb(入参) 运行结果和结论 结果: Hello, playground before go? -- YES in NewTest San in fa before go? -- NO fb in go? -- YES 结论是: 只有第一级函数, 即NewTest()的入参, 在这里是个函数, 是在go之前执行, 对应打印before go? -- YES 要注意, fb的入参函数, 是在go里面执行的. 所以说只有第一级的函数的入参才会在go之前被计算 go get和go mod 测试环境 go1.16, go mod模式 go get go get -u: 升级所有依赖, 递归包括依赖的依赖 go get -u all: 首先all会被扩展成main的所有依赖, 然后升级所有依赖. all是关键词, 详见go help packages go list go list -m all能列出当前用的所有的module(包括版本号) go list all能列出当前用的所有的package(import路径) go mod tidy 清理go.mod用的, 但似乎go.sum还是有很多\"历史\"版本, 这些版本并没有使用. 总结 go list -m all查看main的所有递归依赖版本 go的编译系统一般只有一个版本号. 当出现不同版本依赖时, 比如A依赖(X@v0.0.2), 但某个依赖指定了不同的版本号(比如X@v0.0.1), 我猜测根据兼容性公约, go的编译系统会选择新的版本号(v0.0.1)来编译. go clean 清理编译cache go clean -cache -i -r The -cache flag causes clean to remove the entire go build cache. The -i flag causes clean to remove the corresponding installed archive or binary (what 'go install' would create). The -r flag causes clean to be applied recursively to all the dependencies of the packages named by the import paths. The -n flag causes clean to print the remove commands it would execute, but not run them. The -modcache flag causes clean to remove the entire module download cache, including unpacked source code of versioned dependencies. 又犯了经典的goroutine引用闭包变量错误!!!!! 这是个比较隐蔽的先for在switch的结构, 很容易忘记go引用的闭包变量会异步的变化. for _, vc := range vcs { switch cmd.Cmd { case \"restart\": if vc.stat == vmStatExited { vm := vc.VM vm.In = null{} logFile, err := os.Create(vc.outputFile) if err != nil { greLogger.Errorln(errorHere(err)) break } vm.Out = logFile go func() { defer logFile.Close() vc.RestartedNum++ vc.runVM() }() ids = append(ids, vc.ID) } } } 现象是第15和16行, 在10次for循环里, 每次都是对同一个vc对象进行操做. 原因 这里第15和16行引用的是for的循环变量vc, 会被for循环更改. 解决 只在进入goroutine之前, 重新定义局部变量vc := vc, 这样goroutine里面的vc就引用的是局部变量vc.第13行定义的vc每次for循环都是个新的vc. for _, vc := range vcs { switch cmd.Cmd { case \"restart\": if vc.stat == vmStatExited { vm := vc.VM vm.In = null{} logFile, err := os.Create(vc.outputFile) if err != nil { greLogger.Errorln(errorHere(err)) break } vm.Out = logFile vc := vc go func() { defer logFile.Close() vc.RestartedNum++ vc.runVM() }() ids = append(ids, vc.ID) } } } 理论解释 参考https://stackoverflow.com/questions/39208162/why-i-can-redefine-the-same-variable-multiple-times-in-a-for-loop-but-cant-outs 有人问为什么在循环里可以: func main() { for i := 0; i 但自己手动写就编译不过: func main() { a := 77 fmt.Println(a) a := 77 fmt.Println(a) } 为啥? 专家的解答是: for循环每次进入循环体大括号块{}, 都是一个新的scope The reason is each time you enter a block of curly braces {} you're creating a new nested scope. When you declare the variable x at the top of the loop it is a new variable and it goes out of scope at the end of the loop. When the program comes back around to the top of the loop again it's another new scope. 有人给出了证据: func main() { for i := 0; i output 0x1040e0f8 0x1040e0fc 可以手动加{}来添加scope: func main() { a := 77 fmt.Println(&a) { a := 77 fmt.Println(&a) } } output 0x1040e0f8 0x1040e0fc 上面的例子就可以\"连续\"定义a两次, 但第二次是个新的变量地址 总结 for循环同一行的变量作用域在for里面没错, 但更像是在进入循环前定义的一样: for循环里面对循环变量的引用都是指向同一个东西 for循环里面用var v int或vc := vc定义的变量, 并非同一个地址, 每次循环都是\"临时\"生成的. 所以上面在第13行的修改可以解决问题. 以后检查go出去的函数是否有这个问题, 只检查循环变量就行了 if else if 共享一个变量scope 比如 var msg interface{} if ... { } else if msg, ok := msg.(ExclusiveMessage); ok { } else if msg, ok := msg.(Message); ok { } 第二个else if中的msg.(Message)实际上引用的是第一个else if中的msg, ok变量. 解决: 给if的变量取个不同的名字, 不要总叫msg 经典的append地址错误 背景是在vm运行的过程中, 调用callers()保存当下的调用栈所有栈帧. 下面的代码有错误: func (v *VM) callers() (frames []*frame) { curFrame := *v.curFrame curFrame.ip = v.ip - 1 frames = append(frames, &curFrame) for i := v.framesIndex - 1; i >= 1; i-- { //值复制, 避免v.frames变动造成curFrame变动 //v.frames是个数组 curFrame = v.frames[i-1] //注意这里搞错了, 每次都append同一个地址!!! 这个因为curFrame变量只有一个. frames = append(frames, &curFrame) } return frames } 改正: 这种情况下, 返回值的slice, 而不是指针slice. func (v *VM) callers() (frames []frame) { curFrame := *v.curFrame curFrame.ip = v.ip - 1 frames = append(frames, curFrame) for i := v.framesIndex - 1; i >= 1; i-- { curFrame = v.frames[i-1] frames = append(frames, curFrame) } return frames } 打印指针slice 我有个结构体, 现在想打印一个var wants []*want的slice type want struct { content []string unordered bool } 直接fmt.Printf(\"%v\", wants)会输出一个slice, 但元素都是指针. 怎么才能打印这些指针的值呢? 用fmt.Printf(\"%+v\", wants)和fmt.Printf(\"%#v\", wants)都不行. 解决: 给结构体加String方法 func (wt *want) String() string { var sb strings.Builder if wt.unordered { sb.WriteString(\"//unordered output:\\n\") } else { sb.WriteString(\"//output:\\n\") } for _, str := range wt.content { sb.WriteString(str + \"\\n\") } return sb.String() } 这样fmt.Printf(\"%v\", wants)就可以输出我们想要的内容了. 其原理是如果一个类型有自定义String()方法, Printf会调用这个自定义String()方法. 注意, 这里要用引用的receiver方式, 因为我们要打印指针. 关于wait group sync包的wait group用于主routine等待所有子routine退出. 在使用上需要注意: wg.Add(1)需要在go之前. wg.Done()需要在go里面. 即要严格按照官网的例子来写: var wg sync.WaitGroup var urls = []string{ \"http://www.golang.org/\", \"http://www.google.com/\", \"http://www.somestupidname.com/\", } for _, url := range urls { // Increment the WaitGroup counter. wg.Add(1) //注意这里, 在go之前Add() // Launch a goroutine to fetch the URL. go func(url string) { // Decrement the counter when the goroutine completes. defer wg.Done() //注意这里, 在go里面Done() // Fetch the URL. http.Get(url) }(url) } // Wait for all HTTP fetches to complete. wg.Wait() 下面解释一下: 在go之前Add(), 是要这个wg.Add(1) 一定 能被主routine调用到. 在go里面调wg.Done()很好理解, 表示事情在这个异步的goroutine里已经完成. 反例 通常大家容易犯的错误是把wg.Add(1)放到go的里面去做. 比如下面代码: clone一个VM, 然后在新的goroutine中run这个新的VM. 父VM需要记录这个新VM到其childVM map里面. func govm(fn) { newVM := vm.ShallowClone() gvm := &goroutineVM{ VM: newVM, waitChan: make(chan ret, 1), } vm.addChildVM(gvm.VM) go func() { //vm.addChildVM(gvm.VM) //不能在里面Add() val, err := gvm.RunCompiled(fn, args[1:]...) gvm.waitChan 如果第8行放到第10行做, 好像也在干活之前加了1, 干完活减1. 但实际情况是, 比如: 在父VM Abort时, 需要abort其所有的子VM. // Abort aborts the execution of current VM and all its descendant VMs. func (v *VM) Abort() { atomic.StoreInt64(&v.aborting, 1) close(v.abortChan) // broadcast to all receivers v.childCtl.Lock() for cvm := range v.childCtl.vmMap { cvm.Abort() } v.childCtl.Unlock() v.childCtl.Wait() // waits for all child VMs to exit } 现在假设父这样的操做序列: 新起3个VM然后马上abort govm(fn1) govm(fn2) govm(fn3) abort() 3个govm是异步在跑的, 当父VM routine运行到第4行abort()的时候, 在abort()跑到第10行v.childCtl.Wait()等待这个wait group的时候, 不能保证它的3个子VM都跑到了Add(1), 因为子VM的Add()可能还没有运行. 这样会导致在父routine的Wait()得到wait的个数小于实际的VM routine个数, 虽然VM起来以后这个wait个数是能够加到3的, 但已经过了父VM routine 的wait点(Abort()函数第10行), 最后的结果就是父VM routine不能把这3个VM routine都abort()掉. 从栈上获取变量地址 比如我要从下面的栈中, 取传递给github.com/d5/tengo/v2.(*VM).run的地址 goroutine 1 [running]: github.com/d5/tengo/v2.builtinGo(0xc0001360b0, 0x1, 0x1, 0x0, 0x0, 0xc0001360b0, 0x1) /repo/yingjieb/github/godevsig/tengo/builtins.go:409 +0x69 github.com/d5/tengo/v2.(*BuiltinFunction).Call(0x7993b0, 0xc0001360b0, 0x1, 0x1, 0xc000178010, 0x1, 0x7ff, 0x1) /repo/yingjieb/github/godevsig/tengo/objects.go:349 +0x48 github.com/d5/tengo/v2.(*VM).run(0xc00013e0d0) /repo/yingjieb/github/godevsig/tengo/vm.go:652 +0x3323 github.com/d5/tengo/v2.(*VM).Run(0xc00013e0d0, 0xc00013e0d0, 0x400) /repo/yingjieb/github/godevsig/tengo/vm.go:96 +0xc7 github.com/godevsig/gshellos.(*shell).runREPL(0xc000099ec8) /repo/yingjieb/github/godevsig/gshellos/gshellbuilder.go:101 +0x825 github.com/godevsig/gshellos.ShellMain(0x786a60, 0xc00005e750) /repo/yingjieb/github/godevsig/gshellos/gshellbuilder.go:197 +0x660 main.main() /repo/yingjieb/github/godevsig/gshellos/cmd/gshell/gshell.go:12 +0x26 下面是方法 首先在vm.go里面 var stackIdentifierVM string //应该是github.com/d5/tengo/v2.(*VM).run func init() { stackIdentifierVM = runtime.FuncForPC(reflect.ValueOf((*VM).run).Pointer()).Name() } 然后在需要用到VM地址的函数里: func builtinGo(args ...Object) (Object, error) { ... var buf [1024]byte n := runtime.Stack(buf[:], false) //取stack, 看从哪里调的. 本函数离(*VM).Run不远, 1024的buf够了 stk := string(buf[:n]) idx := strings.Index(stk, stackIdentifierVM) //找到stackIdentifierVM字符串, 也就是上面的\"github.com/d5/tengo/v2.(*VM).run\" stk2 := stk[idx+len(stackIdentifierVM)+1:] idx = strings.Index(stk2, \")\") addr, err := strconv.ParseUint(stk2[:idx], 0, 64) //转为int64 vm := (*VM)(unsafe.Pointer(uintptr(addr))) //用unsafe强制转为*VM newVM := vm.ShallowClone() //然后就可以使用这个vm的方法了 } 参考代码 # in vm.go var stackIdentifierVM = runtime.FuncForPC(reflect.ValueOf((*VM).run).Pointer()).Name() # in group.go func getVM() (*VM, error) { var buf [1024]byte n := runtime.Stack(buf[:], false) stk := string(buf[:n]) // find \"github.com/d5/tengo/v2.(*VM).run\" idx := strings.Index(stk, stackIdentifierVM) stk2 := stk[idx+len(stackIdentifierVM)+1:] idx = strings.Index(stk2, \")\") addr, err := strconv.ParseUint(stk2[:idx], 0, 64) if err != nil { return nil, fmt.Errorf(\"failed to get current VM from %s: %w\\n\", stk, err) } vm := (*VM)(unsafe.Pointer(uintptr(addr))) return vm, nil } 运行时获取函数名 比如我有个方法 func (v *VM) run() { } 现在想在另外一个函数里, 打印这个上面这个方法的名字: 方法1 fmt.Println(runtime.FuncForPC(reflect.ValueOf((*VM)(nil).run).Pointer()).Name()) //结果 github.com/d5/tengo/v2.(*VM).run-fm 方法2 fmt.Println(runtime.FuncForPC(reflect.ValueOf((*VM).run).Pointer()).Name()) //结果 github.com/d5/tengo/v2.(*VM).run 注意方法2和方法1的区别只是传入reflect.ValueOf的值不一样: (*VM)(nil).run的意思是先把nil强转成(*VM), 然后这个对象的run方法做为入参 (*VM).run直接就是方法, 说明本质上go把(receiver).method当成一个func定义. continue能返回N层for continue能够指定跳转lable(只能是for的lable) RowLoop: for y, row := range rows { for x, data := range row { if data == endOfRow { continue RowLoop } row[x] = data + bias(x, y) } } interface的理解 interface{}是青出于蓝 一个典型情况是, 底层对象实现了某些方法集合(蓝方法集合), 通过wrapper层, 提供给用户\"扩展\"版本的方法集合(青方法集合). 举例如下: mangos代码中, 底层通道的pipe抽象是transport.Pipe(TranPipe的别名),提供如下方法(蓝色接口): // TranPipe behaves like a full-duplex message-oriented connection between two // peers. Callers may call operations on a Pipe simultaneously from // different goroutines. (These are different from net.Conn because they // provide message oriented semantics.) // // Pipe is only intended for use by transport implementors, and should // not be directly used in applications. type TranPipe interface { // Send sends a complete message. In the event of a partial send, // the Pipe will be closed, and an error is returned. For reasons // of efficiency, we allow the message to be sent in a scatter/gather // list. Send(*Message) error // Recv receives a complete message. In the event that either a // complete message could not be received, an error is returned // to the caller and the Pipe is closed. // // To mitigate Denial-of-Service attacks, we limit the max message // size to 1M. Recv() (*Message, error) // Close closes the underlying transport. Further operations on // the Pipe will result in errors. Note that messages that are // queued in transport buffers may still be received by the remote // peer. Close() error // GetOption returns an arbitrary transport specific option on a // pipe. Options for pipes are read-only and specific to that // particular connection. If the property doesn't exist, then // ErrBadOption should be returned. GetOption(string) (interface{}, error) } 在internal/core/socket.go中, 用一个结构体\"包装\"了底层的transport.Pipe // pipe wraps the Pipe data structure with the stuff we need to keep // for the core. It implements the Pipe interface. type pipe struct { id uint32 p transport.Pipe //这个就是transport.Pipe的接口实例 l *listener d *dialer s *socket closeOnce sync.Once data interface{} // Protocol private added bool closing bool lock sync.Mutex // held across calls to remPipe and addPipe } 这个pipe实现了protocol.ProtocolPipe接口(青色接口): // ProtocolPipe represents the handle that a Protocol implementation has // to the underlying stream transport. It can be thought of as one side // of a TCP, IPC, or other type of connection. type ProtocolPipe interface { // ID returns a unique 31-bit value associated with this. // The value is unique for a given socket, at a given time. ID() uint32 // Close does what you think. Close() error // SendMsg sends a message. On success it returns nil. This is a // blocking call. SendMsg(*Message) error // RecvMsg receives a message. It blocks until the message is // received. On error, the pipe is closed and nil is returned. RecvMsg() *Message // SetPrivate is used to set protocol private data. SetPrivate(interface{}) // GetPrivate returns the previously stored protocol private data. GetPrivate() interface{} } 通过\"青出于蓝\"的操作, pipe struct对外屏蔽了transport.Pipe实例, 但对外提供了该有的函数. 即\"青出于蓝\"的核心不在于暴露蓝的实例, 而是提供青的功能函数. 所以interface{}的本质是提供方法规约, 同时隐藏了实例细节. 这是一个高度接口化(或者说是标准化)的世界, 比如在纺织厂语境下, 你提供的只有纺织工的操作的双手, 你的个性, 比如喜欢王菲的歌, 根本没必要也不值得被外人知道. 相对C++, go的interface概念摒弃了对象的\"数据\"属性, 只保留\"方法\"规约. 对象的\"数据\"自己来cook, 外人不关心; 外人只要你提供方法\"服务\"就行了. 注: 这里的pipe struct是小写, 其内部所有的field都是小写, 这个pipe不能被外部\"直接\"使用. 但可以通过调用核心层的函数, s.proto.AddPipe(p), \"注册\"自己: AddPipe是protocol实例的规定函数, 原型如下: AddPipe(ProtocolPipe) error 上面的p代表一个ProtocolPipe实例, 虽然全部都是小写, 但也不妨碍能被当作ProtocolPipe的interface来赋值. 即这里就把一个完全\"私有\"的实例, 通过满足ProtocolPipe规定的方法, 当作ProtocolPipe的实例被\"导出\"到外部使用. interface{}是带上下文的方法集合 带方法的interface{}的典型的使用场景是: 该interface{}变量是带上下文的函数集合. 比如mangos里面创建REQ的socket: func NewSocket() (protocol.Socket, error) { return protocol.MakeSocket(NewProtocol()), nil } 其中protocol.MakeSocket(proto Protocol) Socket的入参就是一个规定方法的interface{} type ProtocolBase interface { ProtocolContext // Info returns the information describing this protocol. Info() ProtocolInfo // XXX: Revisit these when we can use Pipe natively. // AddPipe is called when a new Pipe is added to the socket. // Typically this is as a result of connect or accept completing. // The pipe ID will be unique for the socket at this time. // The implementation must not call back into the socket, but it // may reject the pipe by returning a non-nil result. AddPipe(ProtocolPipe) error // RemovePipe is called when a Pipe is removed from the socket. // Typically this indicates a disconnected or closed connection. // This is called exactly once, after the underlying transport pipe // is closed. The Pipe ID will still be valid. RemovePipe(ProtocolPipe) // OpenContext is a request to create a unique instance of the // protocol state machine, allowing concurrent use of states on // a given protocol socket. Protocols that don't support this // should return ErrProtoOp. OpenContext() (ProtocolContext, error) } 所以: 核心层(比如这里的protocol层), 给其下辖的模块规定方法集, 满足这些方法集的实现就能享受核心层提供的好处(比如子模块享受核心层的MakeSocket()方法) 核心层是框架, 框架定好规矩(方法集) 子模块是实现, 实现了规定的方法集, 就能融入框架, 享受框架. 这里的例子是, 子模块调用核心层的函数protocol.MakeSocket(自己的接口实例), 传入自己的interface{}实现. 总结: 核心层定义接口, 针对接口做框架 子模块实现接口, 调用核心层的函数来完成任务. runtime.Caller获取当前运行目录 runtime.Caller(0)的第二个返回值是文件名, 对文件名的路径操作得到当前目录, 定位文件等等. _, f, _, _ := runtime.Caller(0) topDir := f[:strings.Index(f, \"extension\")] covFile := filepath.Base(strings.TrimSuffix(file, filepath.Ext(file))) covFileArg := fmt.Sprintf(\"-test.coverprofile=%sl2_%s.cov\", topDir, covFile) An interface holding nil value is not nil func main() { var a interface{} fmt.Printf(\"a == nil is %t\\n\", a == nil) var b interface{} var p *int = nil b = p fmt.Printf(\"b == nil is %t\\n\", b == nil) } 结果: a == nil is true b == nil is false b被赋值为p, p是nil. 但b不是nil 因为interface为nil的条件是2个: 值和类型都必须是nil \"An interface equals nil only if both type and value are nil.\" 这里b的值是nil, 但类型不是nil. 只声明没赋值的接口变量是nil 显式赋值为nil的接口变量是nil. 注意必须是b = nil这样的赋值才行. 补充: nil既是值, 也是一种类型. 另外, 可以对接口进行类型断言来查看其\"值\"是否为nil var v interface{} var a map[string]int v = a //此时v已经不是nil了, 因为v的类型变成了map[string]int //对v断言成mv, 那么mv就又是nil了 mv := v.(map[string]int) if mv == nil { return nil, nil } net/http导致包变大 现象 我import了网上的库, abs. 编译后发现有8.2M. 用nm命令查看二进制, 发现有很多net/http的符号. 但实际上, 我并没有显式引用任何网络相关的函数. 解决 调查发现, abs内部的一个package util中, 有一个文件引用了\"net/http\"包, 提供了一个函数从httpDownload(). 删除这个文件, 二进制的size直接减小了4M! 编译时间也缩短了很多. 而且还不影响功能 原因 那为什么代码里没有实际引用Download()函数, net/http还是被编译进去了呢? 是go的编译器不够聪明, 不能把\"dead code\"删掉吗? -- 不是. 虽然在编译阶段, 是按照packge来编译成.a的, 但这个阶段一般都会缓存到一个cache目录下. 在链接阶段, go的链接器能做到只链接用得到的符号. 但即使只是空引用import _ \"net/http\", 二进制的size就会增加4M, 说明net/http内部一定是有全局变量或者init()函数, 引用了自身的符号. 这个引用进一步把全部符号都拖入泥潭. 结论 不要引用net/http, 初非必须要http功能. 用nm查看二进制, 排查是否有net/http出现. Options初始化 readlineutil.go是github.com/chzyer/readline的一个简单封装, 提供一个类似bufio.Scaner的迭代器. 它的初始化很有设计感. 要点是 入参是变长的 入参的形式是函数 参数在函数里面执行 type Term struct { inst *readline.Instance io.Writer line string err error prevPrompt string } type Option func(*conf) type conf struct { rc *readline.Config } //options是个变长的入参, 格式是Option, 后者是个函数. func NewTerm(options ...Option) (*Term, error) { var c conf c.rc = new(readline.Config) c.rc.DisableAutoSaveHistory = true //执行入参函数 for _, o := range options { o(&c) } inst, err := readline.NewEx(c.rc) t := new(Term) t.inst = inst t.Writer = inst.Stdout() return t, nil } //返回闭包函数 func WithHistoryFile(filename string) Option { return func(c *conf) { c.rc.HistoryFile = filename } } //使用: t := NewTerm(WithHistoryFile(\"history\")) 相对于普通的设计: 多个入参, 类型不同, 显式指定. 比如func NewTerm(history string, prompt string, search bool, 等等) (*Term, error) 这样做的缺点很多: 如果是对外的API, 那这个API可能会经常变化; 比如增加个属性, 调用者要改代码才能编过 -- 即使老用户并不关心这个新增的属性, 也不准备用这个新功能. 参数通过位置传递, 多了不好看 入参是个结构体, 结构体的字段表述不同的属性; 这样入参不需要变长, 靠结构体的定义, 以及不同field的赋值来传入\"变化\" 解决了上面方案的变化问题, 部分解决了API更改的问题. -- 此时API的更改变更为struct{}的更改, 部分解决了老用户希望代码不变的问题 -- 部分赋值的struct是允许的. 但调用者还是要关心这个入参结构体的定义 结构体字段的赋值没有明显的位置感, key: value的形式可读性好点 入参变长, 全部是interface{} 足够灵活, 如果实在是外部需求变化大, 需要适应变化 繁琐: 需要在实现里不断的搞类型断言 对于本例的场景, 并不适合. 传入的参数需要表明目的. 本例范式: func NewTerm(options ...Option) 是对\"结构体\"入参的一种扩展, Option本质上是一个函数, 这个函数对\"cfg入参结构体\"中的一个字段进行配置 这样定义的对外API有良好的扩展性: 用户不必修改代码; cfg结构体对用户不可见. go-micro中, 就大量使用了Options范式: // Service is an interface for a micro service type Service interface { ... // Init initialises options Init(...Option) // Options returns the current options Options() Options ... } type Option func(*Options) type Options struct { A string B int C bool } 总结 设计一个对外的API时, 比较典型的是NewXxx的API, 或者Init()的API, 最好不用1, 简单点的场景用2, 复杂点的框架用4; 特殊情况用3 空白import不会增加binary size 比如一个empty.go, 本来不需要pidinfo包. 但还是引入了这个包 import _ \"pidinfo\" 可以编译, 编译后的binary只包括一点点pidinfo的代码. 整个size并没有变化. 还是2M. 如果调用了有限几个pidinfo里面的函数, 感觉go的编译器会自动remove dead code. reflect高阶用法 动态构建一个struct 用reflect可以创建一个\"任意没有定义过\"的结构体 核心是func StructOf(fields []StructField) Type API typ := reflect.StructOf([]reflect.StructField{ { Name: \"Height\", Type: reflect.TypeOf(float64(0)), Tag: `json:\"height\"`, }, { Name: \"Age\", Type: reflect.TypeOf(int(0)), Tag: `json:\"age\"`, }, }) v := reflect.New(typ).Elem() v.Field(0).SetFloat(0.4) v.Field(1).SetInt(2) s := v.Addr().Interface() w := new(bytes.Buffer) if err := json.NewEncoder(w).Encode(s); err != nil { panic(err) } fmt.Printf(\"value: %+v\\n\", s) fmt.Printf(\"json: %s\", w.Bytes()) r := bytes.NewReader([]byte(`{\"height\":1.5,\"age\":10}`)) if err := json.NewDecoder(r).Decode(s); err != nil { panic(err) } fmt.Printf(\"value: %+v\\n\", s) 结果: value: &{Height:0.4 Age:2} json: {\"height\":0.4,\"age\":2} value: &{Height:1.5 Age:10} byName API -- 神奇的重名API reflect.Value的MethodByName方法 从一个反射对象reflect.Value可以用方法名查到它的方法对象: func (v Value) MethodByName(name string) Value 返回一个\"function value\". 传参给返回的function不能带receiver, 因为它把Value v当作默认的receiver MethodByName returns a function value corresponding to the method of v with the given name. The arguments to a Call on the returned function should not include a receiver; the returned function will always use v as the receiver. It returns the zero Value if no method was found. stack over flow有个讨论, 里面由示例代码. 还有个更简单的 package main import \"fmt\" import \"reflect\" type T struct {} func (t *T) Foo() { fmt.Println(\"foo\") } func main() { var t T reflect.ValueOf(&t).MethodByName(\"Foo\").Call([]reflect.Value{}) } reflect.Type的MethodByName方法 reflect.Type的反射对象也有个同名的方法 reflect.Type是个接口, 它底层的具体类型必须实现一系列的函数 // MethodByName returns the method with that name in the type's // method set and a boolean indicating if the method was found. // // For a non-interface type T or *T, the returned Method's Type and Func // fields describe a function whose first argument is the receiver. // // For an interface type, the returned Method's Type field gives the // method signature, without a receiver, and the Func field is nil. MethodByName(string) (Method, bool) 返回的Method是个结构体 type Method struct { // Name is the method name. // PkgPath is the package path that qualifies a lower case (unexported) // method name. It is empty for upper case (exported) method names. // The combination of PkgPath and Name uniquely identifies a method // in a method set. // See https://golang.org/ref/spec#Uniqueness_of_identifiers Name string PkgPath string Type Type // method type Func Value // func with receiver as first argument Index int // index for Type.Method } 用返回的Method怎么调用函数??? go调用外部程序 这里以go解释器yaegi为例. dotCmd是dot -Tdot -o ast.dot // dotWriter returns an output stream to a dot(1) co-process where to write data in .dot format. func dotWriter(dotCmd string) io.WriteCloser { if dotCmd == \"\" { return nopCloser{ioutil.Discard} } fields := strings.Fields(dotCmd) //构建cmd cmd := exec.Command(fields[0], fields[1:]...) //cmd有StdinPipe函数, 返回 dotin, err := cmd.StdinPipe() if err != nil { log.Fatal(err) } //开始这个cmd, 但还没有输入 if err = cmd.Start(); err != nil { log.Fatal(err) } //返回输入的句柄 return dotin } 外部对dotWriter返回的io.WriteCloser写就可以pipe到cmd的输入. text/template代码生成举例 const model = `// Code generated by 'yaegi extract {{.PkgName}}'. DO NOT EDIT. {{.License}} {{if .BuildTags}}// +build {{.BuildTags}}{{end}} package {{.Dest}} import ( {{- range $key, $value := .Imports }} {{- if $value}} \"{{$key}}\" {{- end}} {{- end}} \"{{.PkgName}}\" \"reflect\" ) func init() { Symbols[\"{{.PkgName}}\"] = map[string]reflect.Value{ {{- if .Val}} // function, constant and variable definitions {{range $key, $value := .Val -}} {{- if $value.Addr -}} \"{{$key}}\": reflect.ValueOf(&{{$value.Name}}).Elem(), {{else -}} \"{{$key}}\": reflect.ValueOf({{$value.Name}}), {{end -}} {{end}} {{- end}} {{- if .Typ}} // type definitions {{range $key, $value := .Typ -}} \"{{$key}}\": reflect.ValueOf((*{{$value}})(nil)), {{end}} {{- end}} {{- if .Wrap}} // interface wrapper definitions {{range $key, $value := .Wrap -}} \"_{{$key}}\": reflect.ValueOf((*{{$value.Name}})(nil)), {{end}} {{- end}} } } {{range $key, $value := .Wrap -}} // {{$value.Name}} is an interface wrapper for {{$key}} type type {{$value.Name}} struct { {{range $m := $value.Method -}} W{{$m.Name}} func{{$m.Param}} {{$m.Result}} {{end}} } {{range $m := $value.Method -}} func (W {{$value.Name}}) {{$m.Name}}{{$m.Param}} {{$m.Result}} { {{$m.Ret}} W.W{{$m.Name}}{{$m.Arg}} } {{end}} {{end}} ` 使用的时候 base := template.New(\"extract\") parse, err := base.Parse(model) b := new(bytes.Buffer) data := map[string]interface{}{ \"Dest\": e.Dest, \"Imports\": imports, //这里的imports是个map \"PkgName\": importPath, \"Val\": val, \"Typ\": typ, \"Wrap\": wrap, \"BuildTags\": buildTags, \"License\": e.License, } err = parse.Execute(b, data) // gofmt source, err := format.Source(b.Bytes()) //此时source里面就是替换后的代码 注: 替换的变量是通过map[string]interface{}传入的, 其值为万能interface 值可以是map, 比如上面的imports; 对应模板里面用range来遍历. map的重要属性 unaddressable 这样操作map是可以的: type User struct { name string } users := make(map[int]User) users[5] = User{\"Steve\"} 但下面这样不行: users[5].name = \"Mark\" //cannot assign to struct field users[5].name in map 为什么呢? 问答在这里 答: map的value是不能被寻址的(by语言设计), 虽然users[5]看起来像是寻找到了一个User, 但在go里面所有的传递都是值传递, users[5].name = \"Mark\"也隐含发生了值拷贝: 从原User拷到了临时的不可见的User. 对后者的赋值= Mmark\"是没有任何意义的, 即使可以, 也不会改变原User的name. 所以在编译阶段, 就会提示错误. 同样的, 这样也会报错: (&users[5]).name = \"Mark\" //cannot take the address of users[5] 那如何更改value呢? 方法1 整体给map赋值 t := users[5] t.name = \"Mark\" users[5] = t 上面的代码中, 首先显式的值拷贝到t, 更改t, 再把t拷贝进users这个map. 这里User类型的实例的拷贝都发生了2次. 方法2 让value的类型为指针 users := make(map[int]*User) 此时users[5]虽然也是值拷贝, 但拷贝出来的指针还是指向底层数据, 就可以更改了. 结论 map的value为值类型时, 不能被寻址; 所以不能\"原地\"修改 可以把map的value设计为指针类型, 支持\"原地\"修改. 但这样会给gc带来压力 也可以用copy-改-copy进map的方式修改 go的树状表达 在C里面, 定义一个node时, next域必须是ListNode的指针 struct ListNode{ int val; ListNode* next; }; 在go里面, 一个tree的最简单, 最天然的表达是map: map的value还是个tree type astTree map[string]astTree 注意到这里, value实际上是它对自己的表达: 自己包括自己(而不是指针), 能行吗? --可以. 下面的例子说明这么写没有任何问题 var ast astTree = nil ast = astTree{\"hello\":nil, \"wolrd\":{\"111\":nil}} fmt.Println(ast) 结果是 map[hello:map[] wolrd:map[111:map[]]] 解释: 实际上, map是个固定size的结构体, 有着类似指针的性质, 比如上面代码中, var ast astTree = nil, map可以赋值为nil. 既然是固定size, 那astTree的value就可以是自身astTree 但是, type astTree map[string]astTree这样树的表达, 语法上可以, 但没有实际意义: 这个树的节点上, 只有做为string的key能存储有意义的数据 -- 一个树, 除了结构表达, 还需要存储data才能发挥实际的作用. 存储data 把ast定义成一个结构体: type astTree struct { data int ast map[string]astTree } 初始化: func main() { fmt.Println(\"Hello, playground\") //var ast astTree = nil ast := astTree{23, map[string]astTree{\"hello\":astTree{}, \"nihao\":astTree{}}} ast.ast[\"world\"] = astTree{} fmt.Println(ast) } //输出 Hello, playground {23 map[hello:{0 map[]} nihao:{0 map[]} world:{0 map[]}]} 但这样有点太丑了. 而且因为map的value元素为值类型的undressable因素, 不能用ast.ast[\"world\"].data =100这样的原地修改方法. 改进存储 按照map的value为指针类型可以寻址的特点, 改进如下: type asTree struct { data int subTrees map[string]*asTree } func main() { fmt.Println(\"Hello, playground\") ast := asTree{23, map[string]*asTree{\"hello\":&asTree{}, \"nihao\":&asTree{}}} ast.subTrees[\"world\"] = &asTree{} ast.subTrees[\"world\"].data =100 ast.subTrees[\"world\"].subTrees = map[string]*asTree{\"shanghai\":&asTree{}} ast.subTrees[\"world\"].subTrees[\"shanghai\"].subTrees = map[string]*asTree{\"pudong\":&asTree{}} fmt.Println(ast) fmt.Println(ast.subTrees[\"world\"]) ast.subTrees[\"world\"].subTrees[\"shanghai\"].data = 2013 fmt.Println(ast.subTrees[\"world\"].subTrees[\"shanghai\"]) } //输出 Hello, playground {23 map[hello:0xc000010200 nihao:0xc000010210 world:0xc000010220]} &{100 map[shanghai:0xc000010230]} &{2013 map[pudong:0xc000010240]} 如果用key来存储data会怎样? type useFullData struct { data1 int data2 string } type asTree map[useFullData]asTree 这样遍历是可以用k, v := range(asTree)来遍历的 但key本质上是种索引, key应该是某种不变的东西. key用来查询并得到数据. 用key来存储数据的问题是, 如果要存储的数据会变化, 那key就变了. golang的SIGABRT 在配置了GOTRACEBACK=crash的情况下, go程序会在panic的时候, 打印所有goroutine的调用栈(这个和GOTRACEBACK=system效果一样), 而且还会发SIGABRT(6号signal)触发core dump. man 7 signal说的很清楚, 每个signal都有默认的性情(disposition): Signal Value Action Comment ────────────────────────────────────────────────────────────────────── SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard SIGQUIT 3 Core Quit from keyboard SIGILL 4 Core Illegal Instruction SIGABRT 6 Core Abort signal from abort(3) SIGFPE 8 Core Floating-point exception SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers; see pipe(7) SIGALRM 14 Term Timer signal from alarm(2) SIGTERM 15 Term Termination signal SIGUSR1 30,10,16 Term User-defined signal 1 SIGUSR2 31,12,17 Term User-defined signal 2 SIGCHLD 20,17,18 Ign Child stopped or terminated SIGCONT 19,18,25 Cont Continue if stopped SIGSTOP 17,19,23 Stop Stop process SIGTSTP 18,20,24 Stop Stop typed at terminal SIGTTIN 21,21,26 Stop Terminal input for background process SIGTTOU 22,22,27 Stop Terminal output for background process SIGABRT的性情就是触发core dump机制. 内核会走core dump流程. 所以, 在GOTRACEBACK=crash情况下 go程序会先打印panic调用栈(所有go routine) 然后主动调用类似c的abort()函数触发SIGABRT给自己 然后kernel会走core dump流程. 关于syscall syscall提供了对底层os的原始封装. 从go1.4开始 官方推荐使用golang.org/x/sys来代替syscall. 标准库的syscall提供了一些基本的const常量. 比如 EPOLLERR = 0x8 EPOLLET = -0x80000000 EPOLLHUP = 0x10 EPOLLIN = 0x1 EPOLLMSG = 0x400 EPOLLONESHOT = 0x40000000 EPOLLOUT = 0x4 EPOLLPRI = 0x2 EPOLLRDBAND = 0x80 EPOLLRDHUP = 0x2000 EPOLLRDNORM = 0x40 EPOLLWRBAND = 0x200 EPOLLWRNORM = 0x100 EPOLL_CLOEXEC = 0x80000 EPOLL_CTL_ADD = 0x1 EPOLL_CTL_DEL = 0x2 EPOLL_CTL_MOD = 0x3 syscall和CPU arch强相关, 默认显示GOARCH的对应的包. 一般是amd64 用 $GOOS and $GOARCH来切换其他的组合. 实际的代码在对应的组合, 比如/usr/local/go/src/syscall/zerrors_linux_arm64.go 标准syscall库的问题 复杂, 维护的很差, 表现在难于测试, 必须跟随OS的ARCH的改动 缺少文档, 兼容性难以保证 解决 syscall在go1.3code freeze 从go1.4开始, 使用一个新库, 新库分为plan9, unix, windows三个子类 在2014年左右就完成了 Note that we cannot clean up the existing syscall package to any meaningful extent because of the compatibility guarantee. We can freeze and, in effect, deprecate it, however. 更好的syscall库 简介在此, 这个简介在2014年左右 库地址: https://github.com/golang/sys 使用 go get -u golang.org/x/sys ioctl unix/zsyscall_linux.go中 func ioctl(fd int, req uint, arg uintptr) (err error) { _, _, e1 := Syscall(SYS_IOCTL, uintptr(fd), uintptr(req), uintptr(arg)) if e1 != 0 { err = errnoErr(e1) } return } 注意到这里的ioctl是小写的, 外面不能引用. c的ioctl接口 #include int ioctl(int fd, unsigned long request, ...); ioctl是对设备文件用的. 第二个参数是设备相关的request code, 第三个参数是个指针(char *argp). 这个request是个编码, 指示argp是入参还是出参, argp的字节数 ioctl对应驱动的实现: int (*ioctl) (struct inode * node, struct file *filp, unsigned int cmd, unsigned long arg); 这篇文章讲的比较清楚. cmd cmd为32bit, 是个组合, 包括几个部分 分类:8bit 类内序号: 8bit 数据传输方向: 2bit_IOC_NONE _IOC_READ _IOC_WRITE _IOC_READ|_IOC_WRITE 数据大小: 剩下的14bit argp 应用层的ioctl的第三个参数是\"...\"，这个跟printf的\"...\"可不一样，printf中是意味这你可以传任意个数的参数，而ioctl最多也只能传一个，\"...\"的意思是让内核不要检查这个参数的类型。也就是说，从用户层可以传入任何参数，只要你传入的个数是1. 一般会有两种的传参方法： 整数，那可是省力又省心，直接使用就可以了。 指针，通过指针的就传什么类型都可以了，当然用起来就比较烦。在驱动里使用copy_xx_user函数从用户态传输数据 go的ioctl接口 前面说过, go的ioctl是小写的, \"内部专供\" 但在unix/ioctl.go中, 封装了几个对外开放的接口: // ioctl itself should not be exposed directly, but additional get/set // functions for specific types are permissible. // IoctlSetInt performs an ioctl operation which sets an integer value // on fd, using the specified request number. func IoctlSetInt(fd int, req uint, value int) error // IoctlSetPointerInt performs an ioctl operation which sets an // integer value on fd, using the specified request number. The ioctl // argument is called with a pointer to the integer value, rather than // passing the integer value directly. func IoctlSetPointerInt(fd int, req uint, value int) error // IoctlSetWinsize performs an ioctl on fd with a *Winsize argument. // // To change fd's window size, the req argument should be TIOCSWINSZ. func IoctlSetWinsize(fd int, req uint, value *Winsize) error // IoctlSetTermios performs an ioctl on fd with a *Termios. // // The req value will usually be TCSETA or TIOCSETA. func IoctlSetTermios(fd int, req uint, value *Termios) error // IoctlGetInt performs an ioctl operation which gets an integer value // from fd, using the specified request number. // // A few ioctl requests use the return value as an output parameter; // for those, IoctlRetInt should be used instead of this function. func IoctlGetInt(fd int, req uint) (int, error) func IoctlGetWinsize(fd int, req uint) (*Winsize, error) func IoctlGetTermios(fd int, req uint) (*Termios, error) 结合C版本的ioctl分析, 这几个API怕是不够. IoctlSetInt和IoctlSetPointerInt能cover简单的int传输的需求 IoctlGetWinsize和IoctlGetTermios都是传递特殊功能结构体的. unix/syscall_linux.go中, 还有几个API: func IoctlRetInt(fd int, req uint) (int, error) func IoctlSetRTCTime(fd int, value *RTCTime) error func IoctlGetUint32(fd int, req uint) (uint32, error) func IoctlGetRTCTime(fd int) (*RTCTime, error) ioctl的宏定义在哪里? 在unix/zerrors_linux.go和unix/zerrors_linux_amd64.go 不用的OS和ARCH对应不同的文件 比如 //unix/zerrors_linux.go EPOLLIN = 0x1 //unix/zerrors_linux_amd64.go RTC_RD_TIME = 0x80247009 asm files ioctl调用的Syscall是在asm里面手写的 见https://github.com/golang/sys/tree/master/unix The hand-written assembly file at asm_${GOOS}_${GOARCH}.s implements system call dispatch. There are three entry points: func Syscall(trap, a1, a2, a3 uintptr) (r1, r2, err uintptr) func Syscall6(trap, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr) func RawSyscall(trap, a1, a2, a3 uintptr) (r1, r2, err uintptr) The first and second are the standard ones; they differ only in how many arguments can be passed to the kernel. The third is for low-level use by the ForkExec wrapper. Unlike the first two, it does not call into the scheduler to let it know that a system call is running. When porting Go to an new architecture/OS, this file must be implemented for each GOOS/GOARCH pair. mmap unix/syscall_linux.go //返回一个byte切片 func Mmap(fd int, offset int64, length int, prot int, flags int) (data []byte, err error) { return mapper.Mmap(fd, offset, length, prot, flags) } 使用: // +build aix darwin dragonfly freebsd linux netbsd openbsd solaris package unix_test import ( \"runtime\" \"testing\" \"golang.org/x/sys/unix\" ) func TestMmap(t *testing.T) { b, err := unix.Mmap(-1, 0, unix.Getpagesize(), unix.PROT_NONE, unix.MAP_ANON|unix.MAP_PRIVATE) if err != nil { t.Fatalf(\"Mmap: %v\", err) } if err := unix.Mprotect(b, unix.PROT_READ|unix.PROT_WRITE); err != nil { t.Fatalf(\"Mprotect: %v\", err) } //可以直接使用这个byte切片来修改内容 b[0] = 42 if runtime.GOOS == \"aix\" { t.Skip(\"msync returns invalid argument for AIX, skipping msync test\") } else { //msync系统调用是用来flush内容copy到真正的文件, mumap也有这个功能. if err := unix.Msync(b, unix.MS_SYNC); err != nil { t.Fatalf(\"Msync: %v\", err) } } //msync以后, 这块内存就可以\"建议\"内核, 不需要了 if err := unix.Madvise(b, unix.MADV_DONTNEED); err != nil { t.Fatalf(\"Madvise: %v\", err) } //最后使用munmap解除映射 if err := unix.Munmap(b); err != nil { t.Fatalf(\"Munmap: %v\", err) } } 百度上的结论写的不错: 最终被映射文件的内容的长度不会超过文件本身的初始大小，即映射不能改变文件的大小； 可以用于进程通信的有效地址空间大小大体上受限于被映射文件的大小，但不完全受限于文件大小。打开文件被截短为5个people结构大小，而在 map_normalfile1中初始化了10个people数据结构，在恰当时候（map_normalfile1输出initialize over 之后，输出umap ok之前）调用map_normalfile2会发现map_normalfile2将输出全部10个people结构的值，后面将给出详细讨论。 　　注：在linux中，内存的保护是以页为基本单位的，即使被映射文件只有一个字节大小，内核也会为映射分配一个页面大小的内存。当被映射文件小于一个页面大小时，进程可以对从mmap()返回地址开始的一个页面大小进行访问，而不会出错；但是，如果对一个页面以外的地址空间进行访问，则导致错误发生，后面将进一步描述。因此，可用于进程间通信的有效地址空间大小不会超过文件大小及一个页面大小的和。 文件一旦被映射后，调用mmap()的进程对返回地址的访问是对某一内存区域的访问，暂时脱离了磁盘上文件的影响。所有对mmap()返回地址空间的操作只在内存中有意义，只有在调用了munmap()后或者msync()时，才把内存中的相应内容写回磁盘文件，所写内容仍然不能超过文件的大小。 mmap返回的byte切片是哪里来的? unix/syscall_linux.go中, Mmap的实现是 func Mmap(fd int, offset int64, length int, prot int, flags int) (data []byte, err error) { return mapper.Mmap(fd, offset, length, prot, flags) } 而这个mapper的实现在unix/syscall_unix.go func (m *mmapper) Mmap(fd int, offset int64, length int, prot int, flags int) (data []byte, err error) { if length 可以看到 Syscall交互的数据都是uintptr类型 Mmap返回的data []byte, 并不是通常动态申请的buffer, 而是用Syscall返回的地址, 经过黑科技构造成的切片. 注: 复习一下指针转换的知识 unsafe.Pointer有如下性质: unsafe.Pointer和任意的指针类型能互相转换 unsafe.Pointer和uintptr能互相转换 指针和uintptr不能直接互转 fnctl unix/fcntl.go中, 对fnctl也有个简单的封装 RWLock死锁 在持有mtx.Lock()的时候, 在里面再次获取RLock会死锁 比如下面的结构会死锁. mtx.Lock() 调用一个函数() 函数里面再次获取读锁 mtx.RLock() ... mtx.RUnlock() mtx.Unlock() 死锁发生时, goroutine会显示semacquire状态 goroutine 1 [semacquire]: 其调用栈路径上能看到是重复获取锁 官方文档说的很清楚, RLock()不支持重复获取(recursive lock) float强转为int go的强转和C的表现是一致的, 比如把3.1415926强转为int, C和go都是得到3, 即浮点的整数部分. go generate go generate用于执行go代码注释中的动作 中文说明 官方说明 执行go generate时，有一些环境变量可以使用: $GOARCH 体系架构 (arm、amd64等待) $GOOS OS环境(linux、windows等) $GOFILE 当前处理中的文件名 $GOLINE 当前命令在文件中的行号 $GOPACKAGE 当前处理文件的包名 $DOLLAR 固定的\"$\",不清楚用途 假设我们有个main.go文件，内容如下： package main import \"fmt\" //go:generate echo hello //go:generate go run main.go //go:generate echo file=$GOFILE pkg=$GOPACKAGE func main() { fmt.Println(\"main func\") } 执行“go generate”后，输出如下： $ go generate hello main func file=main.go pkg=main 最后的build步骤就两步 all: go generate && go build . go generate常使用的一些工具 在学习go generate的过程中，我还看到了一篇generate的常用工具的wiki，我并没有全部使用过，在此与大家分享，希望能提升开发效率，https://github.com/golang/go/wiki/GoGenerateTools。 go generate仅在您有使用它的工具时才有用！这是生成代码的有用工具的不完整列表。 goyacc – Go的Yacc。 stringer – 实现fmt.Stringer枚举的接口。 gostringer – fmt.GoStringer为枚举实现接口。 jsonenums – 枚举的实现json.Marshaler和json.Unmarshaler接口。 go-syncmap – 使用软件包作为的通用模板生成Go代码sync.Map。 go-syncpool – 使用软件包作为的通用模板生成Go代码sync.Pool。 go-atomicvalue – 使用软件包作为的通用模板生成Go代码atomic.Value。 go-nulljson – 使用包作为实现database/sql.Scanner和的通用模板生成Go代码database/sql/driver.Valuer。 go-enum – 使用包作为实现接口的通用模板生成Go代码fmt.Stringer| binary| json| text| sql| yaml枚举。 go-import – 执行非go文件的自动导入。 gojson – 从示例json文档生成go结构定义。 vfsgen – 生成静态实现给定虚拟文件系统的vfsdata.go文件。 goreuse – 使用包作为通用模板通过替换定义来生成Go代码。 embedfiles – 将文件嵌入Go代码。 ragel – 状态机编译器 peachpy – 嵌入在Python中的x86-64汇编器，生成Go汇编 bundle – Bundle创建适用于包含在特定目标软件包中的源软件包的单一源文件版本。 msgp – MessagePack的Go代码生成器 protobuf – protobuf thriftrw – thrift gogen-avro – avro swagger-gen-types – 从swagger定义中去生成代码 avo – 使用Go生成汇编代码 Wire – Go的编译时依赖注入 sumgen – 从sum-type声明生成接口方法实现 interface-extractor – 生成所需类型的接口，仅在包内使用方法。 deep-copy – 为给定类型创建深度复制方法。 godoc安装 godoc属于golang.org/x/tools/ 根据https://github.com/golang/tools的说法, 最简单的安装: go get -u golang.org/x/tools/... 注意后面的三个点也是要的. 安装完毕后, 在GOPATH的bin下面, 会有很多tools $ ls /repo/yingjieb/go/bin/ authtest callgraph cover findcall gitauth godoc gopackages gotype helper lostcancel present shadow stress toolstash benchcmp compilebench digraph fiximports go-contrib-init goimports gorename goyacc html2article netrcauth present2md splitdwarf stringer unmarshal bundle cookieauth eg getgo godex gomvpkg gostacks guru ifaceassert nilness server ssadump stringintconv 注: go get是先下载后安装packages, 包括依赖的包 godoc使用 在自己的repo下面敲 yingjieb@godev-server /repo/yingjieb/godevsig/compatible $ /repo/yingjieb/go/bin/godoc -http 0.0.0.0:6060 using module mode; GOMOD=/repo/yingjieb/godevsig/compatible/go.mod # 我的repo下面有go.mod, godoc支持gomod yingjieb@godev-server /repo/yingjieb/godevsig/compatible $ ls go.mod LICENSE log msgdriven README.md package通配符和导入路径 package通配符... 比如go get, go install命令最后的packages, 是个import路径. 如果包含特殊的通配格式..., 这个路径就是pattern匹配模式. ...匹配任何字符串, 包括空串. 有两个特例: 结尾的/...匹配任何东西. 比如net/...匹配net net/http ...不匹配vendor. 比如./...不匹配 ./vendor ./mycode/vendor. 想匹配vendor要显式写. 比如./vendor/... 导入路径 支持本地导入路径和远程导入路径 导入路径可以重命名 比如 import \"example.org/pkg/foo\" go get会请求如下的page https://example.org/pkg/foo?go-get=1 (preferred) http://example.org/pkg/foo?go-get=1 (fallback, only with -insecure) 如果取下来的page有如下的元数据 指示example.org实际上是code.org/r/p/exproj, go tool会clone后面这个库, 但路径是example.org the go tool will verify that https://example.org/?go-get=1 contains the same meta tag and then git clone https://code.org/r/p/exproj into GOPATH/src/example.org. go mod模式下, 支持类似的机制: 元数据格式为 obj.function()中obj可以是nil 通常我们会认为如果obj是空指针, 那么这个调用会产生空指针引用, 进而panic. 实际上不是的, obj是nil, 不影响堆function()的调用. 代码如下: type people struct { name string } func (p *people) who() { //注意这一行, 即使p是nil, 这个函数也是会被调用. fmt.Println(\"me\") //访问p.name会panic, 但如果没有下面这一行, 整个程序可以正常执行. //fmt.Println(p.name) } var team map[string]*people = make(map[string]*people) func main() { team[\"wang\"].who() san := people{\"zhang san\"} san.who() } 如何处理? 在C里面, 代码里经常要判断指针是否为空. 那么是不是这里我们也要判断? 答案是不需要. 首先, 如果对象都已经是空了, 说明哪里肯定出了问题. 那不如就让它panic, go会打印调用栈来帮助分析问题. 而为什么在C里面, 我们要判断? 因为C程序只能segmentation fault, 除了coredump没有更多的信息. 而分析coredump成本比较大. 所以C程序员习惯自己来处理空指针错误, 通常也是打印错误. 对于GO程序员, runtime会接管SIGSEGV, 在空指针访问的时候, 自动打印调用栈. 所以, go的理念是: 如果确实有问题, 程序要崩要趁早; 崩在第一现场 切片的reslicing 对于一个切片, 比如ss = [\"stdout\"], 其len为1. 对它进行re slicing的操作ss[1:]是合法的. 即slice[len():len()]是合法的, 比如ss[1:1:1], 本身这样写, 就是矛盾的: 最左边的1要求从1开始, 包括1; 但是中间的1要求到1结束, 不包括1; 最后的1表示只有1个容量. go语法支持这种re slicing, 结果就是len()为0的切片. 再议目录结构 Go 语言项目中的每一个文件目录都代表着一个独立的命名空间，也就是一个单独的包，当我们想要引用其他文件夹的目录时，首先需要使用 import 关键字引入相应的文件目录，再通过 pkg.xxx 的形式引用其他目录定义的结构体、函数或者常量 不要使用src目录 命令行执行程序放在/cmd里: /cmd/server/main.go直接编译出来的文件就是server api定义给外部的接口api └── protobuf-spec └── oceanbookpb ├── oceanbook.pb.go └── oceanbook.proto 参考: 如何写出优雅的 Go 语言代码 具体error判断 下面的代码里, 返回的err不为nil, 但也不好用类型断言来判断err具体类型(可能出错函数直接返回的是errors.New()) 那么还可以通过判断字符串来得到具体的错误, 下面第6行. var err error e.epollFd, err = syscall.EpollCreate(1) switch { case err == nil: break case err.Error() == \"function not implemented\": // Some arch (arm64) do not implement EpollCreate(). if e.epollFd, err = syscall.EpollCreate1(0); err != nil { e.mu.Unlock() return err } default: e.mu.Unlock() return err } 函数赋值给变量 openFileOrig是个函数, 可以直接赋值给变量openFile, 相当于C里面的函数指针. var ( mu sync.Mutex maxSpeed int64 = -1 openFile = openFileOrig ) func openFileOrig(path string, flag int) (io.ReadCloser, error) { f, err := fs.Open(path, flag) if err != nil { return nil, err } return f, nil } gob encode和网络io的结合 goroutine 5 [IO wait]: internal/poll.runtime_pollWait(0x7f1c3dbe3ec8, 0x72, 0xffffffffffffffff) /usr/local/go/src/runtime/netpoll.go:184 +0x55 internal/poll.(*pollDesc).wait(0xc000104218, 0x72, 0x1000, 0x1000, 0xffffffffffffffff) /usr/local/go/src/internal/poll/fd_poll_runtime.go:87 +0x45 internal/poll.(*pollDesc).waitRead(...) /usr/local/go/src/internal/poll/fd_poll_runtime.go:92 internal/poll.(*FD).Read(0xc000104200, 0xc000149000, 0x1000, 0x1000, 0x0, 0x0, 0x0) /usr/local/go/src/internal/poll/fd_unix.go:169 +0x1cf net.(*netFD).Read(0xc000104200, 0xc000149000, 0x1000, 0x1000, 0xc0001c9c50, 0x42e031, 0x5a4478) /usr/local/go/src/net/fd_unix.go:202 +0x4f net.(*conn).Read(0xc000010018, 0xc000149000, 0x1000, 0x1000, 0x0, 0x0, 0x0) /usr/local/go/src/net/net.go:184 +0x68 bufio.(*Reader).Read(0xc0000c86c0, 0xc0000c47d0, 0x1, 0x9, 0x4ad2d8, 0x0, 0x0) /usr/local/go/src/bufio/bufio.go:226 +0x26a io.ReadAtLeast(0x5cb4e0, 0xc0000c86c0, 0xc0000c47d0, 0x1, 0x9, 0x1, 0x6bfea0, 0xc0000fc000, 0x0) /usr/local/go/src/io/io.go:310 +0x87 io.ReadFull(...) /usr/local/go/src/io/io.go:329 encoding/gob.decodeUintReader(0x5cb4e0, 0xc0000c86c0, 0xc0000c47d0, 0x9, 0x9, 0x30, 0x27, 0x0, 0x0) /usr/local/go/src/encoding/gob/decode.go:120 +0x6f encoding/gob.(*Decoder).recvMessage(0xc000104380, 0x0) /usr/local/go/src/encoding/gob/decoder.go:81 +0x57 encoding/gob.(*Decoder).decodeTypeSequence(0xc000104380, 0xc0000c6000, 0x59b95d) /usr/local/go/src/encoding/gob/decoder.go:143 +0x10c encoding/gob.(*Decoder).DecodeValue(0xc000104380, 0x54f1c0, 0xc0000ad900, 0x16, 0x0, 0x0) /usr/local/go/src/encoding/gob/decoder.go:211 +0x10b encoding/gob.(*Decoder).Decode(0xc000104380, 0x54f1c0, 0xc0000ad900, 0x0, 0x0) /usr/local/go/src/encoding/gob/decoder.go:188 +0x16d main.inputDispacher(0x5cdb60, 0xc0000d7840, 0x5cec80, 0xc000010018) /repo/yingjieb/godev/practice/src/tools/topid.go:334 +0x12d created by main.main /repo/yingjieb/godev/practice/src/tools/topid.go:558 +0xe22 单步decoder.Decode(&msg) gob decode使用了反射 gob是二进制编码, 在解码时, 先从io stream读出count, 再根据count读出对应的字节数来解码. var msg messageIn dec.Decode(&msg) // 入参e interface{}被赋值为&msg value := reflect.ValueOf(e) //value.Type().Kind() 必须是reflect.Ptr dec.DecodeValue(value) //入参v reflect.Value被赋值为value dec.mutex.Lock() defer dec.mutex.Unlock() dec.buf.Reset() dec.err = nil id := dec.decodeTypeSequence(false) for dec.err == nil if dec.buf.Len() == 0 if !dec.recvMessage() //先从底层io.Reader读count, 再按照count读具体的字节数 比如典型的:n, err := io.ReadFull(r, buf[0:1]) b := buf[0] // gob编码中, 小于128的用一个字节表示 if b protobuf里面oneof转成go结构体 oneof对应go结构里的interface, 并且自动生成isMessageName_MyField的interface, 和响应格式的签名方法 自动生成GetXxx方法 生成的结构体里面还有些隐藏的字段:XXX_开头的, 可能是protobuf自己用的. 参考: https://developers.google.com/protocol-buffers/docs/reference/go-generated proto定义 // Messages specifically used to retrieve and configure BIP PM counters for GPON message GPONBIPWrapper { string onu_name = 1; // vOnuMgmt -> vProxy uint32 chnl_term_id = 2; // vProxy -> Device (to be changed later to chnl_term_name) uint32 onu_id = 3; // vProxy -> Device oneof msg { ConfigureBERInterval config_ber_interval = 4; // vOnuMgmt -> vProxy -> Device ConfigureBERIntervalResponse config_ber_interval_response = 5; // Device -> vProxy -> vOnuMgmt GetBIPCounters get_bip_counters = 6; // vOnuMgmt -> vProxy -> Device GetBIPCountersResponse get_bip_counters_response = 7; // Device -> vProxy -> vOnuMgmt } } 转成的结构体 // Messages specifically used to retrieve and configure BIP PM counters for GPON type GPONBIPWrapper struct { OnuName string `protobuf:\"bytes,1,opt,name=onu_name,json=onuName,proto3\" json:\"onu_name,omitempty\"` ChnlTermId uint32 `protobuf:\"varint,2,opt,name=chnl_term_id,json=chnlTermId,proto3\" json:\"chnl_term_id,omitempty\"` OnuId uint32 `protobuf:\"varint,3,opt,name=onu_id,json=onuId,proto3\" json:\"onu_id,omitempty\"` // Types that are valid to be assigned to Msg: // *GPONBIPWrapper_ConfigBerInterval // *GPONBIPWrapper_ConfigBerIntervalResponse // *GPONBIPWrapper_GetBipCounters // *GPONBIPWrapper_GetBipCountersResponse Msg isGPONBIPWrapper_Msg `protobuf_oneof:\"msg\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } type isGPONBIPWrapper_Msg interface { isGPONBIPWrapper_Msg() } func (*GPONBIPWrapper_ConfigBerInterval) isGPONBIPWrapper_Msg() {} func (*GPONBIPWrapper_ConfigBerIntervalResponse) isGPONBIPWrapper_Msg() {} func (*GPONBIPWrapper_GetBipCounters) isGPONBIPWrapper_Msg() {} func (*GPONBIPWrapper_GetBipCountersResponse) isGPONBIPWrapper_Msg() {} func (m *GPONBIPWrapper) GetMsg() isGPONBIPWrapper_Msg { if m != nil { return m.Msg } return nil } "},"notes/golang_杂记3.html":{"url":"notes/golang_杂记3.html","title":"Golang 杂记3","keywords":"","body":" go按位取反(bitwise not) go的相等性(==) 普通类型的比较 指针的相等性 channel的相等性 interface的相等性 结构体的相等性 Array的相等性 string []byte用bytes.Equal比较 reflect.DeepEqual万能比较 cmp包 通过unix socket发送fd 发送 接收 发送2 接收2 创建临时文件并mmap成结构体 memfd_create()系统调用 gvisor中的使用场景 用正则表达式 遍历/proc/self/maps 递归缩进打印error 读go micro cmd cmd.APP() 已经注册的cmd cli相关的cmd cli子命令 client接口 pattern match 读fs_linux.go 善用字符串库函数--strings.Join 切片的插入 匿名函数执行 go按位取反(bitwise not) go没有专用的取反操作符, 但用异或可以取反: func main() { var bitwisenot byte = 0x0F // printing the number in 8-Bit fmt.Printf(\"%08b\\n\", bitwisenot) // 00001111 fmt.Printf(\"%08b\\n\", ^bitwisenot) // 11110000 fmt.Printf(\"%08b\\n\", 1^bitwisenot) // 00001110 和上面结果不一样 fmt.Printf(\"%08b\\n\", ^0x0F) // -0010000 默认数字都是int fmt.Printf(\"%08b\\n\", ^(int)(0x0F)) // -0010000 fmt.Printf(\"%08b\\n\", ^(uint)(0x0F)) // 1111111111111111111111111111111111111111111111111111111111110000 不带符号位 } 结果: 00001111 11110000 00001110 -0010000 -0010000 1111111111111111111111111111111111111111111111111111111111110000 go的相等性(==) 首先, map和slice不能用==比较 function也不能比较. f := func(int) int { return 1 } g := func(int) int { return 2 } f == g //这样比较会编译错误 但function可以跟nil比较. 普通类型的比较 boo, int, float, complex的比较就是普通比较. 但需要注意的是float的NaN不等于NaN nan := math.NaN() pos_inf := math.Inf(1) neg_inf := math.Inf(-1) fmt.Println(nan == nan) // false fmt.Println(pos_inf == pos_inf) // true fmt.Println(neg_inf == neg_inf) // true fmt.Println(pos_inf == neg_inf) // false 指针的相等性 要么两个指针都是nil, 要么两个指针指向同样的地址: var p1, p2 *string name := \"foo\" fmt.Println(p1 == p2) // true p1 = &name p2 = &name fmt.Println(p1) // 0x40c148 fmt.Println(p2) // 0x40c148 fmt.Println(&p1) // 0x40c138 fmt.Println(&p2) // 0x40c140 fmt.Println(*p1) // foo fmt.Println(*p2) // foo fmt.Println(p1 == p2) // true 需要注意的是, 两个不同的empty struct(即空的struct实例)的地址可能相等 A struct or array type has size zero if it contains no fields (or elements, respectively) that have a size greater than zero. Two distinct zero-size variables may have the same address in memory. type S struct{} func main() { var p1, p2 *S s1 := S{} s2 := S{} p1 = &s1 p2 = &s2 fmt.Printf(\"%p\\n\", p1) // 0x1e52bc fmt.Printf(\"%p\\n\", p2) // 0x1e52bc fmt.Println(p1) // &{} fmt.Println(p2) // &{} fmt.Println(&p1) // 0x40c138 fmt.Println(&p2) // 0x40c140 fmt.Println(*p1) // {} fmt.Println(*p2) // {} fmt.Println(p1 == p2) // true 本来s1和s2不是一个东西, 当都是空, 他们的地址相同, 所以相等. } 如果结构体非空, S struct {f int}, p1和p2就不相等了. channel的相等性 满足下面条件之一 两个chnnel都是nil 两个都是从同一个make函数生成的 func f(ch1 chan int, ch2 *chan int) { fmt.Println(ch1 == *ch2) // true } func main() { var ch1, ch2 chan int fmt.Println(ch1 == ch2) // true ch1 = make(chan int) ch2 = make(chan int) fmt.Println(ch1 == ch2) // false ch2 = ch1 fmt.Printf(\"%p\\n\", &ch1) // 0x40c138 fmt.Printf(\"%p\\n\", &ch2) // 0x40c140 fmt.Println(ch1 == ch2) // true f(ch1, &ch1) } interface的相等性 两个interface都是nil(注意动态类型也要是nil)type I interface{ m() } type T []byte func (t T) m() {} func main() { var t T fmt.Println(t == nil) // true var i I = t fmt.Println(i == nil) // false fmt.Println(reflect.TypeOf(i)) // main.T fmt.Println(reflect.ValueOf(i).IsNil()) // true } 动态类型相同, 并且动态值相等type A int type B = A type C int type I interface{ m() } func (a A) m() {} func (c C) m() {} func main() { var a I = A(1) var b I = B(1) var c I = C(1) fmt.Println(a == b) // true 这里A和B是强别名(=号别名), 类型是一样的. fmt.Println(b == c) // false 类型不同不相等 fmt.Println(a == c) // false 类型不同不相等 } 类型I的interface变量i可以和普通类型X的实例x比较, 只要 类型X实现了接口I 类型X可以比较 所以i和x比较, 如果i的动态类型是X, i的动态值又等于x, 那么i和x相等 type I interface{ m() } type X int func (x X) m() {} type Y int func (y Y) m() {} type Z int func main() { var i I = X(1) fmt.Println(i == X(1)) // true fmt.Println(i == Y(1)) // false // fmt.Println(i == Z(1)) // mismatched types I and C // fmt.Println(i == 1) // mismatched types I and int } 如果动态类型相等, 但这个类型不能比较, 则会产生panic: type A []byte func main() { var i interface{} = A{} var j interface{} = A{} fmt.Println(i == j) } panic: runtime error: comparing uncomparable type main.A 如果动态类型不一样, 那就直接不等: type A []byte type B []byte func main() { // A{} == A{} // slice can only be compared to nil var i interface{} = A{} var j interface{} = B{} fmt.Println(i == j) // false } 结构体的相等性 首先, 结构体可以直接用==操作符比较. 如果里面的非_域都相等, 则两个结构体相等. 注意, 结构体里面的大写, 小写域都要相等. type A struct { _ float64 f1 int F2 string } type B struct { _ float64 f1 int F2 string } func main() { fmt.Println(A{1.1, 2, \"x\"} == A{0.1, 2, \"x\"}) // true // fmt.Println(A{} == B{}) // mismatched types A and B } 当判断x==y时, 只有x可以赋值给y或者y可以赋值给x才能用==操做符. 所以下面的判断是不行的, 编译时就会报错. A{} == B{} Array的相等性 注意这里说的是Array, 不是slice. Array里面的每个元素都相等的话, 两个array相等. type T struct { name string age int _ float64 } func main() { x := [...]float64{1.1, 2, 3.14} fmt.Println(x == [...]float64{1.1, 2, 3.14}) // true y := [1]T{{\"foo\", 1, 0}} fmt.Println(y == [1]T{{\"foo\", 1, 1}}) // true } string string的比较按照[]byte按字节比较. fmt.Println(strings.ToUpper(\"ł\") == \"Ł\") // true fmt.Println(\"foo\" == \"foo\") // true fmt.Println(\"foo\" == \"FOO\") // false fmt.Println(\"Michał\" == \"Michal\") // false fmt.Println(\"żondło\" == \"żondło\") // true fmt.Println(\"żondło\" != \"żondło\") // false fmt.Println(strings.EqualFold(\"ąĆź\", \"ĄćŹ\")) // true []byte用bytes.Equal比较 切片不能直接比较. 但bytes.Equal可以比较两个[]byte s1 := []byte{'f', 'o', 'o'} s2 := []byte{'f', 'o', 'o'} fmt.Println(bytes.Equal(s1, s2)) // true s2 = []byte{'b', 'a', 'r'} fmt.Println(bytes.Equal(s1, s2)) // false s2 = []byte{'f', 'O', 'O'} fmt.Println(bytes.EqualFold(s1, s2)) // true s1 = []byte(\"źdźbło\") s2 = []byte(\"źdŹbŁO\") fmt.Println(bytes.EqualFold(s1, s2)) // true s1 = []byte{} s2 = nil fmt.Println(bytes.Equal(s1, s2)) // true reflect.DeepEqual万能比较 func DeepEqual(x, y interface{}) bool可以比较任意两个值. 比如map m1 := map[string]int{\"foo\": 1, \"bar\": 2} m2 := map[string]int{\"foo\": 1, \"bar\": 2} // fmt.Println(m1 == m2) // map can only be compared to nil fmt.Println(reflect.DeepEqual(m1, m2)) // true m2 = map[string]int{\"foo\": 1, \"bar\": 3} fmt.Println(reflect.DeepEqual(m1, m2)) // false m3 := map[string]interface{}{\"foo\": [2]int{1,2}} m4 := map[string]interface{}{\"foo\": [2]int{1,2}} fmt.Println(reflect.DeepEqual(m3, m4)) // true var m5 map[float64]string fmt.Println(reflect.DeepEqual(m5, nil)) // false fmt.Println(m5 == nil) // true 比如slice s := []string{\"foo\"} fmt.Println(reflect.DeepEqual(s, []string{\"foo\"})) // true fmt.Println(reflect.DeepEqual(s, []string{\"bar\"})) // false s = nil fmt.Println(reflect.DeepEqual(s, []string{})) // false s = []string{} fmt.Println(reflect.DeepEqual(s, []string{})) // true 比如结构体 type T struct { name string Age int } func main() { t := T{\"foo\", 10} fmt.Println(reflect.DeepEqual(t, T{\"bar\", 20})) // false fmt.Println(reflect.DeepEqual(t, T{\"bar\", 10})) // false fmt.Println(reflect.DeepEqual(t, T{\"foo\", 10})) // true } cmp包 google提供了cmp包, 可以打印两个值的差异 import ( \"fmt\" \"github.com/google/go-cmp/cmp\" ) type T struct { Name string Age int City string } func main() { x := T{\"Michał\", 99, \"London\"} y := T{\"Adam\", 88, \"London\"} if diff := cmp.Diff(x, y); diff != \"\" { fmt.Println(diff) } } 输出 main.T{ - Name: \"Michał\", + Name: \"Adam\", - Age: 99, + Age: 88, City: \"London\", } 通过unix socket发送fd gvisor的pkg/unet/unet.go里面提供了listen, accept等方法 unet是给server端用的. 比如read和write方法, 先尝试用阻塞式的unix.RawSyscall(unix.SYS_RECVMSG, ...), 不行再用对fd的pollunix.Syscall6(unix.SYS_PPOLL, ...) 还提供了通过unix socket发送/接收fd的方法: // PackFDs packs the given list of FDs in the control message. // // This must be used prior to WriteVec. func (c *ControlMessage) PackFDs(fds ...int) { *c = ControlMessage(unix.UnixRights(fds...)) } // ExtractFDs returns the list of FDs in the control message. // // Either this or CloseFDs should be used after EnableFDs. func (c *ControlMessage) ExtractFDs() ([]int, error) { msgs, err := unix.ParseSocketControlMessage(*c) if err != nil { return nil, err } var fds []int for _, msg := range msgs { thisFds, err := unix.ParseUnixRights(&msg) if err != nil { // Different control message. return nil, err } for _, fd := range thisFds { if fd >= 0 { fds = append(fds, fd) } } } return fds, nil } 被extract出来的fd可以用比如下面的函数来生成一个File对象. //比如调用 os.NewFile(uintptr(fd), \"urpc\") // NewFile returns a new File with the given file descriptor and // name. The returned value will be nil if fd is not a valid file // descriptor. On Unix systems, if the file descriptor is in // non-blocking mode, NewFile will attempt to return a pollable File // (one for which the SetDeadline methods work). // // After passing it to NewFile, fd may become invalid under the same // conditions described in the comments of the Fd method, and the same // constraints apply. func NewFile(fd uintptr, name string) *File 发送 //@pkg/lisafs/sock.go // writeTo writes the passed iovec to the UDS and donates any passed FDs. func writeTo(sock *unet.Socket, iovec [][]byte, dataLen int, fds []int) error { w := sock.Writer(true) //这里的fds是可选的, 会做为control msg来发送 if len(fds) > 0 { w.PackFDs(fds...) } for n := 0; n 接收 //@pkg/lisafs/sock.go // readFrom fills the passed buffer with data from the socket. It also returns // any donated FDs. func readFrom(sock *unet.Socket, buf []byte, wantFDs uint8) ([]int, error) { r := sock.Reader(true) r.EnableFDs(int(wantFDs)) n := len(buf) for got := 0; got 发送2 //@pkg/urpc/urpc.go // marshal sends the given FD and json struct. func marshal(s *unet.Socket, v interface{}, fs []*os.File) error { // Marshal to a buffer. data, err := json.Marshal(v) if err != nil { log.Warningf(\"urpc: error marshalling %s: %s\", fmt.Sprintf(\"%v\", v), err.Error()) return err } // Write to the socket. w := s.Writer(true) if fs != nil { var fds []int for _, f := range fs { fds = append(fds, int(f.Fd())) } w.PackFDs(fds...) } ... } 接收2 //@pkg/urpc/urpc.go // unmarhsal receives an FD (optional) and unmarshals the given struct. func unmarshal(s *unet.Socket, v interface{}) ([]*os.File, error) { // Receive a single byte. r := s.Reader(true) r.EnableFDs(maxFiles) firstByte := make([]byte, 1) // Extract any FDs that may be there. if _, err := r.ReadVec([][]byte{firstByte}); err != nil { return nil, err } fds, err := r.ExtractFDs() ... } 创建临时文件并mmap成结构体 memfd_create()系统调用 #include int memfd_create(const char *name, unsigned int flags); 用tmpfs创建一个临时文件, 这个文件和通常文件系统没关系, 但可以支持所有文件操做. 可以用这个文件来共享内存: 进程A调用memfd_create(), 返回fd 进程B去打开/proc//fd/(pid是进程A的pid, fd是A调用memfd_create()返回的fd号.), 打开后可以mmap, 就看到进程A一样的内容了. gvisor中的使用场景 //RTMemoryStats是要被mmap的结构体 // RTMemoryStatsSize is the size of the RTMemoryStats struct. var RTMemoryStatsSize = unsafe.Sizeof(RTMemoryStats{}) // RTMemoryStatsPointer casts addr to a RTMemoryStats pointer. func RTMemoryStatsPointer(addr uintptr) *RTMemoryStats { return (*RTMemoryStats)(unsafe.Pointer(addr)) } const name = \"memory-usage\" fd, err := memutil.CreateMemFD(name, 0) p, err := unix.BytePtrFromString(name) fd, _, e := unix.Syscall(unix.SYS_MEMFD_CREATE, uintptr(unsafe.Pointer(p)), uintptr(flags), 0) file := os.NewFile(uintptr(fd), name) //设置文件大小为结构体RTMemoryStatsSize的大小 file.Truncate(int64(RTMemoryStatsSize)) //mmap这个文件 mmap, err := memutil.MapFile(0, RTMemoryStatsSize, unix.PROT_READ|unix.PROT_WRITE, unix.MAP_SHARED, file.Fd(), 0) //现在全局变量MemoryAccounting.RTMemoryStats就指向了这个文件. //直接用那个结构体不香吗? MemoryAccounting = &MemoryLocked{ File: file, RTMemoryStats: RTMemoryStatsPointer(mmap), } 用正则表达式 遍历/proc/self/maps 比如一个程序想解析当前进程的进程空间: $ cat /proc/self/maps 55cb9cb6b000-55cb9cb73000 r-xp 00000000 fc:02 396 /bin/cat 55cb9cd72000-55cb9cd73000 r--p 00007000 fc:02 396 /bin/cat 55cb9cd73000-55cb9cd74000 rw-p 00008000 fc:02 396 /bin/cat 55cb9e7a2000-55cb9e7c3000 rw-p 00000000 00:00 0 [heap] 7f3367281000-7f336754d000 r--p 00000000 fc:02 7228 /usr/lib/locale/locale-archive 7f336754d000-7f3367734000 r-xp 00000000 fc:02 44721 /lib/x86_64-linux-gnu/libc-2.27.so 7f3367734000-7f3367934000 ---p 001e7000 fc:02 44721 /lib/x86_64-linux-gnu/libc-2.27.so 7f3367934000-7f3367938000 r--p 001e7000 fc:02 44721 /lib/x86_64-linux-gnu/libc-2.27.so 7f3367938000-7f336793a000 rw-p 001eb000 fc:02 44721 /lib/x86_64-linux-gnu/libc-2.27.so 7f336793a000-7f336793e000 rw-p 00000000 00:00 0 7f336793e000-7f3367967000 r-xp 00000000 fc:02 44717 /lib/x86_64-linux-gnu/ld-2.27.so 7f3367b33000-7f3367b57000 rw-p 00000000 00:00 0 7f3367b67000-7f3367b68000 r--p 00029000 fc:02 44717 /lib/x86_64-linux-gnu/ld-2.27.so 7f3367b68000-7f3367b69000 rw-p 0002a000 fc:02 44717 /lib/x86_64-linux-gnu/ld-2.27.so 7f3367b69000-7f3367b6a000 rw-p 00000000 00:00 0 7fff4ad74000-7fff4ad95000 rw-p 00000000 00:00 0 [stack] 7fff4adda000-7fff4addd000 r--p 00000000 00:00 0 [vvar] 7fff4addd000-7fff4addf000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall] 可以用regex // mapsLine matches a single line from /proc/PID/maps. var mapsLine = regexp.MustCompile(\"([0-9a-f]+)-([0-9a-f]+) ([r-][w-][x-][sp]) ([0-9a-f]+) [0-9a-f]{2,3}:[0-9a-f]{2,} [0-9]+\\\\s+(.*)\") func parseMaps() { f, err := os.Open(\"/proc/self/maps\") r := bufio.NewReader(f) for { b, err := r.ReadBytes('\\n') m := mapsLine.FindSubmatch(b) start, err := strconv.ParseUint(string(m[1]), 16, 64) end, err := strconv.ParseUint(string(m[2]), 16, 64) read := m[3][0] == 'r' write := m[3][1] == 'w' execute := m[3][2] == 'x' shared := m[3][3] == 's' offset, err := strconv.ParseUint(string(m[4]), 16, 64) } } 递归缩进打印error type vmError struct { self error children []error } func (vme vmError) Error() string { var b strings.Builder fmt.Fprintf(&b, \"----\\n\") if vme.self != nil { fmt.Fprintf(&b, \"%v\\n\", vme.self) } for _, err := range vme.children { for _, s := range strings.Split(err.Error(), \"\\n\") { if s != \"\\tat -\" { fmt.Fprintf(&b, \"\\t%s\\n\", s) } } } return b.String() } 读go micro cmd micro的入口命令, 只调用了一个函数 这里import中的的v2选择了v2的tag. package main import ( \"github.com/micro/micro/v2/cmd\" ) func main() { cmd.Init() } 这里的Init原型很特别: cmd/cmd.go中, // Init initialised the command line func Init(options ...micro.Option) { Setup(cmd.App(), options...) cmd.Init( cmd.Name(name), cmd.Description(description), cmd.Version(buildVersion()), ) } 这里面的Option是type Option func(*Options), 一个函数签名, 入参是*Options Options是个巨大的结构体, 包含了所有的micro的概念. type Options struct { Auth auth.Auth Broker broker.Broker Cmd cmd.Cmd Config config.Config Client client.Client Server server.Server Store store.Store Registry registry.Registry Runtime runtime.Runtime Transport transport.Transport Profile profile.Profile // Before and After funcs BeforeStart []func() error BeforeStop []func() error AfterStart []func() error AfterStop []func() error // Other options for implementations of the interface // can be stored in a context Context context.Context Signal bool } cmd.APP() func App() *cli.App { //这里的DefaultCmd是个interface, 由newCmd实例化 //即var DefaultCmd = newCmd() return DefaultCmd.App() } type cmd struct { opts Options app *cli.App } //newCmd返回的是Cmd这个interface func newCmd(opts ...Option) Cmd { //实际的cmd是上面的结构体 cmd := new(cmd) cmd.opts = options cmd.app = cli.NewApp() //返回cmd实例, 但从外面看起来是Cmd interface return cmd } //综上, DefaultCmd.App()实际上就是 func (c *cmd) App() *cli.App { return c.app } //那么c.app实际上就是cli.NewApp()的实例化结果. 上面的cli.NewApp()在github.com/micro/cli/v2, 它实际上是urfave/cli的fork. 已经注册的cmd cmd/cmd.go app.Commands = append(app.Commands, new.Commands()...) app.Commands = append(app.Commands, runtime.Commands(options...)...) app.Commands = append(app.Commands, store.Commands(options...)...) app.Commands = append(app.Commands, config.Commands(options...)...) app.Commands = append(app.Commands, api.Commands(options...)...) app.Commands = append(app.Commands, auth.Commands()...) app.Commands = append(app.Commands, bot.Commands()...) app.Commands = append(app.Commands, cli.Commands()...) app.Commands = append(app.Commands, broker.Commands(options...)...) app.Commands = append(app.Commands, health.Commands(options...)...) app.Commands = append(app.Commands, proxy.Commands(options...)...) app.Commands = append(app.Commands, router.Commands(options...)...) app.Commands = append(app.Commands, tunnel.Commands(options...)...) app.Commands = append(app.Commands, network.Commands(options...)...) app.Commands = append(app.Commands, registry.Commands(options...)...) app.Commands = append(app.Commands, debug.Commands(options...)...) app.Commands = append(app.Commands, server.Commands(options...)...) app.Commands = append(app.Commands, service.Commands(options...)...) app.Commands = append(app.Commands, build.Commands()...) app.Commands = append(app.Commands, web.Commands(options...)...) cli相关的cmd cli除了注册了自己的cmd, 还注册了call stream publish等命令. 而且cli.Commands()中, cli命令本身和所有的子命令都是同级的. 只是cli被安排在了第一个 func Commands() []*cli.Command { commands := []*cli.Command{ { Name: \"cli\", Usage: \"Run the interactive CLI\", Action: Run, }, { Name: \"call\", Usage: \"Call a service e.g micro call greeter Say.Hello '{\\\"name\\\": \\\"John\\\"}\", //注意这里的Print是个闭包函数, 返回callService的包装. 类似装饰器 Action: Print(callService), Flags: []cli.Flag{ &cli.StringFlag{ Name: \"address\", Usage: \"Set the address of the service instance to call\", EnvVars: []string{\"MICRO_ADDRESS\"}, }, &cli.StringFlag{ Name: \"output, o\", Usage: \"Set the output format; json (default), raw\", EnvVars: []string{\"MICRO_OUTPUT\"}, }, &cli.StringSliceFlag{ Name: \"metadata\", Usage: \"A list of key-value pairs to be forwarded as metadata\", EnvVars: []string{\"MICRO_METADATA\"}, }, }, }, ... } } cli命令对应的Action是Run. 这个Run是个典型的Read-Eval-Print-Loop, 基本上是 for { //先readline() args, err := r.Readline() //准备参数 args = strings.TrimSpace(args) parts := strings.Split(args, \" \") //找到cmd cmd, ok := commands[name] //执行cmd rsp, err := cmd.exec(c, parts[1:]) println(string(rsp)) } micro的交互式cli很简单, 就是在顶层有个循环来readline, 执行cmd. 没有其他功能. cli子命令 这些子命令, 比如call, publish, 实际调用的是internal/command/cli/command.go中的 func CallService(c *cli.Context, args []string) ([]byte, error) { 参数和返回值都通过json格式编码 d := json.NewDecoder(strings.NewReader(req)) ctx := callContext(c) creq := (*cmd.DefaultOptions().Client).NewRequest(service, endpoint, request, client.WithContentType(\"application/json\")) //实际调用的是client的call接口, 这是个同步调用, 发req, 等rsp err = (*cmd.DefaultOptions().Client).Call(ctx, creq, &rsp, opts...) //等待rsp并打印结果. } client接口 client是micro的核心抽象, 接口定义如下: // Client is the interface used to make requests to services. // It supports Request/Response via Transport and Publishing via the Broker. // It also supports bidirectional streaming of requests. type Client interface { Init(...Option) error Options() Options NewMessage(topic string, msg interface{}, opts ...MessageOption) Message NewRequest(service, endpoint string, req interface{}, reqOpts ...RequestOption) Request Call(ctx context.Context, req Request, rsp interface{}, opts ...CallOption) error Stream(ctx context.Context, req Request, opts ...CallOption) (Stream, error) Publish(ctx context.Context, msg Message, opts ...PublishOption) error String() string } client支持Req/Rsp(通过Transport抽象), 支持Pub/Sub(通过Broker)抽象, 还支持stream模式. pattern match 简单的*通配符匹配 func patternMatchs(str, pattern string) bool { if len(pattern) == 0 { return false } strs := strings.Split(pattern, \"*\") //fmt.Println(strs, len(strs)) var pos, index int if index = strings.Index(str, strs[0]); index != 0 { return false } end := strs[len(strs)-1] if index = strings.LastIndex(str, end); index+len(end) != len(str) { return false } for i, substr := range strs { if i == 0 || i == len(strs)-1 || len(substr) == 0 { continue } index = strings.Index(str[pos:], substr) if index == -1 { return false } pos += index + len(substr) } return true } 读fs_linux.go periph.io/x/periph@v3.6.2+incompatible/host/sysfs/fs_linux.go 善用字符串库函数--strings.Join 比如解析或|关系, 普通的思路是自己组类似flag1|flag2|flag3的字符串. 实际上, 下面的代码片段先把flag1 flag2 flag3分别append到[]string切片, 最后用strings.Join(out, \"|\")加或|操作符. func (e epollEvent) String() string { var out []string for _, b := range bitmaskString { if e&b.e != 0 { out = append(out, b.s) e &^= b.e } } if e != 0 { out = append(out, \"0x\"+strconv.FormatUint(uint64(e), 16)) } if len(out) == 0 { out = []string{\"0\"} } return strings.Join(out, \"|\") } 切片的插入 这个切片是个二叉树的顺序表的表达, 和sort包的思路一致. 先找到r.Name的位置i, 然后用append一个nil的方式把切片扩一个位置出来, 然后用内置的copy函数把i后面的元素往后都挪一个位置; 最后把位置i的元素填上. 为了把位置i后面的元素都挪一个位置, 代价是把这后面的所有元素都copy一遍 这里的copy实际上是overlap的, copy的时候, 源为l[i:], 目的是l[i+1:], 目的比源向后移动一个位置, 而且目的的元素个数也少了一个. 注意, 如果按照普通的for循环式的copy思路, src和dst重叠时不能正常工作的. 有人讨论说golang的copy语义类似memmove memcpy: 当src和dst重叠时, 结果不确定 memmove: src和dst可以重叠, 结果是正确的拷贝; 可以理解成有个临时缓冲做中转. 实际上并不需要中间buffer, 只需要在开始的时候判断是从前往后拷贝还是从后往前拷贝就行了. 结论: golang的copy支持overlap, 可以正确的拷贝 func insertRef(l []*Ref, r *Ref) []*Ref { n := r.Name i := search(len(l), func(i int) bool { return l[i].Name > n }) l = append(l, nil) copy(l[i+1:], l[i:]) l[i] = r return l } 下面是对应的快排 注意在search函数里, 并不需要传入要查找的切片; search调用f的时候, f会比较切片元素, f会访问切片. 在本例中, f能访问insertRef函数的l []*Ref变量, 这也是一种闭包形式? // search implements the same algorithm as sort.Search(). // // It was extracted to to not depend on sort, which depends on reflect. func search(n int, f func(int) bool) int { lo := 0 for hi := n; lo > 1); !f(i) { lo = i + 1 } else { hi = i } } return lo } 匿名函数执行 下面的例子中, 匿名行数在定义后就执行, 就像普通的代码块执行一样: 变量r和err都能直接被匿名函数引用到. 那为什么要用func(){}()这种形式呢? 首先, 匿名函数也是要函数栈的, 多了匿名函数性能上不会更好; 但是, 因为遍历全局的map byName byNumber 有锁操作, 那么使用匿名函数配合defer关键词来加锁和解锁, 代码更优雅. 如果不要这个匿名的壳子, 要么不用defer, 自己找地方加unlock; 还想用defer的话, defer会在r.Open()后执行. 估计这里的逻辑是: 一定要在r.Open()前unlock func Open(name string) (i2c.BusCloser, error) { var r *Ref var err error func() { mu.Lock() defer mu.Unlock() if len(byName) == 0 { err = errors.New(\"i2creg: no bus found; did you forget to call Init()?\") return } if len(name) == 0 { r = getDefault() return } // Try by name, by alias, by number. if r = byName[name]; r == nil { if r = byAlias[name]; r == nil { if i, err2 := strconv.Atoi(name); err2 == nil { r = byNumber[i] } } } }() if err != nil { return nil, err } if r == nil { return nil, errors.New(\"i2creg: can't open unknown bus: \" + strconv.Quote(name)) } return r.Open() } "},"notes/golang_lib选型.html":{"url":"notes/golang_lib选型.html","title":"Golang lib选型","keywords":"","body":" 框架 goframe gf hello Hello World API Service Demo 编解码 性能汇总 排名 bson msgpack go解释器 python装饰器 protobuf cli选型 需求 新增备选 备选 Survey grumble liner readline 我的代码: ishell go-prompt cobra 教程 urfave/cli 简单例子 子命令 其他 go micro的cli 方案1 -- go-prompt 方案2 -- promptui + cobra 方案3 -- readline + cobra 方案4 -- 自己写REPL循环 + cobra 框架 goframe gf 作者是国人 郭强, 应该是某互联网大厂的架构师 https://goframe.org/display/gf 库链接: https://github.com/gogf/gf/issues gf是个go的web应用开发框架, 提供了大一统的企业应用开发的基础库: 封装了常见的基础库: hello 创建一个hello工程 gf init hello 默认就开始编译运行了, 起来之后默认就提供swagger UI的和open api等url. Hello World 视频地址：https://www.bilibili.com/video/BV15R4y1G7hq/ 包含以下内容： 安装GoFrame CLI 使用CLI创建一个Go项目 工程目录介绍 API Service Demo 视频地址：https://www.bilibili.com/video/BV1b44y1M7oL/ 代码地址：https://github.com/gogf/gf-demo-user 我们以一个简单的API Service为例来介绍如何使用GoFrame框架以及相应的CLI工具来开发一个接口项目。 包含以下内容： 包名设计 接口设计 接口文档 配置管理 控制器实现 业务逻辑封装 路由注册 中间件使用 Context及上下文变量 编解码 性能汇总 https://golangrepo.com/repo/alecthomas-go_serialization_benchmarks-go-benchmarks 排名 https://kokizzu.blogspot.com/2020/12/golang-serialization-benchmark-2020.html Format type ns/op bytes/op allocs/op ns/alloc Mum ser 97 48 0 0.00 GencodeUnsafe ser 98 46 48 2.05 Gotiny ser 130 48 0 0.00 GotinyNoTime ser 136 48 0 0.00 Gogoprotobuf ser 147 53 64 2.30 Msgp ser 174 97 128 1.36 Gencode ser 186 53 80 2.33 FlatBuffers ser 298 95 0 0.00 Goprotobuf ser 317 53 64 4.95 Protobuf ser 801 52 152 5.27 ShamatonMapMsgpack ser 819 92 208 3.94 Gob ser 834 63 48 17.38 Format type ns/op bytes/op allocs/op ns/alloc Gencode des 222 53 112 1.98 Gogoprotobuf des 230 53 96 2.40 GotinyNoTime des 241 48 96 2.51 FlatBuffers des 265 95 112 2.37 Gotiny des 267 48 112 2.38 Msgp des 314 97 112 2.80 CapNProto des 443 96 200 2.21 Goprotobuf des 481 53 168 2.86 Protobuf des 790 52 192 4.11 Ikea des 871 55 160 5.44 Gob des 900 63 112 8.04 GoAvro2Binary des 1092 47 560 1.95 Hprose des 1195 85 319 3.75 UgorjiCodecMsgpack des 1398 91 496 2.82 Binary des 1511 61 320 4.72 UgorjiCodecBinc des 1587 95 656 2.42 Bson des 1694 110 232 7.30 Format ns/op bytes Bebop 228 110 GencodeUnsafe 259 92 XDR2 290 120 Mum 313 96 Colfer 321 101 GotinyNoTime 377 96 Gogoprotobuf 377 106 Gotiny 397 96 Gencode 408 106 Msgp 488 194 FlatBuffers 563 190 Goprotobuf 798 106 CapNProto 829 192 Hprose2 1,213 170 ShamatonArrayMsgpack 1,241 100 CapNProto2 1,364 192 Ikea 1,541 110 ShamatonMapMsgpack 1,557 184 Protobuf 1,591 104 Gob 1,734 126 GoAvro2Binary 2,042 94 Hprose 2,110 170 UgorjiCodecMsgpack 2,820 182 VmihailencoMsgpack 2,838 200 Bson 2,865 220 Binary 2,875 122 UgorjiCodecBinc 3,055 190 EasyJson 3,513 299 XDR 4,091 182 JsonIter 4,266 278 GoAvro2Text 5,801 268 Sereal 6,313 264 Json 6,786 299 GoAvro 9,790 94 SSZNoTimeNoStringNoFloatA 12,616 110 bson https://github.com/mongodb/mongo-go-driver/tree/master/bson 对应的文档: https://pkg.go.dev/go.mongodb.org/mongo-driver/bson 还有一个实现: https://pkg.go.dev/labix.org/v2/mgo/bson 实现比较简单, 2014年更新的. msgpack github.com/vmihailenco/msgpack/v4 go解释器 库地址: https://github.com/traefik/yaegi 背景: https://traefik.io/blog/announcing-yaegi-263a1e2d070a/ 看起来很好 Complete support of Go specification Written in pure Go, using only the standard library Simple interpreter API: New(), Eval(), Use() Works everywhere Go works All Go & runtime resources accessible from script (with control) Security: unsafe and syscall packages neither used nor exported by default Support Go 1.13 and Go 1.14 (the latest 2 major releases) python装饰器 本质上是函数闭包: 把返回的函数赋值给原始函数名 @a_new_decorator def a_function_requiring_decoration(): \"\"\"Hey you! Decorate me!\"\"\" print(\"I am the function which needs some decoration to \" \"remove my foul smell\") a_function_requiring_decoration() #outputs: I am doing some boring work before executing a_func() # I am the function which needs some decoration to remove my foul smell # I am doing some boring work after executing a_func() #the @a_new_decorator is just a short way of saying: a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration) 参考这里 其中第一篇笔记很赞 protobuf protobuf项目的go版本现在转到: https://github.com/protocolbuffers/protobuf-go 之前是golang team维护的 https://github.com/golang/protobuf cli选型 需求 自动完成 命令层级 交互式(REPL): The name REPL stands for Read-Eval-Print-Loop - an interactive, typically console-based, programming environment. 命令式 新增备选 https://pkg.go.dev/golang.org/x/term 看起来有点官方 备选 从readline grumble liner里面选一个. 对abs来说, 不需要grumble这样的\"集成\"cli. readline/liner应该够了. 但grumble的思路可以借鉴. 最后选择readline Survey https://github.com/AlecAivazis/survey 感觉不是普通的shell思路, 应该说加了比如check box等功能 grumble https://github.com/desertbit/grumble 本身代码量很少, 但集成了readline等库; 但使用了较新维护的版本https://github.com/desertbit/readline 思路是cobra式的子命令, 但默认加了交互式命令行 更新不多, 但比较稳定? 作者自称受ishell启发 There are a handful of powerful go CLI libraries available (spf13/cobra, urfave/cli). However sometimes an integrated shell interface is a great and useful extension for the actual application. This library offers a simple API to create powerful CLI applications and automatically starts an integrated interactive shell, if the application is started without any command arguments. liner https://github.com/peterh/liner 看起来不错. 类VT100 支持go.mod 支持常规快捷键 支持历史命令 readline https://github.com/chzyer/readline 和c版本的readline套路一样. 有个库包装了readline, 成为一个简单的迭代器, 看起来不错 https://github.com/knieriem/readlineutil func (t *Term) Scan() bool { line, err := t.inst.Readline() if err != nil { t.err = err return false } t.line = line if t.prevPrompt != \"\" && line != \"\" { t.inst.SaveHistory(line) } return true } 我的代码: func repl() { completer := readline.NewPrefixCompleter() liner, err := readline.NewEx(&readline.Config{ Prompt: \"> \", AutoComplete: completer, EOFPrompt: \"exit\", }) if err != nil { panic(err) } defer liner.Close() for { l, err := liner.Readline() if errors.Is(err, io.EOF) { break } if errors.Is(err, readline.ErrInterrupt) { continue } if len(l) != 0 { fmt.Println(l) } } } ishell https://github.com/abiosoft/ishell 看着有点简陋... Sample Interactive Shell >>> help Commands: clear clear the screen greet greet user exit exit the program help display help >>> greet Someone Somewhere Hello Someone Somewhere >>> exit $ go-prompt https://github.com/c-bata/go-prompt 还在更新, 主打交互式, 命令联想, 快捷键, 历史记录上下翻. 好像目标是提供kubectl兼容的kube-prompt go.libhunt上对比 termui 主打文本的图形显示 gocui 主打文本布局, 有点像调试器界面 tview 也是主打界面的 tcell 文本控制台库, 似乎很有用. cobra https://github.com/spf13/cobra 使用广泛 Cobra is used in many Go projects such as Kubernetes, Hugo, and Github CLI to name a few. This list contains a more extensive list of projects using Cobra. cobra是个框架, 典型布局 ▾ appName/ ▾ cmd/ add.go your.go commands.go here.go main.go main.go package main import ( \"{pathToYourApp}/cmd\" ) func main() { cmd.Execute() } 教程 这个教程写的不错 用Cobra的几个好处: Easy to create subcommand-based CLIs and use nested subcommands. Automatic help generation for commands and flags. Increased productivity because of commands such as cobra init appname & cobra add cmdname. Helpful, intelligent suggestions (app srver… did you mean app server?). 这篇文章写的更好 Cobra比标准库flag的功能更丰富 支持-abc -e --example等传统的参数输入 支持子命令. 这个是主要原因. golang标准库里本来有, 但在internal下面, 外部用不了 子命令举例: package main import ( \"github.com/spf13/cobra\" ) func main() { cmd := newCommand() cmd.AddCommand(newNestedCommand()) rootCmd := &cobra.Command{} rootCmd.AddCommand(cmd) if err := rootCmd.Execute(); err != nil { println(err.Error()) } } func newCommand() *cobra.Command { cmd := &cobra.Command{ Run: func (cmd *cobra.Command, args []string) { println(`Foo`) }, Use: `foo`, Short: \"Command foo\", Long: \"This is a command\", } return cmd } func newNestedCommand() *cobra.Command { cmd := &cobra.Command{ Run: func (cmd *cobra.Command, args []string) { println(`Bar`) }, Use: `bar`, Short: \"Command bar\", Long: \"This is a nested command\", } return cmd } urfave/cli https://github.com/urfave/cli 主打轻量化.在积极维护 简单例子 package main import ( \"fmt\" \"log\" \"os\" \"github.com/urfave/cli/v2\" ) func main() { app := &cli.App{ Name: \"greet\", Usage: \"fight the loneliness!\", Action: func(c *cli.Context) error { fmt.Println(\"Hello friend!\") return nil }, } err := app.Run(os.Args) if err != nil { log.Fatal(err) } } 子命令 package main import ( \"fmt\" \"log\" \"os\" \"github.com/urfave/cli/v2\" ) func main() { app := cli.NewApp() app.EnableBashCompletion = true app.Commands = []*cli.Command{ { Name: \"add\", Aliases: []string{\"a\"}, Usage: \"add a task to the list\", Action: func(c *cli.Context) error { fmt.Println(\"added task: \", c.Args().First()) return nil }, }, { Name: \"complete\", Aliases: []string{\"c\"}, Usage: \"complete a task on the list\", Action: func(c *cli.Context) error { fmt.Println(\"completed task: \", c.Args().First()) return nil }, }, { Name: \"template\", Aliases: []string{\"t\"}, Usage: \"options for task templates\", Subcommands: []*cli.Command{ { Name: \"add\", Usage: \"add a new template\", Action: func(c *cli.Context) error { fmt.Println(\"new task template: \", c.Args().First()) return nil }, }, { Name: \"remove\", Usage: \"remove an existing template\", Action: func(c *cli.Context) error { fmt.Println(\"removed task template: \", c.Args().First()) return nil }, }, }, }, } err := app.Run(os.Args) if err != nil { log.Fatal(err) } } 其他 kingpin 似乎不怎么维护了 go micro的cli micro命令似乎进入了一个命令行界面, 有提示符. 但里面的命令和在shell中敲的一样. cli命令对应的Action是Run. 这个Run是个典型的Read-Eval-Print-Loop, 基本上是 for { //先readline() args, err := r.Readline() //准备参数 args = strings.TrimSpace(args) parts := strings.Split(args, \" \") //找到cmd cmd, ok := commands[name] //执行cmd rsp, err := cmd.exec(c, parts[1:]) println(string(rsp)) } micro的交互式cli很简单, 就是在顶层有个循环来readline, 执行cmd. 没有其他功能. 方案1 -- go-prompt 方案2 -- promptui + cobra 讨论在此 看下来感觉不好... 方案3 -- readline + cobra 方案4 -- 自己写REPL循环 + cobra 比如这篇文章里, 就只用了reader.ReadString('\\n') func main() { reader := bufio.NewReader(os.Stdin) for { fmt.Print(\"$ \") cmdString, err := reader.ReadString('\\n') if err != nil { fmt.Fprintln(os.Stderr, err) } err = runCommand(cmdString) if err != nil { fmt.Fprintln(os.Stderr, err) } } } func runCommand(commandStr string) error { commandStr = strings.TrimSuffix(commandStr, \"\\n\") arrCommandStr := strings.Fields(commandStr) switch arrCommandStr[0] { case \"exit\": os.Exit(0) // add another case here for custom commands. } cmd := exec.Command(arrCommandStr[0], arrCommandStr[1:]...) cmd.Stderr = os.Stderr cmd.Stdout = os.Stdout return cmd.Run() } "},"notes/golang_mod_proxy.html":{"url":"notes/golang_mod_proxy.html","title":"go mod和go proxy","keywords":"","body":" go get 和 replace gocenter已经停止维护了 go center使用心得 gitlab ci clone私有库 go mode 使用私有repo 使用ssh方式clone GOPROXY问题和解决 go clean go mod 使用replace指定私有repo artifactory和goproxy的问题汇总 godevsig-gocenter-remote获取不到新版本 gitlab默认不支持goproxy artifactory的repository概念 一个repository只对应一个类型 generic类型 local repository remote repository Virtual Repositories GOPROXY和artifactory 私有repo 私有GOPROXY setup私有GOPROXY go list package模式 module模式 国内可用的proxy go mod模式和普通模式的go get区别 普通模式下 go mod模式下 go mode模式下的help go mod使用记录 go.mod 本地import和外部repo import sum DB审查 go mod机制 定义go module 创建一个go module go.mod文件维护 版本格式 兼容性公约 Using v2 releases Using v1 releases GOPROXY=direct 有时候github的库更新了, 但go get或者go mod等命令不能及时的更新版本号, 比如: go get github.com/godevsig/adaptiveservice@master 总是取到master的次新版本, 即使等了一段时间也不行. 估计是proxy cache还没有更新, 用下面的命令可以及时更新: GOPROXY=direct go get github.com/godevsig/adaptiveservice@master go get 和 replace 有replace的时候, 比如: module github.com/godevsig/gshellos go 1.13 require ( github.com/d5/tengo/v2 v2.7.0 github.com/peterh/liner v1.2.1 ) replace github.com/d5/tengo/v2 => github.com/godevsig/tengo/v2 v2.7.1-0.20210415163628-cf3fef922971 直接go get github.com/godevsig/tengo/v2会报错: go get: github.com/godevsig/tengo/v2@v2.7.0: parsing go.mod: module declares its path as: github.com/d5/tengo/v2 but was required as: github.com/godevsig/tengo/v2 那么怎么更新replace的版本呢? 要先改go.mod, 最后一行指定branch是dev replace github.com/d5/tengo/v2 => github.com/godevsig/tengo/v2 dev 然后执行go get $ go get go: finding github.com/godevsig/tengo/v2 dev go: downloading github.com/godevsig/tengo/v2 v2.7.1-0.20210415163628-cf3fef922971 go: extracting github.com/godevsig/tengo/v2 v2.7.1-0.20210415163628-cf3fef922971 会自动更新go.mod文件, 最后一行变为: replace github.com/d5/tengo/v2 => github.com/godevsig/tengo/v2 v2.7.1-0.20210415163628-cf3fef922971 gocenter已经停止维护了 访问https://search.gocenter.io/, 显示gocenter服务已经终止了. 最后一句 You’ve arrived at this page because the era of Bintray, JCenter, GoCenter, and ChartCenter has ended – as newer and better options have emerged. We’re proud of the benefits they provided, as they helped spur software innovation and bolstered the work of brilliant developers. But you know what they say. Don’t overstay your welcome. Know when it’s time to bow out. Make a graceful exit. The Go team has built a module repository for Go developers called Pkg.go.dev. 所以, 之前的 GOPROXY=\"https://artifactory.com/artifactory/api/go/godevsig-go-virtual,direct\" 要改成默认的: GOPROXY=\"https://proxy.golang.org,direct\" 如果没有你的package, 要自己添加以下: https://go.dev/about#adding-a-package Data for the site is downloaded from proxy.golang.org. We monitor the Go Module Index regularly for new packages to add to pkg.go.dev. If you don’t see a package on pkg.go.dev, you can add it by doing one of the following: Visiting that page on pkg.go.dev, and clicking the “Request” button. For example: https://pkg.go.dev/example.com/my/module Making a request to proxy.golang.org for the module version, to any endpoint specified by the Module proxy protocol. For example: https://proxy.golang.org/example.com/my/module/@v/v1.0.0.info Downloading the package via the go command. For example: GOPROXY=https://proxy.golang.org GO111MODULE=on go get example.com/my/module@v1.0.0 go center使用心得 使用export GOPROXY=https://gocenter.io时, 默认是不包括你的github项目的. 需要在主页https://search.gocenter.io/ 点添加module按钮来手动添加 似乎gocenter不会主动持续的对你的库更新版本, 比如通过gocenter来go get时, latest版本不会更新 直接go get github.com/godevsig/gshellos不会拿到最新版本 对于Pseudo-Versions来说, 需要用户指定版本号, 比如 go get github.com/godevsig/gshellos@6d66a9c 这样才会触发gocenter进行一次缓存 tag规范:https://jfrog.com/blog/go-big-with-pseudo-versions-and-gocenter/ 如果不是proxy管理的, 是可以直接更新的: $ go get gitlab.com/godevsig/system go: finding gitlab.com/godevsig/system latest go: downloading gitlab.com/godevsig/system v0.0.0-20210115023458-e89ec0eae327 go: extracting gitlab.com/godevsig/system v0.0.0-20210115023458-e89ec0eae327 gitlab ci clone私有库 参考网上的经验, 在.gitlab-ci.yml中加入before_script, 这样在每个script执行之前, 都会跑这个git配置, 意思是用git的url替换功能, \"加工\"url地址. default: image: name: godevsig-docker-local.artifactory.com/godevsig/devtool:godevsig-ee82473 entrypoint: [\"\"] before_script: - git config --global url.\"https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.com\".insteadOf \"https://gitlab.com\" 因为go默认使用https方式get代码, 这里被替换的url字符串为https://gitlab.com, 将其前面加上https://gitlab-ci-token:${CI_JOB_TOKEN} 使用gitlab内置的CI_JOB_TOKEN变量, 可以绕过私有库不能clone的限制, 把私有库clone下来. go mode 使用私有repo 比如https://gitlab.com/godev/system.git不是public, 不能直接clone go build等命令没法获取到这个库. 我用replace关键词暂时解决了本地编译的问题 replace ( github.com/d5/tengo/v2 => /repo/yingjieb/godevsig/tengo gitlab.com/godevsig/system => /repo/yingjieb/godevsig/system ) require ( github.com/d5/tengo/v2 v2.6.2 github.com/peterh/liner v1.2.1 gitlab.com/godevsig/system v0.0.0-00010101000000-000000000000 ) 但这里的system版本号似乎很怪异:v0.0.0-00010101000000-00000000000 这个大概是个特殊的版本号, 因为go 命令其实没有办法获取到 因为go命令默认使用https去clone. 对于私有仓库来说, 对外不可见, 当然不能用https去get. 使用ssh方式clone 用git config命令, 把https方式clone改成git方式, 这样如果你本地有ssh权限clone这个私有库, go命令也能成功, 因为go命令底层也是调用git命令clone的. $ git config --global url.\"git@mygitlab.com:\".insteadOf \"http://mygitlab.com/\" // 其实就是在 .gitconfig 增加了配置 $ cat ~/.gitconfig [url \"git@mygitlab.com:\"] insteadOf = http://mygitlab.com/ //注意： git@mygitlab.com: 后面有个冒号 :, 且 http://mygitlab.com 后面有 / 我的成功例子: git config --global url.\"git@gitlab.com:godevsig\".insteadOf \"https://gitlab.com/godevsig\" 会在~/.gitconfig中增加: [url \"git@gitlab.com:godevsig\"] insteadOf = https://gitlab.com/godevsig GOPROXY问题和解决 设置git的ssh方式替换https方式后, 把go.mod里面的特殊行删除: gitlab.com/godevsig/system v0.0.0-00010101000000-000000000000 然后使用go get命令更新go.mod 发现go命令能正确发现这个私有库的版本号v0.0.0-20201225044511-46ec8cf4b75c, 但下载失败, 看起来和GOPROXY的设置有关 yingjieb@73e57632f306 /repo/yingjieb/godevsig/gshell $ go get go: finding gitlab.com/godevsig/system/pidinfo latest go: finding gitlab.com/godevsig/system latest go: downloading gitlab.com/godevsig/system v0.0.0-20201225044511-46ec8cf4b75c build gitlab.com/godevsig/gshell: cannot load gitlab.com/godevsig/system/pidinfo: gitlab.com/godevsig/system@v0.0.0-20201225044511-46ec8cf4b75c: reading https://artifactory.com/artifactory/api/go/godevsig-go-virtual/gitlab.com/godevsig/system/@v/v0.0.0-20201225044511-46ec8cf4b75c.zip: 500 Internal Server Error 修改私有库不经过GOPROXY是可以的: GONOPROXY=\"*.net.com\" go get go clean 下面的命令会把所有的cache都清除, 导致go get下次会全新下载 go clean -cache -modcache -i -r 不加参数的go clean好像都不删除cache的. go mod 使用replace指定私有repo 在gshell中, 引用了库github.com/abs-lang/abs, 但我想修改这个package, 但依旧保留import \"github.com/abs-lang/abs\" 怎么做? -- go.mod中使用replace关键词 $ cat go.mod module gitlab.com/godev/gshell go 1.13 replace github.com/abs-lang/abs => /repo/yingjieb/godev/abs require ( github.com/abs-lang/abs v0.0.0-20190921150801-05c699c0415e github.com/peterh/liner v1.2.1 } 上面replace的意思是用本地路径替换目标package, 但源代码不用修改. 如果希望用私有repo, replace要求私有repo要有版本号. artifactory和goproxy的问题汇总 godevsig-gocenter-remote获取不到新版本 现象: $ GONOSUMDB=\"gitlab.com/*\" GOPROXY=\"https://artifactory.com/artifactory/godevsig-gocenter-remote,direct\" GO111MODULE=on go get -v golang.org/x/tools/gopls go: golang.org/x/tools/gopls upgrade => v0.5.1 问题: 应该获取到新版本v0.5.3 解决: 使用virtual repo, 而且必须加api/go前缀就能下载最新的tag GONOSUMDB=\"gitlab.com/*\" GOPROXY=\"https://artifactory.com/artifactory/api/go/godevsig-go-virtual,direct\" GO111MODULE=on go get -v golang.org/x/tools/gopls go: golang.org/x/tools/gopls upgrade => v0.5.3 注: 对godevsig-gocenter-remote加api/go是不行的. 原因: 不是virtual的repo, 好像只能当cache用. gitlab默认不支持goproxy 现象: godevsig-go-virtual包括godevsig-gocenter-remote和godevsig-gogitlab-remote后者proxy https://gitlab.com 但get gitlab.com/godevsig/compatible失败 GONOSUMDB=\"gitlab.com/*\" GOPROXY=\"https://artifactory.com/artifactory/api/go/godevsig-go-virtual,direct\" GO111MODULE=on go get -v gitlab.com/godevsig/compatible go get gitlab.com/godevsig/compatible: no matching versions for query \"upgrade\" 而不加proxy能成功. GO111MODULE=on go get -v gitlab.com/godevsig/compatible get \"gitlab.com/godevsig/compatible\": found meta tag get.metaImport{Prefix:\"gitlab.com/godevsig/compatible\", VCS:\"git\", RepoRoot:\"https://gitlab.com/godevsig/compatible.git\"} at //gitlab.com/godevsig/compatible?go-get=1 go: downloading gitlab.com/godevsig/compatible v0.1.0 go: gitlab.com/godevsig/compatible upgrade => v0.1.0 原因: https://docs.gitlab.com/ee/user/packages/go_proxy/ 需要管理员手动打开, 在gitlab rails控制台 可以全部打开 Feature.enable(:go_proxy) 可以按project打开 Feature.enable(:go_proxy, Project.find(1)) Feature.disable(:go_proxy, Project.find(2)) 进展: gitlab 13.3才有这个功能 临时方案: 从virtual中去掉gitlab的汇聚. 待gitlab go proxy使能后再加 临时只virtual gocenter的. GONOSUMDB=\"gitlab.com/*\" GOPROXY=\"https://artifactory.com/artifactory/api/go/godevsig-go-virtual,direct\" GO111MODULE=on go get -v gitlab.com/godevsig/compatible get \"gitlab.com/godevsig/compatible\": found meta tag get.metaImport{Prefix:\"gitlab.com/godevsig/compatible\", VCS:\"git\", RepoRoot:\"https://gitlab.com/godevsig/compatible.git\"} at //gitlab.com/godevsig/compatible?go-get=1 go: gitlab.com/godevsig/compatible upgrade => v0.1.0 添加写申请: https://nsnsi.service-now.com/ess?id=create_ticket&sys_id=cdb84691db3d8f80012570600f96196a&returnPage=request_services&returnPage2=browse_it_services artifactory的repository概念 https://www.jfrog.com/confluence/display/JFROG/Repository+Management repository包括: Local Remote Virtual Distribution Release Bundle Repository Bintray Distribution Repository 一个repository只对应一个类型 特别的, virtual的repository只能汇聚同一类型的实体repository. 虽然没有强制要求不能上传不同类型的artifact, 但不推荐这样. generic类型 generic类型的repository能放任何东西, 没有特殊的包管理. local repository 访问路径: http://:/artifactory// remote repository 实际上是个proxy和cache 访问格式 http://:/artifactory// 如果确定artifact已经被cache了, 可以直接访问: http://:/artifactory/-cache/ Virtual Repositories 虚拟的repository是汇聚同类repository用的. 搜索的顺序是先local的, 再remote的. 同类的需要看list顺序, 可以改的. GOPROXY和artifactory JFrog Artifactory推出了https://gocenter.io 使用只需要 export GOPROXY=https://gocenter.io 这个代理完全能取代go1.13开始的默认GOPROXY设置. 注意: GOPROXY只在go mod模式下才有用. 使能go mod模式, 要么目录下有go.mod文件, 要么需要强制指定GO111MODULE=on 使用GoCenter比直接从github上下载更快, GoCenter网页也能显示更详细的信息. 私有repo 使用公共repo时, GOSUMDB要去默认的公共校验中心sum.golang.org校验文件安全性. 而私有库显然没有入这个\"sum DB\", 拉取私有库会报错. 通常使用GOPRIVATE环境变量传入需要bypass sum db的私有库 GoCenter提供了同时使用公共gocenter.io和私有库的方法: export GOPROXY=https://gocenter.io,direct export GOPRIVATE=*.internal.mycompany.com 这样能解决问题, 但没法防止私有库的owner去改tag, 改了tag, 就不能重复构建同样的版本了. 私有GOPROXY 私有GOPROXY能解决上面的问题. 私有GOPROXY能同时cache公共GOPROXY和内部私有repo In Artifactory, a combination of a remote repository for GoCenter, a remote Go module repository that points to private GitHub repos (for private modules) and a local Go module repository can be combined into a single virtual repository, to access as a single unit. 使用私有GOPROXY, GONOSUMDB来完成任务: $ export GOPROXY=\"https://:@my.artifactory.server/artifactory/api/go/go\" $ export GONOSUMDB=\"github.com/mycompany/*,github.com/mypersonal/*\" setup私有GOPROXY 官方参考 这里看起来我需要: go remote repo: proxy gocenter go remote repo: proxy gitlab go vitual repo general local repo: 参考: artifactory的go registry说明 经验文档1 经验文档2 go list package模式 go list 用于显示当前目录下的package的import path yingjieb@f8530ea27843 /repo/yingjieb/3rdparty/delve/pkg/proc $ go list github.com/go-delve/delve/pkg/proc -f选项其实是个模板, 可以显示go tool内部的管理数据, 比如查看package的import了谁, 以及依赖谁(即所有递归的import) % go list -f '{{ .Imports }}' github.com/davecheney/profile [io/ioutil log os os/signal path/filepath runtime runtime/pprof] % go list -f '{{ .Deps }}' github.com/davecheney/profile [bufio bytes errors fmt io io/ioutil log math os os/signal path/filepath reflect runtime runtime/pprof sort strconv strings sync sync/atomic syscall text/tabwriter time unicode unicode/utf8 unsafe] -deps 选项可以递归的list依赖. 也就是说默认不打印依赖. 列出当前目录下的package, 其中...是go tool通用的通配符, 匹配所有. 注意, 不要用go list ..., 意思是列出所有目录下的package yingjieb@f8530ea27843 /repo/yingjieb/3rdparty/delve/pkg/proc $ go list ./... github.com/go-delve/delve/pkg/proc github.com/go-delve/delve/pkg/proc/core github.com/go-delve/delve/pkg/proc/core/minidump github.com/go-delve/delve/pkg/proc/fbsdutil github.com/go-delve/delve/pkg/proc/gdbserial github.com/go-delve/delve/pkg/proc/linutil github.com/go-delve/delve/pkg/proc/native github.com/go-delve/delve/pkg/proc/test github.com/go-delve/delve/pkg/proc/winutil module模式 module对应的是package的自然集合, 通常是一个repo. go list -m all 用于列出所有的import的包(main包和main的依赖包) The arguments to list -m are interpreted as a list of modules, not packages. The main module is the module containing the current directory. The active modules are the main module and its dependencies. With no arguments, list -m shows the main module. With arguments, list -m shows the modules specified by the arguments. Any of the active modules can be specified by its module path. The special pattern \"all\" specifies all the active modules, first the main module and then dependencies sorted by module path. A pattern containing \"...\" specifies the active modules whose module paths match the pattern. 国内可用的proxy https://goproxy.cn/ $ export GO111MODULE=on $ export GOPROXY=https://goproxy.cn go mod模式和普通模式的go get区别 普通模式下 GOPROXY不起作用, go get直接从目标地址下载 go get -v github.com/naoina/go-stringutil # 下载后的代码以src形式保存在GOPATH下的 ./src/github.com/naoina/go-stringutil go mod模式下 # 使用godevsig的proxy GOPROXY=\"https://artifactory.com:443/artifactory/godevsig-go-virtual\" GO111MODULE=on go get -v github.com/naoina/go-stringutil # 下载后的代码以pkg cache的形式保存在GOPATH下的 ./pkg/mod/cache/download/github.com/naoina/go-stringutil 注: GOPROXY生效后, 在 https://artifactory.com/artifactory/webapp/#/artifacts/browse/tree/General/godevsig-gocenter-remote-cache 下, 能找到已经cache的package go mode模式下的help 连help输出也不一样 GO111MODULE=on go help get 第一步解析依赖, 对每个package pattern, 依次做版本检查: 先检查最新的tag, 比如v1.2.3 没有release的tag, 就用pre tag, 比如v0.0.1-pre1 没有tag, 就用最新的commit 可以用@version下载指定版本, @v1会下载v1的最新版本; @commitid也行 对于间接依赖, go get会follow go.mod的指示 go get后面是空的情况下, get当前目录下的依赖 第二步时下载, build, install; 在package下面没有东西可以build的时候, build和install有可能被忽略 可以用...的pattern go mod使用记录 比如库gitlab.com/godevsig/compatible go.mod 使用go mod init gitlab.com/godevsig/compatible 得到如下go.mod: 声明了本库的module名. 官方说法是, 这个名字必须和库地址一致.见cmd/go: Why not separate the module name and the download URL? module gitlab.com/godevsig/compatible go 1.13 本地import和外部repo import 这里的本地引用是说通一个repo下, 不同package之间的引用. 本地引用和外部引用没有任何区别: 在log同一级的目录msg下: main.go package main import \"gitlab.com/godevsig/compatible/log\" func main() { lg := log.DefaultStream.NewLogger(\"msgdriven\") lg.Infolnn(\"hello msg\") } 但本地引用情况下, 本地修改能够立即生效. 比如我在log包里加了一个API, 本地修改不入库, lg.Infolnn(), 本地其他包能够立刻引用新的API. 外部引用的情况下, 只认已经入库的内容. 因为lg.Infolnn()的修改还没有入库, 外部的repo是不知道的. 错误如下: $ go build # main ./hello.go:42:4: lg.Infolnn undefined (type *log.Logger has no field or method Infolnn) sum DB审查 默认配置下, go get/build发现go.mod文件后, 开启go mod模式, 自动拉取依赖的库. 但出现410 Gone错误 $ go build go: finding gitlab.com/godevsig/compatible latest go: finding gitlab.com/godevsig/compatible/log latest go: downloading gitlab.com/godevsig/compatible v0.0.0-20200811070332-66acc8ba0617 verifying gitlab.com/godevsig/compatible@v0.0.0-20200811070332-66acc8ba0617: gitlab.com/godevsig/compatible@v0.0.0-20200811070332-66acc8ba0617: reading htt ps://sum.golang.org/lookup/gitlab.com/godevsig/compatible@v0.0.0-20200811070332-66acc8ba0617: 410 Gone 看过程是它能取到版本, 但verifying checksum出错了. 因为go mod要去sum.golang.org获取校验, 但我们引用的module是私有库, 肯定在golang.org里面没有. 解决: 配置环境变量 GOSUMDB=off 参考: Golang-执行go get私有库提示”410 Gone“ 解决办法 或者使用 GONOSUMDB=\"*.net.com/*\" go mod机制 go help modules go help go.mod go help mod go modules是go的官方包管理机制, 用于替代老的GOPATH环境变量来指定版本依赖的方式. go tools1.13支持go modules. 环境变量GO111MODULE=auto的默认方式下, 如果目录下有go.mod文件, 则使能go modules模式; 否则还是用老的GOPATH模式 GOPATH在go modules模式下, 只用于存放下载的源码: GOPATH/pkg/mod, 和按照后的二进制: GOPATH/bin GO111MODULE=on 强制使用go modules模式 定义go module 一个go module是一个目录, 该目录下有go.mod文件, 该目录也称为module root. 比如下面的go.mod声明了一个module, 它的import path是example.com/m; 它还依赖指定版本的golang.org/x/text和gopkg.in/yaml.v2 module example.com/m require ( golang.org/x/text v0.3.0 gopkg.in/yaml.v2 v2.1.0 ) 创建一个go module 在工程目录下, 执行go mod init example.com/m会创建go.mod go.mod文件创建后, go命令比如go build, go get会自动更新go.mod. go命令会在当前目录找go.mod, 没有再到父目录及其再往上的父目录寻找go.mod 在哪里执行go命令, 那个目录就是main module. 只有main module的go.mod文件的有replace和exclude关键词才有效; 不是main module, 这些关键词被忽略. build list是构建main module的依赖列表 A Go module is a collection of related Go packages that are versioned together as a single unit. go list命令用于查看main module的build list go list -m # print path of main module go list -m -f={{.Dir}} # print root directory of main module go list -m all # print build list go.mod文件维护 go.mod被设计为人和go命令都可读可修改. go命令比如go build, 如果发现源码里面有新增的import example/m关键词, 就会自动添加example/m的最新version到go.mod go mod tidy命令可以整理go.mod文件, 删除不再需要的module. // indirect指示间接依赖 go get命令会更新go.mod的版本. 比如升级一个module, 那其他依赖这个module的modules也会被自动更新到相应版本. 版本格式 版本号的核心思想是版本号是可以比较新旧的. 对没有版本管理的repo, 假的版本号格式是:v0.0.0-yyyymmddhhmmss-abcdefabcdef 其中时间是commit时间, 后面是commit hash. go的命令支持module版本指定: v1.2.3指定了一个具体的版本 v1会被扩展成最新的v1版本 和>=v1.5.6 latest被扩展成最新的tagged版本, 或者, 对于没有版本管理的库, 使用最新的commit upgrade: 和latest差不多 patch: 和latest差不多 其他: commit hash, tag名, 分支名, 会选择源码的指定版本.go get github.com/gorilla/mux@latest # same (@latest is default for 'go get') go get github.com/gorilla/mux@v1.6.2 # records v1.6.2 go get github.com/gorilla/mux@e3702bed2 # records v1.6.2 go get github.com/gorilla/mux@c856192 # records v0.0.0-20180517173623-c85619274f5d go get github.com/gorilla/mux@master # records current meaning of master 兼容性公约 如果一个package的新老版本的import路径一致, 那么新版本必须兼容老版本. 对不兼容的版本, 解决方案是import路径加v2, 比如: go.mod里显式写明: module example.com/m/v2 在引用时也写明引用v2里面的一个package import example.com/m/v2/sub/pkg 比如urfave/cli的v1和v2版本: Using v2 releases $ GO111MODULE=on go get github.com/urfave/cli/v2 ... import ( \"github.com/urfave/cli/v2\" // imports as package \"cli\" ) ... Using v1 releases $ GO111MODULE=on go get github.com/urfave/cli ... import ( \"github.com/urfave/cli\" ) ... "},"notes/golang_汇编_arm64.html":{"url":"notes/golang_汇编_arm64.html","title":"Golang 汇编语法和arm64小知识","keywords":"","body":"golang汇编语法参考: https://go.dev/doc/asm pseudo寄存器 函数 arm64汇编 Register mapping rules ARM64异常处理过程 ARM64上下文切换 什么是VHE pseudo寄存器 SB: Static base pointer 全局基地址. 比如foo(SB)就是foo这个symbol的地址 FP: 帧指针. 用来传参的, 比如 first_arg+0(FP): 第一个参数 second_arg+8(FP): 第二个参数(64bit CPU) SP: 栈指针. 指向栈顶. 用于局部变量. CPU都有物理SP, 语法上看前缀来区分: x-8(SP), y-4(SP): 使用pseudo SP -8(SP)使用物理SP PC: 程序指针 函数 格式: TEXT symbol(SB), [flags,] $framesize[-argsize] symbol: 函数名 SB: SB伪寄存器 flags: 可以是 NOSPLIT: 不让编译器插入栈分裂的代码 WRAPPER: 不增加函数帧计数 NEEDCTXT: 需要上下文参数, 一般用于闭包 framesize: 局部变量大小, 包含要传给子函数的参数部分 argsize: 参数+返回值的大小, 可以省略由编译器自己推导 比如 //go:nosplit func swap(a, b int) (int, int) 可以写为: TEXT ·swap(SB), NOSPLIT, $0-32 或者 TEXT ·swap(SB), NOSPLIT, $0 这里-32是4个8字节的int, 即入参a, b和两个出参.注意go并不区分入参和出参 func swap(a, b int) (int, int) 或 func swap(a, b, c, d int) 或 func swap() (a, b, c, d int) 或 func swap() (a, []int, d int) 汇编都一样 arm64汇编 https://pkg.go.dev/cmd/internal/obj/arm64#pkg-overview Register mapping rules All basic register names are written as Rn. Go uses ZR as the zero register and RSP as the stack pointer. Bn, Hn, Dn, Sn and Qn instructions are written as Fn in floating-point instructions and as Vn in SIMD instructions. ARM64异常处理过程 When an event which causes an exception occurs, the processor hardware automatically performs certain actions. The SPSR_ELn is updated, (where n is the Exception level where the exception is taken), to store the PSTATE information required to correctly return at the end of the exception. PSTATE is updated to reflect the new processor status (and this may mean that the Exception level is raised, or it may stay the same). The return address to be used at the end of the exception is stored in ELR_ELn. 异常发生的时候, CPU会自动的实施如下动作: 将PSTATE保存到SPSR_ELn比如异常发生在EL0, 一般会在EL1处理. 那PSTATE会保存在SPSR_EL1 更新PSTATE以反映新的CPU状态, 比如已经进入EL1 硬件会将返回地址保存在ELR_Eln.还是比如异常发生在EL0, 但在EL1处理, 那返回地址保存在ELR_EL1 The processor has to be told when to return from an exception by software. This is done by executing the ERET instruction. This restores the pre-exception PSTATE from SPSR_ELn and returns program execution back to the original location by restoring the PC from ELR_ELn. eret指令用来从异常处理返回: 从SPSR_ELn恢复异常前的PSTATE 从ELR_ELn恢复PC 异常返回, 从恢复的PC和PSTATE继续执行 ELR_ELn contains the return address which is preferred for the specific exception type. For some exceptions, this is the address of the next instruction after the one which generated the exception. For example, when an SVC (system call) instruction is executed, we simply wish to return to the following instruction in the application. In other cases, we may wish to re-execute the instruction that generated the exception. 在发生异常时, 硬件会自动更新ELR, 根据情况, 返回地址有几种可能: 比如SVC指令触发的同步异常, ELR里保存的是其下一条指令 比如异步异常(即外部中断), ELR里保存的是下一个没被执行(或完全执行)的指令 ELR可以在异常处理程序里面被更改. In addition to the SPSR and ELR registers, each Exception level has its own dedicated Stack Pointer register. These are named SP_EL0, SP_EL1, SP_EL2 and SP_EL3. These registers are used to point to a dedicated stack that can, for example, be used to store registers which are corrupted by the exception handler, so that they can be restored to their original value before returning to the original code. Handler code may switch from using SP_ELn to SP_EL0. For example, it may be that SP_EL1 points to a piece of memory which holds a small stack that the kernel can guarantee to always be valid. SP_EL0 might point to a kernel task stack which is larger, but not guaranteed to be safe from overflow. This switching is controlled by writing to the [SPSel] bit, as shown in the following code: MSR SPSel, #0 // switch to SP_EL0 MSR SPSel, #1 // switch to SP_ELn 每个EL都有独立的SP, 并且异常处理程序可以切换使用SP_EL0和SP_ELn. ARM64上下文切换 Processors that implement the ARMv8-A Architecture are typically used in systems running a complex operating system with many applications or tasks that run concurrently. Each process has its own unique translation tables residing in physical memory. When an application starts, the operating system allocates it a set of translation table entries that map both the code and data used by the application to physical memory. These tables can subsequently be modified by the kernel, for example, to map in extra space, and are removed when the application is no longer running. There might therefore be multiple tasks present in the memory system. The kernel scheduler periodically transfers execution from one task to another. This is called a context switch and requires the kernel to save all execution state associated with the process and to restore the state of the process to be run next. The kernel also switches translation table entries to those of the next process to be run. The memory of the tasks that are not currently running is completely protected from the task that is running. 每个进程都有自己的translation table, 这个table是kernel分配的, 把其物理地址配置到ttbr0寄存器. 上下文切换的时候, kernel会保存/恢复如下上下文: general-purpose registers X0-X30. Advanced SIMD and Floating-point registers V0 - V31. Some status registers. TTBR0_EL1 and TTBR0. Thread Process ID (TPIDxxx) Registers. Address Space ID (ASID). For EL0 and EL1, there are two translation tables. TTBR0_EL1 provides translations for the bottom of Virtual Address space, which is typically application space and TTBR1_EL1 covers the top of Virtual Address space, typically kernel space. This split means that the OS mappings do not have to be replicated in the translation tables of each task. EL0和EL1有两个translation table, TTBR0_EL1负责bottom空间(用户空间), TTBR1_EL1负责top空间(kernel空间). 大家都用TTBR1_EL1做kernel空间, 所以进程切换的时候, TTBR1_EL1不用变, 所以kernel的映射不用变. Translation table entries contain a non-global (nG) bit. If the nG bit is set for a particular page, it is associated with a specific task or application. If the bit is marked as 0, then the entry is global and applies to all tasks. 页表entry里有个nG位, 用来表示non-global, 为0的时候, 这个页表entry就是全局的, 对所有task都有效. For non-global entries, when the TLB is updated and the entry is marked as non-global, a value is stored in the TLB entry in addition to the normal translation information. This value is called the Address Space ID (ASID), which is a number assigned by the OS to each individual task. Subsequent TLB look-ups only match on that entry if the current ASID matches with the ASID that is stored in the entry. This permits multiple valid TLB entries to be present for a particular page marked as non-global, but with different ASID values. In other words, we do not necessarily need to flush the TLBs when we context switch. ASID(Address Space ID)寄存器用来标记页表entry所属的task, 由kernel分配. 当TLB更新的时候, TLB entry除了保存地址翻译信息, 还会包括这个ASID. TLB查询的时候, 只有当前的ASID和TLB entry保存的ASID匹配的时候, 才算TLB命中. 所以上下文切换的时候不需要flush TLB. In AArch64, this ASID value can be specified as either an 8-bit or 16-bit value, controlled by the TCR_EL1.AS bit. The current ASID value is specified in either TTBR0_EL1 or TTBR1_EL1. TCR_EL1 controls which TTBR holds the ASID, but it is normally TTBR0_EL1, as this corresponds to application space. ASID可以8位或16位. 一般配置在TTBR0_EL1中. Having the current value of the ASID stored in the translation table register means that you can atomically modify both the translation tables as well as the ASID in a single instruction. This simplifies the process of changing the table and ASID when compared with the ARMv7-A Architecture. 把ASID值放在TTBR0_EL1里的好处是, 一个指令就可以原子的更改ASID和页表. Additionally, the ARMv8-A Architecture provides Thread ID registers for use by operating system software. These have no hardware significance and are typically used by threading libraries as a base pointer to per-thread data. This is often referred to as Thread Local Storage (TLS). For example, the pthreads library uses this feature and includes the following registers: User Read and Write Thread ID Register (TPIDR_EL0). User Read-Only Thread ID Register (TPIDRRO_EL0). Thread ID Register, privileged accesses only (TPIDR_EL1). TPIDR(Thread ID registers)是给系统软件保存Thread Local Storage (TLS)用的. EL0可以用TPIDR_EL0 EL1还有TPIDR_EL1 什么是VHE Note: The DynamIQ processors (Cortex-A55, Cortex-A75 and Cortex-A76) support Virtualization Host Extensions (VHEs). 通常kernel运行在EL1, 一个同样的kernel, 如果运行在VHE使能了, 硬件会重定向寄存器访问: We saw in the section on Virtualizing generic timers that enabling VHE changes the layout of the EL2 virtual address space. However, we still have a problem with the configuration of the MMU. This is because our kernel will try to access _EL1 registers, such as TTBR0_EL1, rather than _EL2 registers such as TTBR0_EL2. To run the same binary at EL2, we need to redirect the accesses from the EL1 registers to the EL2 equivalents. Setting E2H will do this, so that accesses to _EL1 system registers are redirected to their EL2 equivalents. This redirection illustrated in the following diagram: "},"notes/golang_cgo_swig.html":{"url":"notes/golang_cgo_swig.html","title":"go调用c可以用swig","keywords":"","body":"首先, swig是 - Simplified Wrapper and Interface Generator The swig command is used to create wrapper code to connect C and C++ code to scripting languages like Perl, Python, Tcl etc swig for go中说到, cgo生成的wrapper能调用c, 但不能直接调用c++. 而且, gcgo和gccgo在调用c的接口时, 是完全不一样的. swig同时支持这两种toolchain, 并且保证类型安全. 从go1.1开始就支持swig了, 并且集成在go build命令中. 使用go build -x -work能看到详细使用swig的过程. 2010年的问答: Can I use shared objects with Go? According to the Go FAQ, you can call into C libraries using a \"foreign function interface\": Do Go programs link with C/C++ programs? There are two Go compiler implementations, 6g and friends, generically called gc, and gccgo. Gc uses a different calling convention and linker and can therefore only be linked with C programs using the same convention. There is such a C compiler but no C++ compiler. Gccgo is a GCC front-end that can, with care, be linked with GCC-compiled C or C++ programs. However, because Go is garbage-collected it will be unwise to do so, at least naively. There is a “foreign function interface” to allow safe calling of C-written libraries from Go code. We expect to use SWIG to extend this capability to C++ libraries. There is no safe way to call Go code from C or C++ yet. 根据说法, gccgo是可以不用cgo的接口, 直接调用c的. 但必须要十分清楚调用convention, 栈大小的限制等细节: You can also use cgo and SWIG with Gccgo and gollvm. Since they use a traditional API, it's also possible, with great care, to link code from these compilers directly with GCC/LLVM-compiled C or C++ programs. However, doing so safely requires an understanding of the calling conventions for all languages concerned, as well as concern for stack limits when calling C or C++ from Go. 见gccgo安装使用文档 "},"notes/as_title_golang3.html":{"url":"notes/as_title_golang3.html","title":"Golang 环境和工具链生成","keywords":"","body":"如题 "},"notes/golang_toolchain_ppc.html":{"url":"notes/golang_toolchain_ppc.html","title":"go tools增加ppc32支持.md","keywords":"","body":" gccgo工具链不编译c文件 过程 gcc的gccgo官方文档 编译gc go并上传下载 gccgo toolchain准备, 操作命令版本 build toolchain upload the toolchain to artifactory 命令cache go源码编译 原版go源码编译go tools 修改版源码编译 不改变go tools的尝试 使用gccgo 传入GOARCH=ppc64 传入GOARCH=amd64 总结: 尝试成功, 使用ppc32 gccgo的方法 修改go源码, 支持GOARCH=ppc32 编译gccgo crosstool-ng编译gccgo 使用crosstool-ng的gccgo 修改go源码支持ppc32 power ISA背景知识 Power ISA v.2.06 Power ISA v.2.07 Power ISA v.3.0 default ARCH选项 默认编译器参数 要修改的文件 ppc64无法在e6500上运行 这些缺失的符号应该在哪里? 为什么generic-morestack.c没有编译到? crosstool-ng的bug 关于split stack 上面提到的gold linker是什么意思? 相关链接 上传gccgo工具链到artifactory gccgo的hello size过大问题 相关命令 减小libgo.a的体积 使能lto 改动提交到crosstool-ng go_export小节 static和static-libgo 让gcgo支持ppc32 下一步 补充: fant-f运行topid 补充: ppc64 补充: 为什么gc go工具链的支持ppc64, 但不能在e6500上运行? 补充: elf格式 gccgo工具链不编译c文件 参考: https://github.com/golang/go/issues/41758结论: 没有打开CGO_ENABLED=1的情况下, 不会自动编译c文件. CGO_ENABLED=1就好了. 过程 使用gccgo编译MF14Temp时，由于import了fsnotify这个包，而这个包会使用golang.org/x/sys/这个包 这个包里unix/gccgo.go这个go文件里需要调用gccgoRealSyscall和gccgoRealSyscallNoError这两个函数，而这两个函数的实现在unix/gccgo_c.c这个c文件中。 产生下述链接错误的原因是gccgo_c.c这个c文件没有被gccgo编译，因此需要在编译MF14Temp时，加上“CGO_ENABLED=1”这个选项，使能CGO，让GO能够调用C GOARCH=ppc64 go build -o MF14Temp_ppc64 # gitlabe1.ext.net.nokia.com/godevsig/MF14Temp /opt/crosstool/powerpc64-e6500-linux-gnu/bin/../lib/gcc/powerpc64-e6500-linux-gnu/10.2.0/../../../../powerpc64-e6500-linux-gnu/bin/ld: /home/xming/.cache/go-build/16/16ae8d3fbaffd001bb92ecdbfd968c9b2f078bb1bfd38396335cd0a145ce241c-d(_go_.o): in function `golang.x2eorg..z2fx..z2fsys..z2funix.SyscallNoError': /repo/xming/go/pkg/mod/golang.org/x/sys@v0.0.0-20191005200804-aed5e4c7ecf9/unix/gccgo.go:23: undefined reference to `gccgoRealSyscallNoError' /opt/crosstool/powerpc64-e6500-linux-gnu/bin/../lib/gcc/powerpc64-e6500-linux-gnu/10.2.0/../../../../powerpc64-e6500-linux-gnu/bin/ld: /home/xming/.cache/go-build/16/16ae8d3fbaffd001bb92ecdbfd968c9b2f078bb1bfd38396335cd0a145ce241c-d(_go_.o): in function `golang.x2eorg..z2fx..z2fsys..z2funix.Syscall': /repo/xming/go/pkg/mod/golang.org/x/sys@v0.0.0-20191005200804-aed5e4c7ecf9/unix/gccgo.go:30: undefined reference to `gccgoRealSyscall' gcc的gccgo官方文档 https://gcc.gnu.org/onlinedocs/gcc-10.2.0/gccgo/ 编译gc go并上传下载 到https://artifactory-blr1.int.net.nokia.com/artifactory/godevsig-generic-local/toolchain/gccgo/ 查最新的文件夹 找到commit号, 比如 059da3dd98ab7be871a0556dd53fa4057f7dcf09 下载: #编译 cd golang-go git checkout release-branch.go1.16 cd src GOOS=linux GOARCH=amd64 ./bootstrap.bash #上传 curl -H \"X-JFrog-Art-Api:AKCp8hyinctVijrdqGaFc1YAT7e7KDHWJEaackjuv6oCheipkYU9jU5okRj8rnFkVvcZWnTVc\" -X PUT \"https://artifactory-blr1.int.net.nokia.com:443/artifactory/godevsig-generic-local/toolchain/gcgo/`git rev-parse HEAD`/\" -T ../../go-linux-amd64-bootstrap.tbz #下载 curl https://artifactory-blr1.int.net.nokia.com/artifactory/godevsig-generic-local/toolchain/gcgo/1cabb66796f4529afc615dd466b5667fe1509f6c/go-linux-amd64-bootstrap.tbz -O gccgo toolchain准备, 操作命令版本 Steps to prepare gccgo toolchain build toolchain checkout godev branch cd crosstool-ng git checkout godev make crosstool-ng ```sh install dependencies apt install flex help2man texinfo libtool-bin libncurses-dev gawk bison rsync build ./bootstrap ./configure --enable-local make * build gccgo toolchain e.g. to build ppc64 gccgo ```sh ./ct-ng defconfig DEFCONFIG=samples/Nokia/isam-reborn_godev-ppc64-e6500 ./ct-ng build find the toolchain in targets after successfully buildcd targets ls # powerpc64-e6500-linux-gnu upload the toolchain to artifactory compress the toolchain foldertar cJf godev-gccgo-ppc64.tar.xz powerpc64-e6500-linux-gnu upload to artifactorycurl -H \"X-JFrog-Art-Api:AKCp8hyinctVijrdqGaFc1YAT7e7KDHWJEaackjuv6oCheipkYU9jU5okRj8rnFkVvcZWnTVc\" -X PUT \"https://artifactory-blr1.int.net.nokia.com:443/artifactory/godevsig-generic-local/toolchain/gccgo/`git rev-parse HEAD`/\" -T godev-gccgo-ppc64.tar.xz 同时上传ppc和ppc64 proxyoff cd crosstool-ng/targets tar cJf godev-gccgo-ppc.tar.xz powerpc-e500mc-linux-gnu tar cJf godev-gccgo-ppc64.tar.xz powerpc64-e6500-linux-gnu curl -H \"X-JFrog-Art-Api:AKCp8hyinctVijrdqGaFc1YAT7e7KDHWJEaackjuv6oCheipkYU9jU5okRj8rnFkVvcZWnTVc\" -X PUT \"https://artifactory-blr1.int.net.nokia.com:443/artifactory/godevsig-generic-local/toolchain/gccgo/`git rev-parse HEAD`/\" -T \"{godev-gccgo-ppc.tar.xz,godev-gccgo-ppc64.tar.xz}\" 命令cache # hello yingjieb@9102a93a554e /repo/yingjieb/godev/practice/src/examples $ rm -f hello; rm -rf /home/yingjieb/.cache/go-build/; rm -f gccgo.log; PATH=$PATH:/repo/yingjieb/crosstoolng/github/crosstool-ng/targets/powerpc-e500mc-linux-gnu/bin GOARCH=ppc64 /repo/yingjieb/godev/golang-go/bin/go build -compiler gccgo -gccgoflags '-static -Os' hello.go /repo/yingjieb/crosstoolng/github/crosstool-ng/targets/powerpc-e500mc-linux-gnu/bin/powerpc-linux-strip hello scp yingjieb@10.182.105.138:/repo/yingjieb/godev/practice/src/examples/hello . # topid yingjieb@godev-server /repo/yingjieb/godev/practice $ export GOPATH=\"`pwd`:$GOPATH\" yingjieb@godev-server /repo/yingjieb/godev/practice/src/tools $ PATH=$PATH:/repo/yingjieb/crosstoolng/github/crosstool-ng/targets/powerpc-e500mc-linux-gnu/bin GOARCH=ppc64 /repo/yingjieb/godev/golang-go/bin/go build -compiler gccgo -gccgoflags '-static -Os' topid.go /repo/yingjieb/crosstoolng/github/crosstool-ng/targets/powerpc-e500mc-linux-gnu/bin/powerpc-linux-strip topid scp yingjieb@10.182.105.138:/repo/yingjieb/godev/practice/src/tools/topid . ./topid -record -tag fantf -p 1 -child -chartserver 10.182.105.138:9887 -i 3 目标是修改golang go的编译器源码, 使其支持ppc32的gccgo后端 go源码编译 原版go源码编译go tools git clone https://github.com/golang/go.git go build源码在src/cmd/go/internal/work/build.go 第一次编译需要全部编译. cd golang-go/src ./make.bash -a 再编./make.bash --no-clean似乎更快 编译好的go程序在当前工程的bin目录下 cd golang-go/ export GOPATH=\"$GOPATH:`pwd`\" #编译go主程序, 会在当前目录下生成go的可执行程序 cd src/cmd/go go build 注: go tools是指GOTOOLDIR下面的一些可执行文件: $ ls \"/usr/local/go/pkg/tool/linux_amd64\" addr2line asm buildid cgo compile cover dist doc fix link nm objdump pack pprof test2json trace vet 修改版源码编译 我在godevsig分支修改了源码, 支持ppc的gccgo 现在我想基于go1.13 release分支自己编译一套工具链出来 在godev-tool docker里面操作: git checkout release-branch.go1.13 # e164f53422是godevsig分支上我的commit git cherry-pick e164f53422 #在1.15之前, 需要这个commit来支持ppc的cgo git cherry-pick 5d1378143bc #我还想要1.14的-trimpath功能 git cherry-pick eb6ce1cff4 git rm cmd/go/testdata/script/build_trimpath.txt #gccgo不支持go:linkname 指示符, go module要用就出错`//go:linkname is only supported for functions` #需要下面的patch git cherry-pick baf7d95350 git cherry-pick 45873a242d cd golang-go/src GOOS=linux GOARCH=amd64 ./bootstrap.bash 注: bootstrap支持BOOTSTRAP_FORMAT=mintgz意思是生成干净的工具链, 删掉一些不用的文件. 但似乎删的太狠了, 不推荐 不改变go tools的尝试 通常来讲, 用go tools需要传入GOARCH=, 而ppc32是不在列表里面的: $ go tool dist list | grep linux linux/386 linux/amd64 linux/arm linux/arm64 linux/mips linux/mips64 linux/mips64le linux/mipsle linux/ppc64 linux/ppc64le linux/s390x 使用gccgo 一般的, go build的时候, 加-compiler gccgo选项使能gccgo编译器, 需要有gccgo这个可执行文件. 对ppc32来说, 首先要有crosstool-ng编译出来的gccgo, 比如powerpc-e500mc-linux-gnu-gccgo 在/repo/yingjieb/godev/gccgoppc32/powerpc-e500mc-linux-gnu/bin下面 建一个gccgo的可执行脚本, 内容如下: 这会把所有调用到gccgo的参数打印出来 #!/bin/bash ( echo `pwd` echo \"$@\" ) >> gccgo.log exec powerpc-e500mc-linux-gnu-gccgo $@ 注意, go build会使用cache, 要得到完整过程, 需要删掉cache 传入GOARCH=ppc64 yingjieb@9102a93a554e /repo/yingjieb/godev/practice/src/examples $ rm -f hello; rm -rf /home/yingjieb/.cache/go-build/; rm -f gccgo.log; PATH=$PATH:/repo/yingjieb/godev/gccgoppc32/powerpc-e500mc-linux-gnu/bin GOARCH=ppc64 go build -compiler gccgo hello.go yingjieb@9102a93a554e /repo/yingjieb/godev/practice/src/examples $ cat gccgo.log /repo/yingjieb/godev/practice/src/examples -print-search-dirs /repo/yingjieb/godev/practice/src/examples -dumpversion /repo/yingjieb/godev/practice/src/examples -dumpmachine /repo/yingjieb/godev/practice/src/examples -### -x go -c - /repo/yingjieb/godev/practice/src/examples -c -g -fdebug-prefix-map=/tmp/go-build836929485=/tmp/go-build -gno-record-gcc-switches -fgo-relative-import-path=_/repo/yingjieb/godev/practice/src/examples -o /tmp/go-build836929485/b001/_go_.o -I /tmp/go-build836929485/b001/_importcfgroot_ /repo/yingjieb/godev/practice/src/examples/hello.go /repo/yingjieb/godev/practice/src/examples -xassembler-with-cpp -I /tmp/go-build836929485/b001/ -c -o /tmp/go-build836929485/b001/_buildid.o -D GOOS_linux -D GOARCH_ppc64 /tmp/go-build836929485/b001/_buildid.s /repo/yingjieb/godev/practice/src/examples -### -x go -c - /repo/yingjieb/godev/practice/src/examples -o /tmp/go-build836929485/b001/exe/a.out -Wl,-( -Wl,--whole-archive /tmp/go-build836929485/b001/_pkg_.a -Wl,--no-whole-archive -Wl,-) -Wl,--build-id=0x66534c46775162664f44395036684634392d76552f6f79586f4a615778706b5f78586c7852795069492f476e3059334754737349613869696f4b524356432f66534c46775162664f44395036684634392d7655 虽然传入的是GOARCH=ppc64, 但依然成功生成了hello程序: yingjieb@9102a93a554e /repo/yingjieb/godev/practice/src/examples $ file hello hello: ELF 32-bit MSB executable, PowerPC or cisco 4500, version 1 (SYSV), dynamically linked, interpreter /lib/ld.so.1, for GNU/Linux 4.9.156, with debug_info, not stripped 生成的hello和libgo.so.13考到板子上(FANT-F), 可以正常执行: scp yingjieb@10.182.105.138:/repo/yingjieb/godev/practice/src/examples/hello . scp yingjieb@10.182.105.138:/repo/yingjieb/godev/gccgoppc32/powerpc-e500mc-linux-gnu/powerpc-e500mc-linux-gnu/lib/libgo.so.13 . ~ # LD_LIBRARY_PATH=`pwd` ./hello hello world! 4 0x21192008 4 4 传入GOARCH=amd64 yingjieb@9102a93a554e /repo/yingjieb/godev/practice/src/examples $ rm -f hello; rm -rf /home/yingjieb/.cache/go-build/; rm -f gccgo.log; PATH=$PATH:/repo/yingjieb/godev/gccgoppc32/powerpc-e500mc-linux-gnu/bin GOARCH=amd64 go build -compiler gccgo hello.go # command-line-arguments go1: error: '-m64' not supported in this configuration yingjieb@9102a93a554e /repo/yingjieb/godev/practice/src/examples $ cat gccgo.log /repo/yingjieb/godev/practice/src/examples -print-search-dirs /repo/yingjieb/godev/practice/src/examples -dumpversion /repo/yingjieb/godev/practice/src/examples -dumpmachine /repo/yingjieb/godev/practice/src/examples -### -x go -c - /repo/yingjieb/godev/practice/src/examples -c -g -m64 -fdebug-prefix-map=/tmp/go-build246565517=/tmp/go-build -gno-record-gcc-switches -fgo-relative-import-path=_/repo/yingjieb/godev/practice/src/examples -o /tmp/go-build246565517/b001/_go_.o -I /tmp/go-build246565517/b001/_importcfgroot_ /repo/yingjieb/godev/practice/src/examples/hello.go 说明go build确实会默认传入一些编译选项, 比如这里, 因为指定了GOARCH=amd64, 但我们的交叉编译器是powerpc-e500mc-linux-gnu-gccgo, 不认x86的-m64选项, 报错: go1: error: '-m64' not supported in this configuration 总结: 使用gccgo ppc32的交叉toolchain的时候, 后端编译器powerpc-e500mc-linux-gnu-gccgo就默认使用ppc32. 当go build传入GOARCH=ppc64时, 并没有传入\"默认\"的ABI, 所以没有给编译器造成困扰. 但这里有个隐患: -xassembler-with-cpp -I /tmp/go-build836929485/b001/ -c -o /tmp/go-build836929485/b001/_buildid.o -D GOOS_linux -D GOARCH_ppc64 /tmp/go-build836929485/b001/_buildid.s 这里的-D GOARCH_ppc64是不对的. 尝试成功, 使用ppc32 gccgo的方法 其他都一样, 只是要传入GOARCH=ppc64和-compiler gccgo, 并且保证PATH=$PATH:/repo/yingjieb/godev/gccgoppc32/powerpc-e500mc-linux-gnu/bin下面有gccgo链接到同目录下的powerpc-e500mc-linux-gnu-gccgo 修改go源码, 支持GOARCH=ppc32 前面已经证明, 使用GOARCH=ppc64是可以编译成功的, 并且运行也没发现什么问题. 但注意到ppc64会定义-D GOARCH_ppc64, 这个宏是给C和汇编看的, 有什么影响呢? 搜索一下, 下面的文件使用了GOARCH_ppc64 src/internal/bytealg/compare_ppc64x.s src/internal/bytealg/indexbyte_ppc64x.s src/crypto/md5/md5block_ppc64x.s # 这个文件是go运行时的汇编支持, 比如gcWriteBarrier, 和cgo相关的实现 // +build ppc64 ppc64le src/runtime/asm_ppc64x.s # 这个似乎是syscall的汇编实现 // +build linux // +build ppc64 ppc64le src/runtime/sys_linux_ppc64x.s src/runtime/cgo/asm_ppc64x.s 下面我们要修改go源码, 以支持直接传入GOARCH=ppc32 首先需要crosstool-ng编出gccgo. 编译gccgo gcc源码里面包含了go的源码, 在gcc/libgo下面. 不同版本的gcc的go源码版本如下: cat gcc/libgo/VERSION gcc version go version gcc-8.3.0 go1.10.3 gcc-9.3.0 go1.12 gcc-10.1.0 go1.14.2 crosstool-ng编译gccgo 这里我用官方crosstool-ng git clone https://github.com/crosstool-ng/crosstool-ng 需要先安装些依赖: apt install flex help2man texinfo libtool-bin libncurses-dev gawk bison 要生成ct-ng cd crosstool-ng ./bootstrap ./configure --enable-local make 使用cg-ng ./ct-ng help ./ct-ng list-samples ./ct-ng powerpc-e300c3-linux-gnu ./ct-ng defconfig DEFCONFIG=samples/Nokia/isam-reborn-ppc-P40xx ./ct-ng menuconfig ./ct-ng build #保存config ./ct-ng savedefconfig DEFCONFIG=samples/Nokia/isam-reborn-ppc-e6500 完成后, 会在targets下面生成工具链 使用crosstool-ng的gccgo 为了版本一致, 我选用了 gcc-10.2.0, 里面的go版本是go1.14; go源码使用release-branch.go1.14, 编译go tools. 使用新生成的go tools, 指定gccgo编译hello.go PATH=$PATH:/repo/yingjieb/crosstoolng/github/crosstool-ng/targets/powerpc-e500mc-linux-gnu/bin GOARCH=ppc64 /repo/yingjieb/godev/golang-go/bin/go build -compiler gccgo -gccgoflags '-static -Os' hello.go 修改go源码支持ppc32 power ISA背景知识 power ISA Power ISA v.2.06 The specification for Power ISA v.2.06 was released in February 2009, and revised in July 2010. It is based on Power ISA v.2.05 and includes extensions for the POWER7 processor and e500-mc core. One significant new feature is vector-scalar floating-point instructions (VSX \"AltiVec\")). Book III-E also includes significant enhancement for the embedded specification regarding hypervisor and virtualisation on single and multi core implementations. The spec was revised in November 2010 to the Power ISA v.2.06 revision B spec, enhancing virtualization features. Compliant cores All cores that comply with previous versions of the Power ISA POWER7 A2I e500-mc e5500 Power ISA v.2.07 The specification for Power ISA v.2.07 was released in May 2013. It is based on Power ISA v.2.06 and includes major enhancements to logical partition functionality \"Logical partition (virtual computing platform)\"), transactional memory, expanded performance monitoring, new storage control features, additions to the VMX and VSX vector facilities (VSX-2), along with AES> and Galois Counter Mode (GCM), SHA-224, SHA-256, SHA-384 and SHA-512 (SHA-2) cryptographic extensions and cyclic redundancy check (CRC) algorithms. The spec was revised in April 2015 to the Power ISA v.2.07 B spec. Compliant cores All cores that comply with previous versions of the Power ISA POWER8 e6500 core A2O Power ISA v.3.0 The specification for Power ISA v.3.0 was released in November 2015. It is the first to come out after the founding of the OpenPOWER Foundation and includes enhancements for a broad spectrum of workloads and removes the server and embedded categories while retaining backwards compatibility and adds support for VSX-3 instructions. New functions include 128-bit quad-precision floating-point operations, a random number generator, hardware-assisted garbage collection and hardware-enforced trusted computing. The spec was revised in March 2017 to the Power ISA v.3.0 B spec. Compliant cores All cores that comply with previous versions of the Power ISA POWER9 default ARCH选项 每个arch都有个default选项: 这段代码是自动生成的 // Code generated by go tool dist; DO NOT EDIT. package objabi import \"runtime\" const defaultGO386 = `sse2` const defaultGOARM = `5` const defaultGOMIPS = `hardfloat` const defaultGOMIPS64 = `hardfloat` const defaultGOPPC64 = `power8` const defaultGOOS = runtime.GOOS const defaultGOARCH = runtime.GOARCH const defaultGO_EXTLINK_ENABLED = `` const defaultGO_LDSO = `` const version = `go1.14.9` const stackGuardMultiplierDefault = 1 const goexperiment = `` 值得注意的是, PPC64实际上是power8 默认编译器参数 golang-go/src/cmd/go/internal/work/exec.go // gccArchArgs returns arguments to pass to gcc based on the architecture. func (b *Builder) gccArchArgs() []string { switch cfg.Goarch { case \"386\": return []string{\"-m32\"} case \"amd64\": return []string{\"-m64\"} case \"arm\": return []string{\"-marm\"} // not thumb case \"s390x\": return []string{\"-m64\", \"-march=z196\"} case \"mips64\", \"mips64le\": return []string{\"-mabi=64\"} case \"mips\", \"mipsle\": return []string{\"-mabi=32\", \"-march=mips32\"} case \"ppc64\": if cfg.Goos == \"aix\" { return []string{\"-maix64\"} } } return nil } 要修改的文件 modified: src/cmd/dist/build.go modified: src/cmd/dist/buildruntime.go modified: src/cmd/go/internal/cfg/cfg.go modified: src/internal/cfg/cfg.go ppc64无法在e6500上运行 用gcgo编译出来的GOARCH=ppc64的版本无法在fant-g上运行. 用gccgo编出来的32位程序可以执行. 用gccgo编ppc64时, 编译出错, 报link错误: lib/gcc/powerpc-e6500-linux-gnu/10.2.0/libgcc.a(morestack.o): in function `__morestack': src/gcc/libgcc/config/rs6000/morestack.S:159: undefined reference to `__morestack_block_signals' undefined reference to `__generic_morestack' undefined reference to `__morestack_unblock_signals' undefined reference to `__morestack_block_signals' undefined reference to `__generic_releasestack' undefined reference to `__morestack_unblock_signals' undefined reference to `__generic_findstack' undefined reference to `__generic_morestack_set_initial_sp' undefined reference to `__morestack_load_mmap' 对hello.go来说, 加选项-fno-split-stack可以成功编译ppc64, 成功运行 /repo/yingjieb/crosstoolng/github/crosstool-ng/targets/powerpc-e6500-linux-gnu/bin/powerpc-e6500-linux-gnu-gccgo -fno-split-stack -static hello.go PATH=$PATH:/repo/yingjieb/crosstoolng/github/crosstool-ng/targets/powerpc-e6500-linux-gnu/bin GOARCH=ppc64 /repo/yingjieb/godev/golang-go/bin/go build -compiler gccgo -gccgoflags '-static -Os -fno-split-stack' hello.go 但topid.go即使加了-fno-split-stack, 还是报一样的链接错误 这些缺失的符号应该在哪里? 看起来是libgcc.a的__morestack函数需要调用__morestack_block_signals等函数, 但没有定义. $ nm /repo/yingjieb/crosstoolng/github/powerpc-e6500-linux-gnu/lib/gcc/powerpc-e6500-linux-gnu/10.2.0/libgcc.a | grep more morestack.o: U __generic_morestack U __generic_morestack_set_initial_sp 0000000000000018 D __morestack U __morestack_block_signals 0000000000000048 D __morestack_get_guard U __morestack_load_mmap 0000000000000078 D __morestack_make_guard 0000000000000000 D __morestack_non_split 0000000000000060 D __morestack_set_guard U __morestack_unblock_signals U的意思就是undefined 对ppc来说, gcc的代码libgcc/config/rs6000/morestack.S中, 有__morestack的定义, 确实调用了这些函数. 这些函数的定义在:libgcc/generic-morestack.c 根据下面的参考文章, split stack功能是Ian Lance Taylor在x86上实现的; ppc64上的实现的作者是Alan Modra, [Patch 0/4] PowerPC64 Linux split stack support 对照X86机器上: # 先找到gcc的lib目录, 是/usr/lib/gcc/x86_64-linux-gnu/7 gcc -print-search-dirs # 用nm看符号表 $ nm libgcc.a | grep more nm: _trampoline.o: no symbols nm: __main.o: no symbols nm: _mulhc3.o: no symbols nm: _divhc3.o: no symbols generic-morestack.o: 00000000000002b0 T __generic_morestack 0000000000000240 T __generic_morestack_set_initial_sp 0000000000000940 T __morestack_allocate_stack_space 0000000000000880 T __morestack_block_signals 00000000000000b8 B __morestack_current_segment 0000000000000040 T __morestack_fail 0000000000000020 b __morestack_fullmask U __morestack_get_guard 0000000000000000 B __morestack_initial_sp 0000000000000ad0 T __morestack_load_mmap U __morestack_make_guard 0000000000000110 T __morestack_release_segments 00000000000000c0 B __morestack_segments U __morestack_set_guard 00000000000008e0 T __morestack_unblock_signals generic-morestack-thread.o: U __morestack_fail U __morestack_release_segments U __morestack_segments morestack.o: U __generic_morestack U __generic_morestack_set_initial_sp 0000000000000037 T __morestack U __morestack_block_signals 0000000000000155 T __morestack_get_guard 000000000000011c T __morestack_large_model U __morestack_load_mmap 0000000000000169 T __morestack_make_guard 0000000000000000 T __morestack_non_split 000000000000015f T __morestack_set_guard U __morestack_unblock_signals T表示是代码符号. 确实的函数大部分在generic-morestack.o中. 这和libgcc/generic-morestack.c代码分析是一样的. 说明这部分代码没有编译到libgcc.a中. ppc32没有这个问题, 因为ppc32本来就不支持split stack. 为什么generic-morestack.c没有编译到? libgcc/generic-morestack.c中, 主体代码的编译条件是:#if !defined __powerpc__ || defined __powerpc64____powerpc64__确定是有的. 那为什么还是没有编到呢?原来是libgcc/config/t-stack中, enable_threads控制这个文件是否参与编译: ifeq ($(enable_threads),yes) LIB2ADD_ST += $(srcdir)/generic-morestack.c $(srcdir)/generic-morestack-thread.c endif 在gcc安装手册中, 有使用方法: --enable-threads Specify that the target supports threads. This affects the Objective-C compiler and runtime library, and exception handling for other languages like C++. On some systems, this is the default. In general, the best (and, in many cases, the only known) threading model available will be configured for use. Beware that on some systems, GCC has not been taught what threading models are generally available for the system. In this case, --enable-threads is an alias for --enable-threads=single. --enable-threads=lib Specify that lib is the thread support library. This affects the Objective-C compiler and runtime library, and exception handling for other languages like C++. The possibilities for lib are: aix AIX thread support. dce DCE thread support. lynx LynxOS thread support. mipssde MIPS SDE thread support. no This is an alias for ‘single’. posix Generic POSIX/Unix98 thread support. rtems RTEMS thread support. single Disable thread support, should work for all platforms. tpf TPF thread support. vxworks VxWorks thread support. win32 Microsoft Win32 API thread support. 在gcc/config.gcc中, 每个arch和os都会检查 case ${target} in *-*-linux* | frv-*-*linux* | *-*-kfreebsd*-gnu | *-*-gnu* | *-*-kopensolaris*-gnu | *-*-uclinuxfdpiceabi) case ${enable_threads} in \"\" | yes | posix) thread_file='posix' ;; esac ... esac crosstool-ng的bug 经进一步检查, 原版的gcc没有问题. 问题在于crosstool-ng的一个patch: yingjieb@godev-server /repo/yingjieb/crosstoolng/github/crosstool-ng/packages/gcc/10.2.0 $ cat 0008-libgcc-disable-split-stack-nothreads.patch disable split-stack for non-thread builds Signed-off-by: Waldemar Brodkorb --- libgcc/config/t-stack | 2 ++ 1 file changed, 2 insertions(+) --- a/libgcc/config/t-stack +++ b/libgcc/config/t-stack @@ -1,4 +1,6 @@ # Makefile fragment to provide generic support for -fsplit-stack. # This should be used in config.host for any host which supports # -fsplit-stack. +ifeq ($(enable_threads),yes) LIB2ADD_ST += $(srcdir)/generic-morestack.c $(srcdir)/generic-morestack-thread.c +endif 尝试在crosstool-ng的config里面加: CT_CC_GCC_EXTRA_CONFIG_ARRAY=\"--enable-threads=yes\" 但不起作用, 删除掉这个文件0008-libgcc-disable-split-stack-nothreads.patch就好了. 关于split stack gcc的大神Ian Lance Taylor有两篇文章: gccgo中的split stack Gccgo provides the standard, complete Go library. Many of the core features of the Go runtime are the same in both gccgo and gc, including the goroutine scheduler, channels, the memory allocator, and the garbage collector. Gccgo supports splitting goroutine stacks as the gc compiler does, but currently only on x86 (32-bit or 64-bit) and only when using the gold linker (on other processors, each goroutine will have a large stack, and a deep series of function calls may run past the end of the stack and crash the program). Gccgo distributions do not yet include a version of the go command. However, if you install the go command from a standard Go release, it already supports gccgo via the -compiler option: go build -compiler gccgo myprog. The tools used for calls between Go and C/C++, cgo and SWIG, also support gccgo. gcc中的split stack 上面提到的gold linker是什么意思? gold linker是另一种linker, 并不是ld的某种模式. 相关链接 PowerPC64 Linux split stack support https://gcc.gnu.org/legacy-ml/gcc-patches/2015-05/msg01522.html https://groups.google.com/g/golang-codereviews/c/4T_KQys3XM0/m/VWNk2c6JCgAJ 上传gccgo工具链到artifactory proxyoff cd crosstool-ng/targets tar cJf godev-gccgo-ppc.tar.xz powerpc-e500mc-linux-gnu tar cJf godev-gccgo-ppc64.tar.xz powerpc64-e6500-linux-gnu curl -H \"X-JFrog-Art-Api:AKCp8hyinctVijrdqGaFc1YAT7e7KDHWJEaackjuv6oCheipkYU9jU5okRj8rnFkVvcZWnTVc\" -X PUT \"https://artifactory-blr1.int.net.nokia.com:443/artifactory/godevsig-generic-local/toolchain/gccgo/`git rev-parse HEAD`/\" -T \"{godev-gccgo-ppc.tar.xz,godev-gccgo-ppc64.tar.xz}\" 详见笔记云端环境和go实践.md gccgo的hello size过大问题 gccgo编译出来的静态链接的hello是17M 其中有12M左右是debug info. 用gcc -s或者strip --strip-debug都能减小size, 但问题是运行时panic就不能打印调用栈了. 按照gcc-10.2.0/libgo/README的说法: This library should not be stripped when it is installed. Go code relies on being able to look up file/line information, which comes from the debugging info using the libbacktrace library. go需要debug info来查找file line信息. 注: libbacktrace也是ianlancetaylor大神的作品: github 那有没有其他的办法可以减小hello的size呢? 17M也太大了. strip使用说明 GCC的debug选项 gcc的选项中, -g1和-gz看起来比较有用: -glevel Request debugging information and also use level to specify how much information. The default level is 2. Level 0 produces no debug information at all. Thus, -g0 negates -g. Level 1 produces minimal information, enough for making backtraces in parts of the program that you don’t plan to debug. This includes descriptions of functions and external variables, and line number tables, but no information about local variables. Level 3 includes extra information, such as all the macro definitions present in the program. Some debuggers support macro expansion when you use -g3. If you use multiple -g options, with or without level numbers, the last such option is the one that is effective. -gdwarf does not accept a concatenated debug level, to avoid confusion with -gdwarf-level. Instead use an additional -glevel option to change the debug level for DWARF. -gz[=type] Produce compressed debug sections in DWARF format, if that is supported. If type is not given, the default type depends on the capabilities of the assembler and linker used. type may be one of ‘none’ (don’t compress debug sections), ‘zlib’ (use zlib compression in ELF gABI format), or ‘zlib-gnu’ (use zlib compression in traditional GNU format). If the linker doesn’t support writing compressed debug sections, the option is rejected. Otherwise, if the assembler does not support them, -gz is silently ignored when producing object files. -gz能让hello从17M减小到8.7M 相关命令 #看具体那个section的size size -A hello 这篇文章非常详细的介绍了debug节的方方面面的信息. 特别的, objcopy能够去掉任意指定section /repo/yingjieb/godevsig/crosstool-ng/targets/powerpc-e500mc-linux-gnu/bin/powerpc-e500mc-linux-gnu-objcopy -R .debug_info -R .debug_abbrev -R .debug_aranges -R .debug_ranges -R .debug_loc -R .debug_str hello 减小libgo.a的体积 gcc的libgo在编译的时候, 有自己默认的编译选项-O2 -g 用下面的patch diff --git a/Makefile.in b/Makefile.in index 36e369df6..2e183c702 100644 --- a/Makefile.in +++ b/Makefile.in @@ -619,7 +619,7 @@ CXXFLAGS_FOR_TARGET = @CXXFLAGS_FOR_TARGET@ LIBCFLAGS_FOR_TARGET = $(CFLAGS_FOR_TARGET) LIBCXXFLAGS_FOR_TARGET = $(CXXFLAGS_FOR_TARGET) -fno-implicit-templates LDFLAGS_FOR_TARGET = @LDFLAGS_FOR_TARGET@ -GOCFLAGS_FOR_TARGET = -O2 -g +GOCFLAGS_FOR_TARGET = -Os -g1 -gz GDCFLAGS_FOR_TARGET = -O2 -g FLAGS_FOR_TARGET = @FLAGS_FOR_TARGET@ diff --git a/Makefile.tpl b/Makefile.tpl index efed15117..3c0d2e5e2 100644 --- a/Makefile.tpl +++ b/Makefile.tpl @@ -542,7 +542,7 @@ CXXFLAGS_FOR_TARGET = @CXXFLAGS_FOR_TARGET@ LIBCFLAGS_FOR_TARGET = $(CFLAGS_FOR_TARGET) LIBCXXFLAGS_FOR_TARGET = $(CXXFLAGS_FOR_TARGET) -fno-implicit-templates LDFLAGS_FOR_TARGET = @LDFLAGS_FOR_TARGET@ -GOCFLAGS_FOR_TARGET = -O2 -g +GOCFLAGS_FOR_TARGET = -Os -g1 -gz GDCFLAGS_FOR_TARGET = -O2 -g FLAGS_FOR_TARGET = @FLAGS_FOR_TARGET@ 经过测试, 各个选项组合的size如下: GOCFLAGS_FOR_TARGET webhello size -O2 -g(default) 17M -Os -g1 -gz 13M -Os -g1 -gz -fdata-sections -ffunction-sections -Wl,--gc-sections 12M -Os -g1 -gz -fdata-sections -ffunction-sections -Wl,--gc-sections -flto 12M -Os -g1 -gz -fdata-sections -ffunction-sections 12M 使能lto crosstool-ng默认在static gcc模式下(STATIC_TOOLCHAIN=y), 不使能lto. 但可以自己强制使能: CT_CC_GCC_EXTRA_CONFIG_ARRAY=--enable-lto 但经过验证, 似乎打开lto优化, 并没有减小size. 改动提交到crosstool-ng 在gcc库上commit这个改动, 生成patch # -1表示最新的1个改动 git format-patch -1 拷贝这个patch到gcc下面 cp 0001-gccgo-generate-smaller-libgo-when-building-gccgo.patch crosstool-ng/packages/gcc/10.2.0/ go_export小节 用size -A命令看到, gccgo编译出来的executable中, 有.go_export小节, 在webhello中占1.6M 根据这个讨论, gccgo的作者说.go_export是给shared library用的, 最终的可执行文件不需要. 但他不知道怎么告诉linker把这个小节去掉 那可以在最后手动用objcopy -R .go_export file来删除这个小节. /opt/crosstool/powerpc64-e6500-linux-gnu/bin/powerpc64-e6500-linux-gnu-objcopy --remove-section=.go_export hello static和static-libgo 使用gccgo, 可以传入-gccgoflags为-static-libgo, 只把libgo编译到static的目标文件中, 而libc等库还是动态连接的. 详见这个问答 让gcgo支持ppc32 这个issue就是讨论这个话题的. 但截止目前(2020.10)还没有动静. 有人提到了通常支持一个ARCH需要的工作: Quoting this super useful post from Aram Hăvărneanu from https://groups.google.com/d/msg/golang-dev/SRUK7yJVA0c/JeoCRMwzBwAJ \"I've done many ports now, so the strategy I use is this (very simplified): 1\\. Add GOOS/GOARCH support to the toolchain 2\\. Add some support for GOARCH in cmd/internal/obj 3\\. Add some support for GOARCH in cmd/asm 4\\. Add some support for GOOS/GOARCH in cmd/link 5\\. Iterate through 2-3-4 until you can produse some kind of binaries from assembly files. Depending on the specifics of GOOS/GOARCH you might, or might not need to use external linking. 6\\. Once you can produce binaries, thoroughly test them (link just assembly programs, without runtime). Basically make sure the low-level toolchain works. 7\\. Start working on the Go compiler for GOARCH. 8\\. Write a minimal alternative runtime for Go. The runtime is much too complicated as a first test Go program. Basically write your own runtime in assembly that is just stubbed out, but can run a good bulk of the programs in go/test. 9\\. Once that works well enough, start using the real runtime. This requires implementing a lot of assembly, but you can use the lessons learned from #8. 10\\. Make all the tests in go/test work. 11\\. Make all the stdlib tests work. You are still working amd64 now, and executing on GOARCH with go_GOOS_GOARCH_exec. 12\\. Run bootstrap.bash 13\\. Move over the artifacts on GOOS/GOARCH machine. 14\\. Make sure make.bash works. You will still likely have to fix problems to make this N-generation compiler work. 15\\. Make sure all.bash works. 16\\. Done. As you can see, steps 1-14 are done on amd64 (or some other supported platform), and only step 15 is done on the target architecture. 搜索源码大约有5000行包含ppc64字样... 要完整支持ppc32并非易事. 下一步 用crosstool-ng编译go1.13兼容的工具链 最好全部静态链接 高版本 补充: fant-f运行topid ~ # LD_LIBRARY_PATH=`pwd` ./topid -record -tag fantf -p 1 -child -chartserver 10.182.105.138:9887 -i 3 Hello 你好 Hola Hallo Bonjour Ciao Χαίρετε こんにちは 여보세요 Version: 0.1.3 Visit below URL to get the chart: http://10.182.105.138:9888/fantf/538600174 ~ # ls -lh total 38M -rwxr-xr-x 1 root root 89.7K Jul 8 14:22 hello -rwxr-xr-x 1 root root 37.8M Jul 8 14:24 libgo.so.13 -rwxr-xr-x 1 root root 258.3K Jul 8 15:01 topid ~ # cat /isam/slot_default/devs/rip/boardName FANT-F ~ # ~ # 补充: ppc64 yingjieb@godev-server /repo/yingjieb/godevsig/crosstool-ng/samples/Nokia $ git show 0a81bc37 commit 0a81bc37a9e8d44c9d3f13c0b557854425b018db Author: Bai Yingjie Date: Tue Oct 20 08:55:23 2020 +0000 godev: enable the default split-stack for ppc64, gccgo lib uses split-stack feature, which requires toolchain support split-stack. diff --git a/packages/gcc/10.2.0/0008-libgcc-disable-split-stack-nothreads.patch b/packages/gcc/10.2.0/0008-libgcc-disable-split-stack-nothreads.patch deleted file mode 100644 index df91a9ff..00000000 --- a/packages/gcc/10.2.0/0008-libgcc-disable-split-stack-nothreads.patch +++ /dev/null @@ -1,17 +0,0 @@ -disable split-stack for non-thread builds - -Signed-off-by: Waldemar Brodkorb - ---- - libgcc/config/t-stack | 2 ++ - 1 file changed, 2 insertions(+) - ---- a/libgcc/config/t-stack -+++ b/libgcc/config/t-stack -@@ -1,4 +1,6 @@ - # Makefile fragment to provide generic support for -fsplit-stack. - # This should be used in config.host for any host which supports - # -fsplit-stack. -+ifeq ($(enable_threads),yes) - LIB2ADD_ST += $(srcdir)/generic-morestack.c $(srcdir)/generic-morestack-thread.c -+endif 补充: 为什么gc go工具链的支持ppc64, 但不能在e6500上运行? 因为gc go的ppc64特指power8 zte有人支持了e6500 用法是传入GOPPC 但我试了, 官方源码树里还没有. GOPPC64=e6500 GOARCH=ppc64 _go build hello.g 2020/11/03 01:21:57 Invalid GOPPC64 value. Must be power8 or power9. 还有个说法是, 去掉ppc64的支持, 只保留ppc64le 补充: elf格式 https://stevens.netmeister.org/631/elf.html "},"notes/golang_toolchain_compile_gccgo.html":{"url":"notes/golang_toolchain_compile_gccgo.html","title":"编译gccgo","keywords":"","body":" gccgo和go tools 使用go tools和gccgo crosstool ng(我最后用的是这个) 编译gcc的理论基础 # 编译过程for x86_64 下载gcc9.2源码, release版本就可以 安装依赖 解压gcc 编译 编译完成后 会在指定目录下生成 使用 gccgo和go tools gotools目录是编译go tools的. 到时候要自己改build源码, 自己生成tools? go的代码在libgo https://github.com/golang/go/wiki/GccgoCrossCompilation#build-a-cross-gccgo-aware-version-of-the-go-tool https://github.com/golang/go/wiki/GccgoCrossCompilation https://github.com/karalabe/xgo 使用go tools和gccgo https://medium.com/@chrischdi/cross-compiling-go-for-raspberry-pi-dc09892dc745 crosstool ng(我最后用的是这个) https://crosstool-ng.github.io/docs/configuration/ crosstool-ng可以打印shell的调用栈, 比如: [INFO ] Retrieving needed toolchain components' tarballs [EXTRA] Retrieving 'automake-1.16.1' [EXTRA] Verifying SHA512 checksum for 'automake-1.16.1.tar.xz' [EXTRA] Retrieving 'linux-4.9.156' [ERROR] linux: download failed [ERROR] [ERROR] >> [ERROR] >> Build failed in step 'Retrieving needed toolchain components' tarballs' [ERROR] >> called in step '(top-level)' [ERROR] >> [ERROR] >> Error happened in: CT_Abort[scripts/functions@487] [ERROR] >> called from: CT_DoFetch[scripts/functions@2103] [ERROR] >> called from: CT_PackageRun[scripts/functions@2063] [ERROR] >> called from: CT_Fetch[scripts/functions@2174] [ERROR] >> called from: do_kernel_get[scripts/build/kernel/linux.sh@22] [ERROR] >> called from: main[scripts/crosstool-NG.sh@647] [ERROR] >> 是因为使用了trap ... ERR技术.在crosstool-ng-1.24.0/scripts/functions中, trap CT_OnError ERR, 具体见CT_OnError函数. # Install the fault handler trap CT_OnError ERR # Inherit the fault handler in subshells and functions set -E # Make pipes fail on the _first_ failed command # Not supported on bash help trap和help set说的很清楚 上面这个CT_OnError函数用了下面的思路: bash默认有变量FUNCNAME BASH_SOURCE BASH_LINENO, 这些是数组, 保存了调用栈. https://unix.stackexchange.com/questions/462156/how-do-i-find-the-line-number-in-bash-when-an-error-occured 编译gcc的理论基础 https://preshing.com/20141119/how-to-build-a-gcc-cross-compiler/ https://solarianprogrammer.com/2018/05/06/building-gcc-cross-compiler-raspberry-pi/ 编译过程for x86_64 ============================= 下载gcc9.2源码, release版本就可以 axel http://www.netgull.com/gcc/releases/gcc-9.2.0/gcc-9.2.0.tar.xz 安装依赖 apt install build-essential libgmp-dev libmpfr-dev libmpc-dev 解压gcc 编译 cd gcc-9.2.0 mkdir objdir && cd objdir ../configure --prefix=/home/byj/repo/gorepo/gcc --enable-languages=go --disable-multilib # 虽然没有写明要c和c++, 但默认是肯定有的 make -j 编译完成后 make install 会在指定目录下生成 gcc c++ g++ go gofmt gccgo 使用 export LD_LIBRARY_PATH=~/repo/gorepo/gcc/lib64 PATH=~/repo/gorepo/gcc/bin:$PATH go build hello.go 会生成hello可执行文件, 默认动态链接, 大小70K 动态库libgo.so大小48M, strip后26M 压缩后5M $ ldd hello linux-vdso.so.1 => (0x00007ffe35580000) libgo.so.14 => /home/byj/repo/gorepo/gcc/lib64/libgo.so.14 (0x00007f5fae036000) libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5fadd05000) libgcc_s.so.1 => /home/byj/repo/gorepo/gcc/lib64/libgcc_s.so.1 (0x00007f5fadaee000) libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f5fad723000) /lib64/ld-linux-x86-64.so.2 (0x000055cdd4f10000) libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f5fad506000) "},"notes/golang_go_on_mips.html":{"url":"notes/golang_go_on_mips.html","title":"go on mips boards(not so updated)","keywords":"","body":"记录早期我在mips上跑go的过程.现在看起来走了些弯路, 主要是当时没想到go的交叉编译会那么简单. "},"notes/golang_go_on_mips_part1.html":{"url":"notes/golang_go_on_mips_part1.html","title":"part 1: cross compile go","keywords":"","body":" Background Go toolchains Compile gc go toolchain Compile gc go 1.4 for host Compile gc go cross toolchain Cgo issue Cgo use gcc compiler direct cgo to use N32 ABI How to proceed if cgo is not working? disable cgo support use N64 ABI for MIPS64 hello.go Works next Background In order to run go app on the board, we need to have a go compiler that compiles go sources into go executables. Here we have a simple hello example: hello.go package main import \"fmt\" func main() { fmt.Println(\"Hello, World!\") } For go is a modern programing language, it is pretty straightforward to do this in the cloud/server x86 environment: $ sudo apt install golang-go $ go build hello.go $ ls hello hello.go $ ./hello Hello, World! OK, job is done, you can stop reading if you work on a system that uses Ubuntu/CentOS on X86, or even ARM64. But for people who work in embedded environment like buildroot system on MIPS, please continue. Go toolchains There are two official Go compiler toolchains. gc go toolchain: seems to be more popular, this toolchain itself is written in Go x86/ubuntu reference: sudo apt install golang-go gccgo compiler: the traditional compiler using the GCC back end x86/ubuntu reference: sudo apt install gccgo-go At the moment, for MIPS boards like fglt-b, gcc version is 4.7.0, lower than the minimal gcc prerequisite version 4.7.1, which has support for go 1.0. gcc 8 supports go 1.10. source: https://golang.org/doc/install/gccgo Compile gc go toolchain In order to have go executables runnable on board, a gc go toolchain needs to be compiled first which is a cross toolchain resides on host for embedded world. As mentioned, gc go is written in go, so a go bootstrap compiler is needed to compile gc go toolchain. Actually, the bootstrap compiler is also gc go compiler version 1.4, which was the last distribution in which the toolchain was written in C. Compile gc go 1.4 for host In buildroot, package go-bootstrap is the one does the job. see buildroot/package/go-bootstrap/go-bootstrap.mk buildroot/output/build/host-go-bootstrap-1.4.3/src/make.bash the compiled outputs are installed to: buildroot/output/host/lib/go-1.4.3 Compile gc go cross toolchain In buildroot, package go is the one does the job. Here we need to build a cross compiler, GO_GOARCH is used for this purpose, eg: for fglt-b, GO_GOARCH = mips64 If all things go well, the toolchain will be installed to buildroot/output/host/lib/go. The complete build steps: Building Go cmd/dist using /repo/yingjieb/ms/buildroot/output/host/lib/go-1.4.3. Building Go toolchain1 using /repo/yingjieb/ms/buildroot/output/host/lib/go-1.4.3. Building Go bootstrap cmd/go (go_bootstrap) using Go toolchain1. Building Go toolchain2 using go_bootstrap and Go toolchain1. Building Go toolchain3 using go_bootstrap and Go toolchain2. Building packages and commands for host, linux/amd64. # errors are seen in this step Building packages and commands for target, linux/mips64. Cgo issue cgo is a component of go, which enables the creation of go packages that call C code. In buildroot, package go fails to compile, because go doesn't support CGO linking on MIPS64x platforms. See: https://github.com/karalabe/xgo/issues/46 The consequence is any go app that depends on BR2_PACKAGE_HOST_GO_CGO_LINKING_SUPPORTS is unable to compile, even not selectable by buildroot menuconfig. Nearly all go packages depend on BR2_PACKAGE_HOST_GO_CGO_LINKING_SUPPORTS, they are: docker-proxy, docker-containerd, docker-engine, docker-cli, flannel, mender, runc. For MIPS octeon family specifically, which is used widely by LTs, the error message is: # runtime/cgo /repo/yingjieb/ms/buildroot/output/host/opt/ext-toolchain/bin/../lib/gcc/mips64-octeon-linux-gnu/4.7.0/../../../../mips64-octeon-linux-gnu/bin/ld: skipping incompatible /repo/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot/usr/lib/libpthread.so when searching for -lpthread /repo/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot/usr/lib/libgcc_s.so: could not read symbols: File in wrong format collect2: error: ld returned 1 exit status go tool dist: FAILED: /repo/yingjieb/ms/buildroot/output/build/host-go-1.11.6/pkg/tool/linux_amd64/go_bootstrap install -gcflags=all= -ldflags=all= -v std cmd: exit status 2 Cgo use gcc compiler cgo enables go packages call C code, so it must require a C compiler for the C code part. And we are in embedded world, so here we need a cross C compiler. Gc go make.bash uses CC_FOR_TARGET to specify the C compiler, on my environment, for example: CC_FOR_TARGET=\"/repo/yingjieb/ms/buildroot/output/host/bin/mips64-octeon-linux-gnu-gcc\" In fact, this toolchain is a wrapper executable, which wraps external gcc toolchain to generate n32 ABI objects, like this: ELF 32-bit MSB executable, MIPS, N32 MIPS64 rel2 version 1, dynamically linked (uses shared libs), for GNU/Linux 2.6.32, with unknown capability 0x41000000 = 0xf676e75, with unknown capability 0x10000 = 0x70401, not stripped The 64 bit N64 ABI allows for more arguments in registers for more efficient function calls when there are more than four parameters. There is also the N32 ABI which also allows for more arguments in registers. The return address when a function is called is stored in the $ra register automatically by use of the JAL (jump and link) or JALR (jump and link register) instructions. Basically, N32 uses 64 bit registers in a 32 bit address space, meaning that pointers are 32 bit. However, for some unknown reason, the actual output objects are N64 ABI: #cgo puts its generated C files to /tmp dir: yingjieb@FNSHA190 /tmp/go-build164444988/b032 $ ls _cgo_export.c _cgo_flags _cgo_main.c _x001.o _x003.o _x005.o _x007.o _x009.o cgo.cgo1.go _cgo_export.h _cgo_gotypes.go _cgo_main.o _x002.o _x004.o _x006.o _x008.o _x010.o cgo.cgo2.c # the default ABI is N64 yingjieb@FNSHA190 /tmp/go-build164444988/b032 $ file _cgo_main.o _cgo_main.o: ELF 64-bit MSB relocatable, MIPS, MIPS64 rel2 version 1 (SYSV), with unknown capability 0x410000000f676e75 = 0x1000000070401, not stripped The reason maybe that Gcc toolchain uses N64 ABI by default to generates objects. One can explicitly set -mabi=n32 to CFLAGS, that's how we config buildroot to use N32 ABI for our MIPS octeon family boards. direct cgo to use N32 ABI So, let's get back to the cgo compiling issue, basically, ld complains about the format of the to be linked shared objects are incompatible. We can direct cgo to generate N32 ABI object. Add below to src/runtime/cgo/cgo.go#cgo mips64 CFLAGS: -mabi=n32 #cgo mips64 LDFLAGS: -Wl,-m,elf32btsmipn32 Do export CGO_LDFLAGS_ALLOW=\".*\" before building go package Let's compile it again... This time the previous errors are gone, but we have new errors now: # runtime/cgo In file included from _cgo_export.c:4:0: cgo-gcc-export-header-prolog:25:14: error: size of array '_check_for_64_bit_pointer_matching_GoInt' is negative _cgo_export.c is an generated c file, located in /tmp/go-buildxxxxxxxxx where xxxxxxxxx is a random number generated by the build system, so as _cgo_export.h, which has the assertion: typedef GoInt64 GoInt; /* static assertion to make sure the file is being used on architecture at least with matching size of GoInt. */ typedef char _check_for_64_bit_pointer_matching_GoInt[sizeof(void*)==64/8 ? 1:-1]; src/cmd/cgo/out.go is used to genertate _cgo_export.h: It seems that cgo expects a goInt to be 64 bit, but N32's pointer is 32 bit, that's why we have the error message. Disable the check temporally: //typedef char _check_for_GOINTBITS_bit_pointer_matching_GoInt[sizeof(void*)==GOINTBITS/8 ? 1:-1]; #temporary workaroud this check typedef char _check_for_GOINTBITS_bit_pointer_matching_GoInt[1]; Let's rebuild it... cgo build seems to pass, but after it we have more errors: # plugin /repo/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot/usr/lib/libdl.so: could not read symbols:File in wrong format # os/signal/internal/pty /repo/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot/usr/lib/libgcc_s.so: could not read symbols: File in wrong format # os/user /repo/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot/usr/lib/libgcc_s.so: could not read symbols: File in wrong format # net /repo/yingjieb/ms/buildroot/output/host/mips64-buildroot-linux-gnu/sysroot/usr/lib/libgcc_s.so: could not read symbols: File in wrong format Same issue, apply our changes to these files: src/plugin/plugin_dlopen.go src/os/signal/internal/pty/pty.go src/os/user/cgo_lookup_unix.go src/os/user/getgrouplist_unix.go src/os/user/listgroups_unix.go src/os/user/getgrouplist_darwin.go src/net/cgo_linux.go Rebuild, OK, then the error message leads to: /tmp/go-link-436592583/go.o: In function `runtime.save_g': /repo/yingjieb/ms/buildroot/output/host/lib/go/src/runtime/tls_mips64x.s:20:(.text+0x6d24c): relocation truncated to fit: R_MIPS_TLS_TPREL_LO16 against `runtime.tls_g' It seems to have come to the final stage to build the executable \"go\"(the go compiler), but apparently there is something wrong in tls_mips64x.s. The file is not long, but requires some longer study time to adapt for N32 mode if we still want to go this way... // If !iscgo, this is a no-op. // // NOTE: mcall() assumes this clobbers only R23 (REGTMP). TEXT runtime_save_g(SB),NOSPLIT|NOFRAME,$0-0 MOVB runtime_iscgo(SB), R23 BEQ R23, nocgo MOVV R3, R23 // save R3 MOVV g, runtime_tls_g(SB) // TLS relocation clobbers R3 MOVV R23, R3 // restore R3 nocgo: RET TEXT runtime_load_g(SB),NOSPLIT|NOFRAME,$0-0 MOVV runtime_tls_g(SB), g // TLS relocation clobbers R3 RET GLOBL runtime_tls_g(SB), TLSBSS, $8 There are high possibilities that other .s files need to be adapted. A rough estimation is: there are 505 .s files, with 446 line containing \"cgo\" key word together. yingjieb@FNSHA190 /repo/yingjieb/ms/buildroot/output/build/host-go-1.11.6/src $ find -name \"*.s\" | wc -l 505 yingjieb@FNSHA190 /repo/yingjieb/ms/buildroot/output/build/host-go-1.11.6/src $ find -name \"*.s\" | xargs cat | grep cgo | wc -l 446 So, enabling N32 ABI which is used by all our MIPS boards for Go is not something can be done in a short time. How to proceed if cgo is not working? As analyzed, the effort to resolve cgo problem is not trivial, right now what can be done are: disable cgo support add HOST_GO_CGO_ENABLED = 0 to package/go/go.mk, which means the gc go toolchian will not have cgo support, which further results the libraries that depend on cgo are not available. I am not an export of go, but I did a simple counter, there are 208 lines in the toolchain src use cgo. yingjieb@FNSHA190 /repo/yingjieb/ms/buildroot/output/build/host-go-1.11.6/src $ grep -r 'import \"C\"' | wc -l 208 use N64 ABI for MIPS64 What if cgo is the part of the heart of go toolchain and furthermore, is indispensable for most go apps? The other option we can consider is to use N64 ABI. N64 uses 64 bit registers and 64 bit address space. As said, all our MIPS boards use N32, I don't know exactly the reasons, but a common one would be: the old code are written for 32 bit processor, engineers at 32 bit era had the assumption that pointers are always 32 bit. This option remains to be investigated if decision made to go this way. hello.go Finally, we come to our task, thanks for the complicities of embedded world and weak ecosystem of MIPS. Now that we have the gc go toolchain built, yes, with cgo disabled. In Buildroot, we leverage the make framework to construct go packages, for example, I added below files: M package/Nokia/Config.in M package/go/Config.in.host M package/go/go.mk A package/Nokia/go-prototype/Config.in A package/Nokia/go-prototype/go-prototype.mk And I setup a repo named go-prototype, which has the hello.go src. Here I want to skip the details of how to create a package in Buildroot, which is basically to make a package that depends on package go(the gc go toolchain) OK, build our go-protogype package and all other packages selected in buildroot, make rootfs, and load to our board. Be noted that our hello.go doesn't need cgo, since it does not import \"C\" Then on our board, I am using cfnt-b, which has the same CPU with fglt-b: Type /isam/slot_default/run # hello Hello, World! First go program on cfnt-b! /isam/slot_default/run # which hello /usr/bin/hello /isam/slot_default/run # ls -lh /usr/bin/hello -rwxr-xr-x 1 root root 1.4M Jun 28 2019 /usr/bin/hello OK, we are good to run hello.go Works next run Go simple function with library(with the hope that cgo is not used) Go benchmarking research and selection(with the hope that cgo is not used) Benchmark go(with the hope that cgo is not used) investigate how to enable N64 ABI for MIPS(optional) "},"notes/golang_go_on_mips_part2.html":{"url":"notes/golang_go_on_mips_part2.html","title":"part 2: build native go compiler","keywords":"","body":" Background Native compilation of go src board prerequisites set up Gentoo linux environment build gc go toolchain hello.go and cgotest.go Works next Background In previous article go on mips board part 1, we have set up a go cross compiler and by using of buildroot go package frame work, we added a go-prototype package in buildroot and we are able to generate go \"hello\" executable running on MIPS board cfnt-b. For simple \"hello\" go executable, it is fine. However, Cgo is not supported. The MIPS N32 ABI we are always using for our MIPS board does not seem to support Cgo, see \"Cgo issue\" in go on mips board part 1, hence it prevents us from running go packages that call C code, which is the case that vOnuMgmt expects, according to vOnuMgmt expert. So, What should we do? The errors are link time errors, ld complains about the format of the to be linked shared objects are incompatible. In the end of that article, I proposed to try MIPS N64 ABI. However, in buildroot, one can select \"Target ABI\" to either n32 or n64, but not both. If we use n64 ABI, there will be no lib32 support, for example, no n32 glibc is there which is the corner stone of the 32bit isam applications. The possible options to support N64 ABI: change the whole world from N32 ABI completely to N64 ABI: a fundamental change, big impact, think about Android or ios switching from 32 bit to 64 bit. in buildroot, implement \"multilib\" which supports both N32 and N64, let go packages use n64, everything else use n32: smaller impact, but consuming considerablelly more rootfs size since lib64 will also need to be copied to target rootfs.#toolchain-external-custom/mips64-octeon-linux-gnu/sys-root $ du -sh lib32 lib32-fp lib64 lib64-fp 50M lib32 25M lib32-fp 60M lib64 30M lib64-fp The second option is preferred, but we need more work to investigate the effort. There is indeed another prospect in which we can build go natively on the board, see below... But please note that this approach is for prototype go only for performance evaluation, because it leverages a totally different root filesystem, much larger but has the ability to native build go packages compared to buildroot root filesystem. We will also need to explore more feasible method to build go packages if go performance is accetable on MIPS boards. Native compilation of go src board prerequisites To be able to compile go natively, the board should have: native gcc toolchian: used to compile Cgo go bootstrap: used to build go toolchain It is not the intention of buildroot to have the ablitiy to generate a gcc toolchain for the target board, all it is about is cross compilation. And building a Gcc toolchain is not even an easy task for people working on X86 PC/Server environment, since lots of dependency packages need to install in order to get Gcc toolchain built, let alone we are on a embedded busybox based system. So we need a basic system that can provide minimal development environment, for MIPS in particular. Fortunately, there is one available(maybe one only for MIPS): Gentoo Linux set up Gentoo linux environment I am skipping some details about how to setup the Gentoo Linux environment as it is less relevant to our go topic. So here I just list key points. Gentoo linux base rootfs is a prebuilt rootfs, with size about 2G, so I put it on my PC and let board mount it over nfs. Let board boot to the default Linux environment, and then chroot to Gentoo Linux. chroot let us switch into Gentoo Linux as if it is the root filesystem build gc go toolchain To build a gc go toolchain on the board, following steps are required: build go1.12 toolchain on X86 first on X86, use the go1.12 toolchain to build bootstrap toolchain for MIPS64 copy go bootstrap toolchain for MIPS64 to board use that bootstrap toolchain to build go1.12 toolchain The reason we need 4 steps is go toolchain is written in go, so in order to build go toolchain, we need go bootstrap toolchain, which is go1.4, only X86 can build that toolchain. hello.go and cgotest.go OK, now we have native go toolchain, let's compile and run some go programs: root@yingjieb-VirtualBox ~/work/nfsroot/mipsroot.go.ok/root Linux Mint 19.1 Tessa # cat hello.go package main import \"fmt\" func main() { fmt.Println(\"Hello, World!\") fmt.Println(\"First go program on cfnt-b!\") } root@yingjieb-VirtualBox ~/work/nfsroot/mipsroot.go.ok/root Linux Mint 19.1 Tessa # cat cgotest.go package main //int Add(int a, int b){ // return a+b; //} import \"C\" import \"fmt\" func main() { a := C.int(10) b := C.int(20) c := C.Add(a, b) fmt.Println(c) // 30 } The go programs run as expected. Works next go program with libraries: hello.go is too simple, we need to run a real world go program that uses more libraries, to see if running them on MIPS board has any problems. general benchmarks: for example, lmbench. This is to see how general performance looks like compared to X86. go basic performance compress: tar/zip/bzip2/flate structures: heap/list/ring algorithm: sort/string crypto: aes/des/dsa/md5/sha256 encoding: csv/json/xml image: gif/jpeg/png net: http/socket more complicated go benchmarks. "},"notes/golang_go_on_mips_part3.html":{"url":"notes/golang_go_on_mips_part3.html","title":"part 3: gccgo experiments","keywords":"","body":" What is gccgo Current gcc used by MIPS boards doesn't compile go Build gcc toolchain to support go gcc toolchain needs to be rebuild to support go build gcc toolchain natively on X86 use gccgo C Interoperability Gcc version requirement Conclusion What is gccgo The gccgo compiler is a new frontend for GCC, the widely used GNU compiler. refer to: https://golang.org/doc/install/gccgo Current gcc used by MIPS boards doesn't compile go Build gcc toolchain to support go gcc toolchain needs to be rebuild to support go Building gccgo is just like building GCC with one or two additional options. See the instructions on the gcc web site. When you run configure, add the option --enable-languages=c,c++,go (along with other languages you may want to build). build gcc toolchain natively on X86 gccgo is another branch of gcc, called branches/gccgo: svn checkout svn://gcc.gnu.org/svn/gcc/branches/gccgo gccgo mkdir objdir cd objdir ../gccgo/configure --prefix=/opt/gccgo --enable-languages=c,c++,go --with-ld=/opt/gold/bin/ld make make install use gccgo gccgo -c file.go gccgo -o file file.o C Interoperability When using gccgo there is limited interoperability with C, or with C++ code compiled using extern \"C\". refer to: https://golang.org/doc/install/gccgo#C_Interoperability Gcc version requirement The GCC 4.7.1 release and all later 4.7 releases include a complete Go 1 compiler and libraries. The GCC 4.9 releases include a complete Go 1.2 implementation. The GCC 5 releases include a complete implementation of the Go 1.4 user libraries. The GCC 6 releases include a complete implementation of the Go 1.6.1 user libraries. The GCC 7 releases include a complete implementation of the Go 1.8.1 user libraries. The GCC 8 releases are expected to include a complete implementation of the Go 1.10 release Conclusion Gcc toolchain was compiled and generated by vendors, and by default made without go language support. Current gcc toolchain is provided by Marvell(Cavium), which doesn't support go. gcc version is 4.7.0, which can not support Go 1.0 the toolchain was not compiled with Go language support. To be able to use gcc to compile go code, we need vendor to provide cross gcc toolchain, which must meet 2 prerequisites: with additional go language enabled: add --enable-languages=c,c++,go when compiling the toolchain. better to be Gcc 8, Gcc 7 only supports Go 1.8 "},"notes/golang_go_on_mips_part4.html":{"url":"notes/golang_go_on_mips_part4.html","title":"part 4: Golang json performance","keywords":"","body":" Background Single core performance methodology Multi-core performance methodology Devices under test Take goroutine into account Json performance X86 vs MIPS comparison run official json benchmark the result Writing test code for C vs Go comparision compile and run the benchmark on MIPS board runtime status result Overall observation Background There are many factors that impact the performance of a workload under test: On hardware wise, what is the CPU arch being used(X86, MIPS, ARM…), how is the Cache system like(L1 L2 L3), how many DDR controllers on the system, and how much memory a system has? what is the storage subsystem(SSD, NVME, or distributed storage system), what is the network subsystem(1G, 10G, 25G, which NIC being used?)… On software wise, what is the kernel and root file system? How is the software configured? Is it a single thread app or in multi thread mode? What is the contention model if it is multi-threaded? The factors are too many and any one of them could have big impact on the performance. However, there are indeed methodologies based on experience and practice that could direct the evaluation task of a given workload requirement. Single core performance methodology Generally, unlike multi-thread workloads, single thread app does not involve contentions with other threads that share same resources, which avoids the complicities of lock contention, waiting or sleeping on a resource to be available, or cache coherence issue, etc. A typical example is MySQL, which uses many threads that may access the same row or column of a table. So it is in practice more reliable to use single thread mode to do the benchmark. Multi-core performance methodology As said, multi-core introduces more complicities which are all about contention for shared resources, what resources are shared across multiple cores depends on the given workload. For example, if the target workload has multiple threads that shares a critical memory area, there will be a lock to protect the access to the shared memory, which may result in thread being scheduled out, which will hurt the performance.Even if the target workload is designed to be scale-out, say not using locks, its many threads are running independently sharing nothing from software perspective, that is not often the case in real world, but even it is, the cumulative performance may still not be scaled, that's because the cores actually share hardware buses, like cache bandwidth, ddr bandwidth… So for multi-core performance , we should check the scalability of cores 2 4 6 8…, usually by using taskset to bind the tasks to different cores. Devices under test From hardware perspective, which factor is more important for a given workload? That depends on the nature of the selected test app: For compute intensive workloads, CPU micro architecture(multi-issue, OOO, branch predictor…), frequency(base and turbo), L1 L2 cache are the most important factors. For memory intensive workloads, L3 cache and DDR bandwidth sometimes are more important. For IO intensive workloads, normally CPU is not the problem, but the key is how fast the IO subsystem can be, multi-queue technology is often used to improve the IO performance to make better use of a multi-core system. Most real world workloads are a mix type of the above 3 types, typically a server app waits for network IO to receive requests, the requests go through Kernel TCP/IP stack and then are delivered to app to process, the app invokes \"compute\" functions that normally use in memory data structures, and corresponding algorithm to do the real job. The real job part is often compute and memory intensive, depends on how fast the requests come and wait in queues to be served. Therefor we will further focus on the \"real job\" part. It is usually about how fast the CPU processes the instructions and how fast the data can be retrieved from the cache and memory system. Below is the comparison of the CPU architecture that to be evaluated for json performance: Machine X86-laptop X86-workstation MIPS CPU Intel(R) Core(TM) i5-8350U Intel(R) Xeon(R) E7- 8837 Cavium Octeon III Freq 1.70GHz base/ turbo to 3.58GHz 2.67 GHz/ no turbo 1.2GHz/ no turbo Issue width 4 way 4 way 2 way Execution mode Out of Order Out of Order In Order SIMD AVX SSE SSE NONE Core # 4 32 4 L1 cache dcache 32K, icache 32K dcache 32K, icache 32K dcache 32K, icache 78K L2 cache 256K 256K 512k L3 cache 6M 24M NA DDR Channel # 1 uesd/ 2max ? Typical 4 1 Take goroutine into account We want to focus on single thread single core app, then expands a bit more to multi-thread, however it is fairly hard in Golang to do so, because Go has goroutines. Typically, Go runtime starts several threads on which go routines are scheduled to run by go runtime scheduler, go routines can be started by program by using go key word in your code, or can be started by go runtime like GC routines. For example, in json_load benchmark(detailed in the following sections): there are 6 threads, with 8 goroutines running on top of them. So in the rest of this article, I decide to run go programs in 2 methods: A. run with taskset -c 1 to force it running on 1 core B. free run the program, let go runtime to manage the threads. For X86 vs MIPS comparison, only run A method, as we want to know X86 single core VS MIPS single core. For Go vs C comparison, we focuse on B method, and we also look at A method. Json performance X86 vs MIPS comparison I am using built-in json benchmark. run official json benchmark Gc go toolchain has a test framework that all of its packages have built-in tests, encoding/json as the \"official\" json parser also follows the principle, there are many sub-tests of test and benchmarking.The benchmark takes the input test.json file, and test many operations on the data: encoding, decoding…. The input file test.json is a complicated recursive json file, with 1.9M size. We want ro run it in single core, so: taskset -c 1 ./jsontest -test.bench .* The run time snapshot is something looks like this: 1 core running nearly 100% in user space. The behavior is the same on MIPS, so we can make sure the comparison is apple to apple. the result Writing test code for C vs Go comparision Now that we have a roughly idea of how much slower MIPS than X86 running json in Go, that's may be a reference of the performance loss about running OnuMgmt on the cloud VS on the device. But since we will eventually have our code running on the device, for LT board it is MIPS, regardless what code it is, Go or C/C++… So we got to know what is the performance comparison of Go VS C? There are indeed some programing language comparison data on the internet: some good references are: Go java python lua C/C++ Go vs C++ Go vs python I've got the feeling that Go is a bit slower than C/C++, but much much faster than python. It is all about on X86, then how about on MIPS? So we decided to write a simple json benchmark and run it on our MIPS board. We reuse the input json file test.json, what the simple benchmark does is it opens and reads the json file, parsing json into its internal in memory data structures, and then converts the data structures back to json strings, and outputs back to stdout. For C implementation, we use libjansson which is used by reborn platform widely for json parsing. For Go implementation, we use official encoding/json package in GC go toolchain. When benchmarking, run the program enough times and count the time spent as the performance index. compile and run the benchmark on MIPS board For C implementation, we need libjansson #install libjansson, we are on Gentoo linux, we use emerge #for X86, it is 'apt install libjansson-dev' emerge -av dev-libs/jansson #compile, into a.out gcc -O2 json_load.c -ljansson #benchmark it time ./a.out test.json 100 > /dev/null For Go implementation, no other package needed. #compile go build json_load.go #run, Go run time will automatically starts several threads time ./json_load -fileName test.json -loopNum 100 > /dev/null #force single core taskset -c 1 time ./json_load -fileName test.json -loopNum 100 > /dev/null runtime status There are 6 threads(starts with M), with 8 goroutines(starts with G) running on top of them. P represents Processor on the system. json_load is a simple benchmark that has no awareness that it is multi-threaded, in the above 8 goroutines(sometimes more depends on the runtime), there is only one that does the \"user\" work: encoding and decoding the json file. The runtime resource comparison shows that Go has more memory footprint than C, which is nearly triple in huge json file case, while in the small json file case, C only requires 412K on MIPS, that says the json parsing itself is not memory consuming, but Go requires additional memory for its runtime managed data and thus consumes 6772K. Go also has higher CPU usage, also because Go has runtime scheduler and Gc routines running together with the \"user\" routine. The CPU overhead on MIPS is considerably higher than X86. I would say the extra static size and about 30% of a single core overhead is the Go \"tax\" to use this modern programming language on MIPS. result Overall observation For Go builtin json test, MIPS is roughly 7 times slower than X86 @ real frequency For Go builtin json test, MIPS is roughly 3 times slower than X86 @ 1GHz frequency, by calculation For json parsing test between C and Go X86 go json package is highly optimized, even faster than C implementation libjansson MIPS go json package is less optimized, but still on par with C:libjansson "},"notes/golang_gentoo_on_mips_board_and_build_go.html":{"url":"notes/golang_gentoo_on_mips_board_and_build_go.html","title":"Gentoo on mips board and build go","keywords":"","body":" 准备环境 chroot 更新Gentoo build go toolchain 先在x86上build bootstrap for mips 后在板子上编译go toolchain hello.go 和 cgotest.go go env 截图 准备环境 从以下地址下载stage3: 2014是最后更新的, 说明mips已经5年没人更新了. http://distfiles.gentoo.org/experimental/mips/stages/mips64/2014 git clone portage库 https://anongit.gentoo.org/git/repo/gentoo.git #在mint虚拟机上, 打开相关服务和转发 sudo sysctl -w net.ipv4.ip_forward=1 sudo iptables -t nat -A POSTROUTING -j MASQUERADE -s 192.168.2.12/32 -o enp0s10 cat /etc/exports /home/yingjieb/work 192.168.2.0/24(rw,sync,insecure,no_subtree_check,no_root_squash,fsid=0) sudo systemctl start nfs-kernel-server.service #用root操作 sudo -s cd ~/work/nfsroot mkdir mipsroot tar xvf stage3-mips64_multilib-20140904.tar.bz2 -C mipsroot #把portage库拷到usr/portage, 带.git一起拷, 后面还要操作 cp -a gentoo.git mipsroot/usr/portage #到mipsroot目录下操作, 这也是板子的rootfs cd mipsroot 板子起到linux, 配好网络 #板子上操作, 192.168.2.11是mint的ip, 直连的 ifconfig agl0 192.168.2.12 up ifconfig eth-mgnt 192.168.2.12 up route add default gw 192.168.2.11 /etc/init.d/S50dropbear start #mount nfs4 mkdir -p /root/remote mount -t nfs4 192.168.2.11:nfsroot/mipsroot /root/remote -o nolock //nfsv4 mount 192.168.2.11:/home/yingjieb/work/nfsroot/mipsroot /root/remote -o nolock //nfsv3是这么写 #准备chroot mount /dev/pts -o remount,gid=5 ln -s /proc/self/fd /dev/fd cd /root #mount mount -o bind /dev remote/dev mount -o bind /dev/pts remote/dev/pts mount -o bind /proc remote/proc mount -o bind /sys remote/sys mount -o bind /run remote/run #umount umount remote/dev/pts umount remote/dev umount remote/proc umount remote/sys umount remote/run umount remote chroot #最好ssh到板子操作 ssh root@192.168.2.12 chroot remote /bin/bash #一次性操作 #网络 ping 135.245.48.34 route add default gw 192.168.2.11 echo \"nameserver 172.24.213.251\" > /etc/resolv.conf #时间 date -s 20190705 #profile, 重要 cd /etc/portage ln -sf ../../usr/portage/profiles/default/linux/mips/13.0/multilib/n64 make.profile #一般操作 export http_proxy=\"http://135.245.48.34:8000\" export https_proxy=$http_proxy export ftp_proxy=$http_proxy export rsync_proxy=$http_proxy # gentoo使用 #留core0跑网络 taskset -c 1,2,3 emerge -avtuDN @world 更新Gentoo #etc/portage/make.conf GENTOO_MIRRORS=\"http://distfiles.gentoo.org/ http://bbgentoo.ilb.ru/\" build go toolchain git clone go的源码: git clone https://go.googlesource.com/go go的toolchain编译包括两步: 先用go1.4编bootstrap 用bootstrap再编go高版本的编译器 这么做的原因是, go1.4是c写的, 以后的高版本编译器是go写的. 要编译 >1.4版本的编译器, 先要有go1.4编译器. 先在x86上build bootstrap for mips 在mint机器上, 先编译go1.4 git clone https://go.googlesource.com/go $HOME/go1.4 cd $HOME/go1.4/src git checkout release-branch.go1.4 ./make.bash 再编译go1.12, 这个版本支持生成cross toolchain git clone https://go.googlesource.com/go $HOME/go cd $HOME/go/src git checkout release-branch.go1.12 env GOROOT_BOOTSTRAP=$HOME/go1.4 ./make.bash 用go1.12编译target上的boot strap toolchain #还是go1.12目录 cd $HOME/go/src env GOOS=linux GOARCH=mips64 ./bootstrap.bash #成功后, 会生成一个目录和一个压缩包 go-linux-mips64-bootstrap go-linux-mips64-bootstrap.tbz 后在板子上编译go toolchain 拷贝go-linux-mips64-bootstrap.tbz到板子上, 并解压 注意: cgo默认开启, 但需要板子上有gcc工具链, 以支持cgo #正常应该git clone https://go.googlesource.com/go #但我是在板子上, 没有git; 所以直接用nfs共享的 #cd到go 1.12源码go-mips64 cd go-mips64/src #编译go 1.12 for mips64 #all.bash会做test, 有功能和性能的测试 env GOROOT_BOOTSTRAP=/root/go-linux-mips64-bootstrap ./all.bash isam-reborn src # env GOROOT_BOOTSTRAP=/root/go-linux-mips64-bootstrap ./all.bash Building Go cmd/dist using /root/go-linux-mips64-bootstrap. Building Go toolchain1 using /root/go-linux-mips64-bootstrap. Building Go bootstrap cmd/go (go_bootstrap) using Go toolchain1. Building Go toolchain2 using go_bootstrap and Go toolchain1. Building Go toolchain3 using go_bootstrap and Go toolchain2. Building packages and commands for linux/mips64. hello.go 和 cgotest.go root@yingjieb-VirtualBox ~/work/nfsroot/mipsroot.go.ok/root Linux Mint 19.1 Tessa # cat hello.go package main import \"fmt\" func main() { fmt.Println(\"Hello, World!\") fmt.Println(\"First go program on cfnt-b!\") } root@yingjieb-VirtualBox ~/work/nfsroot/mipsroot.go.ok/root Linux Mint 19.1 Tessa # cat cgotest.go package main //int Add(int a, int b){ // return a+b; //} import \"C\" import \"fmt\" func main() { a := C.int(10) b := C.int(20) c := C.Add(a, b) fmt.Println(c) // 30 } go env Linux Mint 19.1 Tessa # ls bin go-linux-mips64 go-linux-mips64-bootstrap go-linux-mips64-bootstrap.tbz src isam-reborn ~ # go env GOARCH=\"mips64\" GOBIN=\"\" GOCACHE=\"/root/.cache/go-build\" GOEXE=\"\" GOFLAGS=\"\" GOHOSTARCH=\"mips64\" GOHOSTOS=\"linux\" GOOS=\"linux\" GOPATH=\"/root\" GOPROXY=\"\" GORACE=\"\" GOROOT=\"/root/go-linux-mips64\" GOTMPDIR=\"\" GOTOOLDIR=\"/root/go-linux-mips64/pkg/tool/linux_mips64\" GCCGO=\"gccgo\" GOMIPS64=\"hardfloat\" CC=\"gcc\" CXX=\"g++\" CGO_ENABLED=\"1\" GOMOD=\"\" CGO_CFLAGS=\"-g -O2\" CGO_CPPFLAGS=\"\" CGO_CXXFLAGS=\"-g -O2\" CGO_FFLAGS=\"-g -O2\" CGO_LDFLAGS=\"-g -O2\" PKG_CONFIG=\"pkg-config\" GOGCCFLAGS=\"-fPIC -mabi=64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build357009098=/tmp/go-build -gno-record-gcc-switches\" 截图 参考: https://dave.cheney.net/2015/10/16/bootstrapping-go-1-5-on-non-intel-platforms https://golang.org/doc/install/source https://golang.org/cmd/cgo/ "},"notes/as_title_golang4.html":{"url":"notes/as_title_golang4.html","title":"微服务","keywords":"","body":"如题 "},"notes/golang_micro.html":{"url":"notes/golang_micro.html","title":"go-micro和micro","keywords":"","body":" micro server命令流程 makefile main.go cmd 顶层Action server子命令 log ./micro service api执行流程 service命令入口, 代码在cmd/service/service.go api命令入口 总结 urfave的cli使用 新版micro搭建blog服务 建立运行环境 新建posts 服务 写main.go 写handler.go 增加save功能 新版micro 依赖 要先启动server, 再login上去 micro server会默认启动一些服务 run hello world service 调用hello world服务 cli方式 rest API方式 自己写个client 总结 新建service 使用micro new新建个工程 storage服务 每个service都有自己的table 用update命令重新run一个服务 内置config命令 更新2021.6.18 分叉 v2-to-v3-upgrade-guide v3版本也是搞micro run这一套 go micro的网络设计理念 更新2020.11 介绍 框架 服务发现 异步消息 消息编码 其他接口 使用micro模板 生成工程模板代码 安装依赖 运行工程 micro server命令流程 makefile micro的makefile很简单 build: go build -a -installsuffix cgo -ldflags \"-s -w ${LDFLAGS}\" -o $(NAME) 会展开成 $ make build go build -a -installsuffix cgo -ldflags \"-s -w -X github.com/micro/micro/v3/cmd.BuildDate=1624082635 -X github.com/micro/micro/v3/cmd.GitCommit=870f80e7 -X github.com/micro/micro/v3/cmd.GitTag=v3.3.0\" -o micro 注: -a是强制所有包都重编 -ldflags \"-X ...这一堆是给包的\"全局变量\"赋值 main.go main.go的思路和gshellos building很像 通过import的package的init函数来注册, main里面只调用cmd.Run() package main //go:generate ./scripts/generate.sh import ( \"github.com/micro/micro/v3/cmd\" // load packages so they can register commands _ \"github.com/micro/micro/v3/cmd/cli\" _ \"github.com/micro/micro/v3/cmd/server\" _ \"github.com/micro/micro/v3/cmd/service\" _ \"github.com/micro/micro/v3/cmd/usage\" ) func main() { cmd.Run() } 注: go generate命令是独立的, go build不会默认先调用go generate. 之所以这里在makefile里面没有调用go generate命令, 可能是因为生成代码这个步骤是开发者手动完成的 cmd 使用了urfave的cli框架 urfave的cli框架里, 命令是以树的形式组织的:子命令先注册到上级命令, 然后顶层命令的run函数, 会找到合适的子命令来run; 如果没找到, 就调用本层命令的Action函数. cmd包全局变量DefaultCmd Cmd = New()之后, 就调用DefaultCmd.Run()来开始命令行解析. 顶层Action 上面说过, 没有匹配的子命令的时候, 就调用本次的Action. 那么顶层的Action被调用到的时候, 说明用户输入的不是子命令, 按照micro的设计, 而是个自定义服务名. 这里的基本逻辑是查找这个服务名, 调用服务. server子命令 按照教程, 所有micro都要依赖micro服务. ./micro server 这个server是个子命令, 被注册到DefaultCmd的子命令列表中 command := &cli.Command{ Name: \"server\", Usage: \"Run the micro server\", Description: `Launching the micro server ('micro server') will enable one to connect to it by setting the appropriate Micro environment (see 'micro env' && 'micro env --help') commands.`, Flags: []cli.Flag{ &cli.StringFlag{ Name: \"address\", Usage: \"Set the micro server address :10001\", EnvVars: []string{\"MICRO_SERVER_ADDRESS\"}, }, &cli.StringFlag{ Name: \"image\", Usage: \"Set the micro server image\", EnvVars: []string{\"MICRO_SERVER_IMAGE\"}, Value: \"micro/micro:latest\", }, }, Action: func(ctx *cli.Context) error { Run(ctx) return nil }, } server的Action动作是启动如下服务 services = []string{ \"registry\", // :8000 \"broker\", // :8003 \"network\", // :8443 \"runtime\", // :8088 \"config\", // :8001 \"store\", // :8002 \"events\", // :unset \"auth\", // :8010 \"proxy\", // :8081 \"api\", // :8080 } 按照micro service [name]的形式, 注意这里的命令的关键词是service for 每个在上面services列表中的service { // all things run by the server are `micro service [name]` cmdArgs := []string{\"service\"} cmdArgs = append(cmdArgs, service) // runtime based on environment we run the service in args := []runtime.CreateOption{ runtime.WithCommand(os.Args[0]), runtime.WithArgs(cmdArgs...), runtime.WithEnv(env), runtime.WithPort(port), runtime.WithRetries(10), runtime.WithServiceAccount(\"micro\"), runtime.WithVolume(\"store-pvc\", \"/store\"), runtime.CreateImage(context.String(\"image\")), runtime.CreateNamespace(\"micro\"), runtime.WithSecret(\"MICRO_AUTH_PUBLIC_KEY\", auth.DefaultAuth.Options().PublicKey), runtime.WithSecret(\"MICRO_AUTH_PRIVATE_KEY\", auth.DefaultAuth.Options().PrivateKey), } // NOTE: we use Version right now to check for the latest release muService := &runtime.Service{Name: service, Version: \"latest\"} //真正的启动内置service runtimeServer.Create(muService, args...) } 这里有两种类型的runtime: local kubernetes 我们这里走的是local. local的Create函数在 service/runtime/local/local.go func (r *localRuntime) Create(resource runtime.Resource, opts ...runtime.CreateOption) error { 这个Create是个通用的接口, 可以新建比如namespace, NetworkPolicy, 也可以是Service // create new service service := newService(s, options) // 先建log文件, 一般在 /tmp/micro/logs/runtime.log f, err := os.OpenFile(logFile(service.Name), os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644) // start the service err := service.Start() } 注意最后service.Start()是启动一个独立的进程, 这个进程运行了内置的service 这里执行的命令是:./micro service api p, err := s.Process.Fork(s.Exec) cmd := exec.Command(exe.Package.Path, exe.Args...) err := cmd.Start() log micro的log都放在/tmp/micro/logs var ( // The directory for logs to be output LogDir = filepath.Join(os.TempDir(), \"micro\", \"logs\") // The source directory where code lives SourceDir = filepath.Join(os.TempDir(), \"micro\", \"uploads\") ) 比如我的实际例子: $ ls /tmp/micro/logs/ api.log auth.log broker.log config.log events.log network.log proxy.log registry.log runtime.log store.log ./micro service api执行流程 service命令入口, 代码在cmd/service/service.go 命令行入口: 注意这个入口是在下级命令的情况下才会调用的. 所以我看了半天实际是执行不到的. : ( // Run starts a micro service sidecar to encapsulate any app func Run(ctx *ccli.Context) { // new service srv := service.New(opts...) // create new muxer // muxer := mux.New(name, p) // set the router srv.Server().Init( server.WithRouter(p), ) // run service srv.Run() } api命令入口 api命令的入口实际上是 var srvCommands = []srvCommand{ { Name: \"api\", Command: api.Run, Flags: api.Flags, }, ... } 对应代码: service/api/server/server.go 默认值就是这里的8080 var ( Name = \"api\" Address = \":8080\" Handler = \"meta\" Resolver = \"micro\" APIPath = \"/\" ProxyPath = \"/{service:[a-zA-Z0-9]+}\" Namespace = \"\" ACMEProvider = \"autocert\" ACMEChallengeProvider = \"cloudflare\" ACMECA = acme.LetsEncryptProductionCA ) api命令的入口就是: func Run(ctx *cli.Context) error { // initialise service srv := service.New(service.Name(Name)) // create a new api server with wrappers api := httpapi.NewServer(Address) // initialise api.Init(opts...) // register the handler api.Handle(\"/\", h) // Start API if err := api.Start(); err != nil { log.Fatal(err) } // Run server if err := srv.Run(); err != nil { log.Fatal(err) } // Stop API if err := api.Stop(); err != nil { log.Fatal(err) } } 我这里./micro server总是出错, 看log是因为: api这个服务的8080端口被占用了. 2021-06-20 05:20:44 file=server/server.go:341 level=fatal service=api listen tcp :8080: bind: address already in use 这个端口是写死的. 虽然有命令行参数可以改, 但是这个命令行是micro server传入写死的... 这似乎是个死局, 初非直接改代码. 解决办法很简单, 直接改个port: $ git diff diff --git a/service/api/server/server.go b/service/api/server/server.go index 2bcd7651..69f7f31a 100644 --- a/service/api/server/server.go +++ b/service/api/server/server.go @@ -39,7 +39,7 @@ import ( var ( Name = \"api\" - Address = \":8080\" + Address = \":18080\" Handler = \"meta\" Resolver = \"micro\" APIPath = \"/\" 总结 每个service都使用了micro的框架, 都以独立进程的形式存在. urfave的cli使用 注册的action会被调用: 比如 func Run(ctx *cli.Context) error { ... } ctx.String(\"server_name\"): 获取命令行的string类型的flag值 ctx.Bool(\"enable_acme\"): 类似的, 获取Bool类型的值 新版micro搭建blog服务 建立运行环境 先run micro server micro server 看看当前环境 $ micro env * local 127.0.0.1:8081 Local running micro server dev proxy.m3o.dev Cloud hosted development environment platform proxy.m3o.com Cloud hosted production environment 可能需要先运行micro env set local来建立local的环境 新建posts 服务 $ micro new posts $ ls posts Dockerfile Makefile README.md generate.go go.mod handler main.go proto 改proto先 syntax = \"proto3\"; package posts; service Posts { rpc Save(SaveRequest) returns (SaveResponse) {} rpc Query(QueryRequest) returns (QueryResponse) {} rpc Delete(DeleteRequest) returns (DeleteResponse) {} } message SaveRequest { string id = 1; string title = 2; string slug = 3; string content = 4; int64 timestamp = 5; repeated string tags = 6; } message SaveResponse { string id = 1; } message Post { string id = 1; string title = 2; string slug = 3; string content = 4; int64 created = 5; int64 updated = 6; string author = 7; repeated string tags = 8; } proto文件有一定的修改, 目的是让命令访问更简单点 不改的话是这样 micro posts save --post_title=Title --post_content=Content 这里想整成这样: micro posts save --title=Title --content=Content 然后make proto就可以生成代码了 写main.go package main import ( \"posts/handler\" \"github.com/micro/micro/v3/service\" \"github.com/micro/micro/v3/service/logger\" ) func main() { // Create the service srv := service.New( service.Name(\"posts\"), ) // Register Handler srv.Handle(handler.NewPosts()) // Run service if err := srv.Run(); err != nil { logger.Fatal(err) } } 写handler.go package handler import ( \"context\" \"time\" \"github.com/micro/dev/model\" \"github.com/micro/go-micro/errors\" \"github.com/micro/micro/v3/service/logger\" \"github.com/micro/micro/v3/service/store\" proto \"posts/proto\" \"github.com/gosimple/slug\" ) type Posts struct { db model.Model idIndex model.Index createdIndex model.Index slugIndex model.Index } func NewPosts() *Posts { createdIndex := model.ByEquality(\"created\") createdIndex.Order.Type = model.OrderTypeDesc slugIndex := model.ByEquality(\"slug\") idIndex := model.ByEquality(\"id\") idIndex.Order.Type = model.OrderTypeUnordered return &Posts{ db: model.New( store.DefaultStore, \"posts\", model.Indexes(slugIndex, createdIndex), &model.ModelOptions{ IdIndex: idIndex, }, ), createdIndex: createdIndex, slugIndex: slugIndex, idIndex: idIndex, } } 现在run这个服务micro run ., 能得到初步的输出 $ micro logs posts Starting [service] posts Server [grpc] Listening on [::]:53031 Registry [service] Registering node: posts-b36361ae-f2ae-48b0-add5-a8d4797508be 增加save功能 增加save函数 func (p *Posts) Save(ctx context.Context, req *proto.SaveRequest, rsp *proto.SaveResponse) error { logger.Info(\"Received Posts.Save request\") post := &proto.Post{ Id: req.Id, Title: req.Title, Content: req.Content, Slug: req.Slug, Created: time.Now().Unix(), } if req.Slug == \"\" { post.Slug = slug.Make(req.Title) } return p.db.Save(post) } 重新run micro update . micro posts save --id=1 --title=\"Post one\" --content=\"First saved post\" micro posts save --id=2 --title=\"Post two\" --content=\"Second saved post\" 新版micro 依赖 依赖protobuf go版本 # Download latest proto releaes # https://github.com/protocolbuffers/protobuf/releases go get github.com/golang/protobuf/protoc-gen-go go get github.com/micro/micro/v3/cmd/protoc-gen-micro 要先启动server, 再login上去 用户名密码是admin和micro micro server $ micro login Enter username: admin Enter password: Successfully logged in. micro server会默认启动一些服务 $ micro services api auth broker config events network proxy registry runtime server store run hello world service 在github.com/micro/services库中, 有很多\"官方\"写好的服务 比如我们要run个hello world micro run github.com/micro/services/helloworld 现在可以看看状态 $ micro status NAME VERSION SOURCE STATUS BUILD UPDATED METADATA helloworld latest github.com/micro/services/helloworld running n/a 4s ago owner=admin, group=micro 看看log $ micro logs helloworld 2020-10-06 17:52:21 file=service/service.go:195 level=info Starting [service] helloworld 2020-10-06 17:52:21 file=grpc/grpc.go:902 level=info Server [grpc] Listening on [::]:33975 2020-10-06 17:52:21 file=grpc/grpc.go:732 level=info Registry [service] Registering node: helloworld-67627b23-3336-4b92-a032-09d8d13ecf95 调用hello world服务 cli方式 还是使用micro命令来调用, 格式是micro [service] [method], 默认的method是call, 参数可以直接命令行传入 $ micro helloworld --name=Jane { \"msg\": \"Hello Jane\" } 查询这个服务能提供什么服务: $ micro helloworld --help NAME: micro helloworld VERSION: latest USAGE: micro helloworld [command] COMMANDS: call 要看call命令的子命令call的使用 $ micro helloworld call --help NAME: micro helloworld call USAGE: micro helloworld call [flags] FLAGS: --name string rest API方式 curl \"http://localhost:8080/helloworld?name=John\" 自己写个client 是个rpc模式的client 实际上这个client本身也是个\"service\"(使用service.New()得来) package main import ( \"context\" \"fmt\" \"time\" \"github.com/micro/micro/v3/service\" proto \"github.com/micro/services/helloworld/proto\" ) func main() { // create and initialise a new service srv := service.New() // create the proto client for helloworld client := proto.NewHelloworldService(\"helloworld\", srv.Client()) // call an endpoint on the service rsp, err := client.Call(context.Background(), &proto.Request{ Name: \"John\", }) if err != nil { fmt.Println(\"Error calling helloworld: \", err) return } // print the response fmt.Println(\"Response: \", rsp.Msg) // let's delay the process for exiting for reasons you'll see below time.Sleep(time.Second * 5) } run这个client: cd example && go mod init example micro run . run的时候不打印, 用status命令能看到 $ micro status NAME VERSION SOURCE STATUS BUILD UPDATED METADATA example latest example.tar.gz running n/a 2s ago owner=admin, group=micro helloworld latest github.com/micro/services/helloworld running n/a 5m59s ago owner=admin, group=micro 看log能够得到其output $ micro logs example # some go build output here Response: Hello John 总结 使用protobuf来封装message client也是service 新建service 使用micro new新建个工程 配套目录, proto定义, Makefile自动生成 $ micro new helloworld Creating service helloworld . ├── main.go ├── generate.go ├── handler │ └── helloworld.go ├── proto │ └── helloworld.proto ├── Dockerfile ├── Makefile ├── README.md ├── .gitignore └── go.mod download protoc zip packages (protoc-$VERSION-$PLATFORM.zip) and install: visit https://github.com/protocolbuffers/protobuf/releases download protobuf for micro: go get -u github.com/golang/protobuf/proto go get -u github.com/golang/protobuf/protoc-gen-go go get github.com/micro/micro/v3/cmd/protoc-gen-micro compile the proto file helloworld.proto: cd helloworld make proto 根据提示, 改了proto后, make proto来生成go代码 storage服务 没错, 永久存储被内置成了服务, 并配套了专有命令 写key value对$ micro store write key1 value1 读key$ micro store read key1 val1 $ micro store read -v key1 KEY VALUE EXPIRY key1 val1 None pattern读, -p选项$ micro store read --prefix --verbose key KEY VALUE EXPIRY key1 val1 None key2 val2 None 每个service都有自己的table 用--table指定table micro store write --table=example mykey \"Hi there\" 可以在client代码里读key package main import ( \"fmt\" \"time\" \"github.com/micro/micro/v3/service\" \"github.com/micro/micro/v3/service/store\" ) func main() { srv := service.New(service.Name(\"example\")) srv.Init() records, err := store.Read(\"mykey\") if err != nil { fmt.Println(\"Error reading from store: \", err) } if len(records) == 0 { fmt.Println(\"No records\") } for _, record := range records { fmt.Printf(\"key: %v, value: %v\\n\", record.Key, string(record.Value)) } time.Sleep(1 * time.Hour) } 用update命令重新run一个服务 比如之前已经在run的example服务(实际是hello world的client), 改了代码要重新run, 用 micro update . 使用最近代码重run 也可以先kill, 再run micro kill example micro run . 内置config命令 支持类似map式的set $ micro config set key val $ micro config get key val $ micro config set key.subkey val $ micro config get key.subkey val $ micro config get key {\"subkey\":\"val\"} $ micro config set key.othersubkey val2 $ micro config get key {\"othersubkey\":\"val2\",\"subkey\":\"val\"} 用client代码来获取config package main import ( \"fmt\" \"github.com/micro/micro/v3/service\" \"github.com/micro/micro/v3/service/config\" ) func main() { // setup the service srv := service.New(service.Name(\"example\")) srv.Init() // read config value val, _ := config.Get(\"key.subkey\") fmt.Println(\"Value of key.subkey: \", val.String(\"\")) } 更新2021.6.18 分叉 分叉为两个分支: https://github.com/micro/micro: 统一的库, 包括原go-micro(在micro/service目录下, 是原go-micro的拷贝) 这个版本使用了Polyform Shield许可证, 禁止和开源作者竞争 https://github.com/asim/go-micro: 原go-micro, 使用个人地址, 也是v3版本, 使用apache协议 已经没有github.com/micro/go-micro 地址了, 自动跳转到https://github.com/asim/go-micro 创始人(asim)两个库都在维护, 但更多的精力放在了另外一个库中:https://github.com/micro/services 官方的说法是v2-to-v3-upgrade-guide: go-micro已经\"now deprecated\", 由asim个人维护. v2-to-v3-upgrade-guide srv := micro.NewService( micro.Name(\"go.micro.service.foo\") ) 变为 srv := service.New( service.Name(\"foo\") ) v3版本也是搞micro run这一套 要先起个server: 使用命令micro server, 没有server的环境, 可以使用免费的M3O环境: micro env set platform go micro的网络设计理念 从Building a global services network using Go, QUIC and Micro看过来的. 更新2020.11 go micro换了地址 老地址:github.com/micro/go-micro 新地址:https://github.com/asim/nitro 访问老地址会自动跳到新地址 FAQ中说 go-micro重命名为Nitro, 现在由个人维护; 原组织github.com/micro现在加倍下注(doubling down)在Micro项目, 这个项目会集大成 License从Apache 2.0换到了Polyform Noncommercial go-plugins现在地址是github.com/asim/nitro-plugins, 虽然是Apache协议, 但用了Nitro, 所以也不能商用 Nitro的目标是不引入外部依赖, 外部依赖由Nitro Plugins解决. -- 纯框架 defualt的top level services初始化被移出了. 作者认为设置default初始化不好 cmd包也被移出了. 作者认为这部分代码引入了复杂的依赖代码.难于维护. 作者推荐使用google的生成依赖初始化项目wire 介绍见blog 不同于基于reflection的 Uber's dig and Facebook's inject, wire使用代码生成技术, 类似java的 Dagger 2 基本上是代码里声明依赖, 用go generate调用wire生成代码. 作者认为micro和nitro的区别是, 前者现在是大一统的方案, 目标是云; 后者是作者自己维护的框架, 目标是edge, IOT, 嵌入式等. 原来的go-micro开发怎么继续? 答: 使用Micro和m3o.com which starts as a purely free Dev environment in the cloud. go-micro v2还能用吗? 答: 可以. v2还是Apache许可证. import github.com/micro/go-micro/v2 github会自动重定向到https://github.com/asim/nitro 补充: Micro项目的License也换了. 但同样的, 可以用v2版本 介绍 本文介绍go的开源微服务框架https://github.com/micro/go-micro. 原文链接 Micro in Action, Part 2: An Ultimate Guide for Bootstrap Micro In Action, Part 3: Calling a Service Micro In Action, Part 4: Pub/Sub Micro In Action, Part 5: Message Broker Micro In Action, Part 6: Service Discovery Micro In Action, Part 7: Circuit Breaker & Rate Limiter Micro In Action, Coda: Distributed Cron Job The index page of Micro In Action Micro有两个库: go-micro 核心库. 典型的是用gRPC micro 辅助工具集, 比如工程模板生成, 运行状态检查, 微服务调用. 基于go-micro 还有一个重要库: go-plugins 自定义扩展, 比如提供了transport protocols的扩展选择. go-micro是plugin的思路, 不同的扩展可以自由组合. 框架 go-mirco对通用的分布式微服务做了interface抽象.其中service是核心, 负责协调其他interfaces 服务发现 服务发现定义为如下的interface, 只要实现了这些, 就能被框架使用. github.com/micro/go-micro/v2/registry/Registry // The registry provides an interface for service discovery // and an abstraction over varying implementations // {consul, etcd, zookeeper, ...} type Registry interface { Init(...Option) error Options() Options Register(*Service, ...RegisterOption) error Deregister(*Service) error GetService(string) ([]*Service, error) ListServices() ([]*Service, error) Watch(...WatchOption) (Watcher, error) String() string } 实际上, go-plugin库已经有很多实现了, 比如etcd/consul/zookeeper, 默认是多播DNS(mDNS), 不需要配置, 开箱即用. 异步消息 异步消息定义如下: github.com/micro/go-micro/v2/broker/Broker // Broker is an interface used for asynchronous messaging. type Broker interface { Init(...Option) error Options() Options Address() string Connect() error Disconnect() error Publish(topic string, m *Message, opts ...PublishOption) error Subscribe(topic string, h Handler, opts ...SubscribeOption) (Subscriber, error) String() string } 已经实现的broker有: RabbitMQ, Kafka, NSQ, 默认的使用http. 消息编码 github.com/micro/go-micro/v2/codec/Codec // Codec encodes/decodes various types of messages used within go-micro. // ReadHeader and ReadBody are called in pairs to read requests/responses // from the connection. Close is called when finished with the // connection. ReadBody may be called with a nil argument to force the // body to be read and discarded. type Codec interface { Reader Writer Close() error String() string } 目前有json bson msgpack等实现. 其他接口 Server， define the server of microservices Transport， defines the transport protocol Selector，abstracts logic of service selection. you can implement various load balancing strategies with this interface Wrapper，defines middleware which can wrap server/client request go-micro对微服务的抽象很\"正交\"(orthoganal), 比较全面. 使用micro模板 生成工程模板代码 下载micro工具 GO111MODULE=on go get github.com/micro/micro/v2@v2.4.0 创建一个模板工程 micro new --namespace=com.foo --gopath=false hello micro new, create a gRPC service by running the new sub-command of the micro command-line tool hello, specify the service name --namespace=com.foo, provide a namespace to the service --gopath=false, generate code into the current directory instead of $GOPATH (since Golang supports Go Module, new projects should be placed outside of $GOPATH) 命令执行完毕后, 在当前目录会创建工程代码: Creating service com.foo.srv.hello in hello . ├── main.go ├── generate.go ├── plugin.go ├── handler │ └── hello.go ├── subscriber │ └── hello.go ├── proto/hello │ └── hello.proto ├── Dockerfile ├── Makefile ├── README.md ├── .gitignore └── go.mod download protobuf for micro: brew install protobuf go get -u github.com/golang/protobuf/{proto,protoc-gen-go} go get -u github.com/micro/protoc-gen-micro/v2 compile the proto file hello.proto: cd hello protoc --proto_path=.:$GOPATH/src --go_out=. --micro_out=. proto/hello/hello.proto 注意到一个Makefile文件生成了 安装依赖 主要是安装protobuf # install protobuf brew install protobuf # install protoc-gen-go go get -u github.com/golang/protobuf/{proto,protoc-gen-go} # install protoc-gen-micro GO111MODULE=on go get -u github.com/micro/protoc-gen-micro/v2 protoc-gen-micro是protobuf的micro插件 注: protobuf项目的go版本现在转到: https://github.com/protocolbuffers/protobuf-go 之前是golang team维护的 https://github.com/golang/protobuf 运行工程 首先要get go-micro go get github.com/micro/go-micro/v2@v2.4.0 这样go.mod会是 module hello go 1.14 require github.com/micro/go-micro/v2 v2.4.0 运行 make build && ./hello-service 得到如下输出 make build && ./hello-serviceprotoc --proto_path=. --micro_out=Mgithub.com/micro/go-micro/api/proto/api.proto=github.com/micro/go-micro/v2/api/proto:. --go_out=Mgithub.com/micro/go-micro/api/proto/api.proto=github.com/micro/go-micro/v2/api/proto:. proto/hello/hello.protogo build -o hello-service *.go2020-04-02 11:12:47 level=info Starting [service] go.micro.service.hello 2020-04-02 11:12:47 level=info Server [grpc] Listening on [::]:53451 2020-04-02 11:12:47 level=info Broker [eats] Connected to [::]:53453 2020-04-02 11:12:47 level=info Registry [mdns] Registering node: go.micro.service.hello-063d6dae-826b-49f5-9141-df525af8a6b1 2020-04-02 11:12:47 level=info Subscribing to topic: go.micro.service.hello 这里make build会先用protoc编译.proto文件, 然后go build, 生成hello-service可执行程序. "},"notes/golang_微服务概念.html":{"url":"notes/golang_微服务概念.html","title":"微服务概念","keywords":"","body":" 传统的monolithic设计 微服务 微服务通信 API网关 调用service 服务发现 部分失效 微服务IPC 消息格式 微服务API 处理部分失效 本文是2015年5月, Nginx社区的一篇微服务介绍文章的阅读笔记. 原文链接 文章中假设你要从头设计一个打车的系统, 来和uber竞争 传统的monolithic设计 这个框架简单,单一形式部署容易. 绝大部分应用都是这个思路. 但随着软件的长期维护, 这个模式不好维护. 微服务 一个微服务单独开发和部署, 提供独特的功能. 通常使用容器化技术来部署. Each service has a well‑defined boundary in the form of an RPC‑ or message‑driven API. 好处是分而治之, 但加大了通信复杂性 微服务通信 一个client想使用某个服务, 它可以直接访问该微服务 Each microservice would have a public endpoint (https://_serviceName_.api.company.name). 但通常不这样做, 因为client也要关心和不同的微服务打交道, 也难适应变化. API网关 通常client通过API网关来访问微服务. Usually a much better approach is to use what is known as an API Gateway. An API Gateway is a server that is the single entry point into the system. It is similar to the Facade pattern from object‑oriented design. The API Gateway encapsulates the internal system architecture and provides an API that is tailored to each client. It might have other responsibilities such as authentication, monitoring, load balancing, caching, request shaping and management, and static response handling. The API Gateway is responsible for request routing, composition, and protocol translation. All requests from clients first go through the API Gateway. It then routes requests to the appropriate microservice. The API Gateway will often handle a request by invoking multiple microservices and aggregating the results. It can translate between web protocols such as HTTP and WebSocket and web‑unfriendly protocols that are used internally. 关键词: 包装内部服务对外呈现 鉴权 监控 负载均衡 缓存 请求整形 静态响应 请求转发, 响应聚合 调用service 两种风格来调用service: 异步消息. 比如基于massage broker的AMQP, 直接message通信的zeromq 同步调用. 比如RPC HTTP Thrift 通常一个系统中这两个风格都有. 服务发现 API gateway必须能访问Service Registry数据库. 部分失效 The API Gateway should never block indefinitely waiting for a downstream service. 微服务IPC There are the following kinds of one‑to‑one interactions: Request/response – A client makes a request to a service and waits for a response. The client expects the response to arrive in a timely fashion. In a thread‑based application, the thread that makes the request might even block while waiting. Notification (a.k.a. a one‑way request) – A client sends a request to a service but no reply is expected or sent. Request/async response – A client sends a request to a service, which replies asynchronously. The client does not block while waiting and is designed with the assumption that the response might not arrive for a while. There are the following kinds of one‑to‑many interactions: Publish/subscribe – A client publishes a notification message, which is consumed by zero or more interested services. Publish/async responses – A client publishes a request message, and then waits a certain amount of time for responses from interested services. 上面的图例中, trip management发现一个打车请求, 发new trip消息给dispatcher, dispatcher找到一个proposed drvier放到dispatching通道; driver和passenger同时得到通知. REQ-REP模式下, client发送请求, 阻塞等待响应. 在物联网界, 通常用RESTful API(基于HTTP), 这也是阻塞式的. Apache Thrift也是REQ-REP模式. 消息格式 两大类: text类和binery类 前者式json, xml等. 后者是GPB 微服务API API是合同, 但API也注定会变化. 在传统的monolithic模式下, api改了, 每个调用的地方改了就好了. 但微服务场景下, 没有强制手段要求所有的clients都来适配新的API. 所以API要兼容老的, 至少新API和老API都能同时支持. 处理部分失效 一个微服务可能因为故障或维护被down了, 那访问就会不可达. 有时候一个微服务负载太高, 响应很慢. 为了避免一个client因为server的故障而永远等待, 可以: 使用timeout机制 流控, 定义一个limit, 超过limit马上回复 定义一个error比例. 统计成功的请求和error的请求, 大于某个比例就视为服务失效. "},"notes/as_title_golang5.html":{"url":"notes/as_title_golang5.html","title":"解释器和编解码","keywords":"","body":"如题 "},"notes/golang_yeagi.html":{"url":"notes/golang_yeagi.html","title":"解释器yeagi","keywords":"","body":" yeagi执行流程和损耗 先说数据 perf采样 binary topid yaegi topid 优化 interface wrapper 空指针引用panic 解决 reflect出现assignable错误 Methods can not be called by bin go embed和yaegi 嵌入scriptlib.go scriptlib是要解释执行的库 使用时 yaegi如何执行go语句 ast阶段 cfg阶段 run阶段 builtin数组里面, aCall和aCallSlice的action是call case callExpr里面callBin被当作快速路径? yaegi使用 Eval Stop脚本 bin函数执行时不能被cancel 怎么解决呢? 结果 题外 yaegi已知问题 脚本的结构体没有名字 yaegi执行过程 yaegi集成到gshell new 解释器 yaegi使用 编译运行 debug ast和cfg举例图 空敲回车 a := 100 s := \"hello world\" if slice import for range 函数定义 func call 使用例子 动态代码 解释器调用bin符号 -- 代码生成部分 go generate extract包 不用New的对象初始化 Extract方法 go/importer 和 go/types 模板替换 模板 替换结果举例 yaegi 背景知识 AST语法树 1+3*(4-1)+2 xml while for 为什么需要抽象语法树 函数 第一步：词法分析，也叫扫描scanner 第二步：语法分析，也称解析器 main 解释器 解释器调用bin里的符号 使用refect.Value.Call()的例子 查找bin 的method interp/run.go的call()函数 ast的node Eval ast函数 builtin预定义action cfg函数 frame类型 run函数 exec举例 总结 代码生成 REPL循环 EvalWithContext 总结 yeagi执行流程和损耗 同样的go代码topid, 解释执行和编译执行的性能分析和对比如下.原始代码在 https://github.com/godevsig/grepo 先说数据 两个程序做的事情一模一样 跑了一个小时, yaegi的topid实际占用79秒, 平均2.2% 同样一个小时, bin topid实际占用33秒, 平均0.9% 二者大概差两倍, 也就是说对这个case, 解释会增加100%的成本 perf采样 perf record -g -p 18149 -- sleep 60 perf script | /repo/yingjieb/FlameGraph/stackcollapse-perf.pl | /repo/yingjieb/FlameGraph/flamegraph.pl > topid.svg binary topid 实际干活是蓝色框 被pmu中断会算在进程时间里 yaegi topid yaegi的解释执行执行是个递归的过程, 具体来说就是interp.runCfg和interp.call.func6的递归 出现很多fmt.Fprintf和fmt.Sprintf是在递归打印进程信息. 这个过程要不断解释, 递归执行. 真正的printf只占很小比例, 很多时间花在了interp.runCfg和interp.call.func6的递归中, 即: 时间花在了\"解释\"代码而非\"执行\"代码 优化 尽量避免解释, 但又不要把太多\"应用逻辑\"塞到bin里. 目前来看, 如果showPidInfo解释起来成本稍高, 但如果是网络传输, appendProcessInfo函数应该好点. 结论就是保持现状, 不增加bin call api. interface wrapper yaegi对于带方法的iface, 都自动生成了一个interface wrapper 比如stdlib的flag.go: // _flag_Value is an interface wrapper for Value type type _flag_Value struct { IValue interface{} WSet func(a0 string) error WString func() string } func (W _flag_Value) Set(a0 string) error { return W.WSet(a0) } func (W _flag_Value) String() string { return W.WString() } 因为bin call不能直接认识解释模式下的类型定义, yaegi的做法是, 定义一个wrapper 结构体, 例如上面的_flag_Value, 这个结构体实现了规定的方法; 估计yaegi在解释的时候, 把需要满足该接口的类型(flag.Value), 包装成_flag_Value. 这样bin call就可以调用脚本的方法了. -- 注意, >描述的是eface(interface{})的处理, 因为fmt传入的是eface, fmt包里面断言其有String() string方法; 但yaegi没有办法把一个eface也包装, 所以不能输出aaaa -- 但这里, 是明确的带方法的iface, 那么yaegi就能wrap 这个iface 空指针引用panic 对应的应用代码 var pids pidValue func main() { flags := flag.NewFlagSet(\"\", flag.ContinueOnError) flags.SetOutput(os.Stdout) flags.Var(&pids, \"p\", \"process id. Multiple -p is supported, but -tree and -child are ignored (default -1)\") flags.BoolVar(&sf.memMB, \"MB\", false, \"show mem in MB (default in KB)\") ... if err = flags.Parse(args); err != nil { if err == flag.ErrHelp { err = nil } return err } } 上面的pid是自定义的满足falg.Value的接口: 注意我加了调试打印 type pidValue []int func (pids *pidValue) String() string { fmt.Println(\"===== string\") return \"[-1]\" } func (pids *pidValue) Set(value string) error { pid, err := strconv.Atoi(value) if err != nil { return err } *pids = append(*pids, pid) return nil } 解释: 首先, pidValue的String()方法能够被调用, 能打印\"===== string\"; 说明yaegi里面方法明确的iface方法是能够被调用的 代码在flags.Parse(args)里面panic 加-h打印help时出现panic 路径显示在flag.isZeroValue有空引用 runtime error: invalid memory address or nil pointer dereference flag.isZeroValue(0xc00028dd00, 0x0, 0x0, 0xc0000f7801) flag.(*FlagSet).PrintDefaults.func1(0xc00028dd00) 对应的代码isZeroValue()是在打印help的过程中被PrintDefaults()调用的: // isZeroValue determines whether the string represents the zero // value for a flag. func isZeroValue(flag *Flag, value string) bool { // Build a zero value of the flag's Value type, and see if the // result of calling its String method equals the value passed in. // This works unless the Value type is itself an interface type. typ := reflect.TypeOf(flag.Value) var z reflect.Value if typ.Kind() == reflect.Ptr { z = reflect.New(typ.Elem()) } else { z = reflect.Zero(typ) } return value == z.Interface().(Value).String() } 普通bool flag定义: flags.BoolVar(&sf.memMB, \"MB\", false, \"show mem in MB (default in KB)\")这个BoolVar对应到isZeroValue()的入参flag是:&flag.Flag{Name:\"MB\", Usage:\"show mem in MB (default in KB)\", Value:(*flag.boolValue)(0xc000728142), DefValue:\"false\"}显示其底层是*flag.boolValue类型. flag.Value iface定义:而flags.Var(&pids, \"p\", \"process id. Multiple -p is supported, but -tree and -child are ignored (default -1)\")因为func (f *FlagSet) Var(value Value, name string, usage string)函数要求value是flag.Value这个iface, 而pids是pidValue满足接口要求, yaegi就\"构造\"了一个wrapper:&flag.Flag{Name:\"p\", Usage:\"process id. Multiple -p is supported, but -tree and -child are ignored (default -1)\", Value:stdlib._flag_Value{IValue:(*[]int)(0xc000444ed0), WSet:(func(string) error)(0x4c5bc0), WString:(func() string)(0x4c5bc0)}, DefValue:\"\"}注意, 此时在isZeroValue()看来, 这个flag就是stdlib._flag_Value类型了但为什么会panic? 需要往下看:为了比较这个flag值是否为零值, z = reflect.Zero(typ)调用后, z的值为零值, 其Wstring方法也是nil:stdlib._flag_Value{IValue:interface {}(nil), WSet:(func(string) error)(nil), WString:(func() string)(nil)}那么在调用的时候, func (W _flag_Value) String() string { return W.WString() }就会引用nil, 从而产生panic 解决 我的解决办法是在代码生成模板里面加nil的check: We have interface wrapper that has funcion wrappers which may be nil in some cases, for example in stdlib flag.go type _flag_Value struct { IValue interface{} WSet func(a0 string) (r0 error) WString func() (r0 string) } WString can be nil in flag package calling PrintDefaults() in which isZeroValue() is called where String() is called on zero value: func isZeroValue(flag *Flag, value string) bool { ... z = reflect.Zero(typ) return value == z.Interface().(Value).String() } This causes runtime error: invalid memory address or nil pointer dereference because WString is nil: func (W _flag_Value) String() string { return W.WString() } The fix is adding a check before calling the func wrapper: func (W _flag_Value) String() (r0 string) { if W.WString == nil { return } return W.WString() } reflect出现assignable错误 出现错误: reflect.Set: value of type interface {} is not assignable to type interp.valueInterface 出错代码在: // messageOut represents a message to be sent type messageOut interface { isMessageOut() } var outputCh chan messageOut = make(chan messageOut, 30) select { case 这里的valueInterface在interp/run.go type valueInterface struct { node *node value reflect.Value } 看字面意思: interface{}的值不能赋值给interp.valueInterface 我理解应该是interp规定的interface在bin里可能是不认的. Methods can not be called by bin If a struct has String() string method, fmt.Printf will call this method in compiled go: type test struct { a int c string } func (t *test) String() string { return \"aaaa\" } func main() { fmt.Println(\"Hello, world!\") fmt.Printf(\"%v\\n\", &test{1, \"nihao\"}) } $ go run hello.go Hello, world! aaaa But gshell output: $ gsh exec hello.go Hello, world! {1 nihao} 说明在解释器模式下, fmt.Printf()是看不到test类型的任何方法的. 实际上, type定义的结构体其实是没有名字的, 这是reflect的限制, 用reflect只能产生匿名的结构体. go embed和yaegi 嵌入scriptlib.go import _ \"embed\" // embed libs //go:embed scriptlib/scriptlib.go var scriptlib string func (sh *shell) loadScriptLib() error { _, err := sh.Eval(scriptlib) if err != nil { return fmt.Errorf(\"load script libs error: %s\", err) } return nil } scriptlib是要解释执行的库 // Package scriptlib should be used as loadable scripts package scriptlib import ( \"fmt\" \"io\" \"net/http\" ) // HTTPGet gets file from url func HTTPGet(url string) ([]byte, error) { resp, err := http.Get(url) if err != nil { return nil, err } defer resp.Body.Close() if resp.StatusCode != 200 { return nil, fmt.Errorf(\"file not found: %d error\", resp.StatusCode) } body, err := io.ReadAll(resp.Body) if err != nil { return nil, err } return body, nil } 使用时 newShell以后, loadScriptLib就是sh.Eval(scriptlib), 就把embed的scriptlib.go解释编译了.v, err := crs.sh.Eval(\"scriptlib.HTTPGet\")就是把文本的符号scriptlib.HTTPGet解释成一个函数, 然后强制转换成native go能够执行的函数:crs.httpGet = v.Interface().(func(url string) ([]byte, error)) crs := &codeRepoSvc{sh: newShell(interp.Options{}), repoInfo: repoInfo} if err := crs.sh.loadScriptLib(); err != nil { return err } v, err := crs.sh.Eval(\"scriptlib.HTTPGet\") if err != nil { return err } crs.httpGet = v.Interface().(func(url string) ([]byte, error)) if err := s.Publish(\"codeRepo\", codeRepoKnownMsgs, as.OnNewStreamFunc(func(ctx as.Context) { ctx.SetContext(crs) }), ); err != nil { return err } yaegi如何执行go语句 ast阶段 // ast parses src string containing Go code and generates the corresponding AST. // The package name and the AST root node are returned. // The given name is used to set the filename of the relevant source file in the // interpreter's FileSet. func (interp *Interpreter) ast(src, name string, inc bool) (string, *node, error) switch nod.(type) case *ast.GoStmt: st.push(addChild(&root, anc, pos, goStmt, aNop), nod) cfg阶段 // cfg generates a control flow graph (CFG) from AST (wiring successors in AST) // and pre-compute frame sizes and indexes for all un-named (temporary) and named // variables. A list of nodes of init functions is returned. // Following this pass, the CFG is ready to run. func (interp *Interpreter) cfg(root *node, importPath, pkgName string) ([]*node, error) switch n.kind { case deferStmt, goStmt: wireChild(n) run阶段 builtin数组里面, aCall和aCallSlice的action是call func call(n *node) { goroutine := n.anc.kind == goStmt n.exec = func(f *frame) bltn { bf := value(f) if bf.IsValid() { // 如果是bin func if goroutine { go bf.Call(in) // go关键词, 起goroutine来调用bf.Call return tnext } out := bf.Call(in) // 直接调用bf.Call ... } //下面是脚本的func执行流程 nf := newFrame(anc, len(def.types), anc.runid()) // 先建立新frame if goroutine { go runCfg(def.child[3].start, nf, def, n) // goroutine执行函数体 return tnext } runCfg(def.child[3].start, nf, def, n) // 直接执行函数体 } runCfg()函数内部已经recover了panic, 但会再次触发panic, 这点不是我们想要的: func runCfg(n *node, f *frame, funcNode, callNode *node) { defer func() { f.mutex.Lock() f.recovered = recover() for _, val := range f.deferred { val[0].Call(val[1:]) } if f.recovered != nil { oNode := originalExecNode(n, exec) if oNode == nil { oNode = n } fmt.Fprintln(n.interp.stderr, oNode.cfgErrorf(\"panic\")) f.mutex.Unlock() panic(f.recovered) } f.mutex.Unlock() }() } 上面逻辑对应的调用栈: panic: test panic [recovered] //对应上面代码的recover panic: test panic //代码里再次panic github.com/traefik/yaegi/interp.runCfg.func1 //对应runCfg的defer执行的函数 panic github.com/traefik/yaegi/interp._panic.func1 //对应脚本里调用panic github.com/traefik/yaegi/interp.runCfg github.com/traefik/yaegi/interp.call.func6 所以: 脚本里调用的panic, 实际上是interp/run.go里的 func _panic(n *node) { value := genValue(n.child[1]) n.exec = func(f *frame) bltn { panic(value(f)) } } case callExpr里面callBin被当作快速路径? // ast阶段代码 case callExpr: case isBinCall(n): n.gen = callBin // Callbin calls a function from a bin import, accessible through reflect. func callBin(n *node) { case n.anc.kind == goStmt: // Execute function in a goroutine, discard results. n.exec = func(f *frame) bltn { in := make([]reflect.Value, l) for i, v := range values { in[i] = v(f) } go callFn(value(f), in) return tnext } } yaegi使用 Eval interp.Eval()可以认为默认是main包, 单独行的Eval()可以执行, 比如: i := interp.New(interp.Options{}) i.Use(stdlib.Symbols) i.Eval(`import \"fmt\"`) i.Eval(`fmt.Println(\"hello\")`) //会打印 hello 但如果待执行的字符串有import动作, 则\"独立\"的语句不执行: i.Eval(`import \"fmt\"; fmt.Println(\"hello\")`) //不会打印hello, 会有如下error _.go:1:28: expected declaration, found fmt 带import动作的脚本, 需要有main函数才能执行: i.Eval(`import \"fmt\"; func main(){fmt.Println(\"hello\")}`) //可以打印 hello 加上package main效果和上面一样: i.Eval(`package main; import \"fmt\"; func main(){fmt.Println(\"hello\")}`) //可以打印 hello 但是加上package other就不会执行main了, 实际上非main包也不应该有main函数 i.Eval(`package module; import \"fmt\"; func main(){fmt.Println(\"hello\")}`) //不会打印hello, 执行不会出错 每次Eval都会执行main 比如我写了个hello.go, 里面是package main, 打印hello package main import ( \"fmt\" ) func main() { fmt.Println(\"Hello, playground~\") } func Stop() { fmt.Println(\"stopping\") } 我想在框架里调用Stop(), 比如这样: sh := newShell(interp.Options{}) sh.runFile(file) // 就是run这个hello.go sh.Eval(\"Stop()\") // 可以执行Stop, 打印stopping, 但Hello, playground~会再打印一遍 // output Hello, playground~ stopping Hello, playground~ 解释: 对main包来说, 每次Eval都会执行main 引用main包的函数, 不能加main前缀, 比如main.Stop, 加了会报undefined selector: Stop 如果hello.go不是main包, 比如是test. 那么 要加test前缀来引用, 比如sh.Eval(\"test.Stop()\") 因为是非main包, 执行sh.Eval(\"test.Stop()\")不会导致main()函数被二次执行 代码解释: 因为在compile阶段, main()函数每次都会被加到node列表里 func (interp *Interpreter) compile(src, name string, inc bool) (*Program, error) { ... // Add main to list of functions to run, after all inits. if m := gs.sym[mainID]; pkgName == mainID && m != nil { initNodes = append(initNodes, m.node) } ... } Stop脚本 bin函数执行时不能被cancel EvalWithContext()函数在cancel()的时候不能停止bin的函数, 但估计可以停止所有脚本的后续执行.比如脚本在执行一个bin的函数http.Serve(lnr, fs), 这个函数会卡住, 一直accept连接来服务http请求. lnr, err = net.Listen(\"tcp\", \":\"+*port) fs := http.FileServer(http.Dir(*dir)) http.Serve(lnr, fs) 即使用了带ctx的Eval函数: ctx, cancel := context.WithCancel(context.Background()) vc.newShell() vc.sh.EvalWithContext(ctx, \"_main()\") 但在cancel()的时候, 虽然EvalWithContext()能退出来, 但是起里面执行的Eval()却无法退出, 因为Eval在单独的goroutine里面正在执行http.Serve(lnr, fs) // EvalWithContext evaluates Go code represented as a string. It returns // a map on current interpreted package exported symbols. func (interp *Interpreter) EvalWithContext(ctx context.Context, src string) (reflect.Value, error) { var v reflect.Value var err error interp.mutex.Lock() interp.done = make(chan struct{}) interp.cancelChan = !interp.opt.fastChan interp.mutex.Unlock() done := make(chan struct{}) go func() { defer close(done) v, err = interp.Eval(src) }() select { case 怎么解决呢? 思路: 在脚本里定义func Stop()函数: var lnr net.Listener var hello = \"nihao\" func Start() (err error) { hello = \"wo ye hao\" lnr, err = net.Listen(\"tcp\", \":\"+*port) ... http.Serve(lnr, fs) } func Stop() { fmt.Println(\"stopping\") fmt.Println(hello) fmt.Println(lnr == nil) lnr.Close() } 然后在cancel()之前, 由解释器来调用Stop函数: switch msg.Cmd { case \"kill\": if vc.stat == vmStatRunning { // no need to atomic fmt.Fprintln(&vc.stderr, \"calling Stop()\") vc.sh.Eval(\"Stop()\") //注意这里, 解释执行脚本里的Stop函数 vc.cancel() atomic.CompareAndSwapInt32(&vc.stat, vmStatRunning, vmStatAborting) ids = append(ids, vc.ID) } 结果 脚本的stop函数会被调用, 而且, 脚本的内部变量还能\"看到\" /repo1/services/http 8088 stopping //注意这里说明stop函数被解释器\"单独\"的调用了 wo ye hao //但是, 这次\"单独\"的调用依然能看到main()函数里的变量的改变. false // 同上, 能看到变量改变 accept tcp [::]:8088: use of closed network connection //这里的错误信息说明, lnr已经被close了 calling Stop() //我重新安排了输出, 按理说这句应该在前面 56:12: panic // 这两个panic我不知道从哪里来的 1:28: panic context canceled 题外 因为这里的例子是http的file server, 我发现以下事实: chrome浏览器打开了http://10.182.105.179:8088连接, 来访问我这里的http服务. 这是个长连接, 即使页面关闭了, 还会一直存在. 所以表面看起来, 脚本stop了还是可以访问http文件, 是因为这个长连接并没有关闭: 下面是stop后的连接netstat -an | grep 8088 tcp6 0 0 192.168.0.18:8088 10.243.141.21:58826 ESTABLISHED 对比stop前的连接, 明显看到listen的socket没有了, 被关闭了netstat -an | grep 8088 tcp6 0 0 :::8088 :::* LISTEN tcp6 0 0 192.168.0.18:8088 10.243.141.21:58826 ESTABLISHED yaegi已知问题 脚本的结构体没有名字 脚本里定义的结构体, 没有名字. 因为reflect创建的结构体都没有名字 package main import ( \"fmt\" \"reflect\" ) func main() { structFields := []reflect.StructField{ { Name: \"Name\", Type: reflect.TypeOf(\"\"), }, } structDec := reflect.StructOf(structFields) fmt.Printf(\"Type Dynamic : %+v\\n\", structDec) fmt.Printf(\"Type Static : %+v\\n\", reflect.TypeOf(struct{ Name string }{})) } 而yaegi是基于reflect来解释执行代码的: type testA struct { A int B string } fmt.Printf(\"%#v\\n\", testA{}) 所以上面的代码打印: struct { A int; B string }{A:0, B:\"\"} 相关的讨论: Representation of types by reflect and printing values using %T may give different results between compiled mode and interpreted mode. https://github.com/traefik/yaegi/issues/947 stackoveflow 结论: 只有bin里的结构体才能有名字. 如果脚本里需要使用结构体的名字, 比如注册结构体到gotiny, 或者定义结构体的方法, 这个结构体应该在native go代码里定义. yaegi执行过程 interp.(*Interpreter).EvalPath() interp.(*Interpreter).importSrc() interp.(*Interpreter).run() //在这之前已经生成了CFG interp.runCfg() //这里这个node isBinCall()为真, 就是as.RegisterType() //对应这句: callFn := func(v reflect.Value, in []reflect.Value) []reflect.Value { return v.Call(in) } interp.callBin.func10() reflect.Value.Call() reflect.Value.call() adaptiveservice.RegisterType() yaegi集成到gshell new 解释器 type Options struct { // GoPath sets GOPATH for the interpreter. GoPath string // BuildTags sets build constraints for the interpreter. BuildTags []string // Standard input, output and error streams. // They default to os.Stdin, os.Stdout and os.Stderr respectively. Stdin io.Reader Stdout, Stderr io.Writer // SourcecodeFilesystem is where the _sourcecode_ is loaded from and does // NOT affect the filesystem of scripts when they run. // It can be any fs.FS compliant filesystem (e.g. embed.FS, or fstest.MapFS for testing) // See example/fs/fs_test.go for an example. SourcecodeFilesystem fs.FS } yaegi使用 Yet Another Go Interpreter, with the E standing for Elegant, Embedded, Easy 入门文章 使用简单, 只有New() Eval() Use()三个API 完全兼容go规范 编译运行 目前只在go1.14以上才能编 cd yaegi make generate cd cmd/yaegi go build //编译出来yaegi又22M 运行 yingjieb@godev-server /repo/yingjieb/3rdparty/yaegi/cmd/yaegi $ ./yaegi -h Yaegi is a Go interpreter. Usage: yaegi [command] [arguments] The commands are: extract generate a wrapper file from a source package help print usage information run execute a Go program from source test execute test functions in a Go package version print version Use \"yaegi help \" for more information about a command. If no command is given or if the first argument is not a command, then the run command is assumed. debug 打印ast和cfg图: YAEGI_AST_DOT=1 YAEGI_CFG_DOT=1 ./yaegi 输入的每一行都会在当前目录下生成, 覆盖前一个; yaegi-cfg-_.dot yaegi-ast-_.dot 这里的小bug是, 会生成一堆的dot进程. 注, dot是linux命令, 用来绘图. dot文件转成svg后用firefox打开 watch -n1 \"dot -Tsvg yaegi-ast-_.dot -o ast.svg; dot -Tsvg yaegi-cfg-_.dot -o cfg.svg\" firefox \"file:///repo/yingjieb/3rdparty/yaegi/cmd/yaegi/ast.svg\" ast和cfg举例图 空敲回车 对应的ast图cfg图为空 a := 100 s := \"hello world\" if > if s == \"hello world\" { a = 200 } : 200 slice > si := []int{1,2,3,4,5} : [1 2 3 4 5] import import \"fmt\" for range > for i, num := range(si) { fmt.Println(i, num)} 函数定义 func myPrint(p ...interface{}) { fmt.Println(p...) } func call > myPrint(\"ni\", \"hao\", \"ma\", 123) 使用例子 命令循环模式 $ yaegi > 1 + 2 3 > import \"fmt\" > fmt.Println(\"Hello World\") Hello World > 嵌入别的代码里: package main import ( \"github.com/containous/yaegi/interp\" \"github.com/containous/yaegi/stdlib\" ) func main() { i := interp.New(interp.Options{}) i.Use(stdlib.Symbols) i.Eval(`import \"fmt\"`) i.Eval(`fmt.Println(\"hello\")`) } This example demonstrates the ability to use executable pre-compiled symbols in the interpreter. Thanks to the statement i.Use(stdlib.Symbols), the interpreted import \"fmt\"will load the fmt package from the executable itself (wrapped in reflect.Values) instead of trying to parse source files. Yaegi also provides the goexports command to build the binary wrapper of any package from its source. This is the command we used to generate all stdlib wrappers provided by default. 注意, stdlib是预编译好的bin. 这个解释器能够执行已经编译好的代码. i.Use(stdlib.Symbols)的作用是把导入编译后的符号给解释器用. 实现的关键是reflect.Values包装. 动态代码 入前文所说, 解释执行和编译执行能够交互. 下面的例子中, 这个文件都是正常编译的. 但里面的v, _ := i.Eval(\"foo.Bar\")却是解释执行的. package main import \"github.com/containous/yaegi/interp\" const src = `package foo func Bar(s string) string { return s + \"-Foo\" }` func main() { i := interp.New(interp.Options{}) i.Eval(src) //v是reflect.Value v, _ := i.Eval(\"foo.Bar\") //需要先把v还原成interface再断言. bar := v.Interface().(func(string) string) r := bar(\"Kung\") println(r) // Output: // Kung-Foo } 解释器调用bin符号 -- 代码生成部分 在yaegi中, 初始化时用map[\"符号名\"]reflect.Value保存bin中的符号信息. 解释执行过程中, 查找符号名, 得到reflect.Value, 对其进行refect.Value.Call()调用, 就像直接调用这个符号一样. 这个过程如下: go generate 先是go generate ./internal/cmd/extract internal/cmd/extract/extract.go中, 首先第一行就是 //go:generate go build 其实就是go build出来一个叫extract的bin程序, 该程序负责使用模板技术生成标准库的wrapper调用 extract包 extract/extract.go包提供了生成wrapper的模板和相关辅助函数 不用New的对象初始化 做为一个package, 一般的入口函数是New一个对象. 但这里一个简单的结构体初始化就搞定了: ext := extract.Extractor{ Dest: path.Base(wd), } //可以根据情况给ext的其他属性field赋值 ext.Exclude = strings.Split(excludeString, \",\") 相对于一般的NewObj()等API来说, 这个模式简单, 直观, 但就是写法上稍显啰嗦 也有NewObj(Options...)的模式, 可以传入不定长的Options. 本质上和显示结构体初始化一样. Extract方法 var buf bytes.Buffer //这里Extract把模板替换后的代码输出到bytes.Buffer, 后者会被io.Copy到文件中. importPath, err := ext.Extract(pkgIdent, \"\", &buf) f, err := os.Create(prefix + \"_\" + oFile) _, err := io.Copy(f, &buf) f.Close() 从这里开始, 需要有reflect基础. go/importer 和 go/types func ForCompiler(fset *token.FileSet, compiler string, lookup Lookup) types.Importer ForCompiler returns an Importer for importing from installed packages for the compilers \"gc\" and \"gccgo\", or for importing directly from the source if the compiler argument is \"source\". //\"source\"类型的importer pkg, err := importer.ForCompiler(token.NewFileSet(), \"source\", nil).Import(\"pkgname\") //Import以后, \"pkgname\"的包就是`*types.Package`类型的对象 //根据模板生成内容. ipp是import 路径 content, err := e.genContent(ipp, pkg) genContent代码 // Val stores the value name and addressable status of symbols. type Val struct { Name string // \"package.name\" Addr bool // true if symbol is a Var } // Method stores information for generating interface wrapper method. type Method struct { Name, Param, Result, Arg, Ret string } // Wrap stores information for generating interface wrapper. type Wrap struct { Name string Method []Method } func (e *Extractor) genContent(importPath string, p *types.Package) ([]byte, error) { //声明+初始化 typ := map[string]string{} val := map[string]Val{} wrap := map[string]Wrap{} imports := map[string]bool{} //返回这个package的对象空间, 持有这个package的包括的各种对象 //Scope returns the (complete or incomplete) package scope holding the objects declared at package level (TypeNames, Consts, Vars, and Funcs). sc := p.Scope() //import路径从p.Imports()而来 for _, pkg := range p.Imports() { imports[pkg.Path()] = false } //Scope的Names返回其持有的所有对象的名字 for _, name := range sc.Names() { o := sc.Lookup(name) // skip if name不在e.Include里面 or 在e.Exclude里面 switch o := o.(type) { case *types.Const: case *types.Func: //pname是name的全称, 即\"package.name\" val[name] = Val{pname, false} case *types.Var: val[name] = Val{pname, true} //以上Const, Func, Var都算var //typ是什么? case *types.TypeName: typ[name] = pname //底层是interface时, 构造methods if t, ok := o.Type().Underlying().(*types.Interface); ok { var methods []Method for i := 0; i func (s *Scope) Lookup(name string) Object 返回的是types.Object, 是个接口. 一个函数返回接口, 说明这个东西是变化的. An Object describes a named language entity such as a package, constant, type, variable, function (incl. methods), or label. All objects implement the Object interface. type Object interface { Parent() *Scope // scope in which this object is declared; nil for methods and struct fields Pos() token.Pos // position of object identifier in declaration Pkg() *Package // package to which this object belongs; nil for labels and objects in the Universe scope Name() string // package local object name Type() Type // object type Exported() bool // reports whether the name starts with a capital letter Id() string // object name if exported, qualified name if not exported (see func Id) // String returns a human-readable string of the object. String() string // contains filtered or unexported methods } 变化的接口, 代表了一种抽象; 但抽象是不能直接用的, 所以一般会跟类型断言, 来\"具化\"这个接口变量:switch o := o.(type) { case *types.Const: case *types.Func: case *types.Var: case *types.TypeName: } 模板替换 基本上, 符号被分成三类: Val: 变量 函数 Typ: 结构体对象 Wrap: interface的方法集 通过对源码的分析, 把符号分类后, 传给模板 data := map[string]interface{}{ \"Dest\": e.Dest, \"Imports\": imports, \"PkgName\": importPath, \"Val\": val, \"Typ\": typ, \"Wrap\": wrap, \"BuildTags\": buildTags, \"License\": e.License, } base := template.New(\"extract\") parse, err := base.Parse(model) b := new(bytes.Buffer) err = parse.Execute(b, data) // format格式 source, err := format.Source(b.Bytes()) 模板 const model = ` func init() { Symbols[\"{ {.PkgName} }\"] = map[string]reflect.Value{ { {- if .Val} } // function, constant and variable definitions { {range $key, $value := .Val -} } //value是变量的时候, 先取地址, 再Elem() -- 不知道为啥 { {- if $value.Addr -} } \"{ {$key} }\": reflect.ValueOf(&{ {$value.Name} }).Elem(), //value是个函数, 或者常量 { {else -} } \"{ {$key} }\": reflect.ValueOf({ {$value.Name} }), { {end -} } { {end} } { {- end} } //typ类型包装成指针 { {- if .Typ} } // type definitions { {range $key, $value := .Typ -} } \"{ {$key} }\": reflect.ValueOf((*{ {$value} })(nil)), { {end} } { {- end} } { {- if .Wrap} } // interface wrapper definitions { {range $key, $value := .Wrap -} } \"_{ {$key} }\": reflect.ValueOf((*{ {$value.Name} })(nil)), { {end} } { {- end} } } } { {range $key, $value := .Wrap -} } // { {$value.Name} } is an interface wrapper for { {$key} } type type { {$value.Name} } struct { { {range $m := $value.Method -} } W{ {$m.Name} } func{ {$m.Param} } { {$m.Result} } { {end} } } { {range $m := $value.Method -} } func (W { {$value.Name} }) { {$m.Name} }{ {$m.Param} } { {$m.Result} } { { {$m.Ret} } W.W{ {$m.Name} }{ {$m.Arg} } } { {end} } { {end} } ` 替换结果举例 stdlib/go1_14_io.go package stdlib import ( \"go/constant\" \"go/token\" \"io\" \"reflect\" ) func init() { Symbols[\"io\"] = map[string]reflect.Value{ // function, constant and variable definitions \"Copy\": reflect.ValueOf(io.Copy), \"CopyBuffer\": reflect.ValueOf(io.CopyBuffer), \"EOF\": reflect.ValueOf(&io.EOF).Elem(), \"ErrClosedPipe\": reflect.ValueOf(&io.ErrClosedPipe).Elem(), \"SeekCurrent\": reflect.ValueOf(constant.MakeFromLiteral(\"1\", token.INT, 0)), \"SeekEnd\": reflect.ValueOf(constant.MakeFromLiteral(\"2\", token.INT, 0)), // type definitions \"Reader\": reflect.ValueOf((*io.Reader)(nil)), \"Writer\": reflect.ValueOf((*io.Writer)(nil)), // interface wrapper definitions \"_Reader\": reflect.ValueOf((*_io_Reader)(nil)), \"_Writer\": reflect.ValueOf((*_io_Writer)(nil)), } } // _io_Reader is an interface wrapper for Reader type type _io_Reader struct { WRead func(p []byte) (n int, err error) } func (W _io_Reader) Read(p []byte) (n int, err error) { return W.WRead(p) } // _io_Writer is an interface wrapper for Writer type type _io_Writer struct { WWrite func(p []byte) (n int, err error) } func (W _io_Writer) Write(p []byte) (n int, err error) { return W.WWrite(p) } yaegi 背景知识 AST语法树 AST不依赖于具体的文法，不依赖于语言的细节，我们将源代码转化为AST后，可以对AST做很多的操作，包括一些你想不到的操作，这些操作实现了各种各样形形色色的功能，给你带进一个不一样的世界。 抽象语法树（abstract syntax code，AST）是源代码的抽象语法结构的树状表示，树上的每个节点都表示源代码中的一种结构，这所以说是抽象的，是因为抽象语法树并不会表示出真实语法出现的每一个细节，比如说，嵌套括号被隐含在树的结构中，并没有以节点的形式呈现。抽象语法树并不依赖于源语言的语法，也就是说语法分析阶段所采用的上下文无文文法，因为在写文法时，经常会对文法进行等价的转换（消除左递归，回溯，二义性等），这样会给文法分析引入一些多余的成分，对后续阶段造成不利影响，甚至会使合个阶段变得混乱。因些，很多编译器经常要独立地构造语法分析树，为前端，后端建立一个清晰的接口。 1+3*(4-1)+2 xml ShiChuang 12478 Nosic while while b != 0 { if a > b a = a-b else b = b-a } return a for sum=0 for i in range(0,100) sum=sum+i end 为什么需要抽象语法树 当在源程序语法分析工作时，是在相应程序设计语言的语法规则指导下进行的。语法规则描述了该语言的各种语法成分的组成结构，通常可以用所谓的前后文无关文法或与之等价的Backus-Naur范式(BNF)将一个程序设计语言的语法规则确切的描述出来。前后文无关文法有分为这么几类：LL(1)，LR(0)，LR(1)， LR(k) ,LALR(1)等。每一种文法都有不同的要求，如LL(1)要求文法无二义性和不存在左递归。当把一个文法改为LL(1)文法时，需要引入一些隔外的文法符号与产生式。 AST的重要特征就是和语法无关, 只关注抽象. 比如C的if if(condition) { do_something(); } 和fortran的if语法就不一样 If condition then do_somthing() end if 但它们的AST都是一样的: 在源程序中出现的括号，或者是关键字，都会被丢掉。 函数 // 简单函数 function square(n) { return n * n; } // 转换后的AST { type: \"FunctionDeclaration\", id: { type: \"Identifier\", name: \"square\" }, params: [ { type: \"Identifier\", name: \"n\" } ], ... } 第一步：词法分析，也叫扫描scanner 它读取我们的代码，然后把它们按照预定的规则合并成一个个的标识 tokens。同时，它会移除空白符、注释等。最后，整个代码将被分割进一个 tokens 列表（或者说一维数组）。 const a = 5; // 转换成 [{value: 'const', type: 'keyword'}, {value: 'a', type: 'identifier'}, ...] 第二步：语法分析，也称解析器 它会将词法分析出来的数组转换成树形的形式，同时，验证语法。语法如果有错的话，抛出语法错误。 [{value: 'const', type: 'keyword'}, {value: 'a', type: 'identifier'}, ...] // 语法分析后的树形形式 { type: \"VariableDeclarator\", id: { type: \"Identifier\", name: \"a\" }, ... } 当生成树的时候，解析器会删除一些没必要的标识 tokens（比如：不完整的括号），因此 AST 不是 100% 与源码匹配的。 main cmd/yaegi/yaegi.go的main()是入口 func main() run(os.Args[1:]) //New一个解释器 i := interp.New(interp.Options{GoPath: build.Default.GOPATH, BuildTags: strings.Split(tags, \",\")}) // Symbols variable stores the map of stdlib symbols per package. // var Symbols = map[string]map[string]reflect.Value{} //reflect.Value表示任意对象实例 // Symbols[\"github.com/traefik/yaegi/stdlib\"] = map[string]reflect.Value{ \"Symbols\": reflect.ValueOf(Symbols)} //第一个key的值就是自身的reflect.Value表达 // 所以Symbols是个很大的map表的集合 i.Use(stdlib.Symbols) // 使用解释器导出的symbol, 这个很小. 只有\"New\" \"Interpreter\" \"Options\" \"Panic\" i.Use(interp.Symbols) // 使用其他种类的符号表 i.Use(syscall.Symbols) i.Use(unsafe.Symbols) i.Use(unrestricted.Symbols) //cmd是-e选项传过来的要执行的字符串 _, err = i.Eval(cmd) //如果没有cmd就执行REPL循环 i.REPL() map[string]reflect.Value{} //reflect.Value表示任意对象实例 解释器 interp/interp.go 一个解释器包括全局变量和状态 // Interpreter contains global resources and state. type Interpreter struct { // id is an atomic counter counter used for run cancellation, // only accessed via runid/stop // Located at start of struct to ensure proper alignment on 32 bit // architectures. id uint64 // nindex is a node number incremented for each new node. // It is used for debug (AST and CFG graphs). As it is atomically // incremented, keep it aligned on 64 bits boundary. nindex int64 name string // name of the input source file (or main) opt // user settable options cancelChan bool // enables cancellable chan operations fset *token.FileSet // fileset to locate node in source code binPkg Exports // binary packages used in interpreter, indexed by path rdir map[string]bool // for src import cycle detection mutex sync.RWMutex frame *frame // program data storage during execution universe *scope // interpreter global level scope scopes map[string]*scope // package level scopes, indexed by import path srcPkg imports // source packages used in interpreter, indexed by path pkgNames map[string]string // package names, indexed by import path done chan struct{} // for cancellation of channel operations hooks *hooks // symbol hooks } 解释器调用bin里的符号 用i.Use(stdlib.Symbols)可以在yaegi的解释器里面, 调用已经编译好的标准库. 怎么做到的呢? // Exports stores the map of binary packages per package path. type Exports map[string]map[string]reflect.Value // Use loads binary runtime symbols in the interpreter context so // they can be used in interpreted code. func (interp *Interpreter) Use(values Exports) { for k, v := range values { if k == selfPrefix { interp.hooks.Parse(v) continue } if interp.binPkg[k] == nil { interp.binPkg[k] = make(map[string]reflect.Value) } for s, sym := range v { //双map表. 值是reflect.Value interp.binPkg[k][s] = sym } } // Checks if input values correspond to stdlib packages by looking for one // well known stdlib package path. if _, ok := values[\"fmt\"]; ok { fixStdio(interp) } } 比如stdlib/go1_14_bytes.go中, func init() { Symbols[\"bytes\"] = map[string]reflect.Value{ ... //这里bytes.Join指代的是Join这个函数; 有点像函数指针. //是否通过这个函数指针就可以调用bin里的函数了? \"Join\": reflect.ValueOf(bytes.Join), } } 使用refect.Value.Call()的例子 func TestGetFunc(t *testing.T) { i := interp.New(interp.Options{GoPath: \"./_gopath/\"}) i.Use(stdlib.Symbols) if _, err := i.Eval(`import \"github.com/foo/bar\"`); err != nil { t.Fatal(err) } //bar.NewFoo并没有执行, 只是获取了NewFoo的对象的reflect.Value值 //val是reflect.Value类型, 因为Eval返回reflect.Value,error val, err := i.Eval(`bar.NewFoo`) if err != nil { t.Fatal(err) } fmt.Println(val.Call(nil)) } 这里的NewFoo是个函数 type Foo struct { A string } func NewFoo() *Foo { return &Foo{A: \"test\"} } Call的原型如下: func (v Value) Call(in []Value) []Value Call calls the function v with the input arguments in. For example, if len(in) == 3, v.Call(in) represents the Go call v(in[0], in[1], in[2]). Call panics if v's Kind is not Func. It returns the output results as Values. As in Go, each input argument must be assignable to the type of the function's corresponding input parameter. If v is a variadic function, Call creates the variadic slice parameter itself, copying in the corresponding values. 此时v的Kind必须是Func vc可以是func (v Value) MethodByName(name string) Value的返回值; 此时vc.Call()默认第一个参数是v, 其他参数才是Call传入的 查找bin 的method 在cfg阶段, 通过判断node中的信息(typ, child等), 发现这个是个对bin的method的调用. if m, lind, isPtr, ok := n.typ.lookupBinMethod(n.child[1].ident); ok { n.action = aGetMethod if isPtr && n.typ.fieldSeq(lind).cat != ptrT { n.gen = getIndexSeqPtrMethod } else { n.gen = getIndexSeqMethod } // n.recv = &receiver{node: n.child[0], index: lind} n.val = append([]int{m.Index}, lind...) n.typ = &itype{cat: valueT, rtype: m.Type, recv: n.child[0].typ} } 前面说过, 初始化的时候, 对标准库的引用, 是个双map的表 type Exports map[string]map[string]reflect.Value 对一个函数级的符号来说, 就是比如interp.binPkg[\"fmt\"][\"Println\"]查到reflect.ValueOf(fmt.Println), 这是个reflect.Value类型的值, 存储了\"函数指针\"fmt.Println的所有内部表达. // LookupBinMethod returns a method and a path to access a field in a struct object (the receiver). func (t *itype) lookupBinMethod(name string) (m reflect.Method, index []int, isPtr, ok bool) { //指针类型下 if t.cat == ptrT { return t.val.lookupBinMethod(name) } for i, f := range t.field { //是闭包的意思吗? if f.embed { if m2, index2, isPtr2, ok2 := f.typ.lookupBinMethod(name); ok2 { index = append([]int{i}, index2...) return m2, index, isPtr2, ok2 } } } //这里关键的黑魔法来了 //reflect.Value的MethodByName方法查找对象的\"name\"方法, 返回reflect.Method类型. //在go里面, 方法集是绑定在类型上的, m, ok = t.TypeOf().MethodByName(name) if !ok { //没找到就找指针指向的对象的method m, ok = reflect.PtrTo(t.TypeOf()).MethodByName(name) isPtr = ok } return m, index, isPtr, ok } 反射的Method是这样的: type Method struct { // Name is the method name. // PkgPath is the package path that qualifies a lower case (unexported) // method name. It is empty for upper case (exported) method names. // The combination of PkgPath and Name uniquely identifies a method // in a method set. // See https://golang.org/ref/spec#Uniqueness_of_identifiers Name string PkgPath string Type Type // method type Func Value // func with receiver as first argument Index int // index for Type.Method } interp/run.go的call()函数 func call(n *node) { //value是由genValue返回的闭包函数 //n.child[0]就是要call的函数名 value := genValue(n.child[0]) //child[0]是个receiver对象 if n.child[0].recv != nil { ... } 准备in参数 准备out参数 基本上最后每个参数都是个闭包函数的返回值 最后给n.exec赋值, 值为一个闭包函数{ 如果是bin func bf := value(f) //in 和 out都是[]reflect.Value 闭包函数里面调用了out := bf.Call(in) 返回 如果是解释执行的func 最后调用runCfg(def.child[3].start, nf) } } ast的node type node struct { child []*node // child subtrees (AST) anc *node // ancestor (AST) start *node // entry point in subtree (CFG) tnext *node // true branch successor (CFG) fnext *node // false branch successor (CFG) interp *Interpreter // interpreter context frame *frame // frame pointer used for closures only (TODO: suppress this) index int64 // node index (dot display) findex int // index of value in frame or frame size (func def, type def) level int // number of frame indirections to access value nleft int // number of children in left part (assign) or indicates preceding type (compositeLit) nright int // number of children in right part (assign) kind nkind // kind of node pos token.Pos // position in source code, relative to fset sym *symbol // associated symbol typ *itype // type of value in frame, or nil recv *receiver // method receiver node for call, or nil types []reflect.Type // frame types, used by function literals only; 每个type代表一个变量? action action // action exec bltn // generated function to execute gen bltnGenerator // generator function to produce above bltn val interface{} // static generic value (CFG execution) rval reflect.Value // reflection value to let runtime access interpreter (CFG) ident string // set if node is a var or func } // receiver stores method receiver object access path. type receiver struct { node *node // receiver value for alias and struct types val reflect.Value // receiver value for interface type and value type index []int // path in receiver value for interface or value type } // frame contains values for the current execution level (a function context). type frame struct { // id is an atomic counter used for cancellation, only accessed // via newFrame/runid/setrunid/clone. // Located at start of struct to ensure proper aligment. id uint64 anc *frame // ancestor frame (global space) data []reflect.Value // values mutex sync.RWMutex deferred [][]reflect.Value // defer stack recovered interface{} // to handle panic recover done reflect.SelectCase // for cancellation of channel operations } Eval interpreter的Eval方法, 把string解释执行 func (interp *Interpreter) eval(src, name string, inc bool) (res reflect.Value, err error) { // 调用go标准库的语法库解析生成AST. root就是根节点 pkgName, root, err := interp.ast(src, interp.name, inc) // 可以打印这个graph root.astDot(dotWriter(dotCmd), interp.name) // cfg是control flow graph. 控制流的图表达 initNodes, err := interp.cfg(root, pkgName) // Perform global types analysis. err = interp.gtaRetry([]*node{root}, pkgName) gs := interp.scopes[pkgName] // Add main to list of functions to run, after all inits. if m := gs.sym[\"main\"]; pkgName == mainID && m != nil { initNodes = append(initNodes, m.node) } // Generate node exec closures. err = genRun(root) //准备frame内存 // Init interpreter execution memory frame. interp.frame.setrunid(interp.runid()) interp.frame.mutex.Lock() interp.resizeFrame() interp.frame.mutex.Unlock() // Execute node closures. interp.run(root, nil) // Wire and execute global vars. n, err := genGlobalVars([]*node{root}, interp.scopes[pkgName]) interp.run(n, nil) // main也在这个initNodes里面 for _, n := range initNodes { interp.run(n, interp.frame) } //genValue返回一个函数闭包 // v是返回的函数: func(*frame) reflect.Value v := genValue(root) res = v(interp.frame) return res, err } ast函数 在ast.go里面实现interpreter包的ast树功能, 使用了标准库的go/ast go/token go/parser go/scanner // Note: no type analysis is performed at this stage, it is done in pre-order // processing of CFG, in order to accommodate forward type declarations. // ast parses src string containing Go code and generates the corresponding AST. // The package name and the AST root node are returned. // The given name is used to set the filename of the relevant source file in the // interpreter's FileSet. func (interp *Interpreter) ast(src, name string, inc bool) (string, *node, error) { //先parse f, err := parser.ParseFile(interp.fset, name, src, mode) var root *node var anc astNode var st nodestack var pkgName string //Inspect会深度优先遍历ast //这里传入的func是个巨长的函数, 在遍历过程中, 对每个node调用 //它检查node的类型, ast.Inspect(f, func(nod ast.Node) bool { //st是node栈 anc = st.top() //根据node的类型不同, 操作st栈. 一般都是st.push() //type类型断言 switch a := nod.(type) { case nil: case *ast.ArrayType: case *ast.AssignStmt: case *ast.BlockStmt: case *ast.BranchStmt: case *ast.CallExpr: case *ast.CaseClause: case *ast.ChanType: case *ast.CommClause: case *ast.DeclStmt: case *ast.DeferStmt: case *ast.ExprStmt: case *ast.ForStmt: case *ast.File: case *ast.Field: case *ast.GoStmt: case *ast.IfStmt: case *ast.MapType: 还有很多case. 一个语言, 所有支持的操作都在此. } }) } builtin预定义action run.go会提供一个builtin数组, 里面是不同操作码对应的动作: var builtin = [...]bltnGenerator{ aNop: nop, aAddr: addr, aAssign: assign, aAdd: add, aAddAssign: addAssign, aAnd: and, aAndAssign: andAssign, aAndNot: andNot, aAndNotAssign: andNotAssign, aBitNot: bitNot, aCall: call, aCallSlice: call, aCase: _case, aCompositeLit: arrayLit, aDec: dec, aEqual: equal, aGetFunc: getFunc, aGreater: greater, aGreaterEqual: greaterEqual, aInc: inc, aLand: land, aLor: lor, aLower: lower, aLowerEqual: lowerEqual, aMul: mul, aMulAssign: mulAssign, aNeg: neg, aNot: not, aNotEqual: notEqual, aOr: or, aOrAssign: orAssign, aPos: pos, aQuo: quo, aQuoAssign: quoAssign, aRange: _range, aRecv: recv, aRem: rem, aRemAssign: remAssign, aReturn: _return, aSend: send, aShl: shl, aShlAssign: shlAssign, aShr: shr, aShrAssign: shrAssign, aSlice: slice, aSlice0: slice0, aStar: deref, aSub: sub, aSubAssign: subAssign, aTypeAssert: typeAssertShort, aXor: xor, aXorAssign: xorAssign, } 在ast阶段就把操作码的动作\"编译\"进node树里: addChild := func(root **node, anc astNode, pos token.Pos, kind nkind, act action) *node { ... n := &node{anc: anc.node, interp: interp, index: nindex, pos: pos, kind: kind, action: act, val: &i, gen: builtin[act]} ... } 比如所有的\"+\"操做, 都会调用: func add(n *node) { switch typ.Kind() { case reflect.String: case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: //各种类型断言, 使用反射赋值; 注意这里并不是马上执行, 而是返回一个闭包函数 switch { case isInterface: v0 := genValueInt(c0) v1 := genValueInt(c1) n.exec = func(f *frame) bltn { _, i := v0(f) _, j := v1(f) dest(f).Set(reflect.ValueOf(i + j).Convert(typ)) return next } case c0.rval.IsValid(): i := vInt(c0.rval) v1 := genValueInt(c1) n.exec = func(f *frame) bltn { _, j := v1(f) dest(f).SetInt(i + j) return next } case c1.rval.IsValid(): v0 := genValueInt(c0) j := vInt(c1.rval) n.exec = func(f *frame) bltn { _, i := v0(f) dest(f).SetInt(i + j) return next } default: v0 := genValueInt(c0) v1 := genValueInt(c1) n.exec = func(f *frame) bltn { _, i := v0(f) _, j := v1(f) dest(f).SetInt(i + j) return next } } case reflect.Float32, reflect.Float64: } cfg函数 cfg函数根据ast, 生成控制流的图 入参是root node, 返回node类型的切片 // cfg generates a control flow graph (CFG) from AST (wiring successors in AST) // and pre-compute frame sizes and indexes for all un-named (temporary) and named // variables. A list of nodes of init functions is returned. // Following this pass, the CFG is ready to run. func (interp *Interpreter) cfg(root *node, importPath string) ([]*node, error) { sc := tinterp.initScopePkg(importPath) check := typecheck{} var initNodes []*node var err error //这里传了两个函数 //第一个函数, 根据node的类型, 很多地方在调用sc = sc.pushBloc() //比如这样的 //case forStmt0, forStmt1, forStmt2, forStmt3, forStmt4, forStmt5, forStmt6, forStmt7, forRangeStmt: // sc = sc.pushBloc() // sc.loop, sc.loopRestart = n, n.lastChild() // root.Walk(func(n *node) bool {}, func(n *node)) ... } walk函数深度优先遍历树, 对每个节点, 遍历之前调用in函数, 遍历之后调用out函数. in函数又称为pre-ordering函数 out函数又称为post-ordering函数 // Walk traverses AST n in depth first order, call cbin function // at node entry and cbout function at node exit. func (n *node) Walk(in func(n *node) bool, out func(n *node)) { if in != nil && !in(n) { return } for _, child := range n.child { child.Walk(in, out) } if out != nil { out(n) } } frame类型 函数level的frame // frame contains values for the current execution level (a function context). type frame struct { // id is an atomic counter used for cancellation, only accessed // via newFrame/runid/setrunid/clone. // Located at start of struct to ensure proper aligment. id uint64 anc *frame // ancestor frame (global space) data []reflect.Value // values mutex sync.RWMutex deferred [][]reflect.Value // defer stack recovered interface{} // to handle panic recover done reflect.SelectCase // for cancellation of channel operations } //newFrame生成一个frame func newFrame(anc *frame, len int, id uint64) *frame { f := &frame{ anc: anc, data: make([]reflect.Value, len), id: id, } if anc != nil { f.done = anc.done } return f } run函数 func (interp *Interpreter) run(n *node, cf *frame) { if n == nil { return } var f *frame if cf == nil { f = interp.frame } else { //初始frame, 其data切片的大小是len(n.types) //但为什么大小是n.types f = newFrame(cf, len(n.types), interp.runid()) } interp.mutex.RLock() c := reflect.ValueOf(interp.done) interp.mutex.RUnlock() f.mutex.Lock() f.done = reflect.SelectCase{Dir: reflect.SelectRecv, Chan: c} f.mutex.Unlock() for i, t := range n.types { //每种type在f.data里面才有个位置? 还是说f.data[]里面每个type都代表了一个变量? //这里先给每个变量new空间. 用reflect.New f.data[i] = reflect.New(t).Elem() } //调用runCfg. run这个node的流程控制图(CFG) // runCfg executes a node AST by walking its CFG and running node builtin at each step. runCfg(n.start, f) { for exec = n.exec; exec != nil && f.runid() == n.interp.runid(); { exec = exec(f) } } } exec举例 上面的exec是这个node的exec主要是在op.go run.go中定义. 基本上, 每个基础操作都对应一个exec. 比如run.go中的not操作: func not(n *node) { dest := genValue(n) value := genValue(n.child[0]) tnext := getExec(n.tnext) if n.fnext != nil { fnext := getExec(n.fnext) n.exec = func(f *frame) bltn { if !value(f).Bool() { dest(f).SetBool(true) return tnext } dest(f).SetBool(false) return fnext } } else { n.exec = func(f *frame) bltn { dest(f).SetBool(!value(f).Bool()) return tnext } } } 再比如op.go中的 对string的+操作 func add(n *node) { next := getExec(n.tnext) typ := n.typ.concrete().TypeOf() dest := genValueOutput(n, typ) c0, c1 := n.child[0], n.child[1] switch typ.Kind() { case reflect.String: switch { case c0.rval.IsValid(): s0 := vString(c0.rval) v1 := genValue(c1) n.exec = func(f *frame) bltn { dest(f).SetString(s0 + v1(f).String()) return next } case c1.rval.IsValid(): v0 := genValue(c0) s1 := vString(c1.rval) n.exec = func(f *frame) bltn { dest(f).SetString(v0(f).String() + s1) return next } default: v0 := genValue(c0) v1 := genValue(c1) n.exec = func(f *frame) bltn { dest(f).SetString(v0(f).String() + v1(f).String()) return next } } case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: ... } genValue()和genValueOutput(n, typ)都返回一个闭包函数, 底层是value.go基于反射实现的. dest(f)就是调用这个闭包函数. func genValue(n *node) func(*frame) reflect.Value { i := n.sym.index if n.sym.global { return func(f *frame) reflect.Value { return n.interp.frame.data[i] } } return valueGenerator(n, i) } func valueGenerator(n *node, i int) func(*frame) reflect.Value { switch n.level { case 0: return func(f *frame) reflect.Value { return valueOf(f.data, i) } case 1: return func(f *frame) reflect.Value { return valueOf(f.anc.data, i) } case 2: return func(f *frame) reflect.Value { return valueOf(f.anc.anc.data, i) } default: return func(f *frame) reflect.Value { for level := n.level; level > 0; level-- { f = f.anc } return valueOf(f.data, i) } } } // valueOf safely recovers the ith element of data. This is necessary // because a cancellation prior to any evaluation result may leave // the frame's data empty. func valueOf(data []reflect.Value, i int) reflect.Value { if i dest(f)展开就是 f.data[i] 总结 解释执行的过程是 先New一个解释器实例 Eval(string)解释 使用了标准库的go/ast go/token go/parser go/scanner来生成ast. ast表达的是go的抽象语法树; 至此每个node都有个类型, 比如*ast.MapType *ast.BlockStmt *ast.AssignStmt 根据ast生成控制流的图表达(cfg): 深度优先walk这个ast, 对每个node, 遍历之前调用in函数, 遍历之后调用out函数; 生成initNodes的切片. main函数也会被加入这个切片 建立frame: 应该是每个函数有个frame, frame.data[]表示函数内的所有变量, 其类型是反射的通用值类型reflect.Value. frame.data[]和node的types []reflect.Type是对应的. 即先有ast的reflect.Type, 再有frame.data[i] 真正run的是node的exec()函数. 这个函数会循环调用exec() exec = exec(f). exec()是每个node的闭包函数, 比如dest(f).SetBool(!value(f).Bool()), 又比如dest(f).SetString(s0 + v1(f).String()), 这里利用反射对f.data[i]赋值 整个过程只使用了go的标准库 相关标准库go/*的介绍在这里: https://github.com/golang/example/tree/master/gotypes Starting at the bottom, the go/token package defines the lexical tokens of Go. The go/scanner package tokenizes an input stream and records file position information for use in diagnostics or for file surgery in a refactoring tool. The go/ast package defines the data types of the abstract syntax tree (AST). The go/parser package provides a robust recursive-descent parser that constructs the AST. And go/constant provides representations and arithmetic operations for the values of compile-time constant expressions, as we'll see in Constants. 代码生成 op.go的代码是go run ../internal/genop/genop.go生成的 //用{ {$name} }来做模板替换 const model = `代码模板` base := template.New(\"genop\") parse, err := base.Parse(model) b := &bytes.Buffer{} data := map[string]interface{} { 预定义的data } //把data按照模板parse进b parse.Execute(b, data) //gofmt代码 source, err := format.Source(b.Bytes()) //结果写道文件里 ioutil.WriteFile(\"op.go\", source, 0666) REPL循环 // REPL performs a Read-Eval-Print-Loop on input reader. // Results are printed to the output writer of the Interpreter, provided as option // at creation time. Errors are printed to the similarly defined errors writer. // The last interpreter result value and error are returned. func (interp *Interpreter) REPL() (reflect.Value, error) { in, out, errs := interp.stdin, interp.stdout, interp.stderr ctx, cancel := context.WithCancel(context.Background()) end := make(chan struct{}) // channel to terminate the REPL sig := make(chan os.Signal, 1) // channel to trap interrupt signal (Ctrl-C) lines := make(chan string) // channel to read REPL input lines prompt := getPrompt(in, out) // prompt activated on tty like IO stream s := bufio.NewScanner(in) // read input stream line by line var v reflect.Value // result value from eval var err error // error from eval src := \"\" // source string to evaluate //注册Ctrl-C signal.Notify(sig, os.Interrupt) defer signal.Stop(sig) prompt(v) //管输入的routine go func() { defer close(end) //Ctrl+D EOF会退出 for s.Scan() { lines EvalWithContext 这段写的很标准, 有带cancel的context. // EvalWithContext evaluates Go code represented as a string. It returns // a map on current interpreted package exported symbols. func (interp *Interpreter) EvalWithContext(ctx context.Context, src string) (reflect.Value, error) { var v reflect.Value var err error interp.mutex.Lock() interp.done = make(chan struct{}) interp.cancelChan = !interp.opt.fastChan interp.mutex.Unlock() done := make(chan struct{}) //在routine里执行Eval的好处是后面的主循环会马上等待ctx.Done() //这样用户Ctrl+C能马上生效; 要优雅的生效, interp.stop()必不可少. go func() { //close用的漂亮 defer close(done) //直接用外面的变量赋值 v, err = interp.Eval(src) }() select { case 优雅的异步中断pattern: 在新的go routine里执行耗时函数 主循环在select里等待ctx.Done() 收到异步停止信号后(比如超时到, 比如ctx的cancel()触发), 调用stop() 重点是执行方提供一个stop()函数, 比如这里, interp的stop就是close掉done channel // stop sends a semaphore to all running frames and closes the chan // operation short circuit channel. stop may only be called once per // invocation of EvalWithContext. func (interp *Interpreter) stop() { atomic.AddUint64(&interp.id, 1) close(interp.done) } 总结 REPL循环使用bufio的scanner获取输入, 调用interp的Eval来执行. context 以及输入routine, Eval routine, 配合signal, 和interp.stop函数, 能及时退出本次eval执行. "},"notes/golang_tengo.html":{"url":"notes/golang_tengo.html","title":"解释器tengo","keywords":"","body":" 增加VM的并发run 编译函数 VM执行函数 函数怎么调用的? 编译阶段 执行阶段 函数怎么退出的? 如何理解字节码? 运行时会改变/增加/删除 global或constant数组吗? CompiledFunction的Free对象指针数组是干什么的? 运行阶段 结论 例子 tengo指令集和go的操作码 tengo运行过程 初始状态 a := 1 后 b := 2 后 c := a+b 后 f := func(a, b) { return a + b } 后 FormatConstants函数 那么什么是constant? constants在何处使用, 在何处定义 为什么constants要传递给NewCompiler? extension 为什么extension.UserFunction不生效? -- 注意Copy()方法 调查 结论 bash 能够在bash的命令里引用脚本的变量吗? 解决方法 添加新的builtin函数ex() 能否添加到builtinFuncs表 添加到global表 tengo代码 examples/interoperability/main.go 可被注册的对象: native go 可被注册的对象: tengo脚本 native go对象穿越脚本的黑魔法 dlv调查 对象穿越魔法揭秘 使用注意 总结 tengo锁 tengo并发 代码组织: 总结 error.go require/require.go parser/opcodes.go parser/source_file.go parser/parser.go NewParser()函数 ParseFile()函数 formatter.go Pool对象池技术 tengo.go FromInterface()和ToInterface()函数是go和脚本交互的核心 什么是callable对象 CountObjects这个递归函数写的真好 script.go Script.Add用于从native go添加变量到tengo脚本 NewScript Compile函数 preCompile()函数 RemoveDuplicates()去掉重复常量? RunContext()函数 Run函数 variable.go builtins.go compiler.go Compile()函数 emit()函数 Compile之module compileModule()函数 module默认路径 Bytecode()函数返回编译好的字节码 modules.go module类型: 支持import native go对象和tengo代码 symbol_table.go NewSymbolTable()函数 Define()函数 builtin符号除了保存在store中, 还保存在单独的builtinSymbols中 Fork()函数 Resolve()函数 总结 vm.go VM New函数 VM Run函数 vm.run总结 stdlib/func_typedefs.go tengo的后继 tengo 入门 语法 token 一个go调脚本的例子 所有都是值类型 tengo值类型和go类型 error升级为一等类型 值不可变 未定义也是一种类型 广义的array和map 函数也是值, 一等公民, 支持闭包 变量scope 内置类型转换 支持三目操作 []和.能作用于复合类型:array map string和bytes 再切片和go一样 if支持前置执行语句, 和go一样; for的结构也和go一样样的; for支持 for in 其他和native go的不同点 内置函数 被嵌入执行 直接代码执行 代码实时\"编译\"后执行 外部代码里import 用户可以自定义类型 自定义类型举例 增加更多功能 总结 模块化和标准库 标准库 runtime对象类型 安全性 并发 这里说的VM感觉就是编译后的\"表达树\"? 官方例子 更多官方例子 gento代码转成lua 有限状态机 增加VM的并发run 思路是浅拷贝当前的VM, 来run一个compiledFunc 编译函数 在compiler.go中 // Compile compiles the AST node. func (c *Compiler) Compile(node parser.Node) error { case *parser.FuncLit: //函数编译 c.enterScope() for _, p := range node.Type.Params.List { s := c.symbolTable.Define(p.Name) // function arguments is not assigned directly. s.LocalAssigned = true } if err := c.Compile(node.Body); err != nil { return err } // code optimization c.optimizeFunc(node) freeSymbols := c.symbolTable.FreeSymbols() //FreeSymbols函数返回\"原始\"symbol, 即被捕获的symbol, 所以才包含ScopeLocal和ScopeFree两个类型 numLocals := c.symbolTable.MaxSymbols() instructions, sourceMap := c.leaveScope() for _, s := range freeSymbols { //这段没看明白, 这里已经是父函数scope, OpGetFreePtr到栈上? -- 是的. 在定义的时候就把子函数的free变量全部放到栈上备用. 对应VM的OpClosure会把这些free变量放到其 switch s.Scope { case ScopeLocal: if !s.LocalAssigned { c.emit(node, parser.OpNull) c.emit(node, parser.OpDefineLocal, s.Index) s.LocalAssigned = true } c.emit(node, parser.OpGetLocalPtr, s.Index) //捕获了父函数的local变量, s.Index是local变量在栈上相对basePointer的index case ScopeFree: c.emit(node, parser.OpGetFreePtr, s.Index) //捕获了父函数的free变量, index是父函数freeVar数组中的index } } compiledFunction := &CompiledFunction{ Instructions: instructions, NumLocals: numLocals, NumParameters: len(node.Type.Params.List), VarArgs: node.Type.Params.VarArgs, SourceMap: sourceMap, } if len(freeSymbols) > 0 { c.emit(node, parser.OpClosure, c.addConstant(compiledFunction), len(freeSymbols)) } else { c.emit(node, parser.OpConstant, c.addConstant(compiledFunction)) } case *parser.CallExpr: //函数调用 if err := c.Compile(node.Func); err != nil { return err } for _, arg := range node.Args { if err := c.Compile(arg); err != nil { return err } } ellipsis := 0 if node.Ellipsis.IsValid() { ellipsis = 1 } c.emit(node, parser.OpCall, len(node.Args), ellipsis) //OpCall操作码 } VM执行函数 func (v *VM) run() { case parser.OpCall: numArgs := int(v.curInsts[v.ip+1]) spread := int(v.curInsts[v.ip+2]) v.ip += 2 //对应编译阶段的emit Opcall, 参数个数, 是否变长 //比如两个参数的func, 栈是 //sp-->| | //sp-1 | arg1 | //sp-2 | arg0 | //sp-3 | func | value := v.stack[v.sp-1-numArgs] //sp-3的object就是func if !value.CanCall() { v.err = fmt.Errorf(\"not callable: %s\", value.TypeName()) return } if spread == 1 { //如果最后的参数是变长的, 则那个object必然是array; 把array的成员都展开到栈上, 栈向上增长. 更新numArgs的值 v.sp-- switch arr := v.stack[v.sp].(type) { case *Array: for _, item := range arr.Value { v.stack[v.sp] = item v.sp++ } numArgs += len(arr.Value) - 1 case *ImmutableArray: for _, item := range arr.Value { v.stack[v.sp] = item v.sp++ } numArgs += len(arr.Value) - 1 default: v.err = fmt.Errorf(\"not an array: %s\", arr.TypeName()) return } } if callee, ok := value.(*CompiledFunction); ok { if callee.VarArgs { // if the closure is variadic, // roll up all variadic parameters into an array realArgs := callee.NumParameters - 1 varArgs := numArgs - realArgs if varArgs >= 0 { numArgs = realArgs + 1 args := make([]Object, varArgs) spStart := v.sp - varArgs for i := spStart; i =%d, got=%d\", callee.NumParameters-1, numArgs) } else { v.err = fmt.Errorf( \"wrong number of arguments: want=%d, got=%d\", callee.NumParameters, numArgs) } return } // test if it's tail-call if callee == v.curFrame.fn { // recursion nextOp := v.curInsts[v.ip+1] if nextOp == parser.OpReturn || (nextOp == parser.OpPop && parser.OpReturn == v.curInsts[v.ip+2]) { for p := 0; p = MaxFrames { v.err = ErrStackOverflow return } // update call frame v.curFrame.ip = v.ip // store current ip before call v.curFrame = &(v.frames[v.framesIndex]) //framesIndex是提前++的, 所以这里就是下一个frame v.curFrame.fn = callee v.curFrame.freeVars = callee.Free v.curFrame.basePointer = v.sp - numArgs //帧指针的初值是sp的当前值减去参数个数 v.curInsts = callee.Instructions v.ip = -1 v.framesIndex++ v.sp = v.sp - numArgs + callee.NumLocals //给局部变量留好空间, callee.NumLocals包括了arg的个数 } } 举例, 比如2个arg和4个局部变量的栈的初始情况 | | | local obj 0 | | arg obj 1 | | arg obj 0 | 函数怎么调用的? 比如下面的代码 fn := func(i) { return i+1 } ... res := fn(5) //res=6 是如何编译执行的? 编译阶段 编译器发现是case *parser.FuncLit:, 就编译func(i) { return i+1 }, 生成compiledFunction对象, 并把这个对象放到constant数组里, emit OpConstant操作码: c.emit(node, parser.OpConstant, c.addConstant(compiledFunction)); 如果是闭包函数, emit parser.OpClosure操做码. tengo不支持声明函数, 而是用赋值模式. 那么上面一步把func(i) { return i+1 }编译完成后, 把compiledFunction对象保存到constant数组, 并马上放到栈上. 因为接下来就是赋值操做 编译器发现接下来马上是case *parser.AssignStmt:对fn变量赋值: 在确定fn的符号位置后, 发送操作码到字节码:c.emit(node, parser.OpSetGlobal, symbol.Index) //或 c.emit(node, parser.OpSetLocal, symbol.Index) //或 c.emit(node, parser.OpSetFree, symbol.Index) 现在假设代码运行到某处, 需要调用fn:res := fn(5). 编译器需要先解析fn这个符号. 这个例子中fn就是符号引用, 但更复杂的情况可以是res := someMap.fn(5), 下面c.Compile(node.Func)就是解析这个表达式, 最终get到这个compiledFunction对象fn:这里面包括两个过程: 4.1. 先把fn放到栈上 4.2. 依次编译args, 把结果args对象依次放到栈上case *parser.CallExpr: if err := c.Compile(node.Func); err != nil { //最终对应*parser.Ident来确定符号, 对应emit的操做码是OpGetGlobal或OpGetLocal或OpGetBuiltin或OpGetFree, 效果是把compiledFunction对象fn放到栈上 return err } for _, arg := range node.Args { //比如这里的arg就是5, 它是个*parser.IntLit, 对应emit操作码OpConstant, 把5放到栈上 if err := c.Compile(arg); err != nil { return err } } ellipsis := 0 if node.Ellipsis.IsValid() { ellipsis = 1 } c.emit(node, parser.OpCall, len(node.Args), ellipsis) //最后emit OpCall 比如 //比如两个参数的func, 栈是 //sp-->| | //sp-1 | arg1 | //sp-2 | arg0 | //sp-3 | func | 执行阶段 VM负责执行 对应编译阶段的前三步, VM执行fn := func(i) { return i+1 }, 效果是把fn这个compiledFunction对象先临时保存在v.stack[v.sp-1]中, 再根据变量scope, 保存在下面三种变量区的一个 全局对象区, 即vm的globals数组中 栈中: 因为上一步已经把fn放在了v.stack[v.sp-1], 这里做的就是把v.stack[v.sp-1]赋值给v.stack[v.curFrame.basePointer + localIndex]. 这个localIndex是操做码里指定的, 编译阶段就知道了 frame的free数组中: *v.curFrame.freeVars[freeIndex].Value = v.stack[v.sp-1] VM执行到函数执行阶段res := fn(5)时: 对应case parser.OpCall: //比如两个参数的func, 栈是 //sp-->| | //sp-1 | arg1 | //sp-2 | arg0 | //sp-3 | func | 进入下一个栈帧前, 做准备: 保存当前ip到父栈帧 准备空的栈帧, 栈帧切换到子函数 子栈帧的fn为子函数callee 子栈帧的freeVars为callee.Free, 此时callee.Free里面已经准备好了 因为参数是父栈帧准备的, 这里子栈帧的basePointer = v.sp - numArgs 子栈帧的字节码为callee.Instructions 子栈帧的ip从-1开始, 正好下一次从0开始执行 把local变量的位置留出来, 子栈帧的sp从local变量区顶部开始. 进入下一个栈帧, 从ip=0开始执行fn.Instructions | | | local obj 0 | | arg obj 1 | | arg obj 0 | 函数怎么退出的? 在编译阶段, 空的return会c.emit(node, parser.OpReturn, 0) return一个表达式的情况, 会先编译这个表达式, 最终的结果对象放到栈顶, 然后c.emit(node, parser.OpReturn, 1) 执行阶段, 如果发现OpReturn的第二个操作数是1, 就从栈上拿到返回值; 然后还原栈帧, 还原sp, 并把返回值放到栈顶(此时已经是父函数的栈了). 如何理解字节码? 看这个文件compiler_test.go 运行时会改变/增加/删除 global或constant数组吗? 首先, constant数组是在编译时确定的, 在运行时不改变. 其次, global数组是可以更改的, 但不需要增加/删除, 因为对global的操做的index是在编译时就确定的. CompiledFunction的Free对象指针数组是干什么的? 在编译阶段, Resolve symbol的时候, 这个symbol可以是global的, 也可以是自己函数局部的, 还可以是父级函数路径上的局部函数. 最后这种情况就是free变量. if symbol is defined in parent table and if it's not global/builtin then it's free variable. free变量是一路父函数上的局部变量, resolve要化不少代价, 所以在编译生成函数对象CompiledFunction的时候就用Free []*ObjectPtr来保存对\"捕获\"的父函数局部变量的指针. 运行阶段 只有闭包函数有free变量, 闭包在编译的时候, emit的是OpClosure, 在这之前free变量已经从父函数的local区和free区拷贝到栈上了. if len(freeSymbols) > 0 { c.emit(node, parser.OpClosure, c.addConstant(compiledFunction), len(freeSymbols)) } else { c.emit(node, parser.OpConstant, c.addConstant(compiledFunction)) } 那么运行时 case parser.OpClosure: v.ip += 3 constIndex := int(v.curInsts[v.ip-1]) | int(v.curInsts[v.ip-2]) OpClosure操作码后面应该是接着赋值操做. 还有几个case: OpGetFreePtr:从freeVars数组取index对应的ptr到栈顶 OpGetFree: 从freeVars数组取index的ptr的Value到栈顶 OpSetFree: 栈顶obj赋值给freeVars数组index对应的ptr, 注意是赋值给*ptr OpGetLocalPtr: 把本frame的指定local变量v.stack[v.curFrame.basePointer + localIndex], 封装为*ObjectPtr, 放到栈顶. OpSetSelFree: 把selectors序列对应的对象依次写入v.curFrame.freeVars 结论 子函数可以是普通的函数, 也可以是Closure, 他们都是compiledFunction. 区别在于函数声明后, 必定会赋值到一个变量. 到变量赋值这里后, closure的compiledFunction的Free域就有值了, 这些值是指针, 是对free变量的指针表. 闭包对free变量的访问不是去父函数路径上的栈里去找, 而是去自己的free变量的指针表里操做. 例子 比如下面的代码 f1 := func(a, b) { return a + b}; func(fn, ...args){ return fn(args...) }(f1, 1, 2) 会被编译成 0000 CONST 0 //对应第一个函数从const 0取到栈顶 0003 SETG 0 //对应f1的赋值 0006 CONST 1 //对应第二个函数从const 1取到栈顶 0009 GETG 0 //对应f1入参 0012 CONST 2 //对应常量1入参 0015 CONST 3 //对应常量2入参 0018 CALL 3 0 //对应函数调用 0021 POP //对应这块代码无返回值 0022 SUSPEND //对应这块代码的结束 对应的const数组 [ 0] (Compiled Function|0xc0000946e8) 0000 GETL 0 //对应得到入参a 0002 GETL 1 //对应得到入参b 0004 BINARYOP 11 //a+b 0006 RET 1 //return 值 [ 1] (Compiled Function|0xc0000946f0) 0000 GETL 0 //对应得到fn 0002 GETL 1 //对应应该传入的args 0004 CALL 1 1 //call一个参数, 最后一个参数变长 0007 RET 1 //return栈上的值 [ 2] 1 (Int|0xc0000fd1e0) [ 3] 2 (Int|0xc0000fd200) tengo指令集和go的操作码 GO的编译器也有自己一套的”操作码”. 是对所有CPU指令集的common抽象, 比如下面的CALL就相当于MIPS指令集的jal或者ARM的blr Tengo的指令级比这个”高阶”, 比如数据访问永远都是以obj为单位. 但基本上思路是一样的. tengo运行过程 在REPL中加打印, 主要考察符号表, 全局变量, 和constants 初始状态 symbols: &tengo.SymbolTable{parent:(*tengo.SymbolTable)(nil), block:false, store:map[string]*tengo.Symbol{\"ex\":(*tengo.Symbol)(0xc0000df980), \"show\":(*tengo.Symbol)(0xc0000df9e0)}, numDe$ inition:2, maxDefinition:2, freeSymbols:[]*tengo.Symbol(nil), builtinSymbols:[]*tengo.Symbol(nil)} 没有父table, 是全局符号表. 里面有内建的两个函数: ex和show. 没毛病 constant为空 global只有两个, 对应ex和show. 没毛病 globals: []tengo.Object{(*tengo.UserFunction)(0xc0000df9b0), (*tengo.UserFunction)(0xc0000dfa10), ...后面都是nil a := 1 后 symtable由于第一次执行后, 多了内置的很多函数, 也多了变量a symbols: &tengo.SymbolTable{parent:(*tengo.SymbolTable)(nil), block:false, store:map[string]*tengo.Symbol{\"a\":(*tengo.Symbol)(0xc00018e690), \"append\":(*tengo.Symbol)(0xc00018e0f0), \"bool\":(*tengo.Symbol)(0xc00018e1e0), \"bytes\":(*tengo.Symbol)(0xc00018e270), \"char\":(*tengo.Symbol)(0xc00018e240), \"copy\":(*tengo.Symbol)(0xc00018e0c0), \"delete\":(*tengo.Symbol)(0xc00018e120), \"ex\":(*tengo.Symbol)(0xc0000df980), \"float\":(*tengo.Symbol)(0xc00018e210), \"format\":(*tengo.Symbol)(0xc00018e600), \"int\":(*tengo.Symbol)(0xc00018e1b0), \"is_array\":(*tengo.Symbol)(0xc00018e3f0), \"is_bool\":(*tengo.Symbol)(0xc00018e360), \"is_bytes\":(*tengo.Symbol)(0xc00018e3c0), \"is_callable\":(*tengo.Symbol)(0xc00018e5a0), \"is_char\":(*tengo.Symbol)(0xc00018e390), \"is_error\":(*tengo.Symbol)(0xc00018e510), \"is_float\":(*tengo.Symbol)(0xc00018e300), \"is_function\":(*tengo.Symbol)(0xc00018e570), \"is_immutable_array\":(*tengo.Symbol)(0xc00018e420), \"is_immutable_map\":(*tengo.Symbol)(0xc00018e480), \"is_int\":(*tengo.Symbol)(0xc00018e2d0), \"is_iterable\":(*tengo.Symbol)(0xc00018e4b0), \"is_map\":(*tengo.Symbol)(0xc00018e450), \"is_string\":(*tengo.Symbol)(0xc00018e330), \"is_time\":(*tengo.Symbol)(0xc00018e4e0), \"is_undefined\":(*tengo.Symbol)(0xc00018e540), \"len\":(*tengo.Symbol)(0xc00018e090), \"show\":(*tengo.Symbol)(0xc0000df9e0), \"splice\":(*tengo.Symbol)(0xc00018e150), \"string\":(*tengo.Symbol)(0xc00018e180), \"time\":(*tengo.Symbol)(0xc00018e2a0), \"type_name\":(*tengo.Symbol)(0xc00018e5d0)}, numDefinition:3, maxDefinition:3, freeSymbols:[]*tengo.Symbol(nil), builtinSymbols:[]*tengo.Symbol{(*tengo.Symbol)(0xc00018e090), (*tengo.Symbol)(0xc00018e0c0), (*tengo.Symbol)(0xc00018e0f0), (*tengo.Symbol)(0xc00018e120), (*tengo.Symbol)(0xc00018e150), (*tengo.Symbol)(0xc00018e180), (*tengo.Symbol)(0xc00018e1b0), (*tengo.Symbol)(0xc00018e1e0), (*tengo.Symbol)(0xc00018e210), (*tengo.Symbol)(0xc00018e240), (*tengo.Symbol)(0xc00018e270), (*tengo.Symbol)(0xc00018e2a0), (*tengo.Symbol)(0xc00018e2d0), (*tengo.Symbol)(0xc00018e300), (*tengo.Symbol)(0xc00018e330), (*tengo.Symbol)(0xc00018e360), (*tengo.Symbol)(0xc00018e390), (*tengo.Symbol)(0xc00018e3c0), (*tengo.Symbol)(0xc00018e3f0), (*tengo.Symbol)(0xc00018e420), (*tengo.Symbol)(0xc00018e450), (*tengo.Symbol)(0xc00018e480), (*tengo.Symbol)(0xc00018e4b0), (*tengo.Symbol)(0xc00018e4e0), (*tengo.Symbol)(0xc00018e510), (*tengo.Symbol)(0xc00018e540), (*tengo.Symbol)(0xc00018e570), (*tengo.Symbol)(0xc00018e5a0), (*tengo.Symbol)(0xc00018e5d0), (*tengo.Symbol)(0xc00018e600)}} constant多了一个 constants: []tengo.Object{(*tengo.Int)(0xc000138028)} globals多了一个, 多的这个和constant多出来的是一个地址. globals: []tengo.Object{(*tengo.UserFunction)(0xc0000df9b0), (*tengo.UserFunction)(0xc0000dfa10), (*tengo.Int)(0xc000138028) b := 2 后 symbol表多了b, 但似乎其他的符号地址都变了. -- 这里好像有bug symbols: &tengo.SymbolTable{parent:(*tengo.SymbolTable)(nil), block:false, store:map[string]*tengo.Symbol{\"a\":(*tengo.Symbol)(0xc00018e690), \"append\":(*tengo.Symbol)(0xc00020a0f0), \"b\":(*tengo.Symbol)(0xc00020a690), \"bool\":(*tengo.Symbol)(0xc00020a1e0), \"bytes\":(*tengo.Symbol)(0xc00020a270), \"char\":(*tengo.Symbol)(0xc00020a240), \"copy\":(*tengo.Symbol)(0xc00020a0c0), \"delete\":(*tengo.Symbol)(0xc00020a120), \"ex\":(*tengo.Symbol)(0xc0000df980), \"float\":(*tengo.Symbol)(0xc00020a210), \"format\":(*tengo.Symbol)(0xc00020a600), \"int\":(*tengo.Symbol)(0xc00020a1b0), \"is_array\":(*tengo.Symbol)(0xc00020a3f0), \"is_bool\":(*tengo.Symbol)(0xc00020a360), \"is_bytes\":(*tengo.Symbol)(0xc00020a3c0), \"is_callable\":(*tengo.Symbol)(0xc00020a5a0), \"is_char\":(*tengo.Symbol)(0xc00020a390), \"is_error\":(*tengo.Symbol)(0xc00020a510), \"is_float\":(*tengo.Symbol)(0xc00020a300), \"is_function\":(*tengo.Symbol)(0xc00020a570), \"is_immutable_array\":(*tengo.Symbol)(0xc00020a420), \"is_immutable_map\":(*tengo.Symbol)(0xc00020a480), \"is_int\":(*tengo.Symbol)(0xc00020a2d0), \"is_iterable\":(*tengo.Symbol)(0xc00020a4b0), \"is_map\":(*tengo.Symbol)(0xc00020a450), \"is_string\":(*tengo.Symbol)(0xc00020a330), \"is_time\":(*tengo.Symbol)(0xc00020a4e0), \"is_undefined\":(*tengo.Symbol)(0xc00020a540), \"len\":(*tengo.Symbol)(0xc00020a090), \"show\":(*tengo.Symbol)(0xc0000df9e0), \"splice\":(*tengo.Symbol)(0xc00020a150), \"string\":(*tengo.Symbol)(0xc00020a180), \"time\":(*tengo.Symbol)(0xc00020a2a0), \"type_name\":(*tengo.Symbol)(0xc00020a5d0)}, numDefinition:4, maxDefinition:4, freeSymbols:[]*tengo.Symbol(nil), builtinSymbols:[]*tengo.Symbol{(*tengo.Symbol)(0xc00018e090), (*tengo.Symbol)(0xc00018e0c0), (*tengo.Symbol)(0xc00018e0f0), (*tengo.Symbol)(0xc00018e120), (*tengo.Symbol)(0xc00018e150), (*tengo.Symbol)(0xc00018e180), (*tengo.Symbol)(0xc00018e1b0), (*tengo.Symbol)(0xc00018e1e0), (*tengo.Symbol)(0xc00018e210), (*tengo.Symbol)(0xc00018e240), (*tengo.Symbol)(0xc00018e270), (*tengo.Symbol)(0xc00018e2a0), (*tengo.Symbol)(0xc00018e2d0), (*tengo.Symbol)(0xc00018e300), (*tengo.Symbol)(0xc00018e330), (*tengo.Symbol)(0xc00018e360), (*tengo.Symbol)(0xc00018e390), (*tengo.Symbol)(0xc00018e3c0), (*tengo.Symbol)(0xc00018e3f0), (*tengo.Symbol)(0xc00018e420), (*tengo.Symbol)(0xc00018e450), (*tengo.Symbol)(0xc00018e480), (*tengo.Symbol)(0xc00018e4b0), (*tengo.Symbol)(0xc00018e4e0), (*tengo.Symbol)(0xc00018e510), (*tengo.Symbol)(0xc00018e540), (*tengo.Symbol)(0xc00018e570), (*tengo.Symbol)(0xc00018e5a0), (*tengo.Symbol)(0xc00018e5d0), (*tengo.Symbol)(0xc00018e600), (*tengo.Symbol)(0xc00020a090), (*tengo.Symbol)(0xc00020a0c0), (*tengo.Symbol)(0xc00020a0f0), (*tengo.Symbol)(0xc00020a120), (*tengo.Symbol)(0xc00020a150), (*tengo.Symbol)(0xc00020a180), (*tengo.Symbol)(0xc00020a1b0), (*tengo.Symbol)(0xc00020a1e0), (*tengo.Symbol)(0xc00020a210), (*tengo.Symbol)(0xc00020a240), (*tengo.Symbol)(0xc00020a270), (*tengo.Symbol)(0xc00020a2a0), (*tengo.Symbol)(0xc00020a2d0), (*tengo.Symbol)(0xc00020a300), (*tengo.Symbol)(0xc00020a330), (*tengo.Symbol)(0xc00020a360), (*tengo.Symbol)(0xc00020a390), (*tengo.Symbol)(0xc00020a3c0), (*tengo.Symbol)(0xc00020a3f0), (*tengo.Symbol)(0xc00020a420), (*tengo.Symbol)(0xc00020a450), (*tengo.Symbol)(0xc00020a480), (*tengo.Symbol)(0xc00020a4b0), (*tengo.Symbol)(0xc00020a4e0), (*tengo.Symbol)(0xc00020a510), (*tengo.Symbol)(0xc00020a540), (*tengo.Symbol)(0xc00020a570), (*tengo.Symbol)(0xc00020a5a0), (*tengo.Symbol)(0xc00020a5d0), (*tengo.Symbol)(0xc00020a600)}} constant多了一个, 就是b constants: []tengo.Object{(*tengo.Int)(0xc000138028), (*tengo.Int)(0xc00012a178)} globals也多了这一个 globals: []tengo.Object{(*tengo.UserFunction)(0xc0000df9b0), (*tengo.UserFunction)(0xc0000dfa10), (*tengo.Int)(0xc000138028), (*tengo.Int)(0xc00012a178) c := a+b 后 symbol table多了c, 但还是之前的问题: builtin的符号都被重新注册了一次, 地址都变了.但constants没有变化, 即c不是constantglobals多了c, 其他都一样. globals: []tengo.Object{(*tengo.UserFunction)(0xc0000df9b0), (*tengo.UserFunction)(0xc0000dfa10), (*tengo.Int)(0xc000138028), (*tengo.Int)(0xc00012a178), (*tengo.Int)(0xc0001e4228) f := func(a, b) { return a + b } 后 首先符号表多了f这个没有疑问. constants多了f, 类型是CompiledFunction constants: []tengo.Object{(*tengo.Int)(0xc000138028), (*tengo.Int)(0xc00012a178), (*tengo.CompiledFunction)(0xc0000c86e0)} 这个f也出现在globals里面 globals: []tengo.Object{(*tengo.UserFunction)(0xc0000df9b0), (*tengo.UserFunction)(0xc0000dfa10), (*tengo.Int)(0xc000138028), (*tengo.Int)(0xc00012a178), (*tengo.Int)(0xc0001e4228), (*ten go.CompiledFunction)(0xc0000c86e0) FormatConstants函数 这个函数可以查看constant的值, 比如: fmt.Println(bytecode.FormatConstants()) 会打印: [[ 0] 1 (Int|0xc0000f4920) [ 1] 2 (Int|0xc0000f4950) [ 2] (Compiled Function|0xc0000c4730) 0000 GETL 0 0002 GETL 1 0004 BINARYOP 11 0006 RET 1 ] 这个Compiled Function和汇编的风格很像. 同时可以看到FormatConstants函数能够把编译后的字节码打印出来. 那么什么是constant? 为什么constants数组在编译和运行时段都存在?constants是个对象数组, 保存的是固定值的变量, 比如上文的 a := 1 b := 2 f := func(a, b){...} constants在何处使用, 在何处定义 首先, VM在运行的时候, 在当前指令是parser.OpConstant的时候, 回去constants数组找需要的对象: v.stack[v.sp] = v.constants[cidx] func (v *VM) run() { for atomic.LoadInt64(&v.aborting) == 0 { v.ip++ switch v.curInsts[v.ip] { case parser.OpConstant: v.ip += 2 cidx := int(v.curInsts[v.ip]) | int(v.curInsts[v.ip-1]) 注意, constants对象数组是靠index来索引的. 这些constant对象, 是在编译的时候, 编译器发现a := 1变量是个常量, 就把它放到constants数组里, 并emit parser.OpConstant指令. func (c *Compiler) addConstant(o Object) int { if c.parent != nil { // module compilers will use their parent's constants array return c.parent.addConstant(o) } c.constants = append(c.constants, o) if c.trace != nil { c.printTrace(fmt.Sprintf(\"CONST %04d %s\", len(c.constants)-1, o)) } return len(c.constants) - 1 } // Compile compiles the AST node. func (c *Compiler) Compile(node parser.Node) error { ... case *parser.IntLit: c.emit(node, parser.OpConstant, c.addConstant(&Int{Value: node.Value})) case *parser.FloatLit: c.emit(node, parser.OpConstant, c.addConstant(&Float{Value: node.Value})) 为什么constants要传递给NewCompiler? 主要是用在REPL场景下, 把上一次的constants传递给这一次, 因为REPL是增量式编译执行模式, 需要前面的常量表和符号表, 而增量式的run也需要上一次的globals表. srcFile := fileSet.AddFile(\"repl\", -1, len(line)) p := parser.NewParser(srcFile, []byte(line), nil) file, err := p.ParseFile() if err != nil { fmt.Println(err) continue } c := tengo.NewCompiler(srcFile, sh.symbolTable, constants, sh.modules, nil) if err := c.Compile(file); err != nil { fmt.Println(err) continue } bytecode := c.Bytecode() bytecode.RemoveDuplicates() machine := tengo.NewVM(bytecode, sh.globals, -1) if err := machine.Run(); err != nil { fmt.Println(err) continue } constants = bytecode.Constants fmt.Println(bytecode.FormatConstants()) extension 为什么extension.UserFunction不生效? -- 注意Copy()方法 我在eobjects.go里面, 继承了tengo.UserFunction type UserFunction struct { tengo.UserFunction Signature string Help string } 我是想重载String方法, 这样fmt.println会调用到这个String方法, 显示help信息. func (o *UserFunction) TypeName() string { return \"user-function(extended):\" + o.Name } func (o *UserFunction) String() string { return o.Signature + \"\\n\" + o.Help } 在注册function的时候, 类型为&extension.UserFunction 编译通过, 似乎没问题. var module = map[string]tengo.Object{ \"newx\": &extension.UserFunction{ UserFunction: tengo.UserFunction{ Name: \"new\", Value: newStat, }, Signature: \"new({option1:true, option2:false}) => statCollector\", Help: \"option can be \", }, // new({option1:true, option2:false}) => statCollector } 但为什么还是调到了原tengo.UserFunction? 应该会打印帮助文本啊??? pidstat := import(\"pidstat\") fmt := import(\"fmt\") fmt.println(pidstat) {newx: , __module_name__: \"pidstat\"} 调查 用vscode和dlv调查, 发现在注册的时候是对的, object类型是&extension.UserFunction \"newx\": ) data: (0xc0000f8050) 但调用到fmt.println的时候, 传入的object类型就变成了 ) 是go的\"继承\"系统出了问题吗? 看起来像是对象从派生类变成基类了? -- 不是. go的继承没问题. 问题在注册module对象的时候, 返回了原module对象的Copy()方法, 目的是返回一个ImmutableMap // BuiltinModule is an importable module that's written in Go. type BuiltinModule struct { Attrs map[string]Object } // Import returns an immutable map for the module. func (m *BuiltinModule) Import(moduleName string) (interface{}, error) { return m.AsImmutableMap(moduleName), nil } // AsImmutableMap converts builtin module into an immutable map. func (m *BuiltinModule) AsImmutableMap(moduleName string) *ImmutableMap { attrs := make(map[string]Object, len(m.Attrs)) for k, v := range m.Attrs { attrs[k] = v.Copy() } attrs[\"__module_name__\"] = &String{Value: moduleName} return &ImmutableMap{Value: attrs} } 注意AsImmutableMap()函数里面, 调用了attrs[k] = v.Copy() 问题就出在这里: 我继承了基类的Copy()方法: // Copy returns a copy of the type. func (o *UserFunction) Copy() Object { return &UserFunction{Value: o.Value} } 它返回一个tengo.UserFunction对象. 所以后面fmt.println()的时候, 实际打印的是这个对象. 结论 tengo在注册module的时候, 为了不改变原对象, 对原对象进行了Copy(), 返回了immutable对象. immutable对象实际是调用原对象的Copy()方法来的, 要extend的话, 需要自己实现Copy()方法. bash 能够在bash的命令里引用脚本的变量吗? file := \"test\" fmt.println(bash.run(`touch $file`).output()) 简单回答: 不能. 因为bash.run()实际是调用的native go的代码, 调用的时候已经是在tengo的VM中运行的字节码调用的. 在ast阶段是有符号概念的, 但ast编译成字节码之后, 符号已经变成了\"地址\"了.所以, 调用到bash.run()时, 已经没有\"file\"这个变量了, 只有其对应的地址. 除非在ast中新增对$的解析, 在ast编译成字节码的阶段, 把touch $file对file的访问, emit成一个GetLocal的op. 比如c.emit(node, parser.OpGetLocal, symbol.Index) 解决方法 只有分两步走: file := \"testfile\" cmd := fmt.sprintf(\"touch %s\", file) fmt.println(bash.run(cmd).output()) 注: 使用tengo内置的format函数更简单点 pid := 10086 fmt.println(format(\"hello %d\", pid)) 添加新的builtin函数ex() 能否添加到builtinFuncs表 先说结论: tengo的builtin函数表是固定的, 外面无法更改. builtins.go中, 有个固化的表: var builtinFuncs = []*BuiltinFunction{ { Name: \"len\", Value: builtinLen, }, { Name: \"copy\", Value: builtinCopy, }, } 调用这个表里的函数, 不是以字符串方式查找函数的, 而是index: vm.go执行字节码阶段的run()函数: case parser.OpGetBuiltin: v.ip++ builtinIndex := int(v.curInsts[v.ip]) v.stack[v.sp] = builtinFuncs[builtinIndex] v.sp++ 从字节码里面取出builtinIndex, 查找builtinFuncs, 得到函数, 放到stack上. 而字节码的builtinIndex, 是在编译阶段compiler.go中Compile()函数: case *parser.Ident: symbol, _, ok := c.symbolTable.Resolve(node.Name, false) if !ok { return c.errorf(node, \"unresolved reference '%s'\", node.Name) } switch symbol.Scope { case ScopeGlobal: c.emit(node, parser.OpGetGlobal, symbol.Index) case ScopeLocal: c.emit(node, parser.OpGetLocal, symbol.Index) case ScopeBuiltin: c.emit(node, parser.OpGetBuiltin, symbol.Index) case ScopeFree: c.emit(node, parser.OpGetFree, symbol.Index) } 对符号的处理, 统一都是Resolve()当前的symbolTable, 得到symbol的Index信息, emit到字节码中. 字节码里面已经没有符号, 都是通过index来操作的. 这样最高效. 而最开始的符号, 是初始化的时候加的: gshell.go中runREPL()函数 symbolTable := tengo.NewSymbolTable() for idx, fn := range tengo.GetAllBuiltinFunctions() { symbolTable.DefineBuiltin(idx, fn.Name) } 这里只是加了符号表信息, 把符号的名字和index对上. 实际运行阶段, 按前文所述, vm会去builtinFuncs表中找. 添加到global表 函数也是对象, 那就可以当作\"全局变量\"添加到global表: 在symbolTable里定义一个ex的符号, 用返回的index和globals里面实际定义的函数对应起来. globals := make([]tengo.Object, tengo.GlobalsSize) symbolTable := tengo.NewSymbolTable() symbol := symbolTable.Define(\"ex\") globals[symbol.Index] = &tengo.UserFunction{ Name: \"ex\", Value: func(args ...tengo.Object) (ret tengo.Object, err error) { if len(args) != 1 { return nil, tengo.ErrWrongNumArguments } return extension.ExtendObj(args[0]) }, } 然后在NewCompiler()的时候传入symbolTable; 在NewVM()的时候传入globals 因为之前符号表和全局变量对的上, 那么运行时就能找到正确的函数. tengo代码 从简单的例子开始: import \"github.com/d5/tengo/v2\" var code = ` reduce := func(seq, fn) { s := 0 for x in seq { fn(x, s) } return s } print(reduce([1, 2, 3], func(x, s) { s += x })) ` func main() { s := tengo.NewScript([]byte(code)) if _, err := s.Run(); err != nil { panic(err) } } 上面是直接运行的例子. 下面是编译成字节码后运行的例子. s := tengo.NewScript([]byte(`a := b + 20`)) s.Add(\"b\", 10) c, err := s.Compile() err := c.Run() a := c.Get(\"a\") c.Set(\"b\", 20) c.Run() examples/interoperability/main.go func main() { src := ` // goproxy and proxy must be imported. goproxy := import(\"goproxy\") proxy := import(\"proxy\") global := 0 callbacks := { sum: func(a, b) { return a + b }, multiply: func(a, b) { return a * b }, increment: func() { global++ return global } } // Register callbacks to call them in goproxy loop. goproxy.register(callbacks) // goproxy loop waits for new call requests and run them with the help of // \"proxy\" source module. Cancelling the context breaks the loop. for goproxy.next() { proxy(goproxy.args()) } ` // 5 seconds context timeout is enough for an example. ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() script := tengo.NewScript([]byte(src)) moduleMap := tengo.NewModuleMap() goproxy := NewGoProxy(ctx) // register modules moduleMap.AddBuiltinModule(\"goproxy\", goproxy.ModuleMap()) moduleMap.AddSourceModule(\"proxy\", []byte(ProxySource)) script.SetImports(moduleMap) compiled, err := script.Compile() if err != nil { panic(err) } // call \"sum\", \"multiply\", \"increment\" functions from tengo in a new goroutine go func() { callChan := goproxy.CallChan() result := make(chan tengo.Object, 1) // TODO: check tengo error from result channel. loop: for { select { case 可被注册的对象: native go // NewGoProxy creates GoProxy object. func NewGoProxy(ctx context.Context) *GoProxy { mod := new(GoProxy) mod.ctx = ctx mod.callbacks = make(map[string]tengo.Object) mod.callChan = make(chan *CallArgs, 1) mod.moduleMap = map[string]tengo.Object{ \"next\": &tengo.UserFunction{Value: mod.next}, \"register\": &tengo.UserFunction{Value: mod.register}, \"args\": &tengo.UserFunction{Value: mod.args}, } mod.tasks = list.New() return mod } // GoProxy is a builtin tengo module to register tengo functions and run them. type GoProxy struct { tengo.ObjectImpl //匿名包含tengo.ObjectImpl就是tengo的Object ctx context.Context moduleMap map[string]tengo.Object callbacks map[string]tengo.Object callChan chan *CallArgs tasks *list.List mtx sync.Mutex } 上面的mod.next mod.args等符号, 是函数: 比如 func (mod *GoProxy) register(args ...tengo.Object) (tengo.Object, error) { if len(args) == 0 { return nil, tengo.ErrWrongNumArguments } mod.mtx.Lock() defer mod.mtx.Unlock() switch v := args[0].(type) { case *tengo.Map: mod.callbacks = v.Value case *tengo.ImmutableMap: mod.callbacks = v.Value default: return nil, tengo.ErrInvalidArgumentType{ Name: \"first\", Expected: \"map\", Found: args[0].TypeName(), } } return tengo.UndefinedValue, nil } 这些函数必须符合CallableFunc签名: // CallableFunc is a function signature for the callable functions. type CallableFunc = func(args ...Object) (ret Object, err error) 注意, 这里mod.register等函数, 在语法上是*GoProxy的方法, 为什么符合CallableFunc签名呢?tengo脚本里面的goproxy.register(callbacks) 又是怎么调用到func (mod *GoProxy) register(args ...tengo.Object) (tengo.Object, error)的呢? 这里的receiver是从哪里来的呢?还有, New过的goproxy, 怎么在脚本和native go之前共享的呢?goproxy := NewGoProxy(ctx)见 native go对象穿越脚本的黑魔法 注: native go也可以调用tengo的函数脚本的goproxy.register(callbacks)其实会调用到native go的register函数, 而它的callbacks是tengo脚本里的函数 global := 0 callbacks := { sum: func(a, b) { return a + b }, multiply: func(a, b) { return a * b }, increment: func() { global++ return global } } 上面的脚本函数对应native go的*tengo.CompiledFunction对象. 在neitive go里断言得到compiledFunc对象. //mod.callbacks是在register的时候赋值的 //mod.callbacks = v.Value f, ok := mod.callbacks[callArgs.Func] compiledFunc, ok := f.(*tengo.CompiledFunction) 然后return一个不可改的tengo.Map return &tengo.ImmutableMap{ Value: map[string]tengo.Object{ ... \"callable\": compiledFunc, } } 通过这个对象, 脚本就又可以调用\"callable\"函数了, 即最终, 脚本调用\"callable\"是compiledFunc, 而后者就是从tengo脚本compile来的. 通常使用类型断言来解参数, 比如 switch v := args[0].(type) { case *tengo.Map: mod.callbacks = v.Value case *tengo.ImmutableMap: mod.callbacks = v.Value 这需要直到tengo中Object的具体类型的定义: type Map struct { ObjectImpl Value map[string]Object } native go里实现的函数, 可以做参数检查: 在这里是提示tengo脚本里的入参类型不对 default: return nil, tengo.ErrInvalidArgumentType{ Name: \"first\", Expected: \"map\", Found: args[0].TypeName(), } 从tengo脚本调用native go的过程就是解tengo.Object到go对象, 运算, 再返回tengo.Object的过程 函数方法返回的Object, 可以继续是一个BuiltinModule func (mod *GoProxy) args(args ...tengo.Object) (tengo.Object, error) { mod.mtx.Lock() defer mod.mtx.Unlock() if mod.tasks.Len() == 0 { return tengo.UndefinedValue, nil } el := mod.tasks.Front() callArgs, ok := el.Value.(*CallArgs) if !ok || callArgs == nil { return nil, errors.New(\"invalid call arguments\") } mod.tasks.Remove(el) f, ok := mod.callbacks[callArgs.Func] if !ok { return tengo.UndefinedValue, nil } compiledFunc, ok := f.(*tengo.CompiledFunction) if !ok { return tengo.UndefinedValue, nil } params := callArgs.Params if params == nil { params = make([]tengo.Object, 0) } // callable.VarArgs implementation is omitted. return &tengo.ImmutableMap{ Value: map[string]tengo.Object{ \"result\": &tengo.UserFunction{ Value: func(args ...tengo.Object) (tengo.Object, error) { if len(args) > 0 { callArgs.Result 注: native go&tengo.ImmutableMap中的\"params\": &tengo.Array{Value: params},可以被tengo脚本使用: v = callable(args.params[0], args.params[1], args.params[2]) 可被注册的对象: tengo脚本 在main()中import了tengo的源码: moduleMap.AddSourceModule(\"proxy\", []byte(ProxySource)) 脚本源码在此: // ProxySource is a tengo script to handle bidirectional arguments flow between // go and pure tengo functions. Note: you should add more if conditions for // different number of parameters. // TODO: handle variadic functions. var ProxySource = ` export func(args) { if is_undefined(args) { return } callable := args.callable if is_undefined(callable) { return } result := args.result num_params := args.num_params v := undefined // add more else if conditions for different number of parameters. if num_params == 0 { v = callable() } else if num_params == 1 { v = callable(args.params[0]) } else if num_params == 2 { v = callable(args.params[0], args.params[1]) } else if num_params == 3 { v = callable(args.params[0], args.params[1], args.params[2]) } result(v) } ` 注: 要在外部脚本引用的对象, 用export关键词导出 native go对象穿越脚本的黑魔法 在native go中, New了对象后, 只是把goproxy的moduleMap注册给script goproxy := NewGoProxy(ctx) moduleMap.AddBuiltinModule(\"goproxy\", goproxy.ModuleMap()) 其中, 这个map包括了这个模块支持的方法 mod.moduleMap = map[string]tengo.Object{ \"next\": &tengo.UserFunction{Value: mod.next}, \"register\": &tengo.UserFunction{Value: mod.register}, \"args\": &tengo.UserFunction{Value: mod.args}, } 比如, register函数原型是 func (mod *GoProxy) register(args ...tengo.Object) (tengo.Object, error) 被包装成了&tengo.UserFunction{Value: mod.register}保存在goproxy.ModuleMap中 而在tengo脚本中, import只是得到这个map, 就直接调用native go的register方法了 goproxy := import(\"goproxy\") //调用native go的方法 goproxy.register(callbacks) 那么问题是, receiver哪去了? 没有receiver怎么调用 dlv调查 tengo脚本经过compile后, Run这个字节码: compiled.RunContext(ctx) vm.run()的执行循环中 for atomic.LoadInt64(&v.aborting) == 0 { v.ip++ switch v.curInsts[v.ip] { case parser.OpCall: //支持CompiledFunction类型的执行, 这块对应tengo编译后的函数 //但这里走的是UserFunction的分支 var args []Object args = append(args, v.stack[v.sp-numArgs:v.sp]...) //这里就是函数调用了 //value的静态类型是tengo.Object, 动态类型是tengo.UserFunction //其值是main.(*GoProxy).register-fm, 见下图当时value的值 ret, e := value.Call(args...) v.sp -= numArgs + 1 ... 当时value的值到这里有点有意思了: native go注册的函数是mod.register mod.moduleMap = map[string]tengo.Object{ \"register\": &tengo.UserFunction{Value: mod.register}, } 这里看到实际是main.(*GoProxy).register-fm 这个带fm字样的函数应该是编译器生成的, 或者说是编译器内部对\"方法\"到\"函数\"的转换表达: 它自带.this做为receiverdlv对main.(*GoProxy).register-fm的处理也是隐形的: 它直接定位到func (mod *GoProxy) register(args ...tengo.Object) (tengo.Object, error)函数, 但调用栈能看到:register-fm是register的上级函数:注意: register-fm的.this就是注册时候的\"register\": &tengo.UserFunction{Value: mod.register},中的mod, 是同一个对象. 待调用到register时, .this从register-fm变为receiver. 对象穿越魔法揭秘 编译的时候, 编译器发现这里把\"方法\"赋值给\"函数\", 比如:func (mod *GoProxy) register(args ...tengo.Object) (tengo.Object, error)赋值给CallableFunc, 其定义为func(args ...Object) (ret Object, err error)即:\"register\": &tengo.UserFunction{Value: mod.register} 那么此时编译器会给每个obj.function自动生成functionName-fm()函数, obj被保存到.this. 运行的时候functionName-fm()会调用obj.function(), 并把保存的.this做为receiver, 类似调用(.this).fucntion()编译后的符号表有functionName-fm()出现: go tool objdump -S interoperability看的非常清楚: 赋值的时候就是register-fm这样赋值的结果是: mod对象会被带进UserFunction 所以, 这里的魔法是编译器在把\"方法\"转换为\"函数\"时, 生成的-fm包装代码. 详见: src/cmd/compile/internal/gc/closure.go的makepartialcall()函数 -- 真正的黑科技永远是芯片和编译器 使用注意 注意要用实际对象的方法来注册, 即下面的mod是个实例化过的对象. // NewGoProxy creates GoProxy object. func NewGoProxy(ctx context.Context) *GoProxy { mod := new(GoProxy) mod.ctx = ctx mod.callbacks = make(map[string]tengo.Object) mod.callChan = make(chan *CallArgs, 1) mod.moduleMap = map[string]tengo.Object{ \"next\": &tengo.UserFunction{Value: mod.next}, \"register\": &tengo.UserFunction{Value: mod.register}, \"args\": &tengo.UserFunction{Value: mod.args}, } mod.tasks = list.New() return mod } 其实(*GoProxy).next也是语法合法的, 但它是type func(*GoProxy, args ...v2.Object) (v2.Object, error), 第一个参数是*GoProxy; 在这里不能赋值给func(args ...Object) (ret Object, err error) 总结 tengo脚本和自定义native go的交互: native go的入参和返回值都是tengo.Object native go返回的tengo.Object的concrete类型可以是 &tengo.UserFunction: 脚本可以继续调函数 &tengo.Int: 脚本能直接用做Int &tengo.Array: 脚本能按[index]来访问 其他Object类型 估计也可以自定义类型, 见用户可以自定义类型 tengo脚本可以调用native go的代码 native go也可以调用tengo的函数 native go的对象方法能够转换为tengo脚本的函数. 要在tengo脚本里import(\"module_name\"), 这个module必须先在native go代码里注册. 参考stdlib/stdlib.go中的GetModuleMap()函数 tengo锁 tengo并发 script_test.go中, 有个测并发的函数: 用了compiled.Clone()复制一个compiled对象, 每个go routine用复制的compiled对象来运行. for i := 0; i Clone函数主要是clone了golbals全局变量集, 这样routine之前的全局变量不打架.其他的几个filed, 都没有实际拷贝, 而是用了引用. // Clone creates a new copy of Compiled. Cloned copies are safe for concurrent // use by multiple goroutines. func (c *Compiled) Clone() *Compiled { c.lock.Lock() defer c.lock.Unlock() clone := &Compiled{ globalIndexes: c.globalIndexes, bytecode: c.bytecode, globals: make([]Object, len(c.globals)), maxAllocs: c.maxAllocs, } // copy global objects for idx, g := range c.globals { if g != nil { clone.globals[idx] = g } } return clone } 说明: 在vM执行过程中, bytecode是不会变的 global的对照表也不变 代码组织: tengo很多代码都是直接属于tengo包的. 总结 SourceFile和symbolTable和constants和modules是Compiler(*tengo.Compiler)的属性. 而经过parser解析文本内容而来的代表ast的parser.File是Compiler的输入 编译后的bytecode和全局变量array是运行时VM的输入 constants是运行时\"不变\"的Object的集合. 比如:a:=1和b:=2中的a和b是constant; 而c=a+b的c就不是constant. 不变的函数也是constant. 随着bytecode传递, 每次执行会改变 tengo的Script经过compile后的compiled对象, 代表了VM之前的所有步骤. compiled包括globals和bytecode 在没有compiled.Run之前, globals是可以改的. compiled.Set()可以改 compiled.Get()可以得到结果. compiled的Get()和Set()是go调用tengo脚本的时候, 和脚本交互变量用的. error.go 内部预定义错误, 比如ErrStringLimit = errors.New(\"exceeding string size limit\") require/require.go require.go依赖标准库testing, 提供了一些type断言的功能. 主要是tengo的内部test代码在用. parser/opcodes.go opcodes定义了支持的所有操作: 注意这里是\"指令流\"的操作, 没有for, 因为for已经被编译成类似Jump的指令了 // List of opcodes const ( OpConstant Opcode = iota // Load constant OpBComplement // bitwise complement OpPop // Pop OpTrue // Push true OpFalse // Push false OpEqual // Equal == OpNotEqual // Not equal != OpMinus // Minus - OpLNot // Logical not ! OpJumpFalsy // Jump if falsy OpAndJump // Logical AND jump OpOrJump // Logical OR jump OpJump // Jump OpNull // Push null OpArray // Array object OpMap // Map object OpError // Error object OpImmutable // Immutable object OpIndex // Index operation OpSliceIndex // Slice operation OpCall // Call function OpReturn // Return OpGetGlobal // Get global variable OpSetGlobal // Set global variable OpSetSelGlobal // Set global variable using selectors OpGetLocal // Get local variable OpSetLocal // Set local variable OpDefineLocal // Define local variable OpSetSelLocal // Set local variable using selectors OpGetFreePtr // Get free variable pointer object OpGetFree // Get free variables OpSetFree // Set free variables OpGetLocalPtr // Get local variable as a pointer OpSetSelFree // Set free variables using selectors OpGetBuiltin // Get builtin function OpClosure // Push closure OpIteratorInit // Iterator init OpIteratorNext // Iterator next OpIteratorKey // Iterator key OpIteratorValue // Iterator value OpBinaryOp // Binary operation OpSuspend // Suspend VM ) parser/source_file.go 提供了脚本file的表达, 重点是file 集合 // SourceFileSet represents a set of source files. type SourceFileSet struct { Base int // base offset for the next file Files []*SourceFile // list of files in the order added to the set LastFile *SourceFile // cache of last file looked up } 其中SourceFile还可以包括Files // SourceFile represents a source file. type SourceFile struct { // SourceFile set for the file set *SourceFileSet // SourceFile name as provided to AddFile Name string // SourcePos value range for this file is [base...base+size] Base int // SourceFile size as provided to AddFile Size int // Lines contains the offset of the first character for each line // (the first entry is always 0) Lines []int } 所以一个file集合是个树状的. parser/parser.go parser的底层是token, 对下调用token的方法, 对上提供词法分析 parser持有scanner实例, 提供next方法 // Parser parses the Tengo source files. It's based on Go's parser // implementation. type Parser struct { file *SourceFile errors ErrorList scanner *Scanner pos Pos token token.Token tokenLit string exprLevel int // = 0: in expression syncPos Pos // last sync position syncCount int // number of advance calls without progress trace bool indent int traceOut io.Writer } NewParser()函数 NewParser返回一个parser对象, 值得一提的是, 如果trace不是nil, 会输出debug信息到指定的io.Writer // NewParser creates a Parser. func NewParser(file *SourceFile, src []byte, trace io.Writer) *Parser { p := &Parser{ file: file, trace: trace != nil, traceOut: trace, } p.scanner = NewScanner(p.file, src, func(pos SourceFilePos, msg string) { p.errors.Add(pos, msg) }, 0) p.next() return p } ParseFile()函数 ParseFile的核心是返回一个[]Stmt, 即表达式的集合 // ParseFile parses the source and returns an AST file unit. func (p *Parser) ParseFile() (file *File, err error) { defer func() { if e := recover(); e != nil { if _, ok := e.(bailout); !ok { panic(e) } } p.errors.Sort() //这里用defer在最后来排序erros err = p.errors.Err() }() if p.trace { defer untracep(tracep(p, \"File\")) //使用了defer的trace技术 } if p.errors.Len() > 0 { return nil, p.errors.Err() } stmts := p.parseStmtList() if p.errors.Len() > 0 { return nil, p.errors.Err() } file = &File{ InputFile: p.file, Stmts: stmts, } return } stmt是个抽象化的接口 type Stmt interface { Node stmtNode() } parseStmtList()不断调用p.parseStmt(), 并把结果放到[]Stmt中 func (p *Parser) parseStmtList() (list []Stmt) { if p.trace { defer untracep(tracep(p, \"StatementList\")) } for p.token != token.RBrace && p.token != token.EOF { list = append(list, p.parseStmt()) } return } parseStmt()是核心函数, 根据每次根据token的类型做动作, 并且前进(advance) func (p *Parser) parseStmt() (stmt Stmt) { if p.trace { defer untracep(tracep(p, \"Statement\")) } switch p.token { case // simple statements token.Func, token.Error, token.Immutable, token.Ident, token.Int, token.Float, token.Char, token.String, token.True, token.False, token.Undefined, token.Import, token.LParen, token.LBrace, token.LBrack, token.Add, token.Sub, token.Mul, token.And, token.Xor, token.Not: s := p.parseSimpleStmt(false) p.expectSemi() return s case token.Return: return p.parseReturnStmt() case token.Export: return p.parseExportStmt() case token.If: return p.parseIfStmt() case token.For: return p.parseForStmt() case token.Break, token.Continue: return p.parseBranchStmt(p.token) case token.Semicolon: s := &EmptyStmt{Semicolon: p.pos, Implicit: p.tokenLit == \"\\n\"} p.next() return s case token.RBrace: // semicolon may be omitted before a closing \"}\" return &EmptyStmt{Semicolon: p.pos, Implicit: true} default: pos := p.pos p.errorExpected(pos, \"statement\") p.advance(stmtStart) return &BadStmt{From: pos, To: p.pos} } } 比如其中的token.If func (p *Parser) parseIfStmt() Stmt { if p.trace { defer untracep(tracep(p, \"IfStmt\")) } pos := p.expect(token.If) init, cond := p.parseIfHeader() body := p.parseBlockStmt() var elseStmt Stmt if p.token == token.Else { p.next() switch p.token { case token.If: elseStmt = p.parseIfStmt() case token.LBrace: elseStmt = p.parseBlockStmt() p.expectSemi() default: p.errorExpected(p.pos, \"if or {\") elseStmt = &BadStmt{From: p.pos, To: p.pos} } } else { p.expectSemi() } return &IfStmt{ IfPos: pos, Init: init, Cond: cond, Body: body, Else: elseStmt, } } 这个过程存在不少的递归. 比如if的body就是body := p.parseBlockStmt(), 而BlockStmt里面又可以包含任何的语句. func (p *Parser) parseBlockStmt() *BlockStmt { if p.trace { defer untracep(tracep(p, \"BlockStmt\")) } lbrace := p.expect(token.LBrace) list := p.parseStmtList() rbrace := p.expect(token.RBrace) return &BlockStmt{ LBrace: lbrace, RBrace: rbrace, Stmts: list, } } formatter.go 提供了format tengo对象Object的函数, 这些函数是自己实现的, 不依赖标准库fmt. // Format is like fmt.Sprintf but using Objects. func Format(format string, a ...Object) (string, error) Pool对象池技术 这里使用了sync.Pool技术来避免重复分配内存. 注: sync.Pool是标准库提供的对象池化技术, 主要用于cache对象, 减轻gc压力. 要点: 提供Put()和Get()接口, 用来存取pool中对象. Put()和Get()没有相关性. 不保证Put过的对象就一定能Get()到 实际上, 这里是对象池. 使用者不应该关心对象携带的\"历史\"信息. Get()一个就是一个新的对象. 可以提供一个New func() interface{}函数, Get()不到的时候也返回一个新的. 并发安全 标准库fmt使用了Pool技术 pool不能被拷贝 似乎比较heavy. 轻量的free list的需求场景建议自己实现对象free list 新建对象池, 这里提供了默认的New方法: var ppFree = sync.Pool{ New: func() interface{} { return new(pp) }, } 从对象池get: // newPrinter allocates a new pp struct or grabs a cached one. func newPrinter() *pp { p := ppFree.Get().(*pp) p.erroring = false p.fmt.init(&p.buf) return p } free对象, 放到对象池 // free saves used pp structs in ppFree; avoids an allocation per invocation. func (p *pp) free() { // Proper usage of a sync.Pool requires each entry to have approximately // the same memory cost. To obtain this property when the stored type // contains a variably-sized fmtbuf, we add a hard limit on the maximum // fmtbuf to place back in the pool. // // See https://golang.org/issue/23199 if cap(p.buf) > 64 在这里, 被池化的对象是 // pp is used to store a printer's state and is reused with sync.Pool to avoid // allocations. type pp struct { buf fmtbuf // arg holds the current item. arg Object // fmt is used to format basic items such as integers or strings. fmt formatter // reordered records whether the format string used argument reordering. reordered bool // goodArgNum records whether the most recent reordering directive was // valid. goodArgNum bool // erroring is set when printing an error string to guard against calling // handleMethods. erroring bool } tengo.go tengo.go里面的函数, 入参都是统一的Object, 利用类型断言来搞事情, 提供入参是Object, 对外统一的函数. 比如: func ToInt(o Object) (v int, ok bool) func CountObjects(o Object) (c int) ... //最后这对方法很特别, 是native go和tengo类型互转的关键 func ToInterface(o Object) (res interface{}) func FromInterface(v interface{}) (Object, error) 对外提供统一的API, 即入参是Object的API, 有两个思路 本例中, 传入Object接口, 在函数实现里面搞类型断言, 比如:// ToTime will try to convert object o to time.Time value. func ToTime(o Object) (v time.Time, ok bool) { switch o := o.(type) { case *Time: v = o.Value ok = true case *Int: v = time.Unix(o.Value, 0) ok = true } return } 所有的对外API都定义成接口, 比如要求Object全部实现func ToTime(o Object) (v time.Time, ok bool)方法 比较而言, 第一种好点: 并不是所有Object都需要ToTime, 对一个array来搞ToTime有点怪. 而通过一个统一函数, ToTime(Object)的方式, 不怪, 如果传入Array类型的Object, 就返回0值就好了. FromInterface()和ToInterface()函数是go和脚本交互的核心 FromInterface()从go的interface{}推断得到tengo的Object; 而ToInterface()正好相反 // FromInterface will attempt to convert an interface{} v to a Tengo Object func FromInterface(v interface{}) (Object, error) { switch v := v.(type) { case nil: return UndefinedValue, nil //UndefinedValue Object = &Undefined{} 定义于objects.go case string: if len(v) > MaxStringLen { //tengo的String不能大于MaxStringLen, 2G, 已经非常大了 return nil, ErrStringLimit } return &String{Value: v}, nil case int64: return &Int{Value: v}, nil case int: return &Int{Value: int64(v)}, nil case bool: if v { return TrueValue, nil } return FalseValue, nil case rune: return &Char{Value: v}, nil case byte: return &Char{Value: rune(v)}, nil case float64: return &Float{Value: v}, nil case []byte: if len(v) > MaxBytesLen { return nil, ErrBytesLimit } return &Bytes{Value: v}, nil //Bytes就是[]byte的tengo表达 case error: return &Error{Value: &String{Value: v.Error()}}, nil //Error默认是值为string case map[string]Object: //注意这个case, 比下个case更具体, 要放在前面; 从go代码调用下来, 有能力断言Object return &Map{Value: v}, nil case map[string]interface{}: //层层嵌套的map kv := make(map[string]Object) for vk, vv := range v { vo, err := FromInterface(vv) //递归调用 if err != nil { return nil, err } kv[vk] = vo } return &Map{Value: kv}, nil case []Object: //array类型 return &Array{Value: v}, nil case []interface{}: //层层嵌套的arrary arr := make([]Object, len(v)) for i, e := range v { vo, err := FromInterface(e) //递归调用 if err != nil { return nil, err } arr[i] = vo } return &Array{Value: arr}, nil case time.Time: return &Time{Value: v}, nil //Time就是time.Time的包装 case Object: return v, nil case CallableFunc: return &UserFunction{Value: v}, nil } return nil, fmt.Errorf(\"cannot convert to object: %T\", v) } 什么是callable对象 符合这个签名的都是: // CallableFunc is a function signature for the callable functions. type CallableFunc = func(args ...Object) (ret Object, err error) 经过测试 type test struct { a int } 和下面带=号的版本, 都能编过, 效果差不多 type test = struct { a int } 但还是有些差别: =版本的type, 只是别名, an alternate spelling; 别名拥有原名的一切属性 普通版本的type定义, 是一个全新的类型, 不具有原类型的方法 CountObjects这个递归函数写的真好 最里层返回预设值1, 其他情况递归的计算array/map的元素个数 // CountObjects returns the number of objects that a given object o contains. // For scalar value types, it will always be 1. For compound value types, // this will include its elements and all of their elements recursively. func CountObjects(o Object) (c int) { c = 1 switch o := o.(type) { case *Array: for _, v := range o.Value { c += CountObjects(v) } case *ImmutableArray: for _, v := range o.Value { c += CountObjects(v) } case *Map: for _, v := range o.Value { c += CountObjects(v) } case *ImmutableMap: for _, v := range o.Value { c += CountObjects(v) } case *Error: c += CountObjects(o.Value) } return } script.go 依赖parser, 用于嵌入代码和调用go代码的交互. 新建script实例, 从go代码add变量进script 还提供了一些API func (s *Script) EnableFileImport(enable bool) Script.Add用于从native go添加变量到tengo脚本 即把interface{}转换为{name, Object}, 加到script的variablesmap里 // Add adds a new variable or updates an existing variable to the script. func (s *Script) Add(name string, value interface{}) error { obj, err := FromInterface(value) //见tengo.go里的函数实现 if err != nil { return err } s.variables[name] = &Variable{ name: name, value: obj, } return nil } NewScript 在script.go中, NewScript简单到返回一个结构体: // Variable is a user-defined variable for the script. type Variable struct { name string value Object //Object是tengo的数据类型的统一表述(接口) } // Script can simplify compilation and execution of embedded scripts. type Script struct { variables map[string]*Variable //script和外部go的变量交互通过这个map modules *ModuleMap input []byte maxAllocs int64 maxConstObjects int enableFileImport bool importDir string } // NewScript creates a Script instance with an input script. func NewScript(input []byte) *Script { return &Script{ variables: make(map[string]*Variable), input: input, maxAllocs: -1, maxConstObjects: -1, } } Compile函数 Script对象的Compile方法把代码编译成Compiled对象 Compiled对象包括了字节码和全局变量, 和一个锁 // Compiled is a compiled instance of the user script. Use Script.Compile() to // create Compiled object. type Compiled struct { globalIndexes map[string]int // global symbol name to index globals []Object //以上两个域一起, 实际就是map[string]Object; 这里作者这么搞怕不是有什么优化 bytecode *Bytecode maxAllocs int64 lock sync.RWMutex } Compile函数流程 func (s *Script) Compile() (*Compiled, error) { symbolTable, globals, err := s.prepCompile() //新建符号表, 建立全局变量索引 fileSet := parser.NewFileSet() //新建一个fileset, fileset类似文件描述符, 本身不存储文件内容 srcFile := fileSet.AddFile(\"(main)\", -1, len(s.input)) //AddFile加默认的main文件, 此时只是传入len p := parser.NewParser(srcFile, s.input, nil) //到NewParser才把文件和s.input内容联系起来. file, err := p.ParseFile() c := NewCompiler(srcFile, symbolTable, nil, s.modules, nil) c.EnableFileImport(s.enableFileImport) c.SetImportDir(s.importDir) err := c.Compile(file) //优化全局变量 ... // remove duplicates from constants bytecode := c.Bytecode() bytecode.RemoveDuplicates() //检查最大object数目是否超限制 //这里的变量名和域名一样, 是合法的. return &Compiled{ globalIndexes: globalIndexes, bytecode: bytecode, //看起来这个bytecode是关键 globals: globals, maxAllocs: s.maxAllocs, }, nil } preCompile()函数 preCompile函数负责创建初始symbolTable, 并把Script.Add()加入的变量, Define到symbolTable中. 注意Define只是增加个符号, 而不是保存实际值. 实际值保存在单独的变量表中: preCompile还增加builtin的符号 preCompile返回符号表和全局变量表. func (s *Script) prepCompile() ( symbolTable *SymbolTable, globals []Object, err error, ) { var names []string for name := range s.variables { names = append(names, name) } symbolTable = NewSymbolTable() for idx, fn := range builtinFuncs { symbolTable.DefineBuiltin(idx, fn.Name) } globals = make([]Object, GlobalsSize) for idx, name := range names { symbol := symbolTable.Define(name) if symbol.Index != idx { panic(fmt.Errorf(\"wrong symbol index: %d != %d\", idx, symbol.Index)) } globals[symbol.Index] = s.variables[name].value //实际的Object保存在global中 } return } RemoveDuplicates()去掉重复常量? 没细看 RunContext()函数 这是个带超时的Run()函数 // RunContext is like Run but includes a context. func (c *Compiled) RunContext(ctx context.Context) (err error) { c.lock.Lock() defer c.lock.Unlock() v := NewVM(c.bytecode, c.globals, c.maxAllocs) ch := make(chan error, 1) go func() { ch Run函数 script的Run // Run compiles and runs the scripts. Use returned compiled object to access // global variables. func (s *Script) Run() (compiled *Compiled, err error) { compiled, err = s.Compile() if err != nil { return } err = compiled.Run() return } Compiled的Run // Run executes the compiled script in the virtual machine. func (c *Compiled) Run() error { c.lock.Lock() defer c.lock.Unlock() v := NewVM(c.bytecode, c.globals, c.maxAllocs) return v.Run() } VM的Run在vm.go中 variable.go variable.go提供了从Variable类型到native go的转换. type Variable struct { name string value Object } 典型的转换如下: _ = compiled.Set(\"a\", a) _ = compiled.Set(\"b\", b) _ = compiled.Set(\"c\", c) err := compiled.Run() require.NoError(t, err) d = compiled.Get(\"d\").Int() e = compiled.Get(\"e\").Int() builtins.go builtin函数的实现 var builtinFuncs = []*BuiltinFunction{ { Name: \"len\", Value: builtinLen, }, { Name: \"copy\", Value: builtinCopy, }, 等等 又少不了类型断言... compiler.go compiler的作用是把ast转化为字节码 它依赖下层的token和parser. compiler管符号表, 模块, 已经被编译过的函数, // Compiler compiles the AST into a bytecode. type Compiler struct { file *parser.SourceFile parent *Compiler modulePath string importDir string constants []Object symbolTable *SymbolTable scopes []compilationScope scopeIndex int modules *ModuleMap compiledModules map[string]*CompiledFunction allowFileImport bool loops []*loop loopIndex int trace io.Writer indent int } new一个compiler就是 &Compiler{ file: file, symbolTable: symbolTable, constants: constants, scopes: []compilationScope{mainScope}, //每个scope都有自己的字节码 scopeIndex: 0, //scope从0开始 loopIndex: -1, trace: trace, modules: modules, compiledModules: make(map[string]*CompiledFunction), } 特别的, NewCompiler()的时候, 也可以传入一个io.Writer做为trace, 方便debug. io.Writer配合defer的trace功能, 就能详细的log软件逻辑.compiler的allowFileImport和importDir一般和script的对应属性一样. Compile()函数 Compile()的入参是parser.Node, 它是个接口; parser.ParseFile()返回的*parser.File就是其中的一种实现func (c *Compiler) Compile(node parser.Node) errorCompile()的实现又是典型的类型断言+递归的方式, 它支持File树, 支持表达式树, 进一步支持更加拆分细化的基础操作.用递归调用来化整为零, 很经典: // Compile compiles the AST node. func (c *Compiler) Compile(node parser.Node) error { switch node := node.(type) { case *parser.File: //如果是File类型, 只是对其中的每个声明做递归的Compile for _, stmt := range node.Stmts { if err := c.Compile(stmt); err != nil { //再次调用Compile函数 return err } } case *parser.ExprStmt: //如果是表达式 if err := c.Compile(node.Expr); err != nil { return err } c.emit(node, parser.OpPop) //表达式是有具体动作的: emit POP(出栈)字节码到c.scopes[c.scopeIndex].Instructions的最后 其他更加拆分的case, 比如 case *parser.BinaryExpr: //处理加减乘除, emit OpBinaryOp case *parser.IntLit: case *parser.StringLit: case *parser.FloatLit: //以上带Lit后缀的是literature, 即字面值. emit OpConstant case *parser.UnaryExpr:: //一元操作 case *parser.IncDecStmt: //++ -- case *parser.IfStmt: // open new symbol table for the statement c.symbolTable = c.symbolTable.Fork(true) //if块有自己的符号scope, 所以这里fork出一个子的scope defer func() { c.symbolTable = c.symbolTable.Parent(false) //退出if块的时候, 还原父scope }() if node.Init != nil { //和go语法一样, if支持前置表达式 if err := c.Compile(node.Init); err != nil { return err } } if err := c.Compile(node.Cond); err != nil { //条件部分 return err } // first jump placeholder jumpPos1 := c.emit(node, parser.OpJumpFalsy, 0) //不要被这里的0 offset迷惑 后面会修正 //注意, 这里对应 vm.run()中的执行部分: 当前ip是操作码OpJumpFalsy, 判断当前sp的值是否为假, 是的话, ip跳转到pos-1, 这个pos就是指令码里的offset, 看起来是用两个字节表示的. 最大跳转65535字节. //case parser.OpJumpFalsy: // v.ip += 2 // v.sp-- // if v.stack[v.sp].IsFalsy() { // pos := int(v.curInsts[v.ip]) | int(v.curInsts[v.ip-1]) 0 { //如果引用了父scope的局部变量, 那这个函数就是个闭包; emit OpClosure c.emit(node, parser.OpClosure, c.addConstant(compiledFunction), len(freeSymbols)) } else { //普通函数, emit OpConstant //先把上面的compiledFunction加到constant数组里, 然后把这个对象放到栈中. c.emit(node, parser.OpConstant, c.addConstant(compiledFunction)) } case *parser.ReturnStmt: //函数返回 if c.symbolTable.Parent(true) == nil { // outside the function return c.errorf(node, \"return not allowed outside function\") } if node.Result == nil { c.emit(node, parser.OpReturn, 0) //空return } else { if err := c.Compile(node.Result); err != nil { return err } c.emit(node, parser.OpReturn, 1) //return值 } case *parser.CallExpr: //函数调用 if err := c.Compile(node.Func); err != nil { return err } for _, arg := range node.Args { if err := c.Compile(arg); err != nil { return err } } ellipsis := 0 if node.Ellipsis.IsValid() { ellipsis = 1 } c.emit(node, parser.OpCall, len(node.Args), ellipsis) 等等 } 在没有类型断言的语言里, 恐怕最常规的思路是写一堆的CompileXxx函数, 来互相调用, 比如: CompileExprStmt() CompileIfStmt() CompileBinaryExpr() 等等 源文件会比较乱. 注: 实际上, 这里还真有compileForStmt()函数 emit()函数 通常的调用比如: c.emit(node, parser.OpBinaryOp, int(token.Add)) c.emit(node, parser.OpMinus) pos := c.emit(node, parser.OpJump, 0) 第二个参数都是OpCode, 每个OpCode只有1个字节, 被放到字节码中. 全部的OpCode不多, 在parser/opcodes.go中定义最终这个OpCode被写入c.scopes[c.scopeIndex].Instructions: func (c *Compiler) emit( node parser.Node, opcode parser.Opcode, operands ...int, ) int { filePos := parser.NoPos if node != nil { filePos = node.Pos() } inst := MakeInstruction(opcode, operands...) pos := c.addInstruction(inst) c.scopes[c.scopeIndex].SourceMap[pos] = filePos if c.trace != nil { c.printTrace(fmt.Sprintf(\"EMIT %s\", FormatInstructions( c.scopes[c.scopeIndex].Instructions[pos:], pos)[0])) } return pos } func (c *Compiler) addInstruction(b []byte) int { posNewIns := len(c.currentInstructions()) c.scopes[c.scopeIndex].Instructions = append( c.currentInstructions(), b...) return posNewIns } func (c *Compiler) currentInstructions() []byte { return c.scopes[c.scopeIndex].Instructions } Compile之module Compile遇到import语句时, import先从注册过的module查找. 没有的话, 从文件查找 case *parser.ImportExpr: if node.ModuleName == \"\" { return c.errorf(node, \"empty module name\") } if mod := c.modules.Get(node.ModuleName); mod != nil { v, err := mod.Import(node.ModuleName) if err != nil { return err } switch v := v.(type) { case []byte: // module written in Tengo 注册过的tengo代码, 还是原始文本代码 compiled, err := c.compileModule(node, //所以这里需要compile node.ModuleName, v, false) if err != nil { return err } c.emit(node, parser.OpConstant, c.addConstant(compiled)) c.emit(node, parser.OpCall, 0, 0) case Object: // builtin module c.emit(node, parser.OpConstant, c.addConstant(v)) default: panic(fmt.Errorf(\"invalid import value type: %T\", v)) } } else if c.allowFileImport { //如果使能了文件module moduleName := node.ModuleName if !strings.HasSuffix(moduleName, \".tengo\") { moduleName += \".tengo\" //module必须以.tengo结尾 } modulePath, err := filepath.Abs( filepath.Join(c.importDir, moduleName)) //module name是个相对路径, 要和c.importDir结合才能找到文件 if err != nil { return c.errorf(node, \"module file path error: %s\", err.Error()) } moduleSrc, err := ioutil.ReadFile(modulePath) if err != nil { return c.errorf(node, \"module file read error: %s\", err.Error()) } compiled, err := c.compileModule(node, modulePath, moduleSrc, true) if err != nil { return err } c.emit(node, parser.OpConstant, c.addConstant(compiled)) c.emit(node, parser.OpCall, 0, 0) } else { return c.errorf(node, \"module '%s' not found\", node.ModuleName) } case *parser.ExportStmt: // export statement must be in top-level scope if c.scopeIndex != 0 { return c.errorf(node, \"export not allowed inside function\") } // export statement is simply ignore when compiling non-module code if c.parent == nil { break } if err := c.Compile(node.Result); err != nil { return err } c.emit(node, parser.OpImmutable) c.emit(node, parser.OpReturn, 1) compileModule()函数 一个module被编译后, 实际上是个*CompiledFunction, 定义于objects.go // CompiledFunction represents a compiled function. type CompiledFunction struct { ObjectImpl Instructions []byte NumLocals int // number of local variables (including function parameters) NumParameters int VarArgs bool SourceMap map[int]parser.Pos Free []*ObjectPtr } 它先检查是否有循环依赖. 然后检查是否该模块是否已经编译过了, 编译过就不需要再编了, 直接返回之前的.然后才是编译.模块的编译使用了fork的概念, 即fork一个上级compiler的副本, 再填入module的信息来编译.编译完成后还要从字节码的角度来优化代码. 最后保存这个编译好的module简化流程如下: c.checkCyclicImports(node, modulePath) c.loadCompiledModule(modulePath) modFile := c.file.Set().AddFile(modulePath, -1, len(src)) p := parser.NewParser(modFile, src, nil) file, err := p.ParseFile() // inherit builtin functions symbolTable := NewSymbolTable() // no global scope for the module symbolTable = symbolTable.Fork(false) // compile module moduleCompiler := c.fork(modFile, modulePath, symbolTable, isFile) moduleCompiler.Compile(file) // code optimization moduleCompiler.optimizeFunc(node) compiledFunc := moduleCompiler.Bytecode().MainFunction c.storeCompiledModule(modulePath, compiledFunc) return compiledFunc, nil 注: storeCompiledModule()和loadCompiledModule()都是对最顶层的Compiler操作的 func (c *Compiler) loadCompiledModule( modulePath string, ) (mod *CompiledFunction, ok bool) { if c.parent != nil { return c.parent.loadCompiledModule(modulePath) } mod, ok = c.compiledModules[modulePath] return } func (c *Compiler) storeCompiledModule( modulePath string, module *CompiledFunction, ) { if c.parent != nil { c.parent.storeCompiledModule(modulePath, module) } c.compiledModules[modulePath] = module } module默认路径 c.importDir默认为\"\", 即从当前目录找module也可以调用c.SetImportDir(filepath.Dir(inputFile))把import路径设为文件名的路径. 这个主意不错. Bytecode()函数返回编译好的字节码 它调用了上面的c.currentInstructions() // Bytecode returns a compiled bytecode. func (c *Compiler) Bytecode() *Bytecode { return &Bytecode{ FileSet: c.file.Set(), MainFunction: &CompiledFunction{ Instructions: append(c.currentInstructions(), parser.OpSuspend), SourceMap: c.currentSourceMap(), }, Constants: c.constants, } } modules.go tengo的模块支持ModuleMap是个简单的mapImportable是个可以被import的对象: Import()的签名足够简单 // Importable interface represents importable module instance. type Importable interface { // Import should return either an Object or module source code ([]byte). Import(moduleName string) (interface{}, error) } // ModuleMap represents a set of named modules. Use NewModuleMap to create a // new module map. type ModuleMap struct { m map[string]Importable } NewModuleMap()就返回一个初始的ModuleMap module类型: 支持import native go对象和tengo代码 下面代码中有三种module类型 // Add adds an import module. func (m *ModuleMap) Add(name string, module Importable) { m.m[name] = module } // AddBuiltinModule adds a builtin module. func (m *ModuleMap) AddBuiltinModule(name string, attrs map[string]Object) { m.m[name] = &BuiltinModule{Attrs: attrs} } // AddSourceModule adds a source module. func (m *ModuleMap) AddSourceModule(name string, src []byte) { m.m[name] = &SourceModule{Src: src} } 普通的实现了Import()方法的对象 Builtin模块: 用于import native go实现的对象. 就是add一个map[string]Object, import builtin模块返回一个ImmutableMap对象 定义于objects.go // BuiltinModule is an importable module that's written in Go. type BuiltinModule struct { Attrs map[string]Object } // Import returns an immutable map for the module. func (m *BuiltinModule) Import(moduleName string) (interface{}, error) { return m.AsImmutableMap(moduleName), nil } // AsImmutableMap converts builtin module into an immutable map. func (m *BuiltinModule) AsImmutableMap(moduleName string) *ImmutableMap { attrs := make(map[string]Object, len(m.Attrs)) for k, v := range m.Attrs { attrs[k] = v.Copy() } attrs[\"__module_name__\"] = &String{Value: moduleName} return &ImmutableMap{Value: attrs} } // ImmutableMap represents an immutable map object. type ImmutableMap struct { ObjectImpl Value map[string]Object } 源码模块: 用于import tengo脚本代码 // SourceModule is an importable module that's written in Tengo. type SourceModule struct { Src []byte } // Import returns a module source code. func (m *SourceModule) Import(_ string) (interface{}, error) { return m.Src, nil } 注: 这里的Import的源码模块, 是没有经过编译的. 名字被故意去掉了, 可能是后面和\"main\"脚本一起编译. symbol_table.go 提供symbol的定义和存储, define一个symbol的操作 symbol分为四种: // List of symbol scopes const ( ScopeGlobal SymbolScope = \"GLOBAL\" ScopeLocal SymbolScope = \"LOCAL\" ScopeBuiltin SymbolScope = \"BUILTIN\" ScopeFree SymbolScope = \"FREE\" ) SymbolTable应该是个树结构, 但只能往上找parent; 这很好理解: 一般的call stack中, 都是一路向下的调用, caller函数其实更关心怎么return到父级函数, 而不怎么关心callee函数 // Symbol represents a symbol in the symbol table. type Symbol struct { Name string Scope SymbolScope Index int LocalAssigned bool // if the local symbol is assigned at least once } // SymbolTable represents a symbol table. type SymbolTable struct { parent *SymbolTable block bool store map[string]*Symbol numDefinition int maxDefinition int freeSymbols []*Symbol builtinSymbols []*Symbol } 注意这里symbolTable只关注(name, index), 即symbolTable本身只保存符号和index, 并不保存Object. NewSymbolTable()函数 返回一个SymbolTable结构体 // NewSymbolTable creates a SymbolTable. func NewSymbolTable() *SymbolTable { return &SymbolTable{ store: make(map[string]*Symbol), } } Define()函数 func (t *SymbolTable) Define(name string) *Symbol 确定这个Symbol的scope: 如果t有parent, 则这个symbol是local的 否则是global的 然后放在这个t的store中: t.store[name] = symbol builtin符号除了保存在store中, 还保存在单独的builtinSymbols中 builtin符号保存在最顶层的符号表中: // DefineBuiltin adds a symbol for builtin function. func (t *SymbolTable) DefineBuiltin(index int, name string) *Symbol { if t.parent != nil { return t.parent.DefineBuiltin(index, name) } symbol := &Symbol{ Name: name, Index: index, Scope: ScopeBuiltin, } t.store[name] = symbol t.builtinSymbols = append(t.builtinSymbols, symbol) return symbol } Fork()函数 当扫描ast发现是一个block statement时, 调用SymbolTable的Fork函数, 在当前SymbolTable下, 新建一个SymbolTable.退出这个block statement时, 还原SymbolTable到父symbolTablecompile函数片段: case *parser.BlockStmt: if len(node.Stmts) == 0 { return nil } //注意这里, compile到这里发现是个新的语句块, 比如if, for的body部分 //fork一个新的symbolTable, 并做为当前的symbolTable: 注意下面这句 c.symbolTable = c.symbolTable.Fork(true) defer func() { //退出这个block的时候, 还原symbolTable为父symbolTable c.symbolTable = c.symbolTable.Parent(false) }() for _, stmt := range node.Stmts { if err := c.Compile(stmt); err != nil { return err } } Resolve()函数 Resolve()函数从本SymbolTable开始, 递归的沿parent路径查找, 直到在路径上的t.store map中找到该name的symbol.这就像是在本函数作用域查找变量一样: 在本函数没有, 就找父函数, 直到global的符号表.如果本函数没有, 但在某个父函数找到了这个符号, 这个符号就是free scope类型的.找到free scope类型的符号后, 添加到本函数的SymbolTable中. // if symbol is defined in parent table and if it's not global/builtin // then it's free variable. if !t.block && depth > 0 && symbol.Scope != ScopeGlobal && symbol.Scope != ScopeBuiltin { return t.defineFree(symbol), depth, true } free scope这个名字很贴切, \"游离\"的变量: 定义于某个函数, 但在其子函数中被引用到 -- 这就是闭包的概念.对子函数来说, 引用父函数的变量, 是free scope; 但对被引用的这个父函数变量来说, 它在父函数中是local的.在compile生成字节码阶段就调用了Resolve()函数 case *parser.Ident: // 在当前symbolTable查找name, symbolTable有父symbolTable的指针 // 当前找不到会依次往上查找 symbol, _, ok := c.symbolTable.Resolve(node.Name, false) if !ok { return c.errorf(node, \"unresolved reference '%s'\", node.Name) } switch symbol.Scope { case ScopeGlobal: c.emit(node, parser.OpGetGlobal, symbol.Index) case ScopeLocal: c.emit(node, parser.OpGetLocal, symbol.Index) case ScopeBuiltin: c.emit(node, parser.OpGetBuiltin, symbol.Index) case ScopeFree: c.emit(node, parser.OpGetFree, symbol.Index) } 总结 基本上一个symbolTable对应一个block体, 比如for的的代码块主体就是一个symbolTable.一个函数也是一个新的symbolTable, 函数return返回的时候symbolTable也恢复为其父symbolTable. symbolTable在编译阶段发挥作用, 即从ast到字节码的过程中, 根据语义分析的ast, 如果是对变量的引用, 就在当前symbolTable中查找变量的符号, 并最终emit到字节码中(见上面代码).如果是对变量的赋值, 比如对局部变量赋值, 会调用c.emit(node, parser.OpSetLocal, symbol.Index) 看到这里我认为字节码中并没有符号信息, 就像汇编里已经不是变量名了, 而是地址. vm.go 看起来到VM阶段, 字节码已经是解结构化的, 类似汇编式的指令流了. 所以这个run函数的主体是个for, 通过控制ip的移动来\"执行\"指令流. 所以这里不是解释器那种的递归调用Eval的情况. VM是个固定栈大小的(目前是固定2048个Object对象), 有global的引用, 最大1024个frame的VM.注意这里的栈大小是整个VM的栈, 随着fram的增减而上下浮动. 对应C里面ulimit -s的栈大小, 一般是8192K. // VM is a virtual machine that executes the bytecode compiled by Compiler. type VM struct { constants []Object stack [StackSize]Object sp int globals []Object fileSet *parser.SourceFileSet frames [MaxFrames]frame framesIndex int curFrame *frame curInsts []byte ip int aborting int64 maxAllocs int64 allocs int64 err error } //主要限制如下: const ( // GlobalsSize is the maximum number of global variables for a VM. GlobalsSize = 1024 // StackSize is the maximum stack size for a VM. StackSize = 2048 // MaxFrames is the maximum number of function frames for a VM. MaxFrames = 1024 ) frame管状态记录的, 定义如下: // frame represents a function call frame. type frame struct { fn *CompiledFunction //CompiledFunction也是Object对象, 主要包括对应的字节码 freeVars []*ObjectPtr //ObjectPrt是指向Object的结构, 见下面: ip int basePointer int } // CompiledFunction represents a compiled function. type CompiledFunction struct { ObjectImpl Instructions []byte NumLocals int // number of local variables (including function parameters); 这个是在编译阶段就确定了的 NumParameters int VarArgs bool SourceMap map[int]parser.Pos Free []*ObjectPtr //持有ObjectPtr是要干什么呢? 可能是\"堆\"变量 } // ObjectPtr represents a free variable. type ObjectPtr struct { ObjectImpl Value *Object } VM New函数 传入的globals可以在多个VM中共享 // NewVM creates a VM. func NewVM( bytecode *Bytecode, globals []Object, maxAllocs int64, ) *VM { if globals == nil { globals = make([]Object, GlobalsSize) } v := &VM{ constants: bytecode.Constants, sp: 0, globals: globals, fileSet: bytecode.FileSet, framesIndex: 1, ip: -1, maxAllocs: maxAllocs, } v.frames[0].fn = bytecode.MainFunction v.frames[0].ip = -1 v.curFrame = &v.frames[0] v.curInsts = v.curFrame.fn.Instructions return v } VM在run阶段, 会把产生的全局变量放到globals中. case parser.OpSetGlobal: v.ip += 2 v.sp-- globalIndex := int(v.curInsts[v.ip]) | int(v.curInsts[v.ip-1]) VM Run函数 Script, Compiled的Run, 最后都是调用VM.Run();如果去掉错误处理, VM.Run()是下面的样子 // Run starts the execution. func (v *VM) Run() (err error) { // reset VM states //VM真的有sp, ip, frame等操作 v.sp = 0 v.curFrame = &(v.frames[0]) v.curInsts = v.curFrame.fn.Instructions v.framesIndex = 1 v.ip = -1 v.allocs = v.maxAllocs + 1 //重点是这个run函数 v.run() atomic.StoreInt64(&v.aborting, 0) err = v.err ... } VM.run()是真正执行部分 func (v *VM) run() { for atomic.LoadInt64(&v.aborting) == 0 { //对应VM Abort来停止 v.ip++ //字节码的执行依靠ip指针的向后移动 switch v.curInsts[v.ip] { case parser.OpConstant: v.ip += 2 cidx := int(v.curInsts[v.ip]) | int(v.curInsts[v.ip-1])= 0 { numArgs = realArgs + 1 args := make([]Object, varArgs) spStart := v.sp - varArgs for i := spStart; i =%d, got=%d\", callee.NumParameters-1, numArgs) } else { v.err = fmt.Errorf( \"wrong number of arguments: want=%d, got=%d\", callee.NumParameters, numArgs) } return } // test if it's tail-call if callee == v.curFrame.fn { // recursion nextOp := v.curInsts[v.ip+1] if nextOp == parser.OpReturn || (nextOp == parser.OpPop && parser.OpReturn == v.curInsts[v.ip+2]) { for p := 0; p = MaxFrames { v.err = ErrStackOverflow return } //下面非常重要!!!!! 一个CompiledFunction运行在独立的frame里, 全部的frame是个1024个大小的frame数组. // update call frame //进入下一个循环, switch v.curInsts[v.ip] v.curFrame.ip = v.ip // store current ip before call v.curFrame = &(v.frames[v.framesIndex]) v.curFrame.fn = callee v.curFrame.freeVars = callee.Free v.curFrame.basePointer = v.sp - numArgs v.curInsts = callee.Instructions v.ip = -1 //下个循环中, v.ip++刚好为0 v.framesIndex++ v.sp = v.sp - numArgs + callee.NumLocals //注意这里, sp并没有独立的\"frame\". 这里的NumLocals是包括了参数个数的:number of local variables (including function parameters) } else { //难道else对应用户自定义函数? var args []Object args = append(args, v.stack[v.sp-numArgs:v.sp]...) ret, e := value.Call(args...) v.sp -= numArgs + 1 // runtime error if e != nil { if e == ErrWrongNumArguments { v.err = fmt.Errorf( \"wrong number of arguments in call to '%s'\", value.TypeName()) return } if e, ok := e.(ErrInvalidArgumentType); ok { v.err = fmt.Errorf( \"invalid type for argument '%s' in call to '%s': \"+ \"expected %s, found %s\", e.Name, value.TypeName(), e.Expected, e.Found) return } v.err = e return } // nil return -> undefined if ret == nil { ret = UndefinedValue } v.allocs-- if v.allocs == 0 { v.err = ErrObjectAllocLimit return } v.stack[v.sp] = ret v.sp++ } } //这个frame return case parser.OpReturn: v.ip++ var retVal Object if int(v.curInsts[v.ip]) == 1 { //有return 值 retVal = v.stack[v.sp-1] } else { retVal = UndefinedValue // 没有return默认return UndefinedValue } //v.sp-- v.framesIndex-- //撤销这个frame, 下面几句还原上一个frame的现场 v.curFrame = &v.frames[v.framesIndex-1] v.curInsts = v.curFrame.fn.Instructions v.ip = v.curFrame.ip //v.sp = lastFrame.basePointer - 1 v.sp = v.frames[v.framesIndex].basePointer // skip stack overflow check because (newSP) Abort用atomic写入v.aborting // Abort aborts the execution. func (v *VM) Abort() { atomic.StoreInt64(&v.aborting, 1) } 下面的几个操作有什么区别? 使用场景是什么? 栈变量? 堆变量? OpDefineLocal // Define local variable OpSetSelLocal // Set local variable using selectors OpGetFreePtr // Get free variable pointer object OpGetFree // Get free variables OpSetFree // Set free variables OpGetLocalPtr // Get local variable as a pointer 粗看下来, 我感觉变量的第一使用现场都是在stack [StackSize]Object, 虽然这里叫stack, 但实际是Object的运动场, \"堆\"变量也保存在这里. 那有人要问了, 明明看到这个stack有sp, 而且sp会上下浮动. 如果当然frame的堆变量保存在此, 那sp\"回退\"到上个frame怎么办? 感觉OpGetFree和OpSetFree就是来解决这个问题的 case parser.OpGetFree: v.ip++ freeIndex := int(v.curInsts[v.ip]) val := *v.curFrame.freeVars[freeIndex].Value v.stack[v.sp] = val v.sp++ case parser.OpSetFree: v.ip++ freeIndex := int(v.curInsts[v.ip]) *v.curFrame.freeVars[freeIndex].Value = v.stack[v.sp-1] v.sp-- OpGetFree从v.curFrame.freeVars这个map里得到值, 放到v.stack[v.sp]中 OpSetFree相反, 把栈变量放到frame的freeVars中. 注意这里都是值拷贝. 即: 变量都是在stack运动场上参与运动(CPU计算), 变量退场的时候, 其值被拷贝(值拷贝)到当前休息区curFrame.freeVars; 等下次轮到这个变量上场的时候, 再次值拷贝到运动场(stack). 前面说过, callee函数也是做为一个Object被放到stack上的, 所以这里的stack并不是完全的C的\"栈\"的概念. 应该说是对象的运动场/训练场更合适. vm.run总结 ip是字节码的\"pc\"指针, op和op的操作指令放在字节码里 sp向上增长, op的操作数放在栈上. 比如 判断左右两个操作数相等, 左右两个操作数都在栈上, 出栈后判断是否相等, 并把结果压栈.case parser.OpEqual: right := v.stack[v.sp-1] left := v.stack[v.sp-2] v.sp -= 2 if left.Equals(right) { v.stack[v.sp] = TrueValue } else { v.stack[v.sp] = FalseValue } v.sp++ stdlib/func_typedefs.go 提供了go的函数签名到tengo callable对象的包装函数: 比如: // FuncARI transform a function of 'func() int' signature into CallableFunc // type. func FuncARI(fn func() int) tengo.CallableFunc { return func(args ...tengo.Object) (ret tengo.Object, err error) { if len(args) != 0 { return nil, tengo.ErrWrongNumArguments } return &tengo.Int{Value: int64(fn())}, nil } } tengo的后继 https://github.com/ozanh/ugo tengo的v2版本处于维护状态. tengo的一个contributor自己也建了一个类似的解释器: ugo tengo https://github.com/d5/tengo Reddit讨论在此 tengo似乎是个VM模式的解释语言. 语法和go非常相近, 性能似乎不错: fib(35) fibt(35) Language (Type) Tengo 2,931ms 4ms Tengo (VM) go-lua 4,824ms 4ms Lua (VM) GopherLua 5,365ms 4ms Lua (VM) goja 5,533ms 5ms JavaScript (VM) starlark-go 11,495ms 5ms Starlark (Interpreter) Yaegi 15,645ms 12ms Yaegi (Interpreter) gpython 16,322ms 5ms Python (Interpreter) otto 73,093ms 10ms JavaScript (Interpreter) Anko 79,809ms 8ms Anko (Interpreter) - - - - Go 53ms 3ms Go (Native) Lua 1,612ms 3ms Lua (Native) Python 2,632ms 23ms Python 2 (Native) 编译出来size也很小, 4.8M. 没有外部package引用, 没有cgo 被设计成类似lua的被嵌入的语言. -- 更容易集成 似乎扩展性不错. -- 有库的概念. 库是go语言写的 github 2.1k个星 入门 a b c d四个变量求和: package main import ( \"context\" \"fmt\" \"github.com/d5/tengo/v2\" ) func main() { // Tengo script code src := ` each := func(seq, fn) { for x in seq { fn(x) } } sum := 0 mul := 1 each([a, b, c, d], func(x) { sum += x mul *= x })` // create a new Script instance script := tengo.NewScript([]byte(src)) // set values _ = script.Add(\"a\", 1) _ = script.Add(\"b\", 9) _ = script.Add(\"c\", 8) _ = script.Add(\"d\", 4) // run the script compiled, err := script.RunContext(context.Background()) if err != nil { panic(err) } // retrieve values sum := compiled.Get(\"sum\") mul := compiled.Get(\"mul\") fmt.Println(sum, mul) // \"22 288\" } 语法 token 有几个token值得注意: Ellipsis: \"...\", Func: \"func\", Error: \"error\", Immutable: \"immutable\", Export: \"export\", True: \"true\", False: \"false\", In: \"in\", Undefined: \"undefined\", Import: \"import\", 比如... error immutable undefined export import都是关键词 一个go调脚本的例子 package main import ( \"log\" \"github.com/d5/tengo/v2\" \"github.com/d5/tengo/v2/stdlib\" ) func main() { src := ` text := import(\"text\") m := { contains: func(args) { return text.contains(args.str, args.substr) } } out := undefined if f := m[argsMap.function]; !is_undefined(f) { out = f(argsMap) } else { out = error(\"unknown function\") } ` script := tengo.NewScript([]byte(src)) script.SetImports(stdlib.GetModuleMap(\"text\")) argsMap := map[string]interface{}{ \"function\": \"contains\", \"str\": \"foo bar\", \"substr\": \"bar\"} if err := script.Add(\"argsMap\", argsMap); err != nil { log.Fatal(err) } c, err := script.Run() if err != nil { log.Fatal(err) } out := c.Get(\"out\") if err := out.Error(); err != nil { log.Fatal(err) } // If it returns a dynamic type use type switch using out.Value() log.Println(\"result =\", out.Bool()) } 所有都是值类型 19 + 84 // int values \"aomame\" + `kawa` // string values -9.22 + 1e10 // float values true || false // bool values '九' > '9' // char values [1, false, \"foo\"] // array value {a: 12.34, b: \"bar\"} // map value func() { /*...*/ } // function value tengo值类型和go类型 Tengo Type Description Equivalent Type in Go int signed 64-bit integer value int64 float 64-bit floating point value float64 bool boolean value bool char unicode character rune string unicode string string bytes byte array []byte error error value - time time value time.Time array value array (mutable) []interface{} immutable array immutable array - map value map with string keys (mutable) map[string]interface{} immutable map immutable map - undefined undefined value - function function value - user-defined value of user-defined types - error升级为一等类型 err1 := error(\"oops\") // error with string value err2 := error(1+2+3) // error with int value if is_error(err1) { // 'is_error' builtin function err_val := err1.value // get underlying value } 值不可变 除了map和array, 其他类型的值不能改变 s := \"12345\" s[1] = 'b' // illegal: String is immutable a := [1, 2, 3] a[1] = \"two\" // ok: a is now [1, \"two\", 3] 用immutable关键词可以把map和array也设成不可变 b := immutable([1, 2, 3]) b[1] = \"foo\" // illegal: 'b' references to an immutable array. 注意, 重新赋值和immutabiltiy没有关系: 对变量重新赋值永远都是可以的 s := \"abc\" s = \"foo\" // ok a := immutable([1, 2, 3]) a = false // ok immutable改变只是当前变量属性, 对map/array里的元素没有约束 a := immutable({b: 4, c: [1, 2, 3]}) a.b = 5 // illegal a.c[1] = 5 // ok: because 'a.c' is not immutable a = immutable({b: 4, c: immutable([1, 2, 3])}) a.c[1] = 5 // illegal 未定义也是一种类型 没有return的函数是undefined类型 不存在的index/key返回undefined类型 强转失败返回undefined类型 a := func() { b := 4 }() // a == undefined b := [1, 2, 3][10] // b == undefined c := {a: \"foo\"}[\"b\"] // c == undefined d := int(\"foo\") // d == undefined 注: 这里看到, index超过范围返回的是undefined类型, 而不是直接panic 广义的array和map array和map被设计成解释器该有的样子: array的元素可以是任意类型, 支持嵌套: [1, 2, 3][0] // == 1 [1, 2, 3][2] // == 3 [1, 2, 3][3] // == undefined [\"foo\", \"bar\", [1, 2, 3]] // ok: array with an array element array支持加法操作: (array) + (array): return a concatenated array map的元素可以用[]也可以用.来访问 m := { a: 1, b: false, c: \"foo\" } m[\"b\"] // == false m.c // == \"foo\" m.x // == undefined {a: [1,2,3], b: {c: \"foo\", d: \"bar\"}} // ok: map with an array element and a map element array和map都支持比较操作 函数也是值, 一等公民, 支持闭包 my_func := func(arg1, arg2) { return arg1 + arg2 } adder := func(base) { return func(x) { return base + x } // capturing 'base' } add5 := adder(5) nine := add5(4) // == 9 和go的区别是, tengo里的函数必须是值声明的方式来定义 声明函数的方式不支持 func my_func(arg1, arg2) { // illegal return arg1 + arg2 } 支持变长参数: 变长部分被组成一个array variadic := func (a, b, ...c) { return [a, b, c] } variadic(1, 2, 3, 4) // [1, 2, [3, 4]] variadicClosure := func(a) { return func(b, ...c) { return [a, b, c] } } variadicClosure(1)(2, 3, 4) // [1, 2, [3, 4]] 变长部分只能在最后: // illegal, because a is variadic and is not the last parameter illegal := func(a..., b) { /*... */ } 支持go式的...解array: f1 := func(a, b, c) { return a + b + c } f1([1, 2, 3]...) // => 6 f1(1, [2, 3]...) // => 6 f1(1, 2, [3]...) // => 6 f1([1, 2]...) // Runtime Error: wrong number of arguments: want=3, got=2 f2 := func(a, ...b) {} f2(1) // valid; a = 1, b = [] f2(1, 2) // valid; a = 1, b = [2] f2(1, 2, 3) // valid; a = 1, b = [2, 3] f2([1, 2, 3]...) // valid; a = 1, b = [2, 3] 变量scope 和abs不同, tengo支持函数级的scope :=在当前scope定义新变量 =在当前scope重新赋值变量 变量的值类型是动态的:a := 123 // assigned 'int' a = \"123\" // re-assigned 'string' a = [1, 2, 3] // re-assigned 'array' scope例子: a := \"foo\" // define 'a' in global scope func() { // function scope A b := 52 // define 'b' in function scope A func() { // function scope B c := 19.84 // define 'c' in function scope B a = \"bee\" // ok: assign new value to 'a' from global scope b = 20 // ok: assign new value to 'b' from function scope A b := true // ok: define new 'b' in function scope B // (shadowing 'b' from function scope A) } a = \"bar\" // ok: assigne new value to 'a' from global scope b = 10 // ok: assigne new value to 'b' a := -100 // ok: define new 'a' in function scope A // (shadowing 'a' from global scope) c = -9.1 // illegal: 'c' is not defined b := [1, 2] // illegal: 'b' is already defined in the same scope } b = 25 // illegal: 'b' is not defined a := {d: 2} // illegal: 'a' is already defined in the same scope 内置类型转换 比如int转为string是可以的: string(x): tries to convert x into string; returns undefined if failed //在native go里, string(1984)是不行的; 但这里可以 s1 := string(1984) // \"1984\" i2 := int(\"-999\") // -999 f3 := float(-51) // -51.0 b4 := bool(1) // true c5 := char(\"X\") // 'X' 所有类型都有false的概念: Int: n == 0 String: len(s) == 0 Float: isNaN(f) Bool: !b Char: c == 0 Bytes: len(bytes) == 0 Array: len(arr) == 0 Map: len(map) == 0 Time: Time.IsZero() Error: true (Error is always falsy) Undefined: true (Undefined is always falsy) 支持三目操作 a := true ? 1 : -1 // a == 1 min := func(a, b) { return a []和.能作用于复合类型:array map string和bytes 特别的用m.x = 5能新增一个(key value) [\"one\", \"two\", \"three\"][1] // == \"two\" m := { a: 1, b: [2, 3, 4], c: func() { return 10 } } m.a // == 1 m[\"b\"][1] // == 3 m.c() // == 10 m.x = 5 // add 'x' to map 'm' m[\"b\"][5] // == undefined m[\"b\"][5].d // == undefined m.b[5] = 0 // == undefined m.x.y.z // == undefined 再切片和go一样 可惜-1不是倒数第一个的意思, [1, 2, 3, 4, 5][3:-1]是非法的 a := [1, 2, 3, 4, 5][1:3] // == [2, 3] b := [1, 2, 3, 4, 5][3:] // == [4, 5] c := [1, 2, 3, 4, 5][:3] // == [1, 2, 3] d := \"hello world\"[2:10] // == \"llo worl\" c := [1, 2, 3, 4, 5][-1:10] // == [1, 2, 3, 4, 5] if支持前置执行语句, 和go一样; for的结构也和go一样样的; for支持 for in for v in [1, 2, 3] { // array: element // 'v' is value } for i, v in [1, 2, 3] { // array: index and element // 'i' is index // 'v' is value } for k, v in {k1: 1, k2: 2} { // map: key and value // 'k' is key // 'v' is value } 其他和native go的不同点 没有struct -- 有map了, 不需要struct 没有go routine和channel 没有defer 没有switch case 没有panic 没有类型断言 内置函数 不是很多, format len copy append delete等等 a := [1, 2, 3] s := format(\"Foo: %v\", a) // s == \"Foo: [1, 2, 3]\" v := [1, 2, 3] l := len(v) // l == 3 v1 := [1, 2, 3] v2 := v1 v3 := copy(v1) v1[1] = 0 print(v2[1]) // \"0\"; 'v1' and 'v2' referencing the same array print(v3[1]) // \"2\"; 'v3' not affected by 'v1' v := [1] v = append(v, 2, 3) // v == [1, 2, 3] v := {key: \"value\"} delete(v, \"key\") // v == {} delete(v, \"missing\") // v == {\"key\": \"value\"} 还有类型转换和类型判断 string() is_string() v := bytes(\"foo\") // v == [102 111 111] is_bytes() 等等 被嵌入执行 直接代码执行 import \"github.com/d5/tengo/v2\" var code = ` reduce := func(seq, fn) { s := 0 for x in seq { fn(x, s) } return s } print(reduce([1, 2, 3], func(x, s) { s += x })) ` func main() { s := tengo.NewScript([]byte(code)) if _, err := s.Run(); err != nil { panic(err) } } 代码实时\"编译\"后执行 这个例子中, 代码被\"编译\"了一次, 但执行了2次. 全局变量可以通过Compiled.Set和Compiled.Get来从外部访问. import ( \"fmt\" \"github.com/d5/tengo/v2\" ) func main() { s := tengo.NewScript([]byte(`a := b + 20`)) // define variable 'b' _ = s.Add(\"b\", 10) // compile the source c, err := s.Compile() if err != nil { panic(err) } // run the compiled bytecode // a compiled bytecode 'c' can be executed multiple times without re-compiling it if err := c.Run(); err != nil { panic(err) } // retrieve value of 'a' a := c.Get(\"a\") fmt.Println(a.Int()) // prints \"30\" // re-run after replacing value of 'b' if err := c.Set(\"b\", 20); err != nil { panic(err) } if err := c.Run(); err != nil { panic(err) } fmt.Println(c.Get(\"a\").Int()) // prints \"40\" } 外部代码里import 这里的外部代码指调用tengo的go代码: s := tengo.NewScript([]byte(`math := import(\"math\"); a := math.abs(-19.84)`)) s.SetImports(stdlib.GetModuleMap(\"math\")) // or, to include all stdlib at once s.SetImports(stdlib.GetModuleMap(stdlib.AllModuleNames()...)) 也可以引用自定义module: s := tengo.NewScript([]byte(`double := import(\"double\"); a := double(20)`)) mods := tengo.NewModuleMap() mods.AddSourceModule(\"double\", []byte(`export func(x) { return x * 2 }`)) s.SetImports(mods) 用户可以自定义类型 用户实现了Object接口就可以自定义类型. 用户的自定义类型和内置类型待遇一样.即要实现下面的接口: //类型名字 TypeName() string //值的字面字符串表达 String() string //重载操作符: +, -, *, /, %, &, |, ^, &^, >>, , >= //有错误发生时, 可以返回Error值到res(不中断脚本执行) //也可以直接返回err, 比如ErrInvalidOperator, 但会中断VM的执行 BinaryOp(op token.Token, rhs Object) (res Object, err error) IsFalsy() bool Equals(o Object) bool //内建的类型是深拷贝模式, 但用户也可以在自定义类型中搞浅拷贝 Copy() Object //If Object is not indexable, ErrNotIndexable should be returned as error. If nil is returned as value, it will be converted to Undefined value by the runtime. IndexGet(index Object) (value Object, err error) IndexSet(index, value Object) error //对象可以有call方法 CanCall() bool //Call的格式比native go更死板一点, 返回值必须是两个 Call(args ...Object) (ret Object, err error) //对象可以被迭代 CanIterate() bool //Iterator是返回的对象, 实现了Iterator接口 Iterate() Iterator //Iterator接口包括: Next() bool Key() Object Value() Object 自定义类型举例 和native go的核心精神一样, 下面的*StringArray实现了tengo的Object要求: 主要是实现了string array的+操作 注意StringArray匿名包含了tengo.ObjectImpl, 这是个典型的继承操作: 继承了tengo.ObjectImpl的方法集, 后者是个空结构体type ObjectImpl struct {}, 但由tengo/objects.go提供的默认类型方法实现. type StringArray struct { tengo.ObjectImpl Value []string } func (o *StringArray) String() string { return strings.Join(o.Value, \", \") } func (o *StringArray) BinaryOp(op token.Token, rhs tengo.Object) (tengo.Object, error) { if rhs, ok := rhs.(*StringArray); ok { switch op { //这里是核心实现 case token.Add: if len(rhs.Value) == 0 { return o, nil } return &StringArray{Value: append(o.Value, rhs.Value...)}, nil } } return nil, tengo.ErrInvalidOperator } func (o *StringArray) IsFalsy() bool { return len(o.Value) == 0 } func (o *StringArray) Equals(x tengo.Object) bool { if x, ok := x.(*StringArray); ok { if len(o.Value) != len(x.Value) { return false } for i, v := range o.Value { if v != x.Value[i] { return false } } return true } return false } func (o *StringArray) Copy() tengo.Object { return &StringArray{ Value: append([]string{}, o.Value...), } } func (o *StringArray) TypeName() string { return \"string-array\" } 那么一个*StringArray对象可以被直接Add进脚本, 不需要\"注册\"步骤, tengo就知道自动调用*StringArray重载的+方法: // script that uses 'my_list' s := tengo.NewScript([]byte(` print(my_list + \"three\") `)) myList := &StringArray{Value: []string{\"one\", \"two\"}} s.Add(\"my_list\", myList) // add StringArray value 'my_list' s.Run() // prints \"one, two, three\" 增加更多功能 在StringArray的实现里可以增加index的支持. func (o *StringArray) IndexGet(index tengo.Object) (tengo.Object, error) { intIdx, ok := index.(*tengo.Int) if ok { if intIdx.Value >= 0 && intIdx.Value = 0 && intIdx.Value 也可以增加call的支持: func (o *StringArray) CanCall() bool { return true } func (o *StringArray) Call(args ...tengo.Object) (ret tengo.Object, err error) { if len(args) != 1 { return nil, tengo.ErrWrongNumArguments } s1, ok := tengo.ToString(args[0]) if !ok { return nil, tengo.ErrInvalidArgumentType{ Name: \"first\", Expected: \"string\", Found: args[0].TypeName(), } } for i, v := range o.Value { if v == s1 { return &tengo.Int{Value: int64(i)}, nil } } return tengo.UndefinedValue, nil } 这样这个对象就可以被函数式调用: s := tengo.NewScript([]byte(` print(my_list(\"two\")) `)) myList := &StringArray{Value: []string{\"one\", \"two\", \"three\"}} s.Add(\"my_list\", myList) // add StringArray value 'my_list' s.Run() // prints \"1\" (index of \"two\") 也可以增加迭代支持: func (o *StringArray) CanIterate() bool { return true } func (o *StringArray) Iterate() tengo.Iterator { return &StringArrayIterator{ strArr: o, } } type StringArrayIterator struct { tengo.ObjectImpl strArr *StringArray idx int } func (i *StringArrayIterator) TypeName() string { return \"string-array-iterator\" } func (i *StringArrayIterator) Next() bool { i.idx++ return i.idx 总结 tengo使用了go的匿名继承, \"斧头帮\"式的接口概念, 可以非常容易的自定义类型. 自己搞个工程, 定义类型, 和tengo编译在一起, 这样其脚本就支持非常丰富的自定义操作. 但上面的例子中, my_list变量是在go代码里定义的, 在脚本里使用. 如何在脚本里直接定义StringArray类型的变量呢? 模块化和标准库 tengo支持模块化, 有两个层次的: tengo脚本 sum := import(\"./sum\") // load module from a local file fmt.print(sum(10)) // module function sum.tengo的内容如下: 用export来声明要导出的对象, 可以是任何tengo类型, 比如map base := 5 export func(x) { return x + base } export就是文件级别的return, export返回的对象都是immutable的. 没有export声明的module, 返回undefined类型. go代码的模块 标准库是go代码实现的math := import(\"math\") a := math.abs(-19.84) // == 19.84 如何实现的? 标准库 标准库由: os: platform-independent interface to operating system functionality. text: regular expressions, string conversion, and manipulation math: mathematical constants and functions times: time-related functions rand: random functions fmt: formatting functions json: JSON functions enum: Enumeration functions 有each(x, fn) all(x, fn) => bool any(x, fn) => bool等针对array和map调用fn的函数 hex: hex encoding and decoding functions base64: base64 encoding and decoding functions runtime对象类型 Primitive value types: Int, String, Float, Bool, Char, Bytes, Time Composite value types: Array, ImmutableArray, Map, ImmutableMap Functions: CompiledFunction, BuiltinFunction, UserFunction Iterators: StringIterator, ArrayIterator, MapIterator, ImmutableMapIterator Error Undefined Other internal objects: Break, Continue, ReturnValue 安全性 有几个API可以设置最大对象个数, 最大code长度等等 Script.SetMaxAllocs(n int64) //默认关闭从文件load模块 Script.EnableFileImport(enable bool) tengo.MaxStringLen tengo.MaxBytesLen 并发 编译后的tengo代码被Compiled.Clone后, 可以多个goroutine并发执行. for i := 0; i 这里说的VM感觉就是编译后的\"表达树\"? Although it's not recommended, you can directly create and run the Tengo Compiler, and VM for yourself instead of using Scripts and Script Variables. It's a bit more involved as you have to manage the symbol tables and global variables between them, but, basically that's what Script and Script Variable is doing internally. 用tengo默认的cli命令, 可以看到: $ cat test.tengo fmt := import(\"fmt\") fmt.println(\"hello\") #可以直接执行tengo脚本 $ ./tengo test.tengo hello #也可以\"编译\"成字节码, 编译后大概1k $ ./tengo -o test test.tengo $ llh total 4.8M -rwxr-xr-x 1 yingjieb platform 4.8M Dec 4 08:53 tengo -rwxr-xr-x 1 yingjieb platform 1019 Dec 4 08:53 test -rw-r--r-- 1 yingjieb platform 42 Dec 4 08:53 test.tengo #可以直接运行字节码 $ ./tengo test hello vi test看到这个\"字节码\": 官方例子 编译成字节码后执行 tengo -o myapp myapp.tengo # compile 'myapp.tengo' into binary file 'myapp' tengo myapp # execute the compiled binary `myapp` 加#!/usr/local/bin/tengo后脚本方式执行 脚本必须以.tengo结尾. 似乎是这个cli程序的限制 # copy tengo executable to a dir where PATH environment variable includes cp tengo /usr/local/bin/ # add shebang line to source file cat > myapp.tengo 更多官方例子 gento代码转成lua 从 each := func(x, f) { for k, v in x { f(k, v) } } sum := 0 each([1, 2, 3], func(i, v) { sum += v }) 自动转到: 似乎有点丑... function __iter__(v) if v.__a then local idx = 0 return function() if v[idx] == nil then return nil end idx = idx + 1 return idx-1, v[idx-1] end else return pairs(v) end end getmetatable(\"\").__add=function(a,b) return a..b end local each=function(x,f) for k, v in __iter__(x) do local __cont_1__ = false repeat f(k,v) __cont_1__ = true until 1 if not __cont_1__ then break end end end local sum=0 each({[0]=(1),(2),(3), __a=true},function(i,v) sum=sum+v end ) 有限状态机 看起来挺绕的. https://github.com/d5/go-fsm 全是go代码, 但里面调用了tengo, 实现了fsm框架 main函数只依赖\"github.com/d5/go-fsm\"的API, 但状态机的定义是\"文本\"的: package main import ( \"fmt\" \"github.com/d5/go-fsm\" ) var decimalsScript = []byte(` fmt := import(\"fmt\") export { // test if the first character is a digit is_digit: func(src, dst, v) { return v[0] >= '0' && v[0] %s: %q\\n\", src, dst, v) }, // cut the first character enter: func(src, dst, v) { return v[1:] }, enter_end: func(src, dst, v) { return \"valid number\" }, enter_error: func(src, dst, v) { return \"invalid number: \" + v } }`) func main() { // build and compile state machine //这个调用很吊, 注释和.夹杂 machine, err := fsm.New(decimalsScript). State(\"S\", \"enter\", \"\"). // start State(\"N\", \"enter\", \"\"). // whole numbers State(\"P\", \"enter\", \"\"). // decimal point State(\"F\", \"enter\", \"\"). // fractional part State(\"E\", \"enter_end\", \"\"). // end State(\"X\", \"enter_error\", \"\"). // error Transition(\"S\", \"E\", \"is_eol\", \"print_tx\"). Transition(\"S\", \"N\", \"is_digit\", \"print_tx\"). Transition(\"S\", \"X\", \"\", \"print_tx\"). Transition(\"N\", \"E\", \"is_eol\", \"print_tx\"). Transition(\"N\", \"N\", \"is_digit\", \"print_tx\"). Transition(\"N\", \"P\", \"is_dot\", \"print_tx\"). Transition(\"N\", \"X\", \"\", \"print_tx\"). Transition(\"P\", \"F\", \"is_digit\", \"print_tx\"). Transition(\"P\", \"X\", \"\", \"print_tx\"). Transition(\"F\", \"E\", \"is_eol\", \"print_tx\"). Transition(\"F\", \"F\", \"is_digit\", \"print_tx\"). Transition(\"F\", \"X\", \"\", \"print_tx\"). Compile() if err != nil { panic(err) } // test case 1: \"123.456\" res, err := machine.Run(\"S\", \"123.456\") if err != nil { panic(err) } fmt.Println(res) // test case 2: \"12.34.65\" res, err = machine.Run(\"S\", \"12.34.56\") if err != nil { panic(err) } fmt.Println(res) } "},"notes/golang_govaluate.html":{"url":"notes/golang_govaluate.html","title":"解释器govaluate","keywords":"","body":" govaluate parse阶段 parseTokens token类型 planStages operator 执行阶段 总结 govaluate govaluate提供了简单的C类似的表达式的求值功能. expression, err := govaluate.NewEvaluableExpression(\"10 > 0\"); result, err := expression.Evaluate(nil); // result is now set to \"true\", the bool value. 传参需要用个map[string]interface{}来传递. expression, err := govaluate.NewEvaluableExpression(\"(requests_made * requests_succeeded / 100) >= 90\"); parameters := make(map[string]interface{}, 8) parameters[\"requests_made\"] = 100; parameters[\"requests_succeeded\"] = 80; result, err := expression.Evaluate(parameters); // result is now set to \"false\", the bool value. 除了返回bool, 也可以返回数字 expression, err := govaluate.NewEvaluableExpression(\"(mem_used / total_mem) * 100\"); parameters := make(map[string]interface{}, 8) parameters[\"total_mem\"] = 1024; parameters[\"mem_used\"] = 512; result, err := expression.Evaluate(parameters); // result is now set to \"50.0\", the float64 value. 也可以预定义可以执行的函数, 用govaluate.NewEvaluableExpressionWithFunctions注册 functions := map[string]govaluate.ExpressionFunction { \"strlen\": func(args ...interface{}) (interface{}, error) { length := len(args[0].(string)) return (float64)(length), nil }, } expString := \"strlen('someReallyLongInputString') parse阶段 func NewEvaluableExpression(expression string) (*EvaluableExpression, error) { functions := make(map[string]ExpressionFunction) return NewEvaluableExpressionWithFunctions(expression, functions) //也要经过解析token //在循环里逐个查看字符, 获得token的切片: []ExpressionToken -- 这里的token都不是一个ast ret.tokens, err = parseTokens(expression, functions) err = checkBalance(ret.tokens) err = checkExpressionSyntax(ret.tokens) //优化token slice. ret.tokens, err = optimizeTokens(ret.tokens) //planStages把token列表转为执行树 ret.evaluationStages, err = planStages(ret.tokens) } parseTokens 对input的每个字符都parse func newLexerStream(source string) *lexerStream { var ret *lexerStream var runes []rune for _, character := range source { runes = append(runes, character) } ret = new(lexerStream) ret.source = runes ret.length = len(runes) return ret } token类型 token是{Kind, Value} type ExpressionToken struct { //Kind是预定义的int标号 Kind TokenKind //值就是万能interface Value interface{} } type TokenKind int const ( UNKNOWN TokenKind = iota PREFIX NUMERIC BOOLEAN STRING PATTERN TIME VARIABLE FUNCTION SEPARATOR ACCESSOR COMPARATOR LOGICALOP MODIFIER CLAUSE CLAUSE_CLOSE TERNARY ) planStages 从token到执行树. 那么执行树里面有什么呢? /* Creates a `evaluationStageList` object which represents an execution plan (or tree) which is used to completely evaluate a set of tokens at evaluation-time. The three stages of evaluation can be thought of as parsing strings to tokens, then tokens to a stage list, then evaluation with parameters. */ func planStages(tokens []ExpressionToken) (*evaluationStage, error) { stream := newTokenStream(tokens) stage, err := planTokens(stream) if err != nil { return nil, err } // while we're now fully-planned, we now need to re-order same-precedence operators. // this could probably be avoided with a different planning method reorderStages(stage) stage = elideLiterals(stage) return stage, nil } operator 在plan执行树的时候, 所有最底层的操作都被定义为evaluationStage.operator /* A truly special precedence function, this handles all the \"lowest-case\" errata of the process, including literals, parmeters, clauses, and prefixes. */ func planValue(stream *tokenStream) (*evaluationStage, error) { ... //根据token的kind信息决定operator switch token.Kind { ... case VARIABLE: operator = makeParameterStage(token.Value.(string)) case NUMERIC: fallthrough case STRING: fallthrough case PATTERN: fallthrough case BOOLEAN: symbol = LITERAL operator = makeLiteralStage(token.Value) } } 每个具体的操作都有对应的stage func subtractStage(left interface{}, right interface{}, parameters Parameters) (interface{}, error) { return left.(float64) - right.(float64), nil } func multiplyStage(left interface{}, right interface{}, parameters Parameters) (interface{}, error) { return left.(float64) * right.(float64), nil } 它们被放到一个map表里 var stageSymbolMap = map[OperatorSymbol]evaluationOperator{ EQ: equalStage, NEQ: notEqualStage, GT: gtStage, LT: ltStage, GTE: gteStage, LTE: lteStage, REQ: regexStage, NREQ: notRegexStage, AND: andStage, OR: orStage, IN: inStage, BITWISE_OR: bitwiseOrStage, BITWISE_AND: bitwiseAndStage, BITWISE_XOR: bitwiseXORStage, BITWISE_LSHIFT: leftShiftStage, BITWISE_RSHIFT: rightShiftStage, PLUS: addStage, MINUS: subtractStage, MULTIPLY: multiplyStage, DIVIDE: divideStage, MODULUS: modulusStage, EXPONENT: exponentStage, NEGATE: negateStage, INVERT: invertStage, BITWISE_NOT: bitwiseNotStage, TERNARY_TRUE: ternaryIfStage, TERNARY_FALSE: ternaryElseStage, COALESCE: ternaryElseStage, SEPARATE: separatorStage, } 执行阶段 /* Same as `Eval`, but automatically wraps a map of parameters into a `govalute.Parameters` structure. */ func (this EvaluableExpression) Evaluate(parameters map[string]interface{}) (interface{}, error) { if parameters == nil { return this.Eval(nil) } /* Runs the entire expression using the given [parameters]. e.g., If the expression contains a reference to the variable \"foo\", it will be taken from `parameters.Get(\"foo\")`. This function returns errors if the combination of expression and parameters cannot be run, such as if a variable in the expression is not present in [parameters]. In all non-error circumstances, this returns the single value result of the expression and parameters given. e.g., if the expression is \"1 + 1\", this will return 2.0. e.g., if the expression is \"foo + 1\" and parameters contains \"foo\" = 2, this will return 3.0 */ return this.Eval(MapParameters(parameters)) { return this.evaluateStage(this.evaluationStages, parameters) { //这里其实对应的是这个stage, 比如是multiplyStage, 执行 //left.(float64) * right.(float64), nil return stage.operator(left, right, parameters) } } } 总结 这个库是个非常简化版的解释器. 也有标准的三步: token阶段, 对应ast planStage阶段, 每个基础操作都有个operaor, 对应执行树 执行阶段, 执行预定义好的动作operaor. "},"notes/golang_encoding_gotiny.html":{"url":"notes/golang_encoding_gotiny.html","title":"gotiny编解码","keywords":"","body":" 警告 gotiny会直接用传入的buf 原因 解决 问答 gotiny_test baseTyp 带方法的类型 interface变量 构造测试数据 测试流程 补充 interface和reflect.Value 例子 代码阅读之encode Marshal Encoder Encode 总结 encode核心函数 基础类型编码 bool编码 int编码 其他 非基础类型编码 每个类型都对应一个encEngine 获取Interface的实体类型名 从指向interface的指针到其数据(解引用) 疑问 补充: reflect.Value源码 总结 代码阅读之decode Unmarshal 解码engine Decoder类型 解码bool类型 解码string getDecEngine() 总结 encoder和decoder总结 警告 gotiny会直接用传入的buf 遇到一个问题, 用在循环里用gotiny解码, 传入了相同的buf: dec := gotiny.NewDecoderWithPtr((*streamTransportMsg)(nil)) bufSize := make([]byte, 4) bufMsg := make([]byte, 512) for { //从网络读size和buf到bufSize和bufMsg var tm streamTransportMsg dec.Decode(bufMsg, &tm) //解码到tm, tm是个结构体, 里面包括msg域是[]byte // swap src and dst streamChan 奇怪的现象是, for循环里大概收到了几次的报文, 经过解码得到: 3 8 hello world 在第9行, send到channel之前打印tm.msg, 都是对的. 但另外一个goroutine读这个streamChan, 也能得到4次数据, 但是是这样的: 8 8 world world 原因 gotiny对[]byte的编码直接用了用户传入的buf func decBytes(d *Decoder, p unsafe.Pointer) { bytes := (*[]byte)(p) if d.decIsNotNil() { l := int(d.decUint32()) *bytes = d.buf[d.index : d.index+l] //这里的d.buf就是Decode传入的用户buf d.index += l } else if !isNil(p) { *bytes = nil } } 这么来看, 因为bufMsg := make([]byte, 512)是共享的, 另外一个goroutine还没有来得及读channel, 这个bufMsg就会被下次的Decode调用改变掉, 导致tm.msg变化. 除了decBytes, 字符串和int8/uint8都直接用了buf.--2022.09.10更新, string, int8, uint8在赋值的时候发生了值拷贝. 特别的, 把一个byte array强转成string, 虽然string是个胖指针, 但我认为其底层的字符串内容也发生了每个字节的拷贝. func decString(d *Decoder, p unsafe.Pointer) { l, val := int(d.decUint32()), (*string)(p) *val = string(d.buf[d.index : d.index+l]) d.index += l } func decInt8(d *Decoder, p unsafe.Pointer) { *(*int8)(p) = int8(d.buf[d.index]); d.index++ } func decUint8(d *Decoder, p unsafe.Pointer) { *(*uint8)(p) = d.buf[d.index]; d.index++ } 解决 给gotiny增加copy mode, 使能之后在decBytes时就拷贝一份 bytes := (*[]byte)(p) if d.decIsNotNil() { l := int(d.decUint32()) if d.copyMode { buf := make([]byte, l) copy(buf, d.buf[d.index:d.index+l]) *bytes = buf } else { *bytes = d.buf[d.index : d.index+l] } d.index += l } else if !isNil(p) { *bytes = nil 详见这个PR: https://github.com/niubaoshu/gotiny/pull/6 问答 为什么Marshal强制要求入参是指针?答: 估计是性能方面的考虑. 入参是普通对象也是可以的, 但参数也会值拷贝. 强制要求入参是指针, 就避免了用户传入\"值\"带来的深拷贝. 这个设计挺好的. 为什么要用这样的结构?reflect.ValueOf(&接口变量).Elem().InterfaceData()[1]从功能上看, 这和reflect.ValueOf(接口变量).InterfaceData()[1]是不是一样的?猜测: 我认为从功能上看是一样的. 因为对一个接口变量取地址再ValueOf再Elem就还原了这个接口变量. 那么作者这么做的用意是避免拷贝吗? 取地址再ValueOf能避免interface直接赋值带来的拷贝, 但是再Elem不是还要拷贝吗? 比如rv := reflect.ValueOf(a)实际上是得到a的副本的值 那这里空转了一下的意义又是什么呢? gotiny_test baseTyp 首先定义了一个baseTyp, 包含了基本类型, array, interface, 以及匿名包含了另外一个结构体 没有slice, 没有maptype baseTyp struct { ... //基本类型 array [3]uint32 inter interface{} A //匿名包含A, A是个结构体, 里面是name phone等有实际意义的field } 还有无限循环的自定义类型, 很神type ( cirTyp *cirTyp cirStruct struct { a int *cirStruct } cirMap map[int]cirMap cirSlice []cirSlice ) var( vcir cirTyp v2cir cirTyp = &vcir v3cir cirTyp = &v2cir ) 带方法的类型 tint实现了io readWriter type tint int func (tint) Read([]byte) (int, error) { return 0, nil } func (tint) Write([]byte) (int, error) { return 0, nil } func (tint) Close() error { return nil } gotiny自己的编解码接口, 外部类型可以实现这两个接口, 框架会调用注意Encode只返回[]byte, 而Decode返回使用了多少个字节的buf. 两个接口都不返回错误. 那么错误只能通过panic传递. // 只应该由指针来实现该接口 type GoTinySerializer interface { // 编码方法，将对象的序列化结果append到入参数并返回，方法不应该修改入参数值原有的值 GotinyEncode([]byte) []byte // 解码方法，将入参解码到对象里并返回使用的长度。方法从入参的第0个字节开始使用，并且不应该修改入参中的任何数据 GotinyDecode([]byte) int } // 下面这个gotinyTest实现了上面的接口, 演示了 type gotinyTest string func (v *gotinyTest) GotinyEncode(buf []byte) []byte { return append(buf, gotiny.Marshal((*string)(v))...) } func (v *gotinyTest) GotinyDecode(buf []byte) int { return gotiny.Unmarshal(buf, (*string)(v)) } interface变量 vnilptr *int v2nilptr []string vnilptrptr = &vnilptr v0interface interface{} vinterface interface{} = varray v1interface io.ReadWriteCloser = tint(2) v2interface io.ReadWriteCloser = os.Stdin v3interface interface{} = &vinterface v4interface interface{} = &v1interface v5interface interface{} = &v2interface v6interface interface{} = &v3interface v7interface interface{} = &v0interface v8interface interface{} = &vnilptr v9interface interface{} = &v8interface 构造测试数据 这里的数据是个interface切片 vs = []interface{}{ 里面是各种生成的, 预赋值的变量 } length = len(vs) buf = make([]byte, 0, 1 要看懂InterfaceData()函数, 要看源码: 把v.ptr当作[2]uintptr的指针, 取值后返回. 注意最外层的取值*操做, 就是拷贝*[2]uintptr指向的[2]uintptr的意思, 也就是返回\"只读\"的[2]uintptr // InterfaceData returns the interface v's value as a uintptr pair. // It panics if v's Kind is not Interface. func (v Value) InterfaceData() [2]uintptr { // TODO: deprecate this v.mustBe(Interface) // We treat this as a read operation, so we allow // it even for unexported data, because the caller // has to import \"unsafe\" to turn it into something // that can be abused. // Interface value is always bigger than a word; assume flagIndir. return *(*[2]uintptr)(v.ptr) } 测试流程 func TestEncodeDecode(t *testing.T) { buf := gotiny.Marshal(srci...) //srci是interface切片, 底层是指针 gotiny.Unmarshal(buf, reti...) //retiye是interface切片, 实际也是指针 for i, r := range reti { Assert(t, buf, srci[i], r) //最后判断编解码是否一致 } } 补充 interface和reflect.Value 这里的Value就是reflect.Value type Value struct { // typ holds the type of the value represented by a Value. typ *rtype // Pointer-valued data or, if flagIndir is set, pointer to data. // Valid when either flagIndir is set or typ.pointers() is true. ptr unsafe.Pointer //带了一个flag, 表示这个值的metadata // flag holds metadata about the value. // The lowest bits are flag bits: // - flagStickyRO: obtained via unexported not embedded field, so read-only // - flagEmbedRO: obtained via unexported embedded field, so read-only // - flagIndir: val holds a pointer to the data 注意这里, ptr可能是\"值\", 也可能是指针 // - flagAddr: v.CanAddr is true (implies flagIndir) // - flagMethod: v is a method value. // The next five bits give the Kind of the value. // This repeats typ.Kind() except for method values. // The remaining 23+ bits give a method number for method values. // If flag.kind() != Func, code can assume that flagMethod is unset. // If ifaceIndir(typ), code can assume that flagIndir is set. flag // A method value represents a curried method invocation // like r.Read for some receiver r. The typ+val+flag bits describe // the receiver r, but the flag's Kind bits say Func (methods are // functions), and the top bits of the flag give the method number // in r's type's method table. } interface有空的interface和带方法的interface两种: 大小都是两个指针 代码在/usr/local/go/src/reflect/value.go // emptyInterface is the header for an interface{} value. type emptyInterface struct { typ *rtype word unsafe.Pointer } // nonEmptyInterface is the header for an interface value with methods. type nonEmptyInterface struct { // see ../runtime/iface.go:/Itab itab *struct { ityp *rtype // static interface type typ *rtype // dynamic concrete type hash uint32 // copy of typ.hash _ [4]byte fun [100000]unsafe.Pointer // method table } word unsafe.Pointer } 例子 src1, src2 := \"hello\", []byte(\" world!\") ret1, ret2 := \"\", []byte{3, 4, 5} gotiny.Unmarshal(gotiny.Marshal(&src1, &src2), &ret1, &ret2) fmt.Println(ret1 + string(ret2)) // print \"hello world!\" enc := gotiny.NewEncoder(src1, src2) dec := gotiny.NewDecoder(ret1, ret2) Marshal可传入多个interface, 但必须是指针形式 Unmarshal也是是多个interface, 必须是指针 NewEncoder和NewDecoder要传入所有编解码类型的实例 代码阅读之encode Marshal func Marshal(is ...interface{}) []byte { return NewEncoderWithPtr(is...).Encode(is...) } 实际每个Marshal都是先NewEncoder Encoder要预先生成 func NewEncoderWithPtr(ps ...interface{}) *Encoder { l := len(ps) engines := make([]encEng, l) for i := 0; i NewEncoderWithPtr函数的核心是build一个engine切片engines, 按照入参顺序建立编码引擎. engines[i] = getEncEngine(rt.Elem()) NewEncoder函数类似, 但入参不是指针 func NewEncoder(is ...interface{}) *Encoder { l := len(is) engines := make([]encEng, l) for i := 0; i 反正都要rt := reflect.TypeOf(ps[i])或reflect.TypeOf(is[i]), 那这两个函数可以合并的吧 Encoder type Encoder struct { buf []byte //编码目的数组 off int boolPos int //下一次要设置的bool在buf中的下标,即buf[boolPos] boolBit byte //下一次要设置的bool的buf[boolPos]中的bit位 engines []encEng length int } Encode encode的作用是, 按照出参顺序, 依次encode到byte数组里 func (e *Encoder) Encode(is ...interface{}) []byte { engines := e.engines for i := 0; i engines是encEngine数组encEngine是个函数type encEng func(*Encoder, unsafe.Pointer) 这里关键是这句:engines[i](e, (*[2]unsafe.Pointer)(unsafe.Pointer(&is[i]))[1]) 其中(unsafe.Pointer(&is[i]))是把输入的参数取地址后转成unsafe.Pointer(*[2]unsafe.Pointer)强转后的值[1]是把强转后的地址强转成指向unsafe.Pointer的数组 [2]unsafe.Pointer.这里用到了数组go的数组表达: 切片的表达是个结构体, 包括了数组的指针和大小, 但数组还是和C一样的\"原始\"样子: 指向首元素的指针可以代表这个数组.比如 func main() { s := [7]int{1,2,3,4,5,6,7} //数组的指针可以当数组用 sp := &s //如果s是切片, 则下面语句报错:(type *[]int does not support indexing) sp[1] = 999 fmt.Println(sp) //强转成指向数组的指针可以突破数组的length限制 //但要注意, 你明确知道在干什么. 否则, segment fault snp := (*[20]int)(unsafe.Pointer(&s[0])) snp[15] = 995 fmt.Println(snp) } 那么强转成(*[2]unsafe.Pointer)是什么道理呢? 原来这里用到了interface的内部表达: type eface struct { // 16 bytes _type *_type data unsafe.Pointer } type iface struct { // 16 bytes tab *itab data unsafe.Pointer } 注意到这里取的就是interface的data域, 因为不论是eface还是iface, data域都是\"第二个\"unsafe.Pointer所以, 这里我猜, type encEng func(*Encoder, unsafe.Pointer)就是要把这个unsafe.Pointer指向的数据, 保存在encoder的buf中. 总结 每个入参interface都有对应的一个encEngine, 这个engine在NewEncoder的时候build好. encEngine负责把interface的data指针编码到encoder中 encode核心函数 上面说到, 入参interface的encEngine需要预先build好. 那NewEncoder里面, 调用的getEncEngine就是干这个的. 到这里已经是解引用了, 这个reflect.Type就是入参interface的实际类型. func getEncEngine(rt reflect.Type) encEng { encLock.RLock() engine := rt2encEng[rt] encLock.RUnlock() if engine != nil { return engine } encLock.Lock() buildEncEngine(rt, &engine) encLock.Unlock() return engine } 先在全局表里按照reflect.Type来查找encEngine这里写的很统一, 实际上, 为什么不用比如reflect.TypeOf(true)呢? 非要先取指针再Elem()这些都是基础类型 rt2encEng = map[reflect.Type]encEng{ reflect.TypeOf((*bool)(nil)).Elem(): encBool, reflect.TypeOf((*int)(nil)).Elem(): encInt, reflect.TypeOf((*int8)(nil)).Elem(): encInt8, reflect.TypeOf((*int16)(nil)).Elem(): encInt16, reflect.TypeOf((*int32)(nil)).Elem(): encInt32, reflect.TypeOf((*int64)(nil)).Elem(): encInt64, reflect.TypeOf((*uint)(nil)).Elem(): encUint, reflect.TypeOf((*uint8)(nil)).Elem(): encUint8, reflect.TypeOf((*uint16)(nil)).Elem(): encUint16, reflect.TypeOf((*uint32)(nil)).Elem(): encUint32, reflect.TypeOf((*uint64)(nil)).Elem(): encUint64, reflect.TypeOf((*uintptr)(nil)).Elem(): encUintptr, reflect.TypeOf((*unsafe.Pointer)(nil)).Elem(): encPointer, reflect.TypeOf((*float32)(nil)).Elem(): encFloat32, reflect.TypeOf((*float64)(nil)).Elem(): encFloat64, reflect.TypeOf((*complex64)(nil)).Elem(): encComplex64, reflect.TypeOf((*complex128)(nil)).Elem(): encComplex128, reflect.TypeOf((*[]byte)(nil)).Elem(): encBytes, reflect.TypeOf((*string)(nil)).Elem(): encString, reflect.TypeOf((*time.Time)(nil)).Elem(): encTime, reflect.TypeOf((*struct{})(nil)).Elem(): encIgnore, reflect.TypeOf(nil): encIgnore, } 基础的编码函数在encbase.go 特别的, 空的结构体和nil按encIgnore编码(也就是啥也不做) func encIgnore(*Encoder, unsafe.Pointer) {} 基础类型编码 bool编码 比如对bool类型的编码: 作者github上是这么描述的: bool类型占用一位，真值编码为1，假值编码为0。当第一次遇到bool类型时会申请一个字节，将值编入最低位，第二次遇到时编入次低位，第九次遇到bool值时再申请一个字节编入最低位，以此类推。 func encBool(e *Encoder, p unsafe.Pointer) { e.encBool(*(*bool)(p)) } func (e *Encoder) encBool(v bool) { if e.boolBit == 0 { e.boolPos = len(e.buf) e.buf = append(e.buf, 0) e.boolBit = 1 } if v { e.buf[e.boolPos] |= e.boolBit } e.boolBit 解释: 这里的boolBit是个byte类型的变量, 每左移8次后变回0. 每次变回0的时候, 就新申请一个byte. int编码 uint8和int8 类型作为一个字节编入字符串的下一个字节。 uint16,uint32,uint64,uint,uintptr 采用Varints编码方式。 int16,int32,int64,int 采用ZigZag转换成一个无符号数后采用Varints编码方式。 从代码上看, int先转为int64, 再转为uint64, 但不是直接转, 而是用zigzag方式转. zigzag的思想来自于我们常用的int都是小整数, 比如15, 2048等等的. 那么可以\"压缩\"来减小体积: 把符号位移到最后, 再做处理. 因为有符号数是原码取反加一(即补码), 对计算机来说和一个非常大的无符号数差不多, 所以要把符号位挪到最后. 详见zigzag算法详细解释 func encInt(e *Encoder, p unsafe.Pointer) { e.encUint64(int64ToUint64(int64(*(*int)(p)))) } // int -5 -4 -3 -2 -1 0 1 2 3 4 5 6 // uint 9 7 5 3 1 0 2 4 6 8 10 12 func int64ToUint64(v int64) uint64 { return uint64((v > 63)) } // uint 9 7 5 3 1 0 2 4 6 8 10 12 // int -5 -4 -3 -2 -1 0 1 2 3 4 5 6 func uint64ToInt64(u uint64) int64 { v := int64(u) return (-(v & 1)) ^ (v>>1)&0x7FFFFFFFFFFFFFFF } func (e *Encoder) encUint64(v uint64) { switch { case v >7)) case v >7)|0x80, byte(v>>14)) case v >7)|0x80, byte(v>>14)|0x80, byte(v>>21)) case v >7)|0x80, byte(v>>14)|0x80, byte(v>>21)|0x80, byte(v>>28)) case v >7)|0x80, byte(v>>14)|0x80, byte(v>>21)|0x80, byte(v>>28)|0x80, byte(v>>35)) case v >7)|0x80, byte(v>>14)|0x80, byte(v>>21)|0x80, byte(v>>28)|0x80, byte(v>>35)|0x80, byte(v>>42)) case v >7)|0x80, byte(v>>14)|0x80, byte(v>>21)|0x80, byte(v>>28)|0x80, byte(v>>35)|0x80, byte(v>>42)|0x80, byte(v>>49)) default: e.buf = append(e.buf, byte(v)|0x80, byte(v>>7)|0x80, byte(v>>14)|0x80, byte(v>>21)|0x80, byte(v>>28)|0x80, byte(v>>35)|0x80, byte(v>>42)|0x80, byte(v>>49)|0x80, byte(v>>56)) } } 所以一般情况下, 比如一个几百的无符号数, zigzag后也不大, 那走上面的switch case估计要走到第二个. 其实如果不考虑大小, 直接存储应该是最方便快速的. 其他 float32和float64采用gob中对浮点类型的编码方式。 complex64类型会强转为一个uint64后采用uint64的编码方式。 complex128类型分别将虚实部分作为float64类型编码。 字符串类型先将字符串长度强转为uint64类型编码，然后将字符串字节数组自身原样编码。 指针类型判断是否为nil，如果是nil，编入一个bool类型的false值后结束，如果不为nil，编入一个bool类型true值，之后将指针解引用，按解引用后的类型编码。 array和slice类型先将长度强转为一个uint64后采用uint64的编码方式编入，然后将每一个元素安装自身的类型编码。 map同上，先编入长度，然后编入一个健，后面跟健对应的值，在编入一个健，接着是值，以此类推。 struct类型将结构体的所有成员按其类型编码，无论是否导出，非导出的字段也会编码。结构体会严格还原。 对于实现encoding包BinaryMarshaler/BinaryUnmarshaler 或 实现 gob包GobEncoder/GobDecoder 接口的类型会用实现的方法编码。 对于实现了gotiny.GoTinySerialize包的类型将采用实现的方法编码和解码 channel和function不能编码 非基础类型编码 是这个函数负责的:buildEncEngine() 基本上能想象, 这是个不断递归的过程. 这个函数写的神了! func buildEncEngine(rt reflect.Type, engPtr *encEng) { ... //如果实现了其他编解码的接口, 先调用那些接口 kind := rt.Kind() var eEng encEng switch kind { case reflect.Ptr: //解引用然后调用其encEngine defer buildEncEngine(rt.Elem(), &eEng) engine = func(e *Encoder, p unsafe.Pointer) { isNotNil := !isNil(p) e.encIsNotNil(isNotNil) if isNotNil { eEng(e, *(*unsafe.Pointer)(p)) } } case reflect.Array: et, l := rt.Elem(), rt.Len() defer buildEncEngine(et, &eEng) size := et.Size() engine = func(e *Encoder, p unsafe.Pointer) { for i := 0; i 0 { //iface engine = func(e *Encoder, p unsafe.Pointer) { isNotNil := !isNil(p) e.encIsNotNil(isNotNil) if isNotNil { v := reflect.ValueOf(*(*interface { M() //注意这里的M的意思是临时定义一个含有M方法的interface. M无入参, 无返回值 })(p)) et := v.Type() e.encString(getNameOfType(et)) getEncEngine(et)(e, getUnsafePointer(&v)) } } } else { //eface engine = func(e *Encoder, p unsafe.Pointer) { isNotNil := !isNil(p) e.encIsNotNil(isNotNil) if isNotNil { v := reflect.ValueOf(*(*interface{})(p)) et := v.Type() e.encString(getNameOfType(et)) getEncEngine(et)(e, getUnsafePointer(&v)) } } } case reflect.Chan, reflect.Func: panic(\"not support \" + rt.String() + \" type\") default: engine = encEngines[kind] } rt2encEng[rt] = engine //build一次, 永久使用 *engPtr = engine } 注: func buildEncEngine(rt reflect.Type, engPtr *encEng)的递归形式是出参形式, 而非return方式type encEng func(*Encoder, unsafe.Pointer)中的unsafe.Pointer指向的类型就是前面rt reflect.Type, 它们是NewEncoder的入参interface的两种表达, rt reflect.Type是类型, 是入参interface用TypeOf得来; unsafe.Pointer是数据, 是入参interface的data域(*[2]unsafe.Pointer)(unsafe.Pointer(入参的地址))[1] interface类型先编码一个encIsNotNil, 再编码其concrete类型的名字e.encString(getNameOfType(et)), 最后编码其值. 也就是说interface类型的编码前面总是会先编码类型名. 整个buildEncEngine的过程是有锁的. 每个类型都对应一个encEngine unsafe.Pointer指向这个类型的实例在编码Interface时, 先编码是否nil, 再编码类型名, 最后编码实体对象. 这里的类型名怎么来的? 首先要知道engine的数据指针p unsafe.Pointer对应的数据类型就是这个engine对应的类型. case reflect.Interface: if rt.NumMethod() > 0 { engine = func(e *Encoder, p unsafe.Pointer) { isNotNil := !isNil(p) e.encIsNotNil(isNotNil) if isNotNil { v := reflect.ValueOf(*(*interface { M() })(p)) et := v.Type() e.encString(getNameOfType(et)) getEncEngine(et)(e, getUnsafePointer(&v)) } } } else { engine = func(e *Encoder, p unsafe.Pointer) { isNotNil := !isNil(p) e.encIsNotNil(isNotNil) if isNotNil { v := reflect.ValueOf(*(*interface{})(p)) et := v.Type() e.encString(getNameOfType(et)) getEncEngine(et)(e, getUnsafePointer(&v)) } } } 比如上面代码中, p unsafe.Pointer指向的是第一行的reflect.Interface. 已知这个data指针p, 想得到其值v, 有两种情况: 带方法的iface 这里定义了一个临时的方法M(), 用来占位?v := reflect.ValueOf(*(*interface { M() })(p)) 不带方法的efacev := reflect.ValueOf(*(*interface{})(p)) 获取Interface的实体类型名 得到v后, 从et := v.Type()中就能得到类型名getNameOfType(et): func getNameOfType(rt reflect.Type) string { if name, has := type2name[rt]; has { //先从已知表中查找 return name } else { return registerType(rt) } } func Register(i interface{}) string { return registerType(reflect.TypeOf(i)) } func registerType(rt reflect.Type) string { name := GetNameByType(rt) RegisterName(name, rt) return name } func RegisterName(name string, rt reflect.Type) { if name == \"\" { panic(\"attempt to register empty name\") } if rt == nil || rt.Kind() == reflect.Invalid { panic(\"attempt to register nil type or invalid type\") } if _, has := type2name[rt]; has { panic(\"gotiny: registering duplicate types for \" + GetNameByType(rt)) } if _, has := name2type[name]; has { panic(\"gotiny: registering name\" + name + \" is exist\") } name2type[name] = rt type2name[rt] = name } 如果已知表中没有查到, 就要注册这个rt. 注册就是把type和name分别放到两个查找表中(name2type, type2name), 双向可查. 重点是 func GetNameByType(rt reflect.Type) string { return string(getName([]byte(nil), rt)) } func getName(prefix []byte, rt reflect.Type) []byte { if rt == nil || rt.Kind() == reflect.Invalid { return append(prefix, []byte(\"\")...) } if rt.Name() == \"\" { //未命名的，组合类型 switch rt.Kind() { case reflect.Ptr: return getName(append(prefix, '*'), rt.Elem()) case reflect.Array: // array是带大小的, 比如[11] return getName(append(prefix, \"[\"+strconv.Itoa(rt.Len())+\"]\"...), rt.Elem()) case reflect.Slice: // slice只是加个[] return getName(append(prefix, '[', ']'), rt.Elem()) case reflect.Struct: // struct的形式类似struct{f1 f1Type; f2 f2Type} prefix = append(prefix, \"struct {\"...) nf := rt.NumField() if nf > 0 { prefix = append(prefix, ' ') } for i := 0; i 0 { prefix = append(prefix, ' ') } for i := 0; i 0 { prefix = append(prefix, ' ') } if no > 1 { prefix = append(prefix, '(') } for i := 0; i 1 { prefix = append(prefix, ')') } return prefix } } //有名的类型走这里, 直接加上package名和类型名 if rt.PkgPath() == \"\" { prefix = append(prefix, rt.Name()...) } else { prefix = append(prefix, rt.PkgPath()+\".\"+rt.Name()...) } return prefix } 从指向interface的指针到其数据(解引用) 还是下面这段代码, 已知指向interface case reflect.Interface 的指针p p unsafe.Pointer 求其值的解码 case reflect.Interface: //化简为eface engine = func(e *Encoder, p unsafe.Pointer) { isNotNil := !isNil(p) e.encIsNotNil(isNotNil) if isNotNil { v := reflect.ValueOf(*(*interface{})(p)) et := v.Type() e.encString(getNameOfType(et)) getEncEngine(et)(e, getUnsafePointer(&v)) } } 以eface为例. 首先p是个指针, 指向上级interface的data, 而这个data还是个interface. 这里先把p转成指向interface的指针, 那么再取值*(*interface{})(p)就是这个interface了.然后ValueOf这个值得到的v, 就是要被编码的interface的reflect.Value表达.那么接下来先编码代表这个类型的字符串.然后递归的调用getEncEngine(et)(e, getUnsafePointer(&v)) 在unsafe.go中, 作者把reflect.Value以复制代码的形式, 定义成refVal.\"原版\"的reflect.Value的域成员是小写的无法导出, 只有源码拷贝定义才行.这里用到了go的链接指示词//go:linkname flagIndir reflect.flagIndir, 把flagIndir当作reflect.flagIndir来链接. type refVal struct { _ unsafe.Pointer ptr unsafe.Pointer flag flag } type flag uintptr //go:linkname flagIndir reflect.flagIndir const flagIndir flag = 1 这里的逻辑是, 如果rv *reflect.Value的ptr是个Pointer-valued data, 就返回它的地址; 如果是普通的指针, 就返回它本身. 疑问 这里先取到指针p指向的interface, 获得其reflect.Value表达v, 然后用reflect.Value再去生成encEngine来调用, 并传入getUnsafePointer(&v)做为下一级的unsafe.Pointer. 问题是, 为什么不直接用interface呢? 像这样: case reflect.Interface: //化简为eface engine = func(e *Encoder, p unsafe.Pointer) { isNotNil := !isNil(p) e.encIsNotNil(isNotNil) if isNotNil { i := *(*interface{})(p) //先得到这个interface的值 i et := reflect.TypeOf(i) e.encString(getNameOfType(et)) getEncEngine(et)(e, (*[2]unsafe.Pointer)(unsafe.Pointer(&i))[1]) //或者直接用p getEncEngine(et)(e, (*[2]unsafe.Pointer)(p)[1]) } } 有两种可能: 避免值拷贝. 很明显i := *(*interface{})(p)会发生interface的值拷贝, 但拷贝interface似乎不heavy? interface里面存interface的情况下, 不能用\"第二个\"unsafe.Pointer指针找到data. 补充: reflect.Value源码 根据/usr/local/go/src/reflect/value.go的注释: type Value struct { // typ holds the type of the value represented by a Value. typ *rtype // Pointer-valued data or, if flagIndir is set, pointer to data. // Valid when either flagIndir is set or typ.pointers() is true. ptr unsafe.Pointer // flag holds metadata about the value. flag } type flag uintptr const ( flagKindWidth = 5 // there are 27 kinds flagKindMask flag = 1 总结 为什么要defer里面递归?因为这个函数的最后, 把生成的engine要放到全局变量rt2encEng中, 以后再次遇到同类型的值, 就不需要再build一次了.defer的意思是先让本次的结果进到这个rt2encEng里面, 这样如果递归的过程还需要用到这个类型, 就可以直接查找到了. 很神 func buildEncEngine(rt reflect.Type, engPtr *encEng)的递归形式是出参形式, 而非return方式type encEng func(*Encoder, unsafe.Pointer)中的unsafe.Pointer指向的类型就是前面rt reflect.Type, 它们是NewEncoder的入参interface的两种表达, rt reflect.Type是类型, 是入参interface用TypeOf得来; unsafe.Pointer是数据, 是入参interface的data域(*[2]unsafe.Pointer)(unsafe.Pointer(入参的地址))[1] interface类型先编码一个encIsNotNil, 再编码其concrete类型的名字e.encString(getNameOfType(et)), 最后编码其值. 也就是说interface类型的编码前面总是会先编码类型名. 整个buildEncEngine的过程是有锁的. 代码阅读之decode Unmarshal func Unmarshal(buf []byte, is ...interface{}) int { return NewDecoderWithPtr(is...).Decode(buf, is...) } Decode的核心逻辑是按照入参is的类型, 从buf里解码 解码engine 解码engine的形式竟然和编码一样: type decEng func(*Decoder, unsafe.Pointer) 也是从一个全局表里查 rt2decEng = map[reflect.Type]decEng{ reflect.TypeOf((*bool)(nil)).Elem(): decBool, reflect.TypeOf((*int)(nil)).Elem(): decInt, reflect.TypeOf((*int8)(nil)).Elem(): decInt8, reflect.TypeOf((*int16)(nil)).Elem(): decInt16, reflect.TypeOf((*int32)(nil)).Elem(): decInt32, reflect.TypeOf((*int64)(nil)).Elem(): decInt64, reflect.TypeOf((*uint)(nil)).Elem(): decUint, reflect.TypeOf((*uint8)(nil)).Elem(): decUint8, reflect.TypeOf((*uint16)(nil)).Elem(): decUint16, reflect.TypeOf((*uint32)(nil)).Elem(): decUint32, reflect.TypeOf((*uint64)(nil)).Elem(): decUint64, reflect.TypeOf((*uintptr)(nil)).Elem(): decUintptr, reflect.TypeOf((*unsafe.Pointer)(nil)).Elem(): decPointer, reflect.TypeOf((*float32)(nil)).Elem(): decFloat32, reflect.TypeOf((*float64)(nil)).Elem(): decFloat64, reflect.TypeOf((*complex64)(nil)).Elem(): decComplex64, reflect.TypeOf((*complex128)(nil)).Elem(): decComplex128, reflect.TypeOf((*[]byte)(nil)).Elem(): decBytes, reflect.TypeOf((*string)(nil)).Elem(): decString, reflect.TypeOf((*time.Time)(nil)).Elem(): decTime, reflect.TypeOf((*struct{})(nil)).Elem(): decIgnore, reflect.TypeOf(nil): decIgnore, } Decoder类型 type Decoder struct { buf []byte //buf index int //下一个要使用的字节在buf中的下标 boolPos byte //下一次要读取的bool在buf中的下标,即buf[boolPos] boolBit byte //下一次要读取的bool的buf[boolPos]中的bit位 engines []decEng //解码器集合 length int //解码器数量 } 解码bool类型 解码的思路就是把数据还原, 写到unsafe.Pointer中. 比如解码bool, 是编码bool的逆过程 func decBool(d *Decoder, p unsafe.Pointer) { *(*bool)(p) = d.decBool() } func (d *Decoder) decBool() (b bool) { if d.boolBit == 0 { d.boolBit = 1 d.boolPos = d.buf[d.index] d.index++ } b = d.boolPos&d.boolBit != 0 d.boolBit 解码string 先解出len, 把buf的index到index+len的字符串拷贝到指针p里. func decString(d *Decoder, p unsafe.Pointer) { l, val := int(d.decUint32()), (*string)(p) *val = string(d.buf[d.index : d.index+l]) d.index += l } 对比编码string的过程是先编码p指向的string的len, 再依次拷贝s的字符到buf func encString(e *Encoder, p unsafe.Pointer) { s := *(*string)(p) e.encUint32(uint32(len(s))) e.buf = append(e.buf, s...) } getDecEngine() 和Marshal一样, Unmarshal也要先build解码器, 其核心是 engines[i] = getDecEngine(reflect.TypeOf(入参interface)) 也是先从全局表查找, 基础类型的decEngine早已写好到rt2decEng func getDecEngine(rt reflect.Type) decEng { decLock.RLock() engine := rt2decEng[rt] decLock.RUnlock() if engine != nil { return engine } decLock.Lock() buildDecEngine(rt, &engine) decLock.Unlock() return engine } 复合类型需要build func buildDecEngine(rt reflect.Type, engPtr *decEng) { //如果递归调用到这里, 先看看之前是否有新加入的decEng engine, has := rt2decEng[rt] if has { *engPtr = engine return } if _, engine = implementOtherSerializer(rt); engine != nil { rt2decEng[rt] = engine *engPtr = engine return } kind := rt.Kind() var eEng decEng switch kind { case reflect.Ptr: //p指向的是ptr et := rt.Elem() defer buildDecEngine(et, &eEng) engine = func(d *Decoder, p unsafe.Pointer) { if d.decIsNotNil() { //因为是ptr, 在编码的时候是先编码一个bool的, 标识是否nil if isNil(p) { // p指向的值是nil *(*unsafe.Pointer)(p) = unsafe.Pointer(reflect.New(et).Elem().UnsafeAddr()) // 新申请一个et类型的变量空间, 并赋值给p; 现在p指向这个新的空间. 关键是reflect.New函数 } eEng(d, *(*unsafe.Pointer)(p)) //调用指针指向的value的decEng } else if !isNil(p) { //原值(即编码时的值)就是nil *(*unsafe.Pointer)(p) = nil } } case reflect.Array: //和编码结构高度一致 l, et := rt.Len(), rt.Elem() size := et.Size() defer buildDecEngine(et, &eEng) engine = func(d *Decoder, p unsafe.Pointer) { for i := 0; i 总结 解码的思路就是把数据还原, 写到unsafe.Pointer p中 但这里一个明显的不足是, 必须要预先指定要解码的数据类型. 解码器需要按照指定的入参类型来解码. 要decode interface, 需要预先显式注册该interface的实体类型(即建立name到reflect.Type的映射关系). 这点和gob一样. 编码interface的类型字符串只是用来当作key查找其reflect.Type类型的. encoder和decoder总结 encoder根据入参类型build encEngine表; decoder类似 如果要decode interface, 要先注册其concrete类型 encode不需要预先注册interface的concrete类型, encode流程里会自动注册. "},"notes/golang_abs.html":{"url":"notes/golang_abs.html","title":"解释器abs","keywords":"","body":" abs代码细看 为什么stdin能被迭代? evalIdentifier() 回到for in 现在清楚了 abs代码 代码组织 代码风格 从cli开始 全局env NewEnvironment 基础对象 string基础对象 Array 内置函数 内置函数的实现 内置函数如何被调用 -- Eval()的魔法 Run() require和source 第一步 lexer 第二步 parser 举例: ParseNumberLiteral 第三步 ParseProgram parseReturnStatement parseAssignStatement() parseExpressionStatement() 以上三个函数, 都调用了parseExpression() Eval() ast.Node是不是树? 树分叉一般发生在Program和BlockStatement级别 function Eval的实现 ast 总结 abs abs -- go实现的类shell 字符串 内置通用array结构 内置通用hash结构, 不同于go的map, 这里的key只能是string 函数 内置函数 装饰器 自带的标准库 cli cli举例 repl cli 举例 函数cache abs代码细看 为什么stdin能被迭代? 官方文档中, stdin()是个函数, 可以用来从标准输入读取输入 echo(\"What do you like?\") echo(\"Oh, you like %s!\", stdin()) # This line will block until user enters some text 但stdin又能用于for # Will read all input to the # stdin and output it back for input in stdin { echo(input) } # Or from the REPL: ⧐ for input in stdin { echo((input.int() / 2).str() + \"...try again:\") } 10 5...try again: 5 2.5...try again: ... 那么stdin到底是什么呢? 注意到functions.go里面, 注册内置函数的时候, 有: // stdin() \"stdin\": &object.Builtin{ Next: stdinNextFn, Types: []string{}, Fn: stdinFn, }, 是把内置对象放到map[string]*object.Builtin中, 它是全局的变量evaluator.Fns object.Builtin是个结构体: type Builtin struct { Token token.Token Fn BuiltinFunction Next func() (Object, Object) Types []string Iterable bool } stdin有点特殊, 它除了有Fn函数, 还有Next函数. 在所有内置的方法中, 只有stdin有Next函数. // stdin() -- implemented with 2 functions func stdinFn(tok token.Token, env *object.Environment, args ...object.Object) object.Object { v := scanner.Scan() if !v { return EOF } return &object.String{Token: tok, Value: scanner.Text()} } func stdinNextFn() (object.Object, object.Object) { v := scanner.Scan() if !v { return nil, EOF } defer func() { scannerPosition += 1 }() return &object.Number{Value: float64(scannerPosition)}, &object.String{Token: tok, Value: scanner.Text()} } evalIdentifier() 这个函数中, 先是在env变量里面找, 优先返回变量; 其次返回builtin的对象.比如node.Value是\"stdin\"的时候, 就返回上面注册的stdin的对象 func evalIdentifier( node *ast.Identifier, env *object.Environment, ) object.Object { if val, ok := env.Get(node.Value); ok { return val } if builtin, ok := Fns[node.Value]; ok { return builtin } return newError(node.Token, \"identifier not found: \"+node.Value) } env是管变量对象的 Fns是管内建对象的 回到for in for in结构的执行在Eval()函数里 case *ast.ForInExpression: return evalForInExpression(node, env) case *ast.Identifier: return evalIdentifier(node, env) evalForInExpression()函数中, 先调用Eval()得到iterable, 如果这个iterable是*object.Builtin类型, // for k,v in 1..10 {v} func evalForInExpression( fie *ast.ForInExpression, env *object.Environment, ) object.Object { iterable := Eval(fie.Iterable, env) // If \"k\" and \"v\" were already declared, let's keep // them aside... existingKeyIdentifier, okk := env.Get(fie.Key) existingValueIdentifier, okv := env.Get(fie.Value) // ...so that we can restore them after the for // loop is over //这个defer用的漂亮: 后面的loopIterable()函数会给fie.Key和fie.Value赋新值 //这里用defer保证退出的时候, 恢复原Key Value的值, 或者如果当前env就没有这两个变量, 就删掉后面生成的. //漂亮!!!!! 事情还没做, 就把清理工作写好了! defer func() { if okk { env.Set(fie.Key, existingKeyIdentifier) } else { env.Delete(fie.Key) } if okv { env.Set(fie.Value, existingValueIdentifier) } else { env.Delete(fie.Value) } }() switch i := iterable.(type) { case object.Iterable: defer func() { i.Reset() }() return loopIterable(i.Next, env, fie, 0) case *object.Builtin: //这里正好对应\"stdin\"是有Next函数的 if i.Next == nil { return newError(fie.Token, \"builtin function cannot be used in loop\") } return loopIterable(i.Next, env, fie, 0) default: return newError(fie.Token, \"'%s' is a %s, not an iterable, cannot be used in for loop\", i.Inspect(), i.Type()) } } loopIterable()函数要求迭代函数next每次调用都返回一个k, v对. 这也是为什么next函数的签名必须是func() (object.Object, object.Object). 这个k, v对会当作变量保存在env中. // This function iterates over an iterable // represented by the next() function: everytime // we call it, a new kv pair is popped from the // iterable func loopIterable(next func() (object.Object, object.Object), env *object.Environment, fie *ast.ForInExpression, index int64) object.Object { // Let's get the first kv pair out k, v := next() // Let's keep going until there are no // more kv pairs for k != nil && v != EOF { // set the special k v variables in the // environment env.Set(fie.Key, k) env.Set(fie.Value, v) res := Eval(fie.Block, env) if isError(res) { // If we have an error it could be: // * a break, so we get out of the loop // * a continue, so we go ahead with the next execution // * an actual error, so we wreak havoc switch res.(type) { case *object.BreakError: return NULL case *object.ContinueError: case *object.Error: return res } } // We had a return from within the FOR..IN loop switch res.(type) { case *object.ReturnValue: return res default: // do nothing } // Let's increment our index, and // pull the next kv pair index++ k, v = next() } if k == nil || v == EOF { // If the index we're at is 0, it means the iterable // was empty. If so, let's try to eval its else condition // (eg. for x in [] {...} else {...}) if index == 0 && fie.Alternative != nil { return Eval(fie.Alternative, env) } } return NULL } 现在清楚了 stdin()是个内置函数 而stdin本身是个关键词, 内部表达是个Builtin的对象, 这个对象有Next方法, 能够被迭代 所有内建函数的名字都是Builtin对象. abs应该是允许变量名和内置对象名重名的, 这种情况下, 用户的变量名优先. abs代码 代码组织 object: object是abs对象的抽象, 是个interface. 实现了Type() Inspect() Json()这三个基本函数的都是object对象. abs对象是abs语法里面的string array hash等基础数据类型. environment.go 是对函数上下文的抽象, 类似frame的概念. 其核心是per function的env, 里面用store map[string]Object 表示变量 evaluator: 核心是递归调用的Eval()函数, 里面有对语法的底层执行代码 functions.go里面实现了基础数据类型的内置方法, 比如len()函数; lexer: lexer把原始的string输入, 通过nextToken()和peekToken等函数的向后移动, 来遍历原始输入. parser: parser.go提供主要的词法分析. 有优先级的定义; 随着token的向后移动, 目的是生成对每个词法的ast.Statement, 即ast树上的nodeconst ( _ int = iota LOWEST AND // && or || EQUALS // == or != LESSGREATER // > or ast: ast的Node是个树, 但在表现上是以interface存在的. node扩展成statement和expression token: token.go很简单, 定义了token的常量. 就像C里面的宏定义. 比如 PLUS = \"+\"作用是如果改语法的token关键词, 改这个文件就好了 util: 代码风格 代码里面用了大量的map数据结构, map表意清楚, 用法直观, 深受喜爱. 比如. 解释语言中, 什么东西都是从string解释而来, nubmer也不意外. abs支持numberLiteral(即string化的number)后面带k m b等单位 // NumberAbbreviations is a list of abbreviations that can be used in numbers eg. 1k, 20B var NumberAbbreviations = map[string]float64{ \"k\": 1000, \"m\": 1000000, \"b\": 1000000000, \"t\": 1000000000000, } 在parser解析number的时候, 这个number是个数字的字符串, 比如1 1.1 或者1.1k 下面的代码就是看最后一个字符是否带量级标记, 是的话就查map表得到abbr的数字. abs的数字默认是float64类型. func (p *Parser) ParseNumberLiteral() ast.Expression { lit := &ast.NumberLiteral{Token: p.curToken} var abbr float64 var ok bool number := p.curToken.Literal // Check if the last character of this number is an abbreviation if abbr, ok = token.NumberAbbreviations[strings.ToLower(string(number[len(number)-1]))]; ok { number = p.curToken.Literal[:len(p.curToken.Literal)-1] } value, err := strconv.ParseFloat(number, 64) if err != nil { msg := fmt.Sprintf(\"could not parse %q as number\", number) p.reportError(msg, p.curToken) return nil } //到这里做的工作就是把1k转为1 * 1000 if abbr != 0 { value *= abbr } //这里的value已经是float64类型了 lit.Value = value return lit } 从cli开始 go build就能编译出abs // 这个函数支持交互式, 也支持执行脚本, 通过命令参数区分 func BeginRepl(args []string, version string) { //比如是脚本模式 //这里的set就是e.store[name] = val; e是下面的*object.Environment env.Set(\"ABS_INTERACTIVE\", evaluator.FALSE) code, err := ioutil.ReadFile(args[1]) Run(string(code), false) } 全局env 每个function都有一个env 基本上, 一般都只有一个env. 这是类shell解释器的通常思路: 所有变量, 函数等都是全局的. 一个文件就像一个大函数. //这里的env是指interp包的全局的环境, 包括所有变量等 var env *object.Environment // Environment represent the environment associated // with the execution context of an ABS script: it // holds all variables etc. type Environment struct { store map[string]Object // Arguments this environment was created in. // When we call function(1, 2, 3), a new environment // for the function to execute is created, and 1/2/3 // are recorded as arguments for this environment. // // Later, if we need to access the arguments passed // to the function, we can refer back to them // through env.CurrentArgs. This is how ... is // implemented. CurrentArgs []Object //每个function都有一个env, 这里的outer是其上层env outer *Environment // Used to capture output. This is typically os.Stdout, // but you could capture in any io.Writer of choice Writer io.Writer // Dir represents the directory from which we're executing code. // It starts as the directory from which we invoke the ABS // executable, but changes when we call require(\"...\") as each // require call resets the dir to its own directory, so that // relative imports work. // // If we have script A and B in /tmp, A can require(\"B\") // wihout having to specify its full absolute path // eg. require(\"/tmp/B\") Dir string // Version of the ABS runtime Version string } NewEnvironment 按照shell的惯例, 所有变量都应该是一个\"frame\" 在repl的init里会新建env 在require(\"file.abs\")的时候会新建env \"{}\".json()方法里面会新建env. 特别的, 执行用户自定义的function时(applyFunction()函数)也会新建一个env, 但这个env是NewEnclosedEnvironment()创建的, 它能够访问外面的(outer)env // NewEnclosedEnvironment creates an environment // with another one embedded to it, so that the // new environment has access to identifiers stored // in the outer one. func NewEnclosedEnvironment(outer *Environment, args []Object) *Environment { env := NewEnvironment(outer.Writer, outer.Dir, outer.Version) env.outer = outer env.CurrentArgs = args return env } 基础对象 abs种的基础对象有: const ( NULL_OBJ = \"NULL\" ERROR_OBJ = \"ERROR\" NUMBER_OBJ = \"NUMBER\" BOOLEAN_OBJ = \"BOOLEAN\" STRING_OBJ = \"STRING\" RETURN_VALUE_OBJ = \"RETURN_VALUE\" // ANY_OBJ represents any ABS type ANY_OBJ = \"ANY\" FUNCTION_OBJ = \"FUNCTION\" BUILTIN_OBJ = \"BUILTIN\" ARRAY_OBJ = \"ARRAY\" HASH_OBJ = \"HASH\" ) // 对象是个interface抽象 type Object interface { Type() ObjectType Inspect() string Json() string } 比如对Number的抽象就是 type Number struct { Token token.Token Value float64 } func (n *Number) Type() ObjectType { return NUMBER_OBJ } // If the number we're dealing with is // an integer, print it as such (1.0000 becomes 1). // If it's a float, let's remove as many zeroes // as possible (1.10000 becomes 1.1). func (n *Number) Inspect() string { if n.IsInt() { return fmt.Sprintf(\"%d\", int64(n.Value)) } return strconv.FormatFloat(n.Value, 'f', -1, 64) } func (n *Number) IsInt() bool { return n.Value == float64(int64(n.Value)) } func (n *Number) Json() string { return n.Inspect() } func (n *Number) ZeroValue() float64 { return float64(0) } func (n *Number) Int() int { return int(n.Value) } 大部分基础对象有Value, 小部分没有 //Boolean就有Value, 很显然, 是个bool type Boolean struct { Token token.Token Value bool } //Function没有Value, 但也实现了Object的方法, 也是Object对象 type Function struct { Token token.Token Name string Parameters []*ast.Parameter Body *ast.BlockStatement Env *Environment Node *ast.FunctionLiteral } string基础对象 string有点特别. 为了支持直接调用shell命令, 字符串还要实现Ok和Done方法 string的典型用法: // cmd = `ls -la` // type(cmd) // STRING // cmd.ok // TRUE // // cmd = `curlzzzzz` // type(cmd) // STRING // cmd.ok // FALSE // // cmd = `sleep 10 &` // type(cmd) // STRING // cmd.done // FALSE // cmd.wait() // ... // cmd.done // TRUE 所以string有更多的属性 type String struct { Token token.Token Value string Ok *Boolean // A special property to check whether a command exited correctly Cmd *exec.Cmd // A special property to access the underlying command Stdout *bytes.Buffer Stderr *bytes.Buffer Done *Boolean mux *sync.Mutex } // string的几个函数写的漂亮 func (s *String) Type() ObjectType { return STRING_OBJ } func (s *String) Inspect() string { return s.Value } func (s *String) Json() string { return `\"` + strings.ReplaceAll(s.Inspect(), `\"`, `\\\"`) + `\"` } func (s *String) ZeroValue() string { return \"\" } func (s *String) HashKey() HashKey { return HashKey{Type: s.Type(), Value: s.Value} } // Function that ensure a mutex // instance is created on the // string func (s *String) mustHaveMutex() { if s.mux == nil { s.mux = &sync.Mutex{} } } // To be called when the command // is done. Releases the internal // mutex. func (s *String) SetDone() { s.mustHaveMutex() s.mux.Unlock() } // To be called when the command // is starting in background, so // that anyone accessing it will // be blocked. func (s *String) SetRunning() { s.mustHaveMutex() s.mux.Lock() } // To be called when we want to // wait on the background command // to be done. func (s *String) Wait() { s.mustHaveMutex() s.mux.Lock() s.mux.Unlock() } // To be called when we want to // kill the background command func (s *String) Kill() error { err := s.Cmd.Process.Kill() // The command value includes output and possible error // We might want to change this output := s.Stdout.String() outputErr := s.Stderr.String() s.Value = strings.TrimSpace(output) + strings.TrimSpace(outputErr) if err != nil { return err } s.Done = TRUE return nil } // Sets the result of the underlying command // on the string. // 3 things are set: // - the string itself (output of the command) // - str.ok // - str.done func (s *String) SetCmdResult(Ok *Boolean) { s.Ok = Ok var output string if Ok.Value { output = s.Stdout.String() } else { output = s.Stderr.String() } // trim space at both ends of out.String(); works in both linux and windows s.Value = strings.TrimSpace(output) s.Done = TRUE } Array abs的Array是个万能容器 type Array struct { Token token.Token Elements []Object // ... is aliased to an array of arguments. // // Since this is a special case of an array, // we need a flag to make sure we know when // to unpack them, else if we do func(...), // func would receive only one array argument // as opposd to the unpacked arguments. IsCurrentArgs bool position int } 内置函数 evaluator/evaluator.go中, 有内置的函数. 内置函数放在一个全局的map表中 var ( NULL = object.NULL EOF = object.EOF TRUE = object.TRUE FALSE = object.FALSE Fns map[string]*object.Builtin ) func init() { Fns = getFns() } 这个map里面是: 部分例子, 很多都是针对string的. map[string]*object.Builtin{ // len(var:\"hello\") \"len\": &object.Builtin{ Types: []string{object.STRING_OBJ, object.ARRAY_OBJ}, Fn: lenFn, }, // cd() or cd(path) \"cd\": &object.Builtin{ Types: []string{}, Fn: cdFn, }, // echo(arg:\"hello\") \"echo\": &object.Builtin{ Types: []string{}, Fn: echoFn, }, // int(string:\"123\") // int(number:\"123\") \"int\": &object.Builtin{ Types: []string{object.STRING_OBJ, object.NUMBER_OBJ}, Fn: intFn, }, // round(string:\"123.1\") // round(number:\"123.1\", 2) \"round\": &object.Builtin{ Types: []string{object.STRING_OBJ, object.NUMBER_OBJ}, Fn: roundFn, }, // number(string:\"1.23456\") \"number\": &object.Builtin{ Types: []string{object.STRING_OBJ, object.NUMBER_OBJ}, Fn: numberFn, }, // stdin() \"stdin\": &object.Builtin{ Next: stdinNextFn, Types: []string{}, Fn: stdinFn, }, // env(variable:\"PWD\") or env(string:\"KEY\", string:\"VAL\") \"env\": &object.Builtin{ Types: []string{}, Fn: envFn, }, // type(variable:\"hello\") \"type\": &object.Builtin{ Types: []string{}, Fn: typeFn, }, // fn.call(args_array) \"call\": &object.Builtin{ Types: []string{object.FUNCTION_OBJ, object.BUILTIN_OBJ}, Fn: callFn, }, // \"{}\".json() // Converts a valid JSON document to an ABS hash. \"json\": &object.Builtin{ Types: []string{object.STRING_OBJ}, Fn: jsonFn, }, // \"a %s\".fmt(b) \"fmt\": &object.Builtin{ Types: []string{object.STRING_OBJ}, Fn: fmtFn, }, // sum(array:[1, 2, 3]) \"sum\": &object.Builtin{ Types: []string{object.ARRAY_OBJ}, Fn: sumFn, }, // sort(array:[1, 2, 3]) \"sort\": &object.Builtin{ Types: []string{object.ARRAY_OBJ}, Fn: sortFn, }, // map(array:[1, 2, 3], function:f(x) { x + 1 }) \"map\": &object.Builtin{ Types: []string{object.ARRAY_OBJ}, Fn: mapFn, }, // every(array:[1, 2, 3], function:f(x) { x == 2 }) \"every\": &object.Builtin{ Types: []string{object.ARRAY_OBJ}, Fn: everyFn, }, // find(array:[1, 2, 3], function:f(x) { x == 2 }) \"find\": &object.Builtin{ Types: []string{object.ARRAY_OBJ}, Fn: findFn, }, // repeat(\"abc\", 3) \"repeat\": &object.Builtin{ Types: []string{object.STRING_OBJ}, Fn: repeatFn, }, // sleep(3000) \"sleep\": &object.Builtin{ Types: []string{object.NUMBER_OBJ}, Fn: sleepFn, }, // source(\"file.abs\") -- soure a file, with access to the global environment \"source\": &object.Builtin{ Types: []string{object.STRING_OBJ}, Fn: sourceFn, }, // require(\"file.abs\") -- require a file without giving it access to the global environment \"require\": &object.Builtin{ Types: []string{object.STRING_OBJ}, Fn: requireFn, }, // exec(command) -- execute command with interactive stdIO \"exec\": &object.Builtin{ Types: []string{object.STRING_OBJ}, Fn: execFn, }, // eval(code) -- evaluates code in the context of the current ABS environment \"eval\": &object.Builtin{ Types: []string{object.STRING_OBJ}, Fn: evalFn, }, } type Builtin struct { Token token.Token Fn BuiltinFunction Next func() (Object, Object) Types []string //这个函数支持的对象类型, 可以支持多个类型, 这里是个切片 Iterable bool } 内置函数的实现 比如, 内置对象是object.Builtin类型 type Builtin struct { Token token.Token Fn BuiltinFunction Next func() (Object, Object) Types []string Iterable bool } //其中BuiltinFunction接受token, env, 其他变长的Object类型参数. 返回Object type BuiltinFunction func(tok token.Token, env *Environment, args ...Object) Object 比如len(\"hello\")的实现就是 // len(var:\"hello\") \"len\": &object.Builtin{ Types: []string{object.STRING_OBJ, object.ARRAY_OBJ}, Fn: lenFn, }, 这个Fn的实现是 // len(var:\"hello\") func lenFn(tok token.Token, env *object.Environment, args ...object.Object) object.Object { //args的个数应该是1 //如果有多个参数, 第一个参数的类型要在[][]string[0]中, 第二个参数要在[][]string[1]中 //这里只有一个arg, 它的类型要在{object.STRING_OBJ, object.ARRAY_OB}中 err := validateArgs(tok, \"len\", args, 1, [][]string{{object.STRING_OBJ, object.ARRAY_OBJ}}) if err != nil { return err } switch arg := args[0].(type) { case *object.Array: return &object.Number{Token: tok, Value: float64(len(arg.Elements))} case *object.String: return &object.Number{Token: tok, Value: float64(len(arg.Value))} default: return newError(tok, \"argument to `len` not supported, got %s\", args[0].Type()) } } 内置函数如何被调用 -- Eval()的魔法 内置函数的基本调用形式有2种 #函数式 len(\"nihaoma\") 7 #对象方法方式 \"nihaoma\".len() 7 #连续调用 \"nihaoma\".len().type() NUMBER #连续的连续 \"nihaoma\".len().type().type() STRING 调用到Fn的路径是, evaluator/evaluator.go中: func applyMethod(tok token.Token, o object.Object, me *ast.MethodExpression, env *object.Environment, args []object.Object) object.Object { method := me.Method.String() //hash类型可以有用户自定义的方法 hash, isHash := o.(*object.Hash) //hash类型时,调用用户态自定义的函数 // If so, run the user-defined function if isHash && hash.GetKeyType(method) == object.FUNCTION_OBJ { pair, _ := hash.GetPair(method) //自定义的函数就不传obj本身了. 是否可以改进成也传入对象本身? 这样这个函数就能访问对象的属性了. return applyFunction(tok, pair.Value.(*object.Function), env, args) } // Now, check if there is a builtin function with the given name f, ok := Fns[method] //检查参数是否类型合法 if util.Contains(f.Types, string(o.Type())) //真正的黑魔法, 调用这个函数 //第一个参数是对象本身, 其他的参数依次append到后面. 这里写的漂亮 args = append([]object.Object{o}, args...) return f.Fn(tok, env, args...) } func applyFunction(tok token.Token, fn object.Object, env *object.Environment, args []object.Object) object.Object { switch fn := fn.(type) { case *object.Function: extendedEnv, err := extendFunctionEnv(fn, args) if err != nil { return err } evaluated := Eval(fn.Body, extendedEnv) return unwrapReturnValue(evaluated) case *object.Builtin: return fn.Fn(tok, env, args...) default: return newError(tok, \"not a function: %s\", fn.Type()) } } 上面的applyMethod被Eval()调用 evaluator.go里面的Eval是个核心函数, 它根据抽象语法树(ast)的node类型, 执行相应的方法. 实际上, Eval()不仅能执行内置的函数, 它能执行任意的\"文本\"代码. 详见下文 Run() cli刚开始的时候, 调用Run()来解释执行代码 // 这个函数支持交互式, 也支持执行脚本, 通过命令参数区分 func BeginRepl(args []string, version string) { //比如是脚本模式 //这里的set就是e.store[name] = val; e是下面的*object.Environment env.Set(\"ABS_INTERACTIVE\", evaluator.FALSE) code, err := ioutil.ReadFile(args[1]) Run(string(code), false) } 其中, Run()函数使用了lexer parser evaluator等包的功能, 负责解释执行string类型的code. func Run(code string, interactive bool) { //词法器 lex := lexer.New(code) //解析器 p := parser.New(lex) //现在是program了 program := p.ParseProgram() evaluated := evaluator.BeginEval(program, env, lex) } // BeginEval (program, env, lexer) object.Object // REPL and testing modules call this function to init the global lexer pointer for error location // NB. Eval(node, env) is recursive func BeginEval(program ast.Node, env *object.Environment, lexer *lexer.Lexer) object.Object { //这里竟然是属于evaluator的全局变量: 用于出错时定位错误在哪一行 // global lexer lex = lexer // run the evaluator //这里的Eval和上面的就对上了 return Eval(program, env) } Eval program实际上就是对program里的每个statement, 递归调用Eval(), 如果返回ReturnValue, 说明可以提前返回; 或者返回Error也要提前返回 func evalProgram(program *ast.Program, env *object.Environment) object.Object { var result object.Object for _, statement := range program.Statements { result = Eval(statement, env) switch result := result.(type) { case *object.ReturnValue: return result.Value case *object.Error: return result } } return result } require和source evaluator/functions.go里面的doSource()函数和Run的流程类似, 它读取一个source文件, 创建lexer, parser, 然后Eval这个program. 第一步 lexer 基本上, lexer的原理是按每个单个字符读入分析. lexer关注字符, 所以每个字符都是rune类型. lexer关注字符的位置; 基本上是把包含code的string, 转化为input []rune, 和其他一些辅助属性. 用于后续解析. type Lexer struct { position int // current position in input (points to current char) readPosition int // current reading position in input (after current char) ch rune // current rune under examination input []rune // map of input line boundaries used by linePosition() for error location lineMap [][2]int // array of [begin, end] pairs: [[0,12], [13,22], [23,33] ... ] } Run()的第一步就是lex := lexer.New(code) func New(in string) *Lexer { l := &Lexer{input: []rune(in)} // map the input line boundaries for CurrentLine() l.buildLineMap() // read the first char l.readChar() return l } 第二步 parser parser是从原始文法, 到ast树的关键.ast token等基础概念可以interface抽象, 而parser是要实际干活的了, 它是个struct.Parser持有一个Lexer的实例, 当前token, 下个token; 还有index表达, 属性表达.另外还有基础词法parse的函数集. type Parser struct { l *lexer.Lexer errors []string curToken token.Token peekToken token.Token // support assignment to index expressions: a[0] = 1, h[\"a\"] = 1 prevIndexExpression *ast.IndexExpression // support assignment to hash property h.a = 1 prevPropertyExpression *ast.PropertyExpression //如果都是预定义好的解析函数, 直接用全局变量函数不是更好? prefixParseFns map[token.TokenType]prefixParseFn infixParseFns map[token.TokenType]infixParseFn } 两个register函数注册解析函数到parser. func (p *Parser) registerPrefix(tokenType token.TokenType, fn prefixParseFn) { p.prefixParseFns[tokenType] = fn } func (p *Parser) registerInfix(tokenType token.TokenType, fn infixParseFn) { p.infixParseFns[tokenType] = fn } New()方法会注册基础关键词的解析函数 func New(l *lexer.Lexer) *Parser { p := &Parser{ l: l, errors: []string{}, } //每个关键词, 都对应一个预定义的解析函数. 比如token.TRUE对一个ParseBoolean. p.prefixParseFns = make(map[token.TokenType]prefixParseFn) p.registerPrefix(token.IDENT, p.parseIdentifier) //前文中, 分析过ParseNumberLiteral, 是把string的\"1k\"转为float64类型的值 p.registerPrefix(token.NUMBER, p.ParseNumberLiteral) p.registerPrefix(token.STRING, p.ParseStringLiteral) p.registerPrefix(token.NULL, p.ParseNullLiteral) p.registerPrefix(token.BANG, p.parsePrefixExpression) p.registerPrefix(token.MINUS, p.parsePrefixExpression) p.registerPrefix(token.TILDE, p.parsePrefixExpression) p.registerPrefix(token.TRUE, p.ParseBoolean) p.registerPrefix(token.FALSE, p.ParseBoolean) ... //infix又是什么呢? p.infixParseFns = make(map[token.TokenType]infixParseFn) p.registerInfix(token.QUESTION, p.parseQuestionExpression) p.registerInfix(token.DOT, p.parseDottedExpression) p.registerInfix(token.PLUS, p.parseInfixExpression) p.registerInfix(token.MINUS, p.parseInfixExpression) p.registerInfix(token.SLASH, p.parseInfixExpression) p.registerInfix(token.EXPONENT, p.parseInfixExpression) p.registerInfix(token.MODULO, p.parseInfixExpression) ... // 最后读两个token出来, 让事情转起来. // Read two tokens, so curToken and peekToken are both set p.nextToken() p.nextToken() return p } prefix和infix对应的是关键词前 关键词中需要的函数; 区别是infix需要额外的入参. 可能就是前序解析出来的ast.Expression type ( prefixParseFn func() ast.Expression infixParseFn func(ast.Expression) ast.Expression ) 举例: ParseNumberLiteral 在parser解析number的时候, 这个number是个数字的字符串, 比如1 1.1 或者1.1k下面的代码就是看最后一个字符是否带量级标记, 是的话就查map表得到abbr的数字.abs的数字默认是float64类型. func (p *Parser) ParseNumberLiteral() ast.Expression { lit := &ast.NumberLiteral{Token: p.curToken} var abbr float64 var ok bool number := p.curToken.Literal // Check if the last character of this number is an abbreviation if abbr, ok = token.NumberAbbreviations[strings.ToLower(string(number[len(number)-1]))]; ok { number = p.curToken.Literal[:len(p.curToken.Literal)-1] } value, err := strconv.ParseFloat(number, 64) if err != nil { msg := fmt.Sprintf(\"could not parse %q as number\", number) p.reportError(msg, p.curToken) return nil } //到这里做的工作就是把1k转为1 * 1000 if abbr != 0 { value *= abbr } //这里的value已经是float64类型了 lit.Value = value return lit } 第三步 ParseProgram parser现在已经有lexer实例, 从而有了从原始string读token的能力; 也注册了基本词法的解析器.现在perser可以把token转换为ast.Expression后面会看到, p.nextToken()会随着每个词法的解析, 根据情况来向后移动. program := p.ParseProgram() 这个ParseProgram()函数也写的非常漂亮 func (p *Parser) ParseProgram() *ast.Program { program := &ast.Program{} program.Statements = []ast.Statement{} //从头到尾逐个token来解析 for !p.curTokenIs(token.EOF) { //解析完成后放到program.Statements中 stmt := p.parseStatement() if stmt != nil { program.Statements = append(program.Statements, stmt) } //下一个token, 实际是调用底层的lexer的 p.l.NextToken() p.nextToken() } return program } parseStatement()先检查是否是Return类型的, 然后尝试先按Assign类型去解析, 最后用Expression类型去解析. func (p *Parser) parseStatement() ast.Statement { if p.curToken.Type == token.RETURN { return p.parseReturnStatement() } statement := p.parseAssignStatement() if statement != nil { return statement } return p.parseExpressionStatement() } parseReturnStatement 题外: 这个peekToken的设计真是绝了. 让我想起了awk的getline()内置函数. peek这个词用的真是绝了. // return x func (p *Parser) parseReturnStatement() *ast.ReturnStatement { //这里的curToken就是\"return\" stmt := &ast.ReturnStatement{Token: p.curToken} returnToken := p.curToken // return; if p.peekTokenIs(token.SEMICOLON) { stmt.ReturnValue = &ast.NullLiteral{Token: p.curToken} } else if p.peekTokenIs(token.RBRACE) || p.peekTokenIs(token.EOF) { // return stmt.ReturnValue = &ast.NullLiteral{Token: returnToken} } else { // return xyz p.nextToken() //LOWEST的意思是当前的优先级最低. parseExpression里面的token现在还不知道, 每个token都有它的优先级的. stmt.ReturnValue = p.parseExpression(LOWEST) } if p.peekTokenIs(token.SEMICOLON) { p.nextToken() } return stmt } parseAssignStatement() 这个函数处理各种赋值语句. // assign to variable: x = y // destructuring assignment: x, y = [z, zz] // assign to index expressions: a[0] = 1, h[\"a\"] = 1 // assign to hash property expressions: h.a = 1 func (p *Parser) parseAssignStatement() ast.Statement { stmt := &ast.AssignStatement{} // Is this a regular x = y assignment? if p.peekTokenIs(token.COMMA) { lexerPosition := p.l.CurrentPosition() // Let's figure out if we are destructuring x, y = [z, zz] if !p.curTokenIs(token.IDENT) { return nil } stmt.Names = p.parseDestructuringIdentifiers() if !p.peekTokenIs(token.ASSIGN) { p.Rewind(lexerPosition) return nil } } else if p.curTokenIs(token.IDENT) { stmt.Name = &ast.Identifier{Token: p.curToken, Value: p.curToken.Literal} } else if p.curTokenIs(token.ASSIGN) { stmt.Token = p.curToken if p.prevIndexExpression != nil { // support assignment to indexed expressions: a[0] = 1, h[\"a\"] = 1 stmt.Index = p.prevIndexExpression p.nextToken() stmt.Value = p.parseExpression(LOWEST) // consume the IndexExpression p.prevIndexExpression = nil if p.peekTokenIs(token.SEMICOLON) { p.nextToken() } return stmt } if p.prevPropertyExpression != nil { // support assignment to hash properties: h.a = 1 stmt.Property = p.prevPropertyExpression p.nextToken() stmt.Value = p.parseExpression(LOWEST) // consume the PropertyExpression p.prevPropertyExpression = nil if p.peekTokenIs(token.SEMICOLON) { p.nextToken() } return stmt } } if !p.peekTokenIs(token.ASSIGN) { return nil } p.nextToken() stmt.Token = p.curToken p.nextToken() stmt.Value = p.parseExpression(LOWEST) if p.peekTokenIs(token.SEMICOLON) { p.nextToken() } return stmt } parseExpressionStatement() // (x * y) + z func (p *Parser) parseExpressionStatement() *ast.ExpressionStatement { stmt := &ast.ExpressionStatement{Token: p.curToken} stmt.Expression = p.parseExpression(LOWEST) if p.peekTokenIs(token.SEMICOLON) { p.nextToken() } return stmt } 以上三个函数, 都调用了parseExpression() 这个函数写的真好. 它调用了New parser时注册的各个词法的prefixParseFns和infixParseFns, 这是处理语法中最复杂的地方. 别看这个函数不长, 但却是prefix和infix等基础函数的总调用入口. func (p *Parser) parseExpression(precedence int) ast.Expression { prefix := p.prefixParseFns[p.curToken.Type] if prefix == nil { p.noPrefixParseFnError(p.curToken) return nil } leftExp := prefix() for !p.peekTokenIs(token.SEMICOLON) && precedence Eval() parser阶段, 把lexer的token按照字面语法移动, 得到相应的ast. Expression, 这就是个ast.Node节点.node是个抽象, 实现了下面两个方法的实体类型, 都是node. 这个node就是ast的节点.evaluator.go里面的Eval是个核心函数, 它根据抽象语法树(ast)的node类型, 执行相应的方法. 实际上, Eval()不仅能执行内置的函数, 它能执行任意的\"文本\"代码. 经过前面的三步, 每个词法都对应了一个ast.Node树, ast.Node是个抽象, 一般是Statement或Expression. 从表面上看, node只是个节点, 还是interface, 好像不是个树. 其定义如下: type Node interface { TokenLiteral() string String() string } 特别的, Statement和Expression是Node的扩展抽象. // All statement nodes implement this type Statement interface { Node statementNode() } // All expression nodes implement this type Expression interface { Node expressionNode() } 其中, Expression实现了statementNode()方法, 它本身是个Statement type ExpressionStatement struct { Token token.Token // the first token of the expression Expression Expression } func (es *ExpressionStatement) statementNode() {} func (es *ExpressionStatement) TokenLiteral() string { return es.Token.Literal } func (es *ExpressionStatement) String() string { if es.Expression != nil { return es.Expression.String() } return \"\" } ast.Node是不是树? 表面上看, ast.Node跟树完全扯不上关系 -- 它只是个接口, 而且只有两个返回string的方法. 但其具体实现里, 都是有树的. 比如: BlockStatement就包括了子节点Statements type BlockStatement struct { Token token.Token // the { token Statements []Statement } 一个Program也是Statements的集合, 属性里少了token字段, 但比BlockStatement概念要大, 是整个代码段的入口 type Program struct { Statements []Statement } ExpressionStatement很常用: type ExpressionStatement struct { Token token.Token // the first token of the expression Expression Expression } AssignStatement是个相对复杂的结构: type AssignStatement struct { Token token.Token // the token.ASSIGN token Name *Identifier Names []Expression Index *IndexExpression // support assignment to indexed expressions: a[0] = 1, h[\"a\"] = 1 Property *PropertyExpression // support assignment to hash properties: h.a = 1 Value Expression } number是最简单的, 其值为翻译后的float64 type NumberLiteral struct { Token token.Token Value float64 } Boolean也是类似的 type Boolean struct { Token token.Token Value bool } prefix是两个操作符, infix是三个(比如 x = a + b 中, infix指+) type PrefixExpression struct { Token token.Token // The prefix token, e.g. ! Operator string Right Expression } type InfixExpression struct { Token token.Token // The operator token, e.g. + Left Expression Operator string Right Expression } 我最喜欢的for in组合 type ForInExpression struct { Token token.Token // The 'for' token Block *BlockStatement // The block executed inside the for loop Iterable Expression // An expression that should return an iterable ([1, 2, 3] or x in 1..10) Key string Value string Alternative *BlockStatement } 树分叉一般发生在Program和BlockStatement级别 从上文可以看出, BlockStatement包括Statements集合, 一对{...}是个树, 里面的每个块也是个树; 块里面的Statements是顺序的. 这个是最自然的理解 function function地位特别, 但实现上也似乎没有专门对待. 有参数列表, 有函数体(是个 * BlockStatement类型), 就是个函数. type FunctionLiteral struct { Token token.Token // The 'fn' token Name string // identifier for this function Parameters []*Parameter Body *BlockStatement } Eval的实现 Eval()函数我一行都没舍得删, 完全搬到这里来: 下面是已经实现了node抽象的实体类型 *ast.Program *ast.BlockStatement *ast.ExpressionStatement *ast.ReturnStatement *ast.AssignStatement *ast.NumberLiteral *ast.NullLiteral *ast.CurrentArgsLiteral *ast.StringLiteral *ast.Boolean *ast.PrefixExpression *ast.InfixExpression *ast.CompoundAssignment *ast.IfExpression *ast.WhileExpression *ast.ForExpression *ast.ForInExpression *ast.Identifier *ast.FunctionLiteral *ast.Decorator *ast.CallExpression *ast.MethodExpression *ast.PropertyExpression *ast.ArrayLiteral *ast.IndexExpression *ast.HashLiteral *ast.CommandExpression *ast.BreakStatement *ast.ContinueStatement Eval()的作用是根据输入的ast.Node和env, recursive调用相应的Eval()方法, 最后得到abs的对象表达: object.Object func Eval(node ast.Node, env *object.Environment) object.Object { //node重新赋值为实现了node的实体类型 switch node := node.(type) { // Statements case *ast.Program: return evalProgram(node, env) case *ast.BlockStatement: return evalBlockStatement(node, env) //很多地方都是再次调用Eval //首先, *ast.ExpressionStatement实现了node的接口, 是node //其实体包括一个token和一个Expression //但这里直接忽略其本体实现, 直接使用了其Expression域 //type ExpressionStatement struct { // Token token.Token // the first token of the expression // Expression Expression //} case *ast.ExpressionStatement: //如上文交代的, Expression是node的扩展抽象. 很多基础expression都实现了这个抽象, 比如*ForInExpression或者*NumberLiteral //再次进Eval获取实体类型, 就会跑到其实体类型对应的case里面去 return Eval(node.Expression, env) case *ast.ReturnStatement: val := Eval(node.ReturnValue, env) if isError(val) { return val } return &object.ReturnValue{Value: val} case *ast.AssignStatement: err := evalAssignment(node, env) if isError(err) { return err } return NULL // Expressions case *ast.NumberLiteral: return &object.Number{Token: node.Token, Value: node.Value} case *ast.NullLiteral: return NULL case *ast.CurrentArgsLiteral: return &object.Array{Token: node.Token, Elements: env.CurrentArgs, IsCurrentArgs: true} case *ast.StringLiteral: return &object.String{Token: node.Token, Value: util.InterpolateStringVars(node.Value, env)} case *ast.Boolean: return nativeBoolToBooleanObject(node.Value) case *ast.PrefixExpression: right := Eval(node.Right, env) if isError(right) { return right } return evalPrefixExpression(node.Token, node.Operator, right) case *ast.InfixExpression: return evalInfixExpression(node.Token, node.Operator, node.Left, node.Right, env) case *ast.CompoundAssignment: return evalCompoundAssignment(node, env) case *ast.IfExpression: return evalIfExpression(node, env) case *ast.WhileExpression: return evalWhileExpression(node, env) case *ast.ForExpression: return evalForExpression(node, env) case *ast.ForInExpression: return evalForInExpression(node, env) case *ast.Identifier: return evalIdentifier(node, env) case *ast.FunctionLiteral: params := node.Parameters body := node.Body name := node.Name fn := &object.Function{Token: node.Token, Parameters: params, Env: env, Body: body, Name: name, Node: node} if name != \"\" { env.Set(name, fn) } return fn case *ast.Decorator: return evalDecorator(node, env) case *ast.CallExpression: //对应函数调用 function := Eval(node.Function, env) if isError(function) { return function } args := evalExpressions(node.Arguments, env) // Did we pass arguments as ...? // If so, replace arguments with the // environment's CurrentArgs. // If other arguments were passed afterwards // (eg. func(..., x, y)) we also add those. if len(args) > 0 { firstArg, ok := args[0].(*object.Array) if ok && firstArg.IsCurrentArgs { newArgs := env.CurrentArgs args = append(newArgs, args[1:]...) } } if len(args) == 1 && isError(args[0]) { return args[0] } return applyFunction(node.Token, function, env, args) case *ast.MethodExpression: o := Eval(node.Object, env) if isError(o) { return o } args := evalExpressions(node.Arguments, env) if len(args) == 1 && isError(args[0]) { return args[0] } return applyMethod(node.Token, o, node, env, args) case *ast.PropertyExpression: return evalPropertyExpression(node, env) case *ast.ArrayLiteral: elements := evalExpressions(node.Elements, env) if len(elements) == 1 && isError(elements[0]) { return elements[0] } return &object.Array{Token: node.Token, Elements: elements} case *ast.IndexExpression: return evalIndexExpression(node, env) case *ast.HashLiteral: return evalHashLiteral(node, env) case *ast.CommandExpression: //这个函数也很经典! //执行shell命令. 系统中必须有bash, 所有命令都是bash -c运行的 //对输入的string类型的命令, 利用正则技术, 找出里面的所有变量, 在env里查找到变量值后, 替换string中的$VAR和${VAR}; 注意这里是把abs的变量展开为字符替换到cmd string中, 由bash去解析 //如果命令string cmd以 &结尾, 说明是要后台执行; 注意这里, abs会自己处理后台的操作, 如果命令后台执行, 需要用字符串的Wait()方法来等待. //利用标准库的exec包来执行cmd, 把stdout stderror都定向到内部bytes.Buffer中 return evalCommandExpression(node.Token, node.Value, env) // break and continue are treated just like errors: they will stop // the execution of the current code. Within FOR blocks, though, they // are caught and handled accordingly (see evalForExpression). case *ast.BreakStatement: return newBreakError(node.Token, \"break called outside of a loop\") // break and continue are treated just like errors: they will stop // the execution of the current code. Within FOR blocks, though, they // are caught and handled accordingly (see evalForExpression). case *ast.ContinueStatement: return newContinueError(node.Token, \"continue called outside of a loop\") } return NULL } 以上可以看到, Eval()函数根据node的类型断言, 调用相应的实体类型的实现函数; 而在实体类型中, 很多也是包含了node类型的struct, 再次调用Eval()来获取其子实体类型 type typeA struct { others Other //fieldX 是个interface fieldX node } type typeB struct { others Other //fieldY 是个interface fieldY node } func Eval(node ast.Node, env *object.Environment) object.Object { switch node := node.(type) { // typeA是struct类型, 实体类型 case typeA: //fieldX是个node类型的interface Eval(node.fieldX, env) case typeB: Eval(node.fieldY, env) } } 所以, 一次递归的Eval()调用, 就对应了实体strcut{}中嵌套的一个node interface变量. 在设计node的时候, 实体类型里嵌套包括node的interface, 再递归调用Eval(), 这是个很精妙的框架设计 ast ast对语法树的抽象是: //基本的node只有两个方法, 都返回string // The base Node interface type Node interface { TokenLiteral() string String() string } // All statement nodes implement this type Statement interface { Node statementNode() } // All expression nodes implement this type Expression interface { Node expressionNode() } //program就是statement的集合 // Represents the whole program // as a bunch of statements type Program struct { Statements []Statement } 比如for的表达是 type ForExpression struct { Token token.Token // The 'for' token Identifier string // \"x\" Starter Statement // x = 0 Closer Statement // x++ Condition Expression // x 总结 相对yaegi, abs更倾向于使用interface来抽象, 看起来更简洁. 比如对node的抽象, abs就是个interface, 只规定了两个实现函数; 而yaegi则使用了传统的struct类型的node, 缺点是必须在这个struct里面, 定义一组\"大而全\"的字段, 以适应所有情况. 在某些情况下, 某些字段是无意义的. abs 看了yaegi和govaluate的实现, 先简单对比一下: yaegi: 基于go标准库的词法解析树, 最底层利用reflect.Value等反射方法来执行 只依赖标准库, 但受限于此, 只能解释go的语法, 支持所有go语言的特性. 完成度比较高, 能解释执行go代码. 利用反射和go的runtime交互, 底层执行靠runtime的反射来实现. 比如调用fmt.Println实际上是调用的是已经编译好的runtime内部的符号. govaluate: 自造词法解析, 构建执行流, 底层直接调用基本函数. 所谓的基本函数就是left.(float64) * right.(float64)的运算 只支持简单的\"表达式\"计算, 侧重于得到表达式的值. abs是个自定义语法的完整解释器实现. 下面我们来一探究竟. abs -- go实现的类shell 看起来非常不错!!!!!库地址: https://github.com/abs-lang/abs主页: https://www.abs-lang.org 作者简介, 大神 解释器基于Thorsten Ball的Writing An Interpreter In Go, 另外还有一本Writing A Compiler In Go 还有个类似的用go写的解释器:Magpie Programming Language 是国人写的 reddit讨论 特点如下: 解释执行 兼备shell的灵活性和通用语言的准确性 first look # Simple program that fetches your IP and sums it up res = `curl -s 'https://api.ipify.org?format=json'` if !res.ok { echo(\"An error occurred: %s\", res) exit(1) } ip = res.json().ip total = ip.split(\".\").map(int).sum() if total > 100 { echo(\"The sum of [$ip] is a large number, $total.\") } 字符串 字符串是用引号括起来的: \"hello world\" 'hello world' 用转义可以转义引号: \"I said: \\\"hello world\\\"\" 字符串支持切片操作, 也支持python一样的-index; 也支持+操作 \"hello world\"[1] # e \"string\"[-2] # \"n\" \"string\"[0:3] // \"str\" \"string\"[0:3] // \"str\" \"string\"[1:] // \"tring\" \"string\"[0:-1] // \"strin\" \"hello\" + \" \" + \"world\" # \"hello world\" 支持in操作 \"str\" in \"string\" # true \"xyz\" in \"string\" # false 在字符串中引用变量要加$前缀, 比如 file = \"/etc/hosts\" x = \"File name is: $file\" echo(x) # \"File name is: /etc/hosts\" 或者在命令里面引用变量也要加$ var = \"/etc\" out = `ls -la $var` echo(out) 内置通用array结构 array是个万能容器 # array [1, 2, \"hello\", [1, f(x){ x + 1 }]] # array提供很多fancy的内置函数 [0, 1, 2].every(f(x){x == 0}) # false [1, 2, 3].diff([3, 1]) # [2] [[1, 2], 3, [4]].flatten() # [1, 2, 3, 4] [[[1, 2], [[[[3]]]], [4]]].flatten_deep() # [1, 2, 3, 4] [1, 2, 3].join(\"_\") # \"1_2_3\" [1, 2, 3].intersect([3, 1]) # [1, 3] (1..2).keys() # [0, 1] [1, 2].len() # 2 #每个元素调用函数 [0, 1, 2].map(f(x){x+1}) # [1, 2, 3] [0, 5, -10, 100].max() # 100 f odd(n) { return !!(n % 2) } [0, 1, 2, 3, 4, 5].partition(odd) # [[0, 2, 4], [1, 3, 5]] # pop a = [1, 2, 3] a.pop() # 3 a # [1, 2] # shift a = [1, 2, 3] a.shift() # 1 a # [2, 3] [\"b\", \"a\", \"c\"].sort() # [\"a\", \"b\", \"c\"] [1, 1, 1, 2].unique() # [1, 2] 内置通用hash结构, 不同于go的map, 这里的key只能是string h = {\"a\": 1, \"b\": 2, \"c\": 3} h # {a: 1, b: 2, c: 3} # index assignment h[\"a\"] = 99 h # {a: 99, b: 2, c: 3} # property assignment h.a # 99 h.a = 88 h # {a: 88, b: 2, c: 3} # compound operator assignment to property h.a += 1 h.a # 89 h # {a: 88, b: 2, c: 3} # create new keys via index or property h[\"x\"] = 10 h.y = 20 h # {a: 88, b: 2, c: 3, x: 10, y: 20} h = {\"a\": 1, \"b\": 2, \"c\": 3} h # {a: 1, b: 2, c: 3} # extending a hash by += compound operator h += {\"c\": 33, \"d\": 4, \"e\": 5} h # {a: 1, b: 2, c: 33, d: 4, e: 5} # 也有很多fancy的内置函数 h = {\"a\": 1, \"b\": 2, \"c\": 3} h.items() # [[a, 1], [b, 2], [c, 3]] items(h) # [[a, 1], [b, 2], [c, 3]] h = {\"a\": 1, \"b\": 2, \"c\": 3} h.keys() # [a, b, c] keys(h) # [a, b, c] h = {\"a\": 1, \"b\": 2, \"c\": {\"x\": 10, \"y\":20}} h.pop(\"a\") # {a: 1} h # {b: 2, c: {x: 10, y: 20}} h = {\"a\": 1, \"b\": 2, \"c\": 3} h.values() # [1, 2, 3] values(h) # [1, 2, 3] hash = {\"greeter\": f(name) { return \"Hello $name!\" }} hash.greeter(\"Sally\") # \"Hello Sally!\" 函数 #有名字 f greet(name) { echo(\"Hello $name!\") } greet(`whoami`) # \"Hello root!\" #没名字 f(x, y) { x + y } #带不带return都行 f(x, y) { return x + y } #匿名函数 [1, 2, 3].map(f(x){ x + 1}) # [2, 3, 4] #一等公民 func = f(x){ x + 1} [1, 2, 3].map(func) # [2, 3, 4] #默认参数 f greet(name, greeting = \"hello\") { echo(\"$greeting $name!\") } greet(\"user\") # hello user! greet(\"user\", \"hola\") # hola user! #使用...特殊变量做变长参数 f sum_numbers() { s = 0 for x in ... { s += x } return s } sum_numbers(1) # 1 sum_numbers(1, 2, 3) # 6 f echo_wrapper() { echo(..., \"root\") } echo_wrapper(\"hello %s %s\", \"sir\") # \"hello sir root\" 内置函数 type(1) # NUMBER arg(n) args() [\"abs\", \"--flag1\", \"--flag2\", \"arg1\", \"arg2\"] cd(path) #echo直接支持格式化打印 # echo实际上底层调用的是fmt.Fprintf(env.Writer, args[0].Inspect(), arguments...) # 见evaluator/functions.go echo(\"hello %s\", \"world\") env(\"PATH\") # \"/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" #执行文本 eval(\"1 + 1\") # 2 eval('object = {\"x\": 10}; object.x') # 10 exit(code [, message]) #参数 abs --test --test2 2 --test3=3 --test4 -test5 flag(\"test2\") 2 pwd() #随机数 rand(10) # 7 #支持引入外部脚本 mod = require(\"module.abs\") echo(mod.adder(1, 2)) # 3 #展开其他脚本在本脚本 source(path_to_file.abs) #睡眠 sleep(1000) # sleeps for 1 second #读取输入 stdin() #从epoch开始的时间戳 unix_ms() 装饰器 竟然支持装饰器! f uppercase(fn) { return f() { return fn(...).upper() } } @uppercase f stringer(x) { return x.str() } stringer({}) # \"{}\" stringer(12) # \"12\" stringer(\"hello\") # \"HELLO\" 如果函数执行太久就打印 f log_if_slow(treshold_ms) { return f(original_fn) { return f() { start = `date +%s%3N`.int() res = original_fn(...) end = `date +%s%3N`.int() if end - start > treshold_ms { echo(\"mmm, we were pretty slow...\") } return res } } } @log_if_slow(500) f return_random_number_after_sleeping(seconds) { `sleep $seconds` return rand(1000) } 自带的标准库 abs自带标准库, 也是用require来引用 mod = require('@module') # Loads \"module\" from the standard library mod = require('./module') # Loads \"module\" from the current directory mod = require('module') # Loads \"module\" that was installed through the ABS package manager 目前的标准库只有cli runtime 和util cli cli库本身只有100行! cli = require('@cli') #注册一个cmd @cli.cmd(\"date\", \"prints the current date\", {format: ''}) f date(args, flags) { format = flags.format return `date ${format}` } #运行这个cli cli.run() #交互式运行cli cli.repl() cli举例 #!/usr/bin/env abs cli = require('@cli') @cli.cmd(\"ip\", \"finds our IP address\", {}) f ip_address(arguments, flags) { return `curl icanhazip.com` } @cli.cmd(\"date\", \"Is it Friday already?\", {\"format\": \"\"}) f date(arguments, flags) { format = flags.format return `date ${format}` } cli.run() 把上面脚本保存为./cli, 调用这个脚本 $ ./cli Available commands: * date - Is it Friday already? * help - print this help message * ip - finds our IP address $ ./cli help Available commands: * date - Is it Friday already? * help - print this help message * ip - finds our IP address $ ./cli ip 87.201.252.69 $ ./cli date Sat Apr 4 18:06:35 +04 2020 $ ./cli date --format +%s 1586009212 repl cli 举例 #!/usr/bin/env abs cli = require('@cli') res = {\"count\": 0} @cli.cmd(\"count\", \"prints a counter\", {}) f counter(arguments, flags) { echo(res.count) } @cli.cmd(\"incr\", \"Increment our counter\", {}) f incr(arguments, flags) { res.count += 1 return \"ok\" } @cli.cmd(\"incr_by\", \"Increment our counter\", {}) f incr_by(arguments, flags) { echo(\"Increment by how much?\") n = stdin().number() res.count += n return \"ok\" } cli.repl() 使用 $ ./cli help Available commands: * count - prints a counter * help - print this help message * incr - Increment our counter * incr_by - Increment our counter count 0 incr ok incr ok count 2 incr_by Increment by how much? -10 ok count -8 函数cache 比如一个函数要算个东西, 很久. 第一次执行的时候算, 后面如果入参都一样, 就直接用cache里面保存的. 这里的cache是指util.memoize这个装饰器 util = require('@util') @util.memoize(60) f expensive_task(x, y, z) { # do something very expensive here... } "},"notes/as_title_golang6.html":{"url":"notes/as_title_golang6.html","title":"调试和性能","keywords":"","body":"如题 "},"notes/golang_调试记录.html":{"url":"notes/golang_调试记录.html","title":"Golang 调试记录","keywords":"","body":" 命令记录 dlv使用 dlv headless模式 修改内存 打印unsafe.Pointer 记录debug gvisor 设置package级别的断点 gvisor syscall调试 为什么date -s命令返回Operation not permitted 记录debug adaptiveservice 记录tengo debug go-prompt 性能问题调查 内存还给OS MADV_FREE和MADV_DONTNEED GOGC比例 topid申请内存错误 背景 错误1 错误2 错误3 调查 释放内存后运行 重启后运行 -- nok golang版本1.13换到1.16问题依旧 和版本相关? gshell server内存调试 问题场景 操做1: 首次运行50个govm_test 操做2: 50个govm_test同时restart 第一次内存优化 操做1 加上第90行的效果 操做2 加上第90行的效果 进一步优化思路 goroutine泄漏调试 vscode debug模式 为何dlv总是提示Could not load source 正常的dlv使用 一次空指针访问的panic 调用栈解析和参数解读 切片, 字符串, 和int 变长参数 interface 方法 参数packing 返回值 结构体 指针 slice的make和append性能 只声明切片 -- 400 ns/op make 1 -- 444 ns/op make 10 -- 604 ns/op make 100 -- 2527 ns/op 更正 更正后最终版 最终版结果 结论 pidinfo调试 执行测试 in docker out docker 打开cpu分析 host上profile CPU pprof的refine功能 tooManyTimer调试 命令记录 go tool pprof的http方式 内存 goroutine的情况 同步 锁 阻塞应该也支持, 待调查 trace 代码打开trace 不同的profile文件对比 传统火焰图方式 代码里加调试支持 命令行调试: \"runtime/pprof\" 网页调试: \"net/http/pprof\" 使用debug网页 使用go tool pprof top命令 list命令 svg命令和perf火焰图对比 对照代码片段分析: handleOnu, 主要是select pprof svg perf 火焰图 对照代码片段分析: timerproc notetsleepg 问题: futex唤醒路径是干啥用的? 对照代码片段分析:sysmon 结论 打印调用栈 以json_load为例 pstack打印不了go的调用栈 用-SIGQUIT 注册信号handler 结果 goroutine 0 goroutine 1 用taskset -c 1强制一个核跑 如果只有两个goroutine, 但为什么要起6个线程呢? 命令记录 # 手动访问随机debug端口 http://10.182.105.138:41259/debug/pprof # pprof生成内存视图服务 go tool pprof -http=0.0.0.0:8000 http://10.182.105.138:41259/debug/pprof/heap # 访问内存视图 http://10.182.105.138:64321/ui/?si=inuse_space # debug gshell内存申请导致CPU 100%的问题. perf record -g -p `pidof gshell` -- sleep 60 perf script | /repo/yingjieb/FlameGraph/stackcollapse-perf.pl | /repo/yingjieb/FlameGraph/flamegraph.pl > gshell.svg go tool pprof -http=0.0.0.0:8000 http://10.182.105.138:9998/debug/pprof/profile?seconds=30 最后访问 http://10.182.105.138:60080/ui/ go test -c -o pidinfotest ./pidinfotest -test.run xxxxxx -test.bench BenchmarkP1InfoUpdate ./pidinfotest -test.run xxxxxx -test.bench . -test.benchtime 10s go test -run xxxxxx -bench BenchmarkP1InfoUpdate -cpuprofile cpuiter.out go test -run xxxxxx -bench BenchmarkMapInter -cpuprofile cpuiter.out go test -run xxxxxx -bench BenchmarkMapInter -benchtime 30s -cpuprofile cpuiter.out go tool pprof -http=0.0.0.0:8000 cpuiter.out go tool pprof -http=0.0.0.0:8000 --base cpuiter1.out cpuiter10.out dlv使用 dlv headless模式 启动dlv调试server dlv --headless exec bin/runsc -- -h #成功后会打印 API server listening at: 127.0.0.1:37619 也可以指定port: dlv --headless -l 127.0.0.1:37619 exec bin/runsc -- -h 在client侧: dlv connect 127.0.0.1:37619 (dlv) # 像正常使用dlv一样 修改内存 比如我要修改一个地址的内存 (dlv) x -fmt hex -size 4 0x7bf6c 0x7bf6c: 0x97ff9ee9 //也可以这样显示 (dlv) p %x *(*uint32)(0x7bf6c) //改为0x17ff9ee8 set *(*uint32)(0x7bf6c) = 0x17ff9ee9 修改前:修改后: 可以看到, call指令换成了jmp指令 参考: arm64指令查看在线工具 打印unsafe.Pointer 我在这个函数里: func bluepillHandler(context unsafe.Pointer) 我想打印context (dlv) p context unsafe.Pointer(0x4000460e20) context类型实际上是*arch.UContext64, 但直接p会报错: (dlv) p *(*arch.UContext64)(context) Command failed: can not convert \"context\" to *struct gvisor.dev/gvisor/pkg/sentry/arch.UContext64 但这样可以: (dlv) p *(*arch.UContext64)(0x4000460e20) gvisor.dev/gvisor/pkg/sentry/arch.UContext64 { Flags: 0, Link: 0, Stack: gvisor.dev/gvisor/pkg/abi/linux.SignalStack {Addr: 274882469888, Flags: 0, _:64, Size: 32768}, Sigset: 0, _pad: [120]uint8 [88,245,3,0,64,0,0,0,0,0,0,0,0,0,0,0,8,0,0,0,0,0,0,0,0,16,0,0,0,0,0,0,8,0,0,0,0,0,0,0,8,0,0,0,0,0,0,0,8,0,0,0,0,0,0,0,32,4,7,0,64,0,0,0,...+56 more], _pad2: [8]uint8 [72,15,70,0,64,0,0,0], MContext: gvisor.dev/gvisor/pkg/sentry/arch.SignalContext64 { FaultAddr: 0, Regs: [31]uint64 [274879089664,0,2,274878169088,0,274882992368,1,1,274879089664,18446462873611930624,0,1648372627,815018159,0,0,0,274879438312,274879438160,0,0,274879438648,274879437192,274882469888,0,0,0,274879436488,25907712,274882992224,274879437992,9805828], Sp: 274879438000, Pc: 9815768, Pstate: 536875008, _pad: [8]uint8 [24,16,70,0,64,0,0,0], Fpsimd64: (*\"gvisor.dev/gvisor/pkg/sentry/arch.FpsimdContext\")(0x4000460ff0), }, } 还可以16进制显示: p %x *(*arch.UContext64)(0x4000460e20) 记录debug gvisor 设置package级别的断点 比如调试gvisor, 要给这个函数打断点: package kernel //@pkg/sentry/kernel/task_syscall.go func (t *Task) executeSyscall(sysno uintptr, args arch.SyscallArguments) (rval uintptr, ctrl *SyscallControl, err error) { ... } 在dlv里: 断点的格式是b package.(type).func b pkg/sentry/kernel.(*Task).executeSyscall gvisor syscall调试 先用runsc启动一个docker container docker run --cpus=2 -m 2g --rm --runtime=runsc -it --name=test centos:7 bash // 先找到runsc-sandbox进程pid ./dlv attach 7014 (dlv) b kernel.(*Task).executeSyscall //在container里面敲个回车都会触发断点 //next执行几次 (dlv) p s.Table //设置print命令显示array的个数, 用config -list看具体各项的值 (dlv) config max-array-values 1000 (dlv) next next next ... (dlv) p sysno 1 (dlv) p s.lookup[1] gvisor.dev/gvisor/pkg/sentry/syscalls/linux/vfs2.Write 对应代码 func (t *Task) executeSyscall(sysno uintptr, args arch.SyscallArguments) (rval uintptr, ctrl *SyscallControl, err error) { s := t.SyscallTable() } 解释p s.Table的部分结果: 20: { Name: \"writev\", Fn: gvisor.dev/gvisor/pkg/sentry/syscalls/linux/vfs2.Writev, SupportLevel: 3, Note: \"Fully Supported.\", URLs: []string len: 0, cap: 0, nil,}, 59: { Name: \"execve\", Fn: gvisor.dev/gvisor/pkg/sentry/syscalls/linux/vfs2.Execve, SupportLevel: 3, Note: \"Fully Supported.\", URLs: []string len: 0, cap: 0, nil,}, 41: { Name: \"socket\", Fn: gvisor.dev/gvisor/pkg/sentry/syscalls/linux/vfs2.Socket, SupportLevel: 3, Note: \"Fully Supported.\", URLs: []string len: 0, cap: 0, nil,}, 321: { Name: \"bpf\", Fn: gvisor.dev/gvisor/pkg/sentry/syscalls.CapError.func1, SupportLevel: 1, Note: \"Returns \\\"operation not permitted\\\" if the process URLs: []string len: 0, cap: 0, nil,}, 9: { Name: \"mmap\", Fn: gvisor.dev/gvisor/pkg/sentry/syscalls/linux/vfs2.Mmap, SupportLevel: 3, Note: \"Fully Supported.\", URLs: []string len: 0, cap: 0, nil,}, 96: { Name: \"gettimeofday\", Fn: gvisor.dev/gvisor/pkg/sentry/syscalls/linux.Gettimeofday, SupportLevel: 3, Note: \"Fully Supported.\", URLs: []string len: 0, cap: 0, nil,}, 227: { Name: \"clock_settime\", Fn: gvisor.dev/gvisor/pkg/sentry/syscalls/linux.ClockSettime, SupportLevel: 3, Note: \"Fully Supported.\", URLs: []string len: 0, cap: 0, nil,}, 14: { Name: \"rt_sigprocmask\", Fn: gvisor.dev/gvisor/pkg/sentry/syscalls/linux.RtSigprocmask, SupportLevel: 3, Note: \"Fully Supported.\", URLs: []string len: 0, cap: 0, nil,}, 为什么date -s命令返回Operation not permitted 首先, date -s调用的是系统调用clock_settime(), 由上面的表得知, gvisor对应的函数是gvisor.dev/gvisor/pkg/sentry/syscalls/linux.ClockSettime 打断点到这个函数, 然后在container里执行date -s 12:00:00, 就会触发断点: > gvisor.dev/gvisor/pkg/sentry/syscalls/linux.ClockSettime() pkg/sentry/syscalls/linux/sys_time.go:160 (hits goroutine(600):1 total:1) (PC: 0xa140c0) Warning: debugging optimized function (dlv) bt 0 0x0000000000a140c0 in gvisor.dev/gvisor/pkg/sentry/syscalls/linux.ClockSettime at pkg/sentry/syscalls/linux/sys_time.go:160 1 0x000000000096b776 in gvisor.dev/gvisor/pkg/sentry/kernel.(*Task).executeSyscall at pkg/sentry/kernel/task_syscall.go:103 2 0x000000000096c2ed in gvisor.dev/gvisor/pkg/sentry/kernel.(*Task).doSyscallInvoke at pkg/sentry/kernel/task_syscall.go:238 3 0x000000000096bfa5 in gvisor.dev/gvisor/pkg/sentry/kernel.(*Task).doSyscallEnter at pkg/sentry/kernel/task_syscall.go:198 4 0x000000000096bcda in gvisor.dev/gvisor/pkg/sentry/kernel.(*Task).doSyscall at pkg/sentry/kernel/task_syscall.go:173 5 0x0000000000960c45 in gvisor.dev/gvisor/pkg/sentry/kernel.(*runApp).execute at pkg/sentry/kernel/task_run.go:254 6 0x000000000095f78c in gvisor.dev/gvisor/pkg/sentry/kernel.(*Task).run at pkg/sentry/kernel/task_run.go:95 7 0x000000000096a2ca in gvisor.dev/gvisor/pkg/sentry/kernel.(*Task).Start·dwrap·230 at pkg/sentry/kernel/task_start.go:339 8 0x0000000000469321 in runtime.goexit 而这个函数直接返回linuxerr.EPERM // ClockSettime implements linux syscall clock_settime(2). func ClockSettime(*kernel.Task, arch.SyscallArguments) (uintptr, *kernel.SyscallControl, error) { return 0, nil, linuxerr.EPERM } 记录debug adaptiveservice # 注意要用./binary的形式 # 给binary传参数用-- dlv exec ./echo -- -c -cmd timeout -d # 断点 b main.main b (*timeouter).SetTimeout 记录tengo debug # dlv exec执行一个程序, 可以带参数 dlv exec bin/gshell example/govm_test.gsh # 程序停在最开始阶段 # 设断点 (dlv) b SourcePos Breakpoint 1 set at 0x52670f for github.com/d5/tengo/v2.(*CompiledFunction).SourcePos() /repo/yingjieb/github/godevsig/tengo/objects.go:609 # continue, 程序停在断点 (dlv) c # 开始debug, bt, p等常用gdb命令是一样的 (dlv) l > github.com/d5/tengo/v2.(*CompiledFunction).SourcePos() /repo/yingjieb/github/godevsig/tengo/objects.go:610 (PC: 0x52671d) Warning: debugging optimized function 605: return false 606: } 607: 608: // SourcePos returns the source position of the instruction at ip. 609: func (o *CompiledFunction) SourcePos(ip int) parser.Pos { => 610: for ip >= 0 { 611: if p, ok := o.SourceMap[ip]; ok { 612: return p 613: } 614: ip-- 615: } (dlv) p o *github.com/d5/tengo/v2.CompiledFunction { ObjectImpl: github.com/d5/tengo/v2.ObjectImpl {}, Instructions: []uint8 len: 43, cap: 64, [22,0,3,0,0,2,18,0,0,8,20,1,0,2,34,33,0,0,6,0,0,6,0,0,6,0,0,6,0,0,6,20,5,0,2,30,0,20,0,0,2,21,0], NumLocals: 0, NumParameters: 0, VarArgs: false, SourceMap: map[int]github.com/d5/tengo/v2/parser.Pos nil, Free: []*github.com/d5/tengo/v2.ObjectPtr len: 1, cap: 1, [ *(*\"github.com/d5/tengo/v2.ObjectPtr\")(0xc00000e938), ],} (dlv) p ip 32 # 可以调用任意函数 (dlv) call FormatInstructions(o.Instructions, 0) > github.com/d5/tengo/v2.(*CompiledFunction).SourcePos() /repo/yingjieb/github/godevsig/tengo/objects.go:610 (PC: 0x52671d) Warning: debugging optimized function Values returned: ~r2: []string len: 18, cap: 32, [ \"0000 GETG 3 \", \"0003 CONST 2 \", \"0006 INDEX \", \"0007 CONST 8 \", \"0010 CALL 1 0 \", \"0013 POP \", \"0014 BUILTIN 33 \", \"0016 CONST 6 \", \"0019 CONST 6 \", \"0022 CONST 6 \", \"0025 CONST 6 \", \"0028 CONST 6 \", \"0031 CALL 5 0 \", \"0034 POP \", \"0035 GETF 0 \", \"0037 CALL 0 0 \", \"0040 POP \", \"0041 RET 0 \", ] # 新增断点, 重新run这个程序 (dlv) b postRun (dlv) restart //会停在刚开始 (dlv) c # 停在func (v *VM) postRun() (err error) (dlv) p v.framesIndex 3 (dlv) p v.curFrame *github.com/d5/tengo/v2.frame { fn: *github.com/d5/tengo/v2.CompiledFunction { ObjectImpl: github.com/d5/tengo/v2.ObjectImpl {}, Instructions: []uint8 len: 43, cap: 64, [22,0,3,0,0,2,18,0,0,8,20,1,0,2,34,33,0,0,6,0,0,6,0,0,6,0,0,6,0,0,6,20,5,0,2,30,0,20,0,0,2,21,0], NumLocals: 0, NumParameters: 0, VarArgs: false, SourceMap: map[int]github.com/d5/tengo/v2/parser.Pos nil, Free: []*github.com/d5/tengo/v2.ObjectPtr len: 1, cap: 1, [ *(*\"github.com/d5/tengo/v2.ObjectPtr\")(0xc00000e9b0), ],}, freeVars: []*github.com/d5/tengo/v2.ObjectPtr len: 1, cap: 1, [ *(*\"github.com/d5/tengo/v2.ObjectPtr\")(0xc00000e9b0), ], ip: 0, basePointer: 5,} (dlv) p v.ip 33 go-prompt 性能问题调查 go-prompt时一个交互式cli框架, 但似乎性能有点问题. 一个简单的cli例子程序CPU就占了10%.基于go-prompt的abs也一样空跑都占比10%以上. 看火焰图似乎是timer的使用问题.原因在于 func (p *Prompt) Run() { for { select { default: //有default就是非阻塞, 这里sleep 10ms time.Sleep(10 * time.Millisecond) } } } 内存还给OS MADV_FREE和MADV_DONTNEED go1.12版本中, gc把不用的内存还给OS时, 调用了int madvise(void *addr, size_t length, int advice), 使用了MADV_FREE标记. man madvise说的很清楚, 虽然内存还给了OS, 但OS只有在系统内存不够时才回收. 导致go程序虽然认为自己已经还了内存给OS, 但其RSS一直居高不下. On Linux, the runtime now uses MADV_FREE to release unused memory. This is more efficient but may result in higher reported RSS. The kernel will reclaim the unused data when it is needed. To revert to the Go 1.11 behavior (MADV_DONTNEED), set the environment variable GODEBUG=madvdontneed=1. go1.11及之前, gc用MADV_DONTNEED来还内存给OS, RSS会马上下降. 理论说法是, MADV_FREE性能更好, 但会导致RSS的值不准确. 实际上, 随着gc算法的提高, OS的MADV_FREE lazy回收模式, 就没有太大意义了. go1.16版本又把MADV_DONTNEED的行为做为默认了. 注意: 如果用GODEBUG=madvdontneed=1还是不能让RSS下降, 需要搭配强制gc的方法, 否则一般情况下, gc要很长时间, 比如几分钟, 才工作. go1.13里面有个优化提到: 之前, gc会持有内存5分钟或更长时间才开始还给OS, 但1.13会更积极的还内存. The runtime is now more aggressive at returning memory to the operating system to make it available to co-tenant applications. Previously, the runtime could retain memory for five or more minutes following a spike in the heap size. It will now begin returning it promptly after the heap shrinks. However, on many OSes, including Linux, the OS itself reclaims memory lazily, so process RSS will not decrease until the system is under memory pressure. 参考: https://golang.org/pkg/runtime/ 搜索madvdontneed 下面就是最新的1.16版本的说法: madvdontneed: setting madvdontneed=0 will use MADV_FREE instead of MADV_DONTNEED on Linux when returning memory to the kernel. This is more efficient, but means RSS numbers will drop only when the OS is under memory pressure. https://golang.org/doc/go1.12#runtime https://vec.io/posts/golang-and-memory GOGC比例 GOGC默认是100, 即新分配的内存和上次gc完成时剩下的内存的比例是1:1时, gc才启动. 也就是说, 只有内存翻了一倍时, gc才开始收集垃圾. 想控制RSS占用的话, GOGC设置小一点. topid申请内存错误 背景 某次版本后, topid启动马上出错: 每次栈还不一样, 但最后都是runtime.mallocgc的栈 错误1 ~ # ./topid -tag ZAPPING_MCASTV4_v2 -p 1 -tree Hello 你好 Hola Hallo Bonjour Ciao Χαίρετε こんにちは 여보세요 Version: 0.1.4 runtime: s.allocCount= 3 s.nelems= 5 fatal error: s.allocCount != s.nelems && freeIndex == s.nelems goroutine 1 [running]: runtime.throw(0x1a8caa, 0x31) runtime/panic.go:774 +0x54 fp=0x40000a0ca0 sp=0x40000a0c70 pc=0x3bc84 runtime.(*mcache).nextFree(0x7f82a2a6d0, 0xa2f47, 0x4000000180, 0x200000003, 0x4000000180) runtime/malloc.go:852 +0x204 fp=0x40000a0cf0 sp=0x40000a0ca0 pc=0x1aa44 runtime.mallocgc(0x600, 0x15d600, 0x1, 0x0) runtime/malloc.go:1022 +0x688 fp=0x40000a0db0 sp=0x40000a0cf0 pc=0x1b0e8 runtime.makeslice(0x15d600, 0x600, 0x600, 0x0) runtime/slice.go:49 +0x74 fp=0x40000a0de0 sp=0x40000a0db0 pc=0x51364 bytes.makeSlice(0x600, 0x0, 0x0, 0x0) bytes/buffer.go:229 +0x64 fp=0x40000a0e50 sp=0x40000a0de0 pc=0x7af84 bytes.(*Buffer).grow(0x40000a0f80, 0x200, 0x200) bytes/buffer.go:142 +0x12c fp=0x40000a0ea0 sp=0x40000a0e50 pc=0x7aabc bytes.(*Buffer).ReadFrom(0x40000a0f80, 0x1d39e0, 0x4000081268, 0x3, 0x0, 0x0) bytes/buffer.go:202 +0x48 fp=0x40000a0f10 sp=0x40000a0ea0 pc=0x7ada8 io/ioutil.readAll(0x1d39e0, 0x4000081268, 0x200, 0x0, 0x0, 0x0, 0x0, 0x0) io/ioutil/ioutil.go:36 +0xb0 fp=0x40000a0fb0 sp=0x40000a0f10 pc=0x12bb80 io/ioutil.ReadAll(...) io/ioutil/ioutil.go:45 pidinfo.(*TidInfo).updateStat(0x40001830e0, 0x0, 0x0) pidinfo/pidinfo.go:486 +0x190 fp=0x40000a10f0 sp=0x40000a0fb0 pc=0x12d510 pidinfo.(*PidInfo).update(0x40001830e0, 0x40001472a0, 0x0) pidinfo/pidinfo.go:799 +0x2c fp=0x40000a1190 sp=0x40000a10f0 pc=0x13017c pidinfo.newPidInfo(0x28e, 0x40000bee10, 0x28e, 0x2f4f20, 0x0) pidinfo/pidinfo.go:631 +0x2f8 fp=0x40000a1280 sp=0x40000a1190 pc=0x12e478 错误2 ~ # Hello 你好 Hola Hallo Bonjour Ciao Χαίρετε こんにちは 여보세요 Version: 0.1.4 Visit below URL to get the chart: http://10.182.105.138:9888/ZAPPING_MCASTV4_v2/1992780262 runtime: s.allocCount= 3 s.nelems= 5 fatal error: s.allocCount != s.nelems && freeIndex == s.nelems goroutine 1 [running]: runtime.throw(0x1a8caa, 0x31) runtime/panic.go:774 +0x54 fp=0x40000a0b00 sp=0x40000a0ad0 pc=0x3bc84 runtime.(*mcache).nextFree(0x7f887a26d0, 0xa2f47, 0x4000000180, 0x200000003, 0x4000000180) runtime/malloc.go:852 +0x204 fp=0x40000a0b50 sp=0x40000a0b00 pc=0x1aa44 runtime.mallocgc(0x600, 0x15d600, 0x1, 0x0) runtime/malloc.go:1022 +0x688 fp=0x40000a0c10 sp=0x40000a0b50 pc=0x1b0e8 runtime.makeslice(0x15d600, 0x600, 0x600, 0x0) runtime/slice.go:49 +0x74 fp=0x40000a0c40 sp=0x40000a0c10 pc=0x51364 bytes.makeSlice(0x600, 0x0, 0x0, 0x0) bytes/buffer.go:229 +0x64 fp=0x40000a0cb0 sp=0x40000a0c40 pc=0x7af84 bytes.(*Buffer).grow(0x40000a0de0, 0x200, 0x200) bytes/buffer.go:142 +0x12c fp=0x40000a0d00 sp=0x40000a0cb0 pc=0x7aabc bytes.(*Buffer).ReadFrom(0x40000a0de0, 0x1d39e0, 0x40000851b8, 0xb7bc4, 0x40001354e0, 0x12) bytes/buffer.go:202 +0x48 fp=0x40000a0d70 sp=0x40000a0d00 pc=0x7ada8 io/ioutil.readAll(0x1d39e0, 0x40000851b8, 0x200, 0x0, 0x0, 0x0, 0x0, 0x0) io/ioutil/ioutil.go:36 +0xb0 fp=0x40000a0e10 sp=0x40000a0d70 pc=0x12bb80 io/ioutil.ReadFile(0x40001354e0, 0x12, 0x0, 0x0, 0x0, 0x0, 0x0) io/ioutil/ioutil.go:73 +0xc4 fp=0x40000a0eb0 sp=0x40000a0e10 pc=0x12bcf4 pidinfo.(*PidInfo).init(0x40001a84b0, 0xa, 0x1d6860) 错误3 ./topid -tag ZAPPING_MCASTV4_v2 -p 1 -chartserver 10.182.105.138:9887 -i 3 -c 3600 -record -sys -child & ~ # Hello \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd Hola Hallo Bonjour Ciao \\xce\\xa7\\xce\\xb1\\xce\\xaf\\xcf\\x81\\xce\\xb5\\xcf\\x84\\xce\\xb5 \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf \\xec\\x97\\xac\\xeb\\xb3\\xb4\\xec\\x84\\xb8\\xec\\x9a\\x94 Version: 0.1.4 Visit below URL to get the chart: http://10.182.105.138:9888/ZAPPING_MCASTV4_v2/2938934727 runtime: s.allocCount= 492 s.nelems= 512 fatal error: s.allocCount != s.nelems && freeIndex == s.nelems goroutine 1 [running]: runtime.throw(0x1a8caa, 0x31) runtime/panic.go:774 +0x54 fp=0x4000040b50 sp=0x4000040b20 pc=0x3bc84 runtime.(*mcache).nextFree(0x7f9c7ab6d0, 0x51305, 0x4000040c08, 0x38e8c, 0x4000040c18) runtime/malloc.go:852 +0x204 fp=0x4000040ba0 sp=0x4000040b50 pc=0x1aa44 runtime.mallocgc(0x8, 0x15d400, 0x0, 0x0) runtime/malloc.go:998 +0x4f4 fp=0x4000040c60 sp=0x4000040ba0 pc=0x1af54 runtime.convT64(0x1, 0x7f9a53ef28) runtime/iface.go:352 +0x5c fp=0x4000040c90 sp=0x4000040c60 pc=0x18d0c internal/poll.errnoErr(...) internal/poll/errno_unix.go:32 internal/poll.(*pollDesc).init(0x4000219338, 0x4000219320, 0x1, 0x4000219320) internal/poll/fd_poll_runtime.go:45 +0x8c fp=0x4000040cd0 sp=0x4000040c90 pc=0xb312c internal/poll.(*FD).Init(0x4000219320, 0x19fa28, 0x4, 0x1, 0x40001fe800, 0x0) internal/poll/fd_unix.go:63 +0x68 fp=0x4000040d00 sp=0x4000040cd0 pc=0xb3888 os.newFile(0x5, 0x40001fe7e0, 0x12, 0x1, 0x4000000000) os/file_unix.go:151 +0xe8 fp=0x4000040d60 sp=0x4000040d00 pc=0xb6ed8 os.openFileNolog(0x40001fe7e0, 0x12, 0x0, 0x0, 0x8, 0x12, 0x40001fe7e0) os/file_unix.go:222 +0x19c fp=0x4000040dc0 sp=0x4000040d60 pc=0xb715c os.OpenFile(0x40001fe7e0, 0x12, 0x0, 0x0, 0x2, 0x40001fe7e0, 0x40001fe7ea) os/file.go:300 +0x54 fp=0x4000040e10 sp=0x4000040dc0 pc=0xb6964 os.Open(...) os/file.go:280 io/ioutil.ReadFile(0x40001fe7e0, 0x12, 0x0, 0x0, 0x0, 0x0, 0x0) io/ioutil/ioutil.go:53 +0x48 fp=0x4000040eb0 sp=0x4000040e10 pc=0x12bc78 pidinfo.(*PidInfo).init(0x400020d1d0, 0xa, 0x1d6860) pidinfo/pidinfo.go:587 +0x130 fp=0x4000040f50 sp=0x4000040eb0 pc=0x12dfe0 调查 释放内存后运行 重启后运行 -- nok 看起来是内存申请失败, 那么先系统内存的情况: ~ # free -m total used free shared buff/cache available Mem: 853 533 8 115 312 179 Swap: 0 0 0 强制释放内存试一下: echo 3 > /proc/sys/vm/drop_caches 再看一下内存: ~ # free -m total used free shared buff/cache available Mem: 853 537 103 115 212 175 Swap: 0 0 0 有103M了, topid通常是占用RSS 6M, 那么100M应该是够了. 但运行还是一样的runtime.mallocgc之后runtime.(*mcache).nextFree错误. 重启板子再运行, 也是出错. golang版本1.13换到1.16问题依旧 注意1.16默认使用go mod, 需要手动off掉 export GOPATH=`pwd` GOARCH=arm64 GO111MODULE=off go build topid.go 和版本相关? 288版本是好的, 289版本就不行了.. kernel改了什么? gshell server内存调试 问题场景 正常一个govm_test应该是11秒结束. 这里的问题主要是内存问题.有人可能问了, go不是自动gc的吗? 为什么内存还有问题?这个要仔细看下面的内容了. 操做1: 首次运行50个govm_test 起50个govm_testhtop显示内存46M显示 Inuse 62M # HeapAlloc = 59398296 # HeapSys = 133005312 # HeapIdle = 70754304 # HeapInuse = 62251008 # HeapReleased = 64618496 # HeapObjects = 31588 go tool pprof -http=0.0.0.0:8000 http://10.182.105.138:42459/debug/pprof/heap得到:大部分是shallowClone产生的 操做2: 50个govm_test同时restart 现象是同时restart后, 系统24核CPU占用100%持续大概1分钟, 每个govm_test的结束时间长至50秒左右.htop显示物理内存上升到129M, 多做几次restart会升到200多M. 200多M的时候, 有的时候就不会100%CPU了. perf record -g -p `pidof gshell` -- sleep 60 perf script | /repo/yingjieb/FlameGraph/stackcollapse-perf.pl | /repo/yingjieb/FlameGraph/flamegraph.pl > gshell.svg 火焰图显示绝大部分时间在runtime.newobject函数里.用go tool的objdump可以看到, runtime.newobject是从堆分配的关键函数, 由编译器插入. 第一次内存优化 源码的逻辑是新建个clone的VM来运行, 运行结束后其实就不用了.注意:第90行是新加的, 上面的实验数据是没有第90行的数据.这个函数在最后把gvm返回了, 被当作一个对象保存在上级VM的stack上. 操做1 加上第90行的效果 既然gvm会被引用到, 那gvm.VM也不会被gc. 但实际上, 这个子VM已经执行完毕, 可以安全的释放了.加了第90行来让gc可以回收这个VM. 同样运行50个govm_test, htop显示物理内存45M, 和之前一样.但pprof显示, Inuse小了不少, 整体的HeapSys也比之前小了一半. # HeapAlloc = 34263112 # HeapSys = 65961984 # HeapIdle = 29892608 # HeapInuse = 36069376 # HeapReleased = 18497536 # HeapObjects = 46161 有经过一小会, 稳定后Inuse更小了, 为11M左右. # HeapAlloc = 9485592 # HeapSys = 65994752 # HeapIdle = 54206464 # HeapInuse = 11788288 # HeapReleased = 52912128 # HeapObjects = 25051 pprof的top也显示, 没有了50M的shallowClone的占用. 效果非常明显! 操做2 加上第90行的效果 运行2次后, 还是会100%, 但感觉时间短了些. govmtest会在15秒左右结束.看火焰图还是在runtime.newobject![](img/golang调试记录_20220913224711.png) pprof显示HeapInuse为14M.也不是多很多, 但HeapIdle为117M, 说明有大量的内存申请过但已经使用完毕. # HeapAlloc = 10403200 # HeapSys = 131563520 # HeapIdle = 117334016 # HeapInuse = 14229504 # HeapReleased = 116457472 # HeapObjects = 37464 第三次以后再全部restart, 占用就很低了, 最多只占单核的10%左右, 一般2%. 内存稳定再110M左右.我很好奇现在的火焰图, 于是抓了一个:runtime.newobject变得非常小了 govm_test的\"应用逻辑\"其实占比非常小, 占的多的主要是: park_m stop_m等熟悉的goroutine和M的结合动作. time.Sleep futext相关. 进一步优化思路 需要在VM运行结束后, 重置stack和frame的空间.去掉object的引用, 触发gc的回收. 固定size的array stack [StackSize]Object frames [MaxFrames]frame 变为slice, 按需append stack []Object frames []frame 把静态的内存分配改成动态append方式, 给个比较小的初始值, 动态扩展. 在VM运行完毕后, 通过把frame和stack置为nil的方式, 告诉gc释放内存. 具体修改见这个commit 结果是全部restart场景下, 完全没有24核100%占用的情况;htop观察到物理内存稳定峰值从之前的200M降低到20M.实际上, idle后使用的内存大约为10M.但并发的峰值减小更能说明, 在重启50个VM时候, 由于govm_test内部会起大概10个VM, 一共500个 VM, 并没有同时申请大量内存. goroutine泄漏调试 topid程序改成goroutine版本之后, 出了很多很典型的并发问题. 其中有channel和goroutine的生命周期没配合好, 导致的卡死问题. 即channel发送或者读取的\"对端\"goroutine异常条件退出了, 但没有处理好channel的善后, 导致对方卡住在channel操作. 但goroutine还是在泄漏. 问题的根源在于, 靠树形结构的上一级节点来触发下一级动作的时候, 程序的思路是:假设进程A的子进程有[B1 B2 B3], 在下一次触发的时候, A的子进程变成了[B2 B3 B4]其内在逻辑是, kernel报告B1进程退出了, 但是这里的逻辑是只新建B4的pidinfo结构.那么B1的守护进程处于什么状态呢?答案是B1会永远等待触发channel, 但永远没有人会写这个channel. 因为B1已经从A的子树里拿掉了.这样就造成了goroutine的泄漏.解决办法是在B1的守护进程里, 同时监听(select) cleanup channel, A除了要处理新建的B4, 还要负责触发B1的cleanup 另外一个场景是 A -> B -> C的进程链, 如果B和C同时(一秒内)都消亡了, 那么按照A触发B, B再触发C的cleanup逻辑, C永远不能被触发, 因为树从顶向下, 先触发B的cleanup, 而C就没有机会被B cleanup.解决办法是再A触发B的cleanup时, 递归的触发B的所有子树的cleanup. vscode debug模式 参考文档: https://github.com/golang/vscode-go/blob/master/docs/debugging.md#remote-debugging 修改lauch.json { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Launch\", \"type\": \"go\", \"request\": \"launch\", \"mode\": \"auto\", \"program\": \"/repo/yingjieb/godev/practice/src/tools/topid.go\", \"env\": {\"GOPATH\":\"/go:/repo/yingjieb/godev/practice\"}, \"args\": [\"-p\", \"1\", \"-tree\"] } ] } 注意env和args的用法 另外一个更通用的配置: 下面这个默认的配置直接就能用. { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Launch\", \"type\": \"go\", \"request\": \"launch\", \"mode\": \"auto\", \"program\": \"${fileDirname}\", \"env\": {}, \"args\": [] } ] } 还是这个链接有详细的说明: https://github.com/golang/vscode-go/blob/master/docs/debugging.md#using-vs-code-variables 为何dlv总是提示Could not load source 即使直接启动dlv也有这个问题: dlv exec examples/interoperability/interoperability (dlv) b main.main Breakpoint 1 set at 0x5014eb for main.main() github.com/d5/tengo/v2@/examples/interoperability/main.go:185 (dlv) c //能断点, 能bt (dlv) bt 0 0x00000000005014eb in main.main at github.com/d5/tengo/v2@/examples/interoperability/main.go:185 1 0x000000000042cbf6 in runtime.main at runtime/proc.go:203 2 0x0000000000456c31 in runtime.goexit at runtime/asm_amd64.s:1357 //但不能看代码, why???? (dlv) l > main.main() github.com/d5/tengo/v2@/examples/interoperability/main.go:185 (hits goroutine(1):1 total:1) (PC: 0x5014eb) Warning: debugging optimized function Command failed: open github.com/d5/tengo/v2@/examples/interoperability/main.go: no such file or directory 因为我hack了go命令, 添加了-trimpath, 这个选项把文件名搞成了相对路径了(短路径). 而dlv依赖绝对路径来debug... 去掉-trimpath就好了. 正常的dlv使用 dlv exec ./interoperability (dlv) b main.main Breakpoint 1 set at 0x5014eb for main.main() ./main.go:185 (dlv) c > main.main() ./main.go:185 (hits goroutine(1):1 total:1) (PC: 0x5014eb) Warning: debugging optimized function //能直接显示代码 一次空指针访问的panic yingjieb@3a9f377eee5d /repo/yingjieb/godev/practice/src/pidinfo $ go test -run xxxxxx -bench BenchmarkP1InfoUpdate -cpuprofile cpuiter.out goos: linux goarch: amd64 pkg: pidinfo BenchmarkP1InfoUpdate-23 panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x50 pc=0x5046c9] goroutine 54 [running]: pidinfo.(*PidInfo).updateTime(0x0, 0x0, 0x0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:227 +0x59 pidinfo.(*PidInfo).Update(0x0, 0xc0000905d0, 0x6f7e) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:302 +0x32 pidinfo.(*PidInfo).updateChildren(0xc0001e03c0, 0x0, 0x0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:206 +0x35e pidinfo.(*PidInfo).Update(0xc0001e03c0, 0xc000090570, 0x4507) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:307 +0x70 pidinfo.(*PidInfo).updateChildren(0xc0001e0320, 0x0, 0x0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:206 +0x35e pidinfo.(*PidInfo).Update(0xc0001e0320, 0xc000090450, 0x11) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:307 +0x70 pidinfo.(*PidInfo).updateChildren(0xc0001e00a0, 0x0, 0x0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:206 +0x35e pidinfo.(*PidInfo).Update(0xc0001e00a0, 0xc000090270, 0xc000451d68) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:307 +0x70 pidinfo.(*PidInfo).updateChildren(0xc0001e0000, 0x0, 0x0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:216 +0x2c1 pidinfo.(*PidInfo).Update(0xc0001e0000, 0x0, 0x0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:307 +0x70 pidinfo.BenchmarkP1InfoUpdate(0xc0001f41c0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo_test.go:77 +0x16f testing.(*B).runN(0xc0001f41c0, 0x5b8) /usr/local/go/src/testing/benchmark.go:190 +0xcc testing.(*B).launch(0xc0001f41c0) /usr/local/go/src/testing/benchmark.go:320 +0x10c created by testing.(*B).doBench /usr/local/go/src/testing/benchmark.go:275 +0x55 exit status 2 FAIL pidinfo 0.592s error应该是占两个\"位置\" 能打出调用栈来的, 那返回值的位置没有意义, 因为底层函数还没有返回. 调用栈解析和参数解读 经常看到, 调用栈打印的参数比函数定义时的参数个数要多, 为什么呢?原文: https://www.ardanlabs.com/blog/2015/01/stack-traces-in-go.html 这要理解不同类型参数的传递规则: 切片, 字符串, 和int // Declaration main.Example(slice []string, str string, i int) // Call to Example by main. slice := make([]string, 2, 4) Example(slice, \"hello\", 10) // Stack trace main.Example(0x2080c3f50, 0x2, 0x4, 0x425c0, 0x5, 0xa) //解释切片: // Slice header values Pointer: 0x2080c3f50 Length: 0x2 Capacity: 0x4 //解释字符串 // String parameter value \"hello\" // String header values Pointer: 0x425c0 Length: 0x5 我实验的版本 package main import \"fmt\" func main() { slice := make([]string, 2, 4) m := map[string]int{\"1st\": 1, \"2nd\": 100} Example(slice, \"hello\", 0x88, m) } func Example(slice []string, str string, i int, m map[string]int) { fmt.Println(slice, str, i, m) //没有这行fmt, 似乎结果里的参数只显示..., 应该是编译器自动优化过了 panic(\"Want stack trace\") } //结果: $ ./tmp [ ] hello 136 map[1st:1 2nd:100] panic: Want stack trace goroutine 1 [running]: main.Example(0xc0000c8040, 0x2, 0x4, 0x4c24f9, 0x5, 0x88, 0xc0000cc000) /repo/yingjieb/godev/practice/src/misc/tmp.go:13 +0x16c main.main() /repo/yingjieb/godev/practice/src/misc/tmp.go:8 +0x114 注意0x88后面那个map, 只占一个位置, 所以map是个指针. 切片是个三个值的结构, 指针, len和cap 字符串是2个值的结构, 指针, len 整型参数就是一个值 map参数是一个位置 变长参数 比如 func panicFmt(v ...interface{}) { ... 主动panic } #在main里调用 panicFmt(1,2,3) 或 panicFmt(1, 2, 3, 4, \"hello\") #结果 goroutine 1 [running]: main.panicFmt(0xc0000a2150, 0x3, 0x3) 结论: 不管有多少个入参, 都是先组成一个slice, 再传给panicFmt函数. slice的表达是3个值. interface interface是2个值, 准确的说是两个指针 type eface struct { // 16 bytes _type *_type data unsafe.Pointer } type iface struct { // 16 bytes tab *itab data unsafe.Pointer } 方法 对象的方法调用, call trace的第一个参数是那个对象 参数packing 如果参数拼在一起能够放在一个word里, 那么go会自动的把这几个参数pack在一个word里, 放在栈上传递下面的例子中, 3个bool变量和一个uint8被拼在一个32位的\"参数\"里 bool类型也是8bit 01 package main 02 03 func main() { 04 Example(true, false, true, 25) 05 } 06 07 func Example(b1, b2, b3 bool, i uint8) { 08 panic(\"Want stack trace\") 09 } //结果: 01 goroutine 1 [running]: 02 main.Example(0x19010001) /Users/bill/Spaces/Go/Projects/src/github.com/goinaction/code/ temp/main.go:8 +0x64 03 main.main() /Users/bill/Spaces/Go/Projects/src/github.com/goinaction/code/ temp/main.go:4 +0x32 解释: 感觉是先拼第一个原始参数, 移位, 再拼第二个原始参数... // Parameter values true, false, true, 25 // Word value Bits Binary Hex Value 00-07 0000 0001 01 true 08-15 0000 0000 00 false 16-23 0000 0001 01 true 24-31 0001 1001 19 25 // Declaration main.Example(b1, b2, b3 bool, i uint8) // Stack trace main.Example(0x19010001) 返回值 返回值在对象, 入参之后. 这个例子来说, 最后的两个值就是返回值. 如果是返回int和string, 那string是两个值(指针和长度) func f(a, b int) (sum int, tag int) { c := a + b fmt.Println(c) if c > 64 { panic(\"aaaaa\") } fmt.Printf(\"%p\\n\", &c) return c, c+10 } func main() { f(2, 5) f(64, 8) } //输出: 7 0xc000020028 72 panic: aaaaa goroutine 1 [running]: //返回值好像也是在栈里的, 第三个值和第四个值其实是上次的返回值. //本次还没有返回值就panic了, 返回值是栈里的值. 正好是上次留下来的返回值 main.f(0x40, 0x8, 0x7, 0x11) /repo/yingjieb/godev/practice/src/benchmarks/hashmap/pidinfomap.go:9 +0x167 main.main() /repo/yingjieb/godev/practice/src/benchmarks/hashmap/pidinfomap.go:18 +0x49 exit status 2 结构体 结构体(非结构体指针)不管是入参还是返回值, 都直接展开在栈上传递 比如 type eee struct { a,b,c,d,e int s string } func f(a, b int, e eee) { c := a + b fmt.Println(c) if c > 64 { panic(\"aaaaa\") } fmt.Printf(\"%p\\n\", &c) //return eee{1, 2, 3, 4, 5, \"sssssss\"} } f(64, 8, eee{1, 2, 3, 4, 5, \"sssssss\"}) //panic的时候 goroutine 1 [running]: main.f(0x40, 0x8, 0x1, 0x2, 0x3, 0x4, 0x5, 0x4c3839, 0x7) 指针 callstack里面, 指针只占一个位置. slice的make和append性能 还是在pidinfo代码里面 被测函数根据map, 创建个切片返回 // Children returns first level children nodes func (pi *PidInfo) Children() []*PidInfo { //后面的几个测试都只改这里, 主要是改children切片的初始化方法 //不同的方式差别非常大 var children []*PidInfo //children := make([]*PidInfo, 100) for _, child := range pi.children { children = append(children, child) } return children } 其测试代码如下: func BenchmarkMapInter(b *testing.B) { pi, err := NewPidInfo(1, true) if err != nil { b.Errorf(\"main: %v\\n\", err) return } if err := pi.Update(); err != nil { b.Errorf(\"main: %v\\n\", err) return } b.ResetTimer() for i := 0; i 本意是想测试map表的遍历效率, 没想到切片的make和append效率才是瓶颈. 应该和切片的类型是结构体指针有关. 只声明切片 -- 400 ns/op 迭代map占比25%, 切片操作占比56%. 为什么切片比map还费事呢?看起来对切片的操作append是runtime.growslice, 而它会调用mallocgc申请内存.为什么要创建gc呢? 所有类型的切片都要创建GC吗? 这个创建gc的动作和切片的size有关吗? // growslice handles slice growth during append. // It is passed the slice element type, the old slice, and the desired new minimum capacity, // and it returns a new slice with at least that capacity, with the old data // copied into it. // The new slice's length is set to the old slice's length, // NOT to the new requested capacity. // This is for codegen convenience. The old slice's length is used immediately // to calculate where to write new values during an append. // TODO: When the old backend is gone, reconsider this decision. // The SSA backend might prefer the new length or to return only ptr/cap and save stack space. func growslice(et *_type, old slice, cap int) slice { //根据et的类型确定扩大比例 //et.ptrdata的解释是 size of memory prefix holding all pointers if et.ptrdata == 0 p = mallocgc(capmem, nil, false) else p = mallocgc(capmem, et, true) memmove(p, old.array, lenmem) return slice{p, old.len, newcap} } // Allocate an object of size bytes. // Small objects are allocated from the per-P cache's free lists. // Large objects (> 32 kB) are allocated straight from the heap. func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { //type是nil或者不包括指针, 就不用scan了 noscan := typ == nil || typ.ptrdata == 0 //要分配的内存大小 dataSize := size c := gomcache() //小于32K在M的cache里面分 //大于32K从堆里分 //如果要scan // It is known that the type has pointers somewhere; //能调到这里, 说明元素类型里面某些地方有指针 //这个函数非常非常复杂!!!!! // heapBitsSetType records that the new allocation [x, x+size) // holds in [x, x+dataSize) one or more values of type typ. // (The number of values is given by dataSize / typ.size.) //size是元素的size, datasize是刚申请的字节数 heapBitsSetType(uintptr(x), size, dataSize, typ) ...省略 } append实际在运行时是growslice, 这个函数负责切片扩容 传入切片元素类型, 原切片, 返回最小cap大小的新切片, 并拷贝老切片 切片的length决定下一个元素写在哪 mallocgc的意思是要申请内存了, go是有垃圾回收的, 那任意的内存申请都是要gc知道的 小的对象被malloc到per CPU的空闲内存中 大的对象(32KB以上)则从堆中分配 malloc之后, 如果元素类型里面有指针, 要scan, 调用的heapBitsSetType函数非常复杂 make 1 -- 444 ns/op 比上一个多了makeslice函数调用, makeslice主要也是调用mallocgc() func (pi *PidInfo) Children() []*PidInfo { //var children []*PidInfo children := make([]*PidInfo, 1) for _, child := range pi.children { children = append(children, child) } return children } 对于上面的被测函数的逻辑看, make 1次, 要append好几次. make 10 -- 604 ns/op 比make 1稍稍多了一点 make 100 -- 2527 ns/op 平均一次的执行时间多了好几倍! 为什么呢? 下面是make 100 case的图: 注意, 这个图默认不显示runtime.makeslice, 很奇怪, 这部分占比挺大的. 可以点下图选中函数的REFINE, 就能看全了. 起始pprof把makeslice省略也是说得通的, 因为它本身的执行时间是0, 而且和growslice都调用mallocgc, 从采样和执行时间分析的角度, 这两个通路是重叠的, 图上默认显示一个通路就好了 仔细对比发现, runtime.growslice和runtime.mallocgc比make 1的时候, 从时间上看, 有所浮动, 但应该是下降了点. runtime.mallocgc在make 1的时候花了0.92s runtime.mallocgc在make 1的时候花了0.66s ~ 0.82s 但gc的占比飙升了.对比两个火焰图, make 100的时候, 可以看到, runtime.gcBgMarkWorker占比明显提高. make 100时, runtime.gcBgMarkWorker占比20% make 1时, runtime.gcBgMarkWorker占比才0.53% 改一下代码: func (pi *PidInfo) Children() []*PidInfo { //var children []*PidInfo children := make([]*PidInfo, 100) i := 0 for _, child := range pi.children { //children = append(children, child) children[i] = child i++ } return children } 把append改成直接赋值, 快了很多! 2062 ns/op -> 461 ns/op 但要预先make好一个足够大的切片, 否则会运行时错误, 说超出切片的index范围 这样说, make 100的时候, 预留了足够的大小, 从执行效率上看, make([]*PidInfo, 100)和make([]*PidInfo, 1) 本身并没有差多少, 可能make 100确实稍稍快了一点, 理论上减少了切片\"扩容\"的次数; 但其代价是增大GC的压力. 从结果上看得不偿失. 更正 实际上, 对make([]*PidInfo, 100)和make([]*PidInfo, 1)的append操作是不一样的: append的元素是在len后面的, 对len=100来说, append的扩容会比初始len=1的情况, 拷贝很多元素.所以上面的例子的对比思路是有问题的.下面的例子说明, append是在10个0之后 func main() { sl := make([]int, 10) sl = append(sl, 1) sl = append(sl, 2) fmt.Println(sl) } 输出 [0 0 0 0 0 0 0 0 0 0 1 2] 更正后最终版 使用make([]T, 0, max)和append的搭配 // Threads returns threads of the process func (pi *PidInfo) Threads() []*TidInfo { if pi.checkLevel&CheckThread != CheckThread { return nil } if pi.pid == -1 { return nil } //threads := make([]*TidInfo, pi.threads.Len()) //申请已知的len, 结合下面的直接赋值, 理论性能最佳 threads := make([]*TidInfo, 0, pi.threads.Len()) //申请已知cap, 从0开始append, 和上面性能差不多 //var threads []*TidInfo //从0开始生长, 每次翻倍扩容都有开销, 性能最差. for i, tid := range pi.threads.Keys() { thrdi, _ := pi.threads.Get(tid) //threads[i] = thrdi.(*TidInfo) threads = append(threads, thrdi.(*TidInfo)) //如果没有发生slice扩容, append的性能很好, 几乎和直接赋值差不多. } return threads } 最终版结果 slice初始化方式 性能 threads := make([]*TidInfo, pi.threads.Len()) + threads[i] = thrdi.(*TidInfo) 726 ns/op threads := make([]*TidInfo, 0, pi.threads.Len()) + append 729 ns/op var threads []*TidInfo + append 1114 ns/op 结论 make([]T, length, capacity)的原型中 make切片对应的时runtime.makeslice() append切片对应runtime.growslice() 如果slice的元素里面包括指针, 则slice里面的元素越多, gc压力越大 如果已知length大小, 推荐make直接填入length, 在后面用for循环给切片元素赋值, 不用append. 这样理论上性能最好; 但要注意, 超过切片的len的赋值会panic. 而append则不会. 切片的append操作如果不发生扩容, 和直接赋值性能差不多; 但如果有扩容, 则性能就差太多了. 应该说任何内存申请, 都会伴随gc相关的工作. 除非申请的内存明确知道没有指针. 切片并不廉价, 初始化切片大小时, 要考虑对GC的影响 推荐length为0, 但设置足够的capacity方式初始化切片: make([]*PidInfo, 0, 100); 后面用append\"追加\"元素. 这样即兼顾了性能, 又不会出现index越界的panic pidinfo调试 主要用testing框架来调试. 我是先写的testing, 再写package代码, 同步修改, 相辅相成.test case里加了benchmarking的case, 主要想测Update()的效率. 因为Update()里面有递归的创建子进程的pidinfo和更新执行时间等操作, 最复杂. func BenchmarkP1InfoUpdate(b *testing.B) { pi, err := NewPidInfo(1, true) if err != nil { b.Errorf(\"main: %v\\n\", err) return } b.ResetTimer() for i := 0; i 执行测试 in docker 可以直接在开发目录下执行 yingjieb@3a9f377eee5d /repo/yingjieb/godev/practice/src/pidinfo $ go test -run xxxxxx -bench . -benchtime 10s goos: linux goarch: amd64 pkg: pidinfo BenchmarkP1InfoUpdate-23 86350 168270 ns/op PASS ok pidinfo 15.962s 我不想run普通的test项, 所以用随意的xxxx来匹配test项, 匹配不到, 所以不run 用-bench .执行所有的benchmark项 用-benchtime指定稳定时间, testing框架默认1s docker里面的pid1进程树, 大概0.2ms运行一轮 out docker 先docker里面编译 yingjieb@3a9f377eee5d /repo/yingjieb/godev/practice/src/pidinfo $ go test -c -o pidinfotest 然后在host上运行 用testing框架编译出来的测试程序, 都带了testing框架的选项, 选项比普通的go test命令要多test.前缀, 其他都一样 $ ./pidinfotest -h Usage of ./pidinfotest: -test.bench regexp run only benchmarks matching regexp -test.benchmem print memory allocations for benchmarks -test.benchtime d run each benchmark for duration d (default 1s) -test.blockprofile file write a goroutine blocking profile to file -test.blockprofilerate rate set blocking profile rate (see runtime.SetBlockProfileRate) (default 1) -test.count n run tests and benchmarks n times (default 1) #最后这样运行 ./pidinfotest -test.run xxxxxx -test.bench . -test.benchtime 10s 结果: 大概1ms $ ./pidinfotest -test.run xxxxxx -test.bench . -test.benchtime 30s goos: linux goarch: amd64 pkg: pidinfo BenchmarkP1InfoUpdate-24 37700 1066736 ns/op PASS 能看到htop显示这个程序占比109%左右. 说明这个程序的主体的活只能单核干, 加上调度开销, 就是110%左右. 打开cpu分析 go test -run xxxxxx -bench . -benchtime 10s -cpuprofile cpu.out go tool pprof -http=0.0.0.0:8000 cpu.out 大部分都是在系统调用耗时 没看到有runtime hash等函数的身影? -- 多试几次能看到 用perf top -p $(pidof pidinfotest)能看到 runtime.mapiternext这个map迭代器的占比: 0.58% pidinfotest [.] runtime.mapiternext host上profile CPU yingjieb@godev-server /repo/yingjieb/godev/practice/src/pidinfo $ ./pidinfotest -test.run xxxxxx -test.bench . -test.benchtime 30s -test.cpuprofile cpuhost.o ut goos: linux goarch: amd64 pkg: pidinfo BenchmarkP1InfoUpdate-24 33567 1115273 ns/op PASS 从pprof看来, 系统调用占比较大. pprof的refine功能 默认的调用图是调整过的, 比如有些占比小的函数用虚线代替, 有的就干脆不显示了. 但我想看看map的迭代器在运行时的情况, 那就要用refine功能: 先选中要看的函数, 如上图, 我选中了runtime.mapiternext 点REFINE->focus pprof会根据采样记录的调用栈关系, 以选中的函数为主体, 重新显示调用; 之前不显示的相关调用会显示出来 沿途的CPU占比都做过了调整, 都是根据选中的函数来调整的. tooManyTimer调试 命令记录 go tool pprof -http=0.0.0.0:8000 http://10.182.105.138:9999/debug/pprof/profile wget http://10.182.105.138:9999/debug/pprof/trace?seconds=10 -O testtrace go tool trace -http=0.0.0.0:8000 testtrace 最后访问http://10.182.105.138:60080/ui/ go tool pprof的http方式 用go tool pprof -http=0.0.0.0:8000 /home/yingjieb/pprof/pprof.tooManyTimer.samples.cpu.003.pb.gz可以在docker实例内启动一个http,port是8000; 把8000映射到外部的一个端口号, 比如60080, 就可以从外部访问这个http服务了.注意, 必须要指明0.0.0.0:port, 否则只能在docker实例内部访问. 访问http://10.182.105.138:60080/ui/有火焰图, 这个是go1.14版本 内存 go tool pprof -http=0.0.0.0:8000 http://10.182.105.138:9999/debug/pprof/heap内存有几个视角: 已经分配的对象个数和内存占用, 这个好像是累加的 正在使用的对象个数和内存占用 goroutine的情况 go tool pprof -http=0.0.0.0:8000 http://10.182.105.138:9999/debug/pprof/goroutine可以看到谁在哪里创建了多少个goroutine, 这是抓的当时时刻的情况. 同步 锁 阻塞应该也支持, 待调查 待调查 trace go test框架已经把trace功能集成进去了.引入import _ \"net/http/pprof\"也是很方便的触发trace的手段.下面是用pprof包触发并分析trace的记录: #先触发并下载30秒的trace wget http://10.182.105.138:9999/debug/pprof/trace?seconds=10 -O testtrace #用trace tool来分析, 不加http就默认使用随机端口起http go tool trace -http=0.0.0.0:8000 testtrace 用chrome打开链接指定链接. 需要注意的是, go tool trace使用了chrome已经不支持的技术, 导致80版本后的chrome不能显示trace. 详见: https://github.com/golang/go/issues/34374 经过我的验证, 下载个79版本的旧chrome可以显示trace页面.可以在docker里面打开这个工具, 在docker外面访问:http://10.182.105.138:60080/ 使用wsad等快捷键来浏览. 代码打开trace 上面介绍的是pprof提供的网页式打开trace的方法, 好处是比较方便, 但要引入http等上层协议. 在代码里可以直接打开trace. 引入“runtime/trace”包，调用方法trace.Start()/trace.Stop() 不同的profile文件对比 #对比1和2的差异 go tool pprof -http=:8080 --base dumps/heap-profile-cmp-001.pb.gz dumps/heap-profile-cmp-002.pb.gz 传统火焰图方式 perf record -g -p `pidof tooManyTimer` -- sleep 60 perf script | /repo/yingjieb/FlameGraph/stackcollapse-perf.pl | /repo/yingjieb/FlameGraph/flamegraph.pl > tooManyTimer60s.svg perf top -p `pidof tooManyTimer` x86下面-g --call-graph dwarf选项效果不好, 调用栈不全 代码里加调试支持 go doc pprof可以查到两种debug方式, 原理都差不多, 区别是使用方式不同. 这两种方式都能生成profile文件, 用于go tool pprof进一步检查 命令行调试: \"runtime/pprof\" go test框架自带pprof支持, 用下面的命令可以得到cpu和mem的profile. go test -cpuprofile cpu.prof -memprofile mem.prof -bench . 自己的程序要得到这个效果, 需要runtime/pprof 加类似下面的代码: var cpuprofile = flag.String(\"cpuprofile\", \"\", \"write cpu profile to `file`\") var memprofile = flag.String(\"memprofile\", \"\", \"write memory profile to `file`\") func main() { flag.Parse() if *cpuprofile != \"\" { f, err := os.Create(*cpuprofile) if err != nil { log.Fatal(\"could not create CPU profile: \", err) } defer f.Close() if err := pprof.StartCPUProfile(f); err != nil { log.Fatal(\"could not start CPU profile: \", err) } defer pprof.StopCPUProfile() } // ... rest of the program ... if *memprofile != \"\" { f, err := os.Create(*memprofile) if err != nil { log.Fatal(\"could not create memory profile: \", err) } defer f.Close() runtime.GC() // get up-to-date statistics if err := pprof.WriteHeapProfile(f); err != nil { log.Fatal(\"could not write memory profile: \", err) } } } 网页调试: \"net/http/pprof\" import ( \"net/http\" _ \"net/http/pprof\" ) func main() { go func() { fmt.Println(http.ListenAndServe(\":9999\", nil)) }() ... } 这里的port随便填 \"net/http/pprof\"会注册一个default的httpHandler, 访问http://ip:port/debug/pprof/会使用此默认的handler 使用debug网页 Web网页http://ip:port/debug/pprof/能提供很多有用的信息: /debug/pprof/profile：访问这个链接会自动进行 CPU profiling，持续 30s，并生成一个文件供下载; 用go tool pprof 这个文件可以进行后续分析 /debug/pprof/block：Goroutine阻塞事件的记录，默认每发生一次阻塞事件时取样一次。 /debug/pprof/goroutines：活跃Goroutine的信息的记录，仅在获取时取样一次。 /debug/pprof/heap： 堆内存分配情况的记录，默认每分配512K字节时取样一次。 /debug/pprof/mutex: 查看争用互斥锁的持有者。 /debug/pprof/threadcreate: 系统线程创建情况的记录，仅在获取时取样一次。 /debug/pprof/trace: 和profile使用方式类似, 产生当前程序的执行trace文件. 用go tool trace可以分析这个trace 这个网页的基本逻辑是点一次, 产生一次数据. 结合go tool pprof可以做更多的事 使用go tool pprof go tool pprof的基本命令格式是:pprof [options] [binary] ...如果没有指定output format, 则进入交互模式.source是网页产生的profile文件. 一般就直接写URL地址go tool pprof http://10.182.105.138:9999/debug/pprof/profile默认的取样时间是30s ，可以通过-seconds 命令来指定取样时间 ，取样完成后会进入命令行状态 $ go tool pprof http://10.182.105.138:9999/debug/pprof/profile Fetching profile over HTTP from http://10.182.105.138:9999/debug/pprof/profile Saved profile in /home/yingjieb/pprof/pprof.tooManyTimer.samples.cpu.002.pb.gz File: tooManyTimer Type: cpu Time: Apr 8, 2020 at 10:56am (UTC) Duration: 30s, Total samples = 1.35s ( 4.50%) Entering interactive mode (type \"help\" for commands, \"o\" for options) 上面例子中, 会写明profile文件放在用户目录下的~/pprof路径下. 默认采样30s, 一共采到了1.35s, 占比4.5% top命令 (pprof) top 20 Showing nodes accounting for 1.19s, 88.15% of 1.35s total Showing top 20 nodes out of 70 flat flat% sum% cum cum% 0.65s 48.15% 48.15% 0.65s 48.15% runtime.futex 0.09s 6.67% 54.81% 0.25s 18.52% runtime.notetsleep_internal 0.07s 5.19% 60.00% 0.07s 5.19% runtime.siftdownTimer 0.07s 5.19% 65.19% 0.07s 5.19% runtime.usleep 0.03s 2.22% 67.41% 0.08s 5.93% runtime.selectgo 0.03s 2.22% 69.63% 0.19s 14.07% runtime.startm 0.02s 1.48% 71.11% 0.02s 1.48% runtime.(*gList).pop 0.02s 1.48% 72.59% 0.19s 14.07% runtime.findrunnable 0.02s 1.48% 74.07% 0.02s 1.48% runtime.gopark 0.02s 1.48% 75.56% 0.23s 17.04% runtime.mcall 0.02s 1.48% 77.04% 0.02s 1.48% runtime.osyield 0.02s 1.48% 78.52% 0.02s 1.48% runtime.retake 0.02s 1.48% 80.00% 0.21s 15.56% runtime.schedule 0.02s 1.48% 81.48% 0.02s 1.48% runtime.sellock 0.02s 1.48% 82.96% 0.17s 12.59% runtime.stopm 0.02s 1.48% 84.44% 0.24s 17.78% runtime.sysmon 0.02s 1.48% 85.93% 0.67s 49.63% runtime.timerproc 0.01s 0.74% 86.67% 0.11s 8.15% main.handleOnu 0.01s 0.74% 87.41% 0.01s 0.74% runtime.(*gList).empty 0.01s 0.74% 88.15% 0.01s 0.74% runtime.(*mcache).prepareForSweep profile文件的基本原理还是采样, 在上面例子中: flat是本函数执行的时间, 不包括其他已经被采样的函数 cum包括本函数及从其调用出去的所有时间, 包括所有子函数, 当然包括其他已经被采样的函数 默认按照%flat排序, 这很好理解, flat基本表示了这个函数本身的执行时间. 如果它的子函数也被采样到了, 是不计入本函数的时间的. flat和cum重要, 分别反应了本函数的效率和所有子函数的效率 sum%只是排序累加, 没啥大意思. list命令 list命令和gdb的list看源码用户体验很像, 但能显示采样信息 (flat, cum)的意思是: 第一列时间是采样发生在本函数的; 第二列是采样发生在子函数里的. (pprof) list runtime.timerproc Total: 1.35s ROUTINE ======================== runtime.timerproc in /usr/local/go/src/runtime/time.go 20ms 670ms (flat, cum) 49.63% of Total . . 260: delta = t.when - now . . 261: if delta > 0 { . . 262: break . . 263: } . . 264: ok := true 10ms 10ms 265: if t.period > 0 { . . 266: // leave in heap but adjust next time to fire 10ms 10ms 267: t.when += t.period * (1 + -delta/t.period) . 70ms 268: if !siftdownTimer(tb.t, 0) { . . 269: ok = false . . 270: } svg命令和perf火焰图对比 svg命令可以生成采样分布图和调用关系图 perf的火焰图 pprof和perf对比几点说明: svg显示的采样占比和top一致. 比如也是分自身时间和累积时间 perf版本的火焰图调用栈解析细节不完整, 比如最左边runtime.notetsleep_internal是从timeproc调用下来的. perf有内核态的路径, pprof没有 perf和pprof的图要结合看, 互补. 对照代码片段分析: handleOnu, 主要是select pprof svg select会被编译器转化为运行时的(*waitq)enqueue和gopark把当前goroutine调度出去 select有一把锁(sellock), 锁住所有case? handleOnu代码占比8.15%, 大部分是在搞goroutine的切换准备.perf 火焰图 perf对go是无感知的, 它的调用栈解析相对不是那么精确, 但感觉大致是对的. runtime.mcall是func mcall(fn func(*g)), 作用是从当前g切换到g0栈, 并执行fn; mcall是CPU ARCH相关的汇编入口. pprof的svg图里面, mcall不是在selectgo()函数里调用的, 而是一个独立的runtime._System入口.这个入口似乎是调度器的入口, 被单独提出来了. 但实际上, mcall(parkm)就是在selectgo里面调用的. park_m会调用schedule()触发调度.![](img/golang调试记录_20220914083305.png)结合上面perf火焰图, 调度器最终会调用futex系统调用来触发kernel级别的调度.我估计这里的逻辑是: 当没有活真正要干时, 调用park_m进而层层调用futex来让M进入休眠状态. 从这点上看, perf火焰图把它放在selectgo里面是合理的. 实际上, 这里的采样点这么多, 是因为timer多, 每个timer每秒都要调这么一圈. 对照代码片段分析: timerproc timerproc是goroutine, 所以在这里是个入口. pprof报告这部分占49.63%. perf报告这部分占27%. 但结合来看, 应该把perf的runtime.notetsleep_internal部分的23%也算在一起, 合起来也是差不多50% time.sendTime做为回调函数, 在timerproc里被调用. 它是个简单的非阻塞的channel写 select { //有default是非阻塞发送 case c.(chan Time) 对应go代码runtime.ChanSend -> 从channel的等待队列取goroutine:runtime.(*waitq).dequeue 唤醒等待的goroutine:goready -> 放到runq:runqput这部分没有系统调用, 完全是用户态.并且占比不高, 只有3%. shiftdownTimer函数负责对timer堆排序. 在2000个timer的情况下, 占5%. 也不是很多 大头在notetsleep, 这是timerproc循环中处理完本次timer堆后调用的, 阻塞等待下次timer超时.再往下分, entersyscallblock占14%, exitsyscall占14%. 这个函数都要调用runtime.systemstack, 在进入系统调用阻塞前, 先handoffp和P解绑定, 然后M就休眠了; 但解绑后的P不能闲着, 会再次绑定一个M; 这就是为什么handoffp后, 马上会调用startm启动或唤醒一个新的M; 下图中, 唤醒路径(左箭头)和睡眠路径(右箭头)都是调用futex系统调用 notetsleepg timerproc里面, 回调函数占一小部分, timer堆排序占一小部分, 大头都是在notetsleep前后的准备工作, 包括: entersyscallblock: 此时已经知道本G所在的M要调用阻塞的系统调用而休眠, 要做一些准备工作: 在进入休眠之前, 先交出P的使用权, 即handoffp; 此时空出来的P要求绑定一个新的M, 即上图中的startm, 后者调用futexwakeup去唤醒其他的M.哪些M见下个小节 exitsyscall: 正常来讲, 一个M被唤醒时, 要acquirep, 也许就是刚刚让出来的P, 来绑定本M; 但上面的统计得出, 本M被唤醒后, 走fast_pidle流程, 就是说很多时候P是idle的, 这和htop的4.5%CPU占用率相符; 一个程序, 可能大部分时间比例都不真正占用CPU. P是idle的情况下, 继续走notewakeup流程, 但看起来不太对啊, 此时不是应该真正休眠吗? 也许是个bug? 怎么有反复wakup的嫌疑? // same as runtime·notetsleep, but called on user g (not g0) // calls only nosplit functions between entersyscallblock/exitsyscall func notetsleepg(n *note, ns int64) bool { gp := getg() if gp == gp.m.g0 { throw(\"notetsleepg on g0\") } entersyscallblock() ok := notetsleep_internal(n, ns) exitsyscall() return ok } 问题: futex唤醒路径是干啥用的? 按理说, timerproc的futex都是带超时的, 那等待超时, kernel自然会wakeup这个M. 那上文中, P和M分离后, 要选一个新的M来wakeup来干活, 那么是哪些M会被wakeup呢?首先, 从htop中看到, 一共由4个线程, 那么就是4个M timerproc那个routine所在的M应该最忙, 也就很可能是pid 21217 第二个忙的M应该是sysmon所在的M 其他的可能跑GC啥的 那如果timerproc让出了P, 很可能是给sysmon用. 对照代码片段分析:sysmon sysmon在>中有所说明 从图上看, 几条路径都会走到阻塞的系统调用来休眠. 结论 从handleOnu的select等待路径和timerproc中time.sendTime唤醒路径看 goroutine的调度开销很小, 因为gopark, goready等函数都在用户态就完成了goroutine切换 channel的开销也比较小, 同样也是用户态. channel的send和receive会触发goroutine调度 M的切换和调度依赖kernel的阻塞的系统调用, go在系统调用前后都有\"埋伏\"函数, 这些函数是调度器的入口点. 打印调用栈 以json_load为例 #运行后, 有6个线程 ./json_load -fileName test.json -loopNum 10000 > /dev/null cat /proc/17212/status Threads: 6 pstack打印不了go的调用栈 提示could not find _DYNAMIC symbol, 因为pstack底层使用gdb, 而gdb对go的理解不好. 用-SIGQUIT 在go程序运行时, 用kill -SIGQUIT 17167给go进程发信号, 会导致go进程终止, 该进程终止时会打印调用栈或者在进程执行时, 按ctrl+\\发SIGQUIT信号, 效果一样 注册信号handler 参考 https://www.jianshu.com/p/abbe6663b672 结果 显示只有2个goroutine goroutine 0 goroutine 1 中间很长, 这个是个多层函数调用嵌套的过程, 因为输入test.json里面有多层嵌套 用taskset -c 1强制一个核跑 还是2个goroutine 如果只有两个goroutine, 但为什么要起6个线程呢? 实际上, goroutine不止2个. 加GOTRACEBACK=system环境变量, 能看到系统级的goroutine GOTRACEBACK=system ./json_load -fileName test.json -loopNum 100000 > /dev/null 然后按ctrl+\\, 得到调用栈 全部goroutine如下: goroutine 0 [idle]: //用户goroutine goroutine 1 [runnable]: //以下是system级别的 goroutine 2 [force gc (idle)]: goroutine 3 [GC sweep wait]: goroutine 4 [finalizer wait]: goroutine 5 [GC worker (idle)]: goroutine 17 [GC worker (idle)]: goroutine 18 [GC worker (idle)]: goroutine 19 [GC worker (idle)]: 用 GODEBUG=schedtrace=10000,scheddetail=1可以10s打印一次调度信息 P: 代表CPU M: 代表OS线程 G: 代表goroutine "},"notes/golang_topid性能优化.html":{"url":"notes/golang_topid性能优化.html","title":"Golang topid性能优化","keywords":"","body":" topid性能优化(倒叙阅读) 单核和多核数据背离 24核运行时 单核运行时 对比多核, 单核运行时 新版本 老版本 结论 pprof数据和实际观测的背离(还是不全面) htop采集的运行数据 pprof的采集数据 哪个数据是准确的? perf火焰图数据来仲裁 新版本在内核态多搞了什么事情? 结论 topid性能优化(不全面) topid和htop性能对比 数据 阶段解读1 benchmark关键函数 分析热点调用链 递归的routine化改造 改造思路和细节 docker instance里面的结果 在host上运行 系统调用次数 open的fd数 goroutine数量 pprof数据 同时运行新老版本 结论 再改进: 保存fd句柄, 但不在goroutine上下文中. 题外: 尝试加goroutine topid是纯go写的类似top的进程性能统计工具, 和topidchart联用可以实时图形化显示性能. gshell topid代码 虽然本文调试的时候topid还没有使用gshell框架, 但是它们的主体代码都是lib pidinfo, 主要的活是lib干的. topid性能优化(倒叙阅读) 单核和多核数据背离 背景见下面. 还是新版本和老版本, 一点都没变. 当用taskset绑定单核运行时, 结果有惊人的变化. 24核运行时 见下面 htop采集的运行数据 小节 左侧是新版本, 右侧是老版本 单核运行时 分别将新老两个版本绑定到15 16核运行 注: taskset和GOMAXPROCS=1都能够限制单核, 实测效果一样. 但后者支持跨核调度, 更好点. GOMAXPROCS=1 ./topid.dd0301e -p 1 -tree -thread taskset -c 15 ./topid.dd0301e -p 1 -tree -thread 左侧是新版本, 右侧是老版本 对比多核, 单核运行时 新版本和老版本的CPU占用都下来了. 新版本的CPU占用下降更多; 甚至比老版本还快. 新老版本使用的OS线程数量都明显下降了, 新版本减小的更多. taskset1以后, 采样数量都小了很多 新版本 多核运行时:单核运行时: 解读: runtime.mcall大大减少, 从24核的2759次减小为31次 runtime.futex大大减小, 从24核的4397次减小为51次 runtime.sellock大大减小, 从24核的1131次减小为3次 gc相关的比例还是30%多, 但绝对数量也大大减小 老版本 多核运行时:单核运行时: 和上面类似 runtime.mcall大大减少 runtime.futex大大减小 runtime.sellock本来也没有啥 连gc相关的比例都减小了 结论 go的性能和P和M的数量关系巨大, 即PMG三者中, P是执行者, M是执行环境, 代码逻辑是要干的活; M的数量受G的数量和P的数量约束 P的数量可以用GOMAXPROCS=n来静态配置, 也可以调用runtime的GOMAXPROCS()函数来配置. 默认和CPU核数相等. M是runtime根据需要动态生成的, 和P的数量看起来是成比例相关性. 应该说G的数量是需求, P是比例系数, 最终确定M的数量 主要影响M个数的还是P, 因为P是执行者, P要结合执行环境M才能干活. P越多, 环境也要求越多. 活分给1个G或者多个G, 对性能影响不大. G和业务逻辑自然对应就好.不要苛求. G越多, M的数量也会多. 并不是G越多并发就越好, 但其实G多了也没啥大影响. -- 补充, 在P数量很少的情况下是的. 但如果P数量大, 比如24个, 然后你的程序的G又多, 那么效果就是M会大大增多, P和M的分离和结合是go调度器的核心, 也是性能开销巨大的地方. P的数量影响锁. 因为P是真正并发的个数, 并发才需要锁. 上面的数据中, P为1的时候, 原来的\"广播\"channel的锁的开销也变得非常小了. P为24的时候, \"广播\"channel的锁开销很大 总的来说, 活不多的时候, 多少个G做都无所谓, 主要是P越小越好. 即一个P能干完的活, 用一个P干比让多个P干更省系统资源. 活多的时候, 一个P干不过来, 表现是一个CPU长期100%也干不完, 那就要增大P. 增大P能多出活, 但要代价是要承担多P的调度开销, 通常来讲这个开销很不小. 多核环境下, 默认的P等于核数的策略, 开销很大. 建议每个go程序都要根据情况手动配置GOMAXPROCS pprof数据和实际观测的背离(还是不全面) 背景见下面: topid性能优化(不全面) 但里面的分析落入了\"用户态\"陷阱. 有很多数据对不上 htop采集的运行数据 左边是新版本topid运行情况右边是老版本topid运行情况无论是从观察CPU占用的情况, 还是TIME时间统计, 新版本都比老版本CPU占用高, 累计运行时间长. 这两个程序我同时起的, 但左边新版本累计运行(即在R状态的时间)了55分钟, 右边是32分钟. 即新版本比老版本CPU占用几乎高2倍. pprof的采集数据 同样采样10分钟:新版本: 老版本: 可以看到, 同样是采样10分钟, pprof采样认为, 新版本真正运行了13.86s ( 2.31%); 而老版本真正运行了24.73s ( 4.12%) 即pprof认为新版本性能更好. 哪个数据是准确的? pprof的采集数据和htop显示的数据是矛盾的吗? 新版本比老版本真的性能更好吗?从代码逻辑上说, 新版本是做了优化的: 优化了文件open的逻辑, 打开不关闭, 下次继续用. 新增每个线程一个goroutine, 来更新status情况. 大概有900个这样的goroutine 感觉上, 新版本应该性能更好. 从pprof的数据看来, 确实系统调用占比下降了. 其他也没有明显的增加, 只大概gc部分和systemstack部分有所增加. 真的吗? perf火焰图数据来仲裁 同样采样60秒:新版本: 老版本: 新版本的采样数为9048老版本的采样数为4338 这个数据很硬, 说明新版比老版本慢了. 和htop的观测数据基本一致的.那么怎么看pprof数据呢? 它错了吗?pprof也没错, 但它只报告用户态的数据. 从用户态来看, 新版本更快了.但从用户态加内核态来看, 新版本反而慢了一倍多. 另外, pprof的采样频率不高:10分钟期间新版本采样1386个, 每秒才采集到2个点.老版本采样2473个, 每秒才采集到4个点.采样频率太低了!而perf的采样每秒能到150个点. 这个差距太大了 似乎pprof只关注用户态, 即使采样到本进程的内核态函数, 似乎pprof就忽略了, 并不计入统计. 就好像这个进程的内核态时间不算一样. 这样就可以解释, 比如在pprof看来, 新老版本的runtime.futex的采样数差不多, 但perf看来确实新版本比老版本多两倍. 如果pprof采样时, 只看用户态的栈, 那么比如某个时刻pprof采样到内核态, 它不管了; 但实际上, 此时是内核在做runtime.futex的事情. perf会通过回溯内核栈知道是runtime.futex, 但pprof没有能力知道这个信息, 就没有统计进. PS: 网上说runtime的 func SetCPUProfileRate(hz int)可以修改采样频率. -- 好像加了没有用? 还是默认的100 新版本在内核态多搞了什么事情? 新版本用户态的逻辑确实快了, 但却带了更大的内核态开销.要对比一下:新版本的采样数为9048老版本的采样数为4338 runtime.futex, 这个函数是goroutine睡眠和唤醒的关键函数 新版本: 9048*48.6% = 4397 老版本: 4338*53.7% = 2329 新版比老版多了近一倍 runtime.sellock 新版本: 9048*12.5% = 1131 老版本: 几乎可以忽略 因为新版本中, 900个gotouine都\"监听\"一个channel, 这个channel用close来\"广播\"事件. 这个channel的锁开销非常大 runtime.mcall 新版本: 9048*30.5% = 2759 老版本: 4338*36.7% = 1592 新版本也是老版本的2倍, 是因为线程多吗? runtime.sysmon 新版本: 4.9%, 443个 老版本: 8.5%, 368个 sysmon并没有太大影响 gc相关(统计正则\"bg|gc\") 新版本: 35% 老版本: 29% 还是新版本多了不少 结论 新版本比老版本慢. 主要原因是goroutine调度和channel的锁, 以及gc开销变大 新版本的线程数也多, mcall也有开销. pprof工具不统计目标进程的内核态的开销, 导致和实际观测结果不一致. 说三遍 pprof工具不统计目标进程的内核态的开销, 导致和实际观测结果不一致. 说三遍 pprof工具不统计目标进程的内核态的开销, 导致和实际观测结果不一致. 说三遍 topid性能优化(不全面) topid和htop性能对比 测试命令: ./topid -p 1 -tree -thread和htop或者GOMAXPROCS=1 ./topid -p 1 -tree -thread 数据 目测htop的CPU占用在3%左右. 偶尔能到7% htop的系统调用 perf stat --log-fd 1 -e 'syscalls:sys_enter_*' -p 17470 -- sleep 10 | egrep -v \"[[:space:]]+0\" Performance counter stats for process id '17470': 62 syscalls:sys_enter_poll 4,044 syscalls:sys_enter_getdents 12 syscalls:sys_enter_newstat 2,046 syscalls:sys_enter_newfstat 23,928 syscalls:sys_enter_read 413 syscalls:sys_enter_write 17,946 syscalls:sys_enter_openat 13,992 syscalls:sys_enter_close 12 syscalls:sys_enter_rt_sigaction 10.221686634 seconds time elapsed topid, e86db98(opt1)的CPU占用总体较高, 跳跃幅度大, 最小3%, 最大有11%. topid的系统调用 #连续3次执行 perf stat --log-fd 1 -e 'syscalls:sys_enter_*' -p 28187 -- sleep 10 | egrep -v \"[[:space:]]+0\" #1 Performance counter stats for process id '28187': 40,904 syscalls:sys_enter_epoll_ctl 243 syscalls:sys_enter_epoll_pwait 2,174 syscalls:sys_enter_getdents64 24 syscalls:sys_enter_fcntl 27,688 syscalls:sys_enter_read 9,368 syscalls:sys_enter_write 18,749 syscalls:sys_enter_openat 18,748 syscalls:sys_enter_close 1,770 syscalls:sys_enter_madvise 6,026 syscalls:sys_enter_futex 2,927 syscalls:sys_enter_nanosleep 6,749 syscalls:sys_enter_sched_yield 14.298493125 seconds time elapsed #2 Performance counter stats for process id '28187': 30,728 syscalls:sys_enter_epoll_ctl 198 syscalls:sys_enter_epoll_pwait 1,782 syscalls:sys_enter_getdents64 20 syscalls:sys_enter_fcntl 22,691 syscalls:sys_enter_read 7,677 syscalls:sys_enter_write 15,364 syscalls:sys_enter_openat 15,364 syscalls:sys_enter_close 1,294 syscalls:sys_enter_madvise 3,415 syscalls:sys_enter_futex 1,668 syscalls:sys_enter_nanosleep 25,145 syscalls:sys_enter_sched_yield 11.999374271 seconds time elapsed #3 Performance counter stats for process id '28187': 30,730 syscalls:sys_enter_epoll_ctl 197 syscalls:sys_enter_epoll_pwait 1,978 syscalls:sys_enter_getdents64 24 syscalls:sys_enter_fcntl 27,721 syscalls:sys_enter_read 9,379 syscalls:sys_enter_write 18,771 syscalls:sys_enter_openat 18,770 syscalls:sys_enter_close 1,650 syscalls:sys_enter_madvise 4,870 syscalls:sys_enter_futex 2,535 syscalls:sys_enter_nanosleep 166 syscalls:sys_enter_sched_yield 12.300680961 seconds time elapsed topid, f7a11ed(opt2+other): 相比较opt1, 没有明显CPU利用率的变化, 最小3.3%, 7.2%, 9.1%, 偶尔到11.1% 阶段解读1 从文件open和read来看, htop和topid数量级差不多 文件close上htop要少 从以上看似乎htop并没有采取\"保持open\"的策略 topid的opt2对比opt1没有性能上的变化, opt2只是去掉了map的遍历以及多调用的一次pi.Children()调用, 都属于很小的优化点. opt2的代码可以做为baseline benchmark关键函数 关键函数是Update() 该baseline的benchmark性能在2ms左右, 这是docker实例里的pid1的 //benchmark: for i := 0; i #结果 yingjieb@3a9f377eee5d /repo/yingjieb/godev/practice/src/pidinfo $ go test -run xxxxxx -bench BenchmarkP1InfoUpdate goos: linux goarch: amd64 pkg: pidinfo BenchmarkP1InfoUpdate-23 712 1539443 ns/op PASS ok pidinfo 1.293s 分析热点调用链 #先利用go test记录cpu的profile文件 go test -run xxxxxx -bench BenchmarkP1InfoUpdate -cpuprofile cpu.out #打开pprof工具分析 go tool pprof -http=0.0.0.0:8000 cpu.out #网页浏览 http://10.182.105.138:60080 从图中得出, 关键在频繁的open read close系统调用.而且, go的runtime特别喜欢在系统调用路径上埋桩, 比如open的时候会把fd加到epoll里面, 导致epollctl比较高, 几乎和open+close相当. 10秒3万次.相对于C的htop, 虽然open和close也在每秒万次级别, 但htop没有使用epollctl; htop只有几十次的poll系统调用 到这里, 可以看到, GO程序的开销还是比较大的, 主要有: 系统调用开销: go的open read write系统调用, 首先是受runtime用户态调度的; 即read()发生时, runtime把这个goroutine调度出去, 待该fd可以read了再调度回来. 之所以能做到用户态调度, 是因为runtime记录了每个open的文件的fd, 并加到epoll_ctl里面管理. GC的开销 实际上, 可以利用go的\"同步\"特性, 用保持open的方式来避免反复的open read close文件. 如果去掉上图的 1 2 3, 只留read, 估计至少可以提高性能一倍. 即下面这个函数相关的逻辑要改造 func readFile(filename string) ([]byte, error) { f, err := os.Open(filename) if err != nil { return nil, fmt.Errorf(\"%s: %w\", here(), err) } defer f.Close() return ioutil.ReadAll(f) } 关键是怎么利用goroutine, 让open的文件保持. 整个程序的思路要从C的递归思路, 转变为go的routine思路. 递归的routine化改造 改造思路和细节 docker instance里面的结果 benchmarking的结果是: 同一个环境, 老版本和新版本都run三次 新版本(并发版本)大概在2ms左右 $ go test -run xxxxxx -bench BenchmarkP1InfoUpdate goos: linux goarch: amd64 pkg: pidinfo BenchmarkP1InfoUpdate-23 732 2105706 ns/op PASS ok pidinfo 4.919s 老版本(递归版本)大概4ms $ go test -run xxxxxx -bench BenchmarkP1InfoUpdate goos: linux goarch: amd64 pkg: pidinfo BenchmarkP1InfoUpdate-23 273 4075231 ns/op PASS ok pidinfo 1.557s 从这个数据看, 新版本优于老版本. 在host上运行 系统调用次数 系统调用次数统计中, epoll_ctl, open和close次数减小的非常明显, 符合预期. root@godev-server:/repo/yingjieb/godev/practice/src/tools# perf stat --log-fd 1 -e 'syscalls:sys_enter_*' -p 3632 -- sleep 10 | egrep -v \"[[:space:]]+0\" Performance counter stats for process id '3632': 24 syscalls:sys_enter_epoll_ctl 277 syscalls:sys_enter_epoll_pwait 2,954 syscalls:sys_enter_getdents64 8 syscalls:sys_enter_fcntl 22,029 syscalls:sys_enter_lseek 30,716 syscalls:sys_enter_read 2,204 syscalls:sys_enter_write 6 syscalls:sys_enter_openat 8 syscalls:sys_enter_close 1,600 syscalls:sys_enter_madvise 5,544 syscalls:sys_enter_futex 2,615 syscalls:sys_enter_nanosleep 708 syscalls:sys_enter_sched_yield 21.666616483 seconds time elapsed open的fd数 大概在2K数量级, 超过了默认的1024. 要先ulimit -n 10240扩大打开文件的上限. yingjieb@godev-server /proc/3632/fd $ ll | wc -l 1972 goroutine数量 大概2K个 pprof数据 是系统调用, 主要是read; 占33% 是GC, 占35% 是调度, 占7% 综合来看, 这么多个goroutine(2K)情况下, 调度其实占比不多. 而实际上, 系统调用次数和开销明显减小了; 但GC的情况变得很糟糕, 有1/3的时间全部在搞GC. 同时运行新老版本 同时运行两个版本, 用htop观察 可以看到, 新版本并没有变好, 甚至观察到比老版本还稍差. 打开的fd多, 需要先修改ulimit的文件打开上限数 占用内存多, 新版本21M, 老版本8M 结论 新版本的思路是用goroutine长期持有fd, open了不关闭, 需要read的时候, 用channel来触发. 但新版本并没有保存fd到记录, 而是每个新的fd都开一个goroutine来守护, 同时搭配channel来协同工作. 这是用goroutine+channel的组合来替换反复的open close文件操作. 当进程数较少时, 新版本有性能优势. 因为避免了老版本的重复open close文件的开销 当进程数很多时, 新版本反而性能更差. 其实调度的开销并不是很大, 这么多个goroutine才7%左右; 补充, 有的时候也能到20% 开销大的是GC. 异步程序设计复杂, 程序逻辑更多. 也加剧了GC的开销.大量的map使用, 应该是主要原因. 所以凡事无绝对, 要看什么场景. 再改进: 保存fd句柄, 但不在goroutine上下文中. 减小goroutine的数量. 减小异步交互. 减小逻辑复杂度. 最终的目的是减小GC压力. 题外: 尝试加goroutine 在调用点 for循环里得到线程列表, 调用线程的update函数的时候, 用goroutine for循环里得到子进程的历表, 调用子进程的update函数的时候, 用goroutine 类似这样: //fmt.Printf(\"Process %d's child processes:%v\\n\", pi.pid, pi.children) for _, childpid := range pi.childrenIds { childpid := childpid childpi := pi.children[childpid] //wrong use of goroutine, 并发panic go func() { //fmt.Println(\"Updating PidInfo for\", childpid) if err := childpi.Update(); err != nil { delete(pi.children, childpid) } }() } 注意, 这样使用会有并发问题 go test -run xxxxxx -bench BenchmarkP1InfoUpdate执行后, 会报错: yingjieb@3a9f377eee5d /repo/yingjieb/godev/practice/src/pidinfo $ go test -run xxxxxx -bench BenchmarkP1InfoUpdate goos: linux goarch: amd64 pkg: pidinfo BenchmarkP1InfoUpdate-23 fatal error: concurrent map read and map write goroutine 13 [running]: runtime.throw(0x5623be, 0x21) /usr/local/go/src/runtime/panic.go:774 +0x72 fp=0xc0000b5d98 sp=0xc0000b5d68 pc=0x42d232 runtime.mapaccess1_fast64(0x52d800, 0xc0000b2450, 0x2900, 0xc000150828) /usr/local/go/src/runtime/map_fast64.go:21 +0x1a6 fp=0xc0000b5dc0 sp=0xc0000b5d98 pc=0x410136 pidinfo.(*PidInfo).updateThreads(0xc0000fc300, 0x0, 0x0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:315 +0x3e0 fp=0xc0000b5f08 sp=0xc0000b5dc0 pc=0x506f30 pidinfo.(*PidInfo).Update(0xc0000fc300, 0x0, 0x0) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:486 +0x16e fp=0xc0000b5fa0 sp=0xc0000b5f08 pc=0x50885e pidinfo.(*PidInfo).updateChildren.func1(0xc0000fc300, 0xc0000fc000, 0x2885) /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:393 +0x2b fp=0xc0000b5fc8 sp=0xc0000b5fa0 pc=0x509eab runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:1357 +0x1 fp=0xc0000b5fd0 sp=0xc0000b5fc8 pc=0x45b8b1 created by pidinfo.(*PidInfo).updateChildren /repo/yingjieb/godev/practice/src/pidinfo/pidinfo.go:391 +0x8e9 因为在更新线程状态的时候, 会对同一个map读写.说明runtime有一定能力检测并发问题, 并进行运行时的panic处理. "},"notes/golang_gshell性能调试.html":{"url":"notes/golang_gshell性能调试.html","title":"Golang gshell性能调试","keywords":"","body":" asbench调试 背景 在x86服务器上性能不稳定 试验 目前的结论: topid性能调试 baseline CPU chart 火焰图 float改int有效果吗? asbench调试 背景 测试adaptiveservice性能 在x86服务器上性能不稳定 测试命令: 启动daemon, 注意这里限定了GOMAXPROCS=8, 就是说默认grg的最大proc数为8 GSHELL_NOUPDATE=1 GOMAXPROCS=8 bin/gshell -loglevel info daemon -registry 10.182.105.179:11985 -bcast 9923 启动asbenchserver: gsh run -group asbench benchmark/asbench/server/asbenchserver.go 启动client, 测试性能 gsh run -group asbench -i -rm benchmark/asbench/client/asbenchclient.go -t 10 -scope os -s32 -n 1 发现有时候TPS数据差异很大: Transaction Per Second(TPS): 15088.94 Transaction Per Second(TPS): 13968.29 Transaction Per Second(TPS): 16901.58 但有时候又很稳定: Transaction Per Second(TPS): 24853.32 Transaction Per Second(TPS): 25749.64 Transaction Per Second(TPS): 24608.96 试验 经过简单的调整golang运行时的参数, 比如: GODEBUG=madvdontneed=0,schedtrace=1000,scheddetail=1,asyncpreemptoff=1 GOGC=off taskset -c 11 ./asbench -packet -t 10 -mode c GODEBUG=madvdontneed=0,schedtrace=1000,scheddetail=1,asyncpreemptoff=1 GOGC=off GOMAXPROCS=1 ./asbench -packet -t 10 -mode c taskset似乎能起一定的作用, 但TPS还是不稳. 目前的结论: 本测试是在云服务器(VM)上进行的, 似乎是KVM虚拟机. 在VM上用top看到, 性能稳定的时候, 都是0.0st(红框部分).但性能不稳定的时候, 是零点几的st: 0.3st, 虽然只有零点几, 但可能是host上有什么负载, 或者是同一台物理机上其他人的VM有大load.因为KVM的核也是host上qemu的一个线程, 在VM中跑load, 会受到host的影响, 但这种影响似乎在VM上很难察觉. topid性能调试 baseline gshell版本v21.11.02.rc2 启动命令: ~/gshell # gsh run -rt 91 -i app-plat/topid/topid.go -chart -snapshot -sys -i 3 -tag eonuTest -info \"cat /tmp/boardname,typeA_panda_show,typeA_squirrel_show\" CPU chart CPU chart(on ppc32, gccgo): CPU chart(on x86, gcgo): 火焰图 火焰图上看, 占比比较大的有: yaegi解释器 进程update 调度 GC 而gotidy编解码占比非常小 float改int有效果吗? topid client在向server端传输数据的时候, CPU占用率用的是float表示. 我们直到, 即使是简单的float值, 比如1.05, 其实在内存中的表达也是一个很大的\"数值\", 那么能不能传输int, 比如105, 这样可以大大简化提高gotiny的编码效率. 那float改成对应的int, 实际效果如何呢?-- 几乎没改善, 和perf火焰图一致.从火焰图上看, 应该效果非常小. 真的吗?把float64改成uint64 CPU实际更擅长\"纯数值\"计算, 在一个应用逻辑中, 除了计算, 还有IO操作, 比如topid中, 更多的工作是proc文件系统操作和网络IO, 比例远远大于\"纯数值\"计算. 所以理论上float变成int能够提高编码效率, 但因为这部分计算占应用的比例太低, 实际上性能没有什么提高. "},"notes/as_title_golang7.html":{"url":"notes/as_title_golang7.html","title":"网络和消息中间件","keywords":"","body":"如题 "},"notes/golang_zmq.html":{"url":"notes/golang_zmq.html","title":"消息中间件基本概念和zero mq","keywords":"","body":" zmq和nanomsg nanomsg支持的socket类型 nanomsg的transport类型 nanomsg和nng nng的纯go版本 高级REQ-REP 经过ROUTER会加\"address\" frame 地址的生成 Router再把标识符剥掉, 传给REQ 总结 搭配 合法的搭配 不合法的搭配 总结 zmq使用 transport类型 线程间 进程间 TCP pgm zmq socket server和client zmq socket类型 zmq的send和recv和tcp的不同 zmq的message framing Frames 兼容性 IO thread和context zmq_poll()函数 不用poll 使用poll 分片的msg 服务发现 多对多的client和server 其他模型 并发支持 zmq的应用代码可以run在线程, 进程, 或node上. pub到已知数量的subscriber 零拷贝 pub-sub实际上是msg filter 高水位 丢包怎么定位? 消息交互模式 PAIR类型 为社么要用pair req-rep server端 client端 pub-sub server侧 client侧 push-pull 生产者 ventilator 消费者worker sink 总结 context pipeline fanout 灵魂几问 阻塞还是异步? 谁当server谁当client? 怎么在wire上表示一个message? 如果对端没准备好的时候, 要发送的数据缓存在哪里? 拥塞控制策略? 消息丢失怎么办? 要保证送达吗? 如果有新的transport方法怎么办? app要改吗? 比如增加支持yipc? 消息怎么路由? 多语言怎么适配? encoding怎么选择? 网络错误怎么处理? 作者观点 zero mq(zmq, 0mq) 问题: REQ-REP模式下的RPC, 怎么把异步同步化的? zmq使用最基本的模式, send后马上recv go标准库的rpc 问题和改进思路 zmq和nanomsg zmq的作者之一后来自立门户, 创立了nanomsg https://nanomsg.org/documentation-zeromq.html 上面文章详细写了nanomsg和zmq的不同点. 总的来说, zmq的作者之一对2008年当时的实现有所反思, 然后在实现方式上有所改进, 但整体思想是有延续性的, 比如都是以lib形式提供的. 特别的, nanomsg是MIT的license nanomsg支持的socket类型 PAIR - simple one-to-one communication BUS - simple many-to-many communication REQREP - allows to build clusters of stateless services to process user requests PUBSUB - distributes messages to large sets of interested subscribers PIPELINE - aggregates messages from multiple sources and load balances them among many destinations SURVEY - allows to query state of multiple applications in a single go nanomsg的transport类型 INPROC - transport within a process (between threads, modules etc.) IPC - transport between processes on a single machine TCP - network transport via TCP WS - websockets over TCP nanomsg和nng nng又是nanomsg的改版: https://github.com/nanomsg/nng 看起来不错: https://github.com/nanomsg nng的纯go版本 https://github.com/nanomsg/mangos 目前是v3版本 看起来完成度不错. 高级REQ-REP REP都有envelop, envelop其实就是加\"报文头\". We already looked briefly at multipart messages. Let’s now look at a major use case, which is reply message envelopes. An envelope is a way of safely packaging up data with an address, without touching the data itself. By separating reply addresses into an envelope we make it possible to write general purpose intermediaries such as APIs and proxies that create, read, and remove addresses no matter what the message payload or structure is. In the request-reply pattern, the envelope holds the return address for replies. It is how a ZeroMQ network with no state can create round-trip request-reply dialogs. When you use REQ and REP sockets you don’t even see envelopes; these sockets deal with them automatically. But for most of the interesting request-reply patterns, you’ll want to understand envelopes and particularly ROUTER sockets. We’ll work through this step-by-step. zmq用多frame描述一个msg, 地址和data在不同的frame中. The ZeroMQ reply envelope formally consists of zero or more reply addresses, followed by an empty frame (the envelope delimiter), followed by the message body (zero or more frames). The envelope is created by multiple sockets working together in a chain. We’ll break this down. We’ll start by sending “Hello” through a REQ socket. The REQ socket creates the simplest possible reply envelope, which has no addresses, just an empty delimiter frame and the message frame containing the “Hello” string. This is a two-frame message. 简单的REQ-REP没地址, 但有个空的分隔frame. 第一个数字是字节数zmq会把前面的envelop头剥掉, 只传递data frame给应用层.every request and every reply is in fact two frames, an empty frame and then the body 经过ROUTER会加\"address\" frame 这里的address是用来识别connection的. 因为router面对的是多个connection. 地址的生成 The ROUTER socket invents a random identity for each connection with which it works. If there are three REQ sockets connected to a ROUTER socket, it will invent three random identities, one for each REQ socket. router的socket会给每个连接生成随机的标识符. 比如ABC. router内部用map来跟踪这个标识符和connection的对应关系.然后这三个frame被发到DEALER socket出去. 从而REP的socket也收到这3个frame.REP的应用层不关心这个标识frame, 所以zmq暂存这个标识, 剥掉前两个frame, 只返回Hello给应用层.应用层处理后, zmq把保存的frame重新包装回frame, 回去还是3个frame.为什么不用底层socket的地址做为标识? 这样就不用生成随机的标识了呀???? 是安全性考虑吗? Router再把标识符剥掉, 传给REQ DEALER还是把3个frame都给ROUTER, ROUTER查表得到connection, 剥掉标识符那个frame, 只发2个frame给REQ socket. ROUTER sockets don’t care about the whole envelope. They don’t know anything about the empty delimiter. All they care about is that one identity frame that lets them figure out which connection to send a message to. 总结 The REQ socket sends, to the network, an empty delimiter frame in front of the message data. REQ sockets are synchronous. REQ sockets always send one request and then wait for one reply. REQ sockets talk to one peer at a time. If you connect a REQ socket to multiple peers, requests are distributed to and replies expected from each peer one turn at a time. The REP socket reads and saves all identity frames up to and including the empty delimiter, then passes the following frame or frames to the caller. REP sockets are synchronous and talk to one peer at a time. If you connect a REP socket to multiple peers, requests are read from peers in fair fashion, and replies are always sent to the same peer that made the last request. The DEALER socket is oblivious to the reply envelope and handles this like any multipart message. DEALER sockets are asynchronous and like PUSH and PULL combined. They distribute sent messages among all connections, and fair-queue received messages from all connections. The ROUTER socket is oblivious to the reply envelope, like DEALER. It creates identities for its connections, and passes these identities to the caller as a first frame in any received message. Conversely, when the caller sends a message, it uses the first message frame as an identity to look up the connection to send to. ROUTERS are asynchronous. 搭配 详见: https://zguide.zeromq.org/docs/chapter3/#Request-Reply-Combinations 合法的搭配 REQ to REP DEALER to REP REQ to ROUTER DEALER to ROUTER DEALER to DEALER ROUTER to ROUTER不合法的搭配 REQ to REQ REQ to DEALER REP to REP REP to ROUTER总结 DEALER is like an asynchronous REQ socket, and ROUTER is like an asynchronous REP socket. Where we use a REQ socket, we can use a DEALER; we just have to read and write the envelope ourselves. Where we use a REP socket, we can stick a ROUTER; we just need to manage the identities ourselves. Think of REQ and DEALER sockets as “clients” and REP and ROUTER sockets as “servers”. zmq使用 libzmq是zmq的核心lib server端用zmq_bind()绑定到\"well known\"的地址 client端从是不固定的, 动态的地址zmq_connect()到server 所有的zmq socket都是在zmq的context里面. zmq的context是一个IO线程池加底层socket的合体. API void *zmq_init (int io_threads)可以来配置io thread的个数. transport类型 transport用下面的格式来说明: transport://地址 线程间 inproc://名字 实际上是直接线程间内存传递, 此时zmq没有IO thread参与, 即void *zmq_init (int io_threads);可以传0. 其他的transport需要背景io线程至少一个. // Assign the in-process name \"#1\" rc = zmq_bind(socket, \"inproc://#1\"); assert (rc == 0); // Assign the in-process name \"my-endpoint\" rc = zmq_bind(socket, \"inproc://my-endpoint\"); assert (rc == 0); // Connect to the in-process name \"#1\" rc = zmq_connect(socket, \"inproc://#1\"); assert (rc == 0); // Connect to the in-process name \"my-endpoint\" rc = zmq_connect(socket, \"inproc://my-endpoint\"); assert (rc == 0); 不支持disconnected模式, 即一定要server先起. 可能后面版本会支持 进程间 ipc://路径 实际上是unix domain socket // Assign the pathname \"/tmp/feeds/0\" rc = zmq_bind(socket, \"ipc:///tmp/feeds/0\"); assert (rc == 0); // Connect to the pathname \"/tmp/feeds/0\" rc = zmq_connect(socket, \"ipc:///tmp/feeds/0\"); assert (rc == 0); 也支持disconnected模式. 即server后起起, client先起, 但还是连的上. TCP 对tcp的地址做了简化封装 // TCP port 5555 on all available interfaces rc = zmq_bind(socket, \"tcp:/// :5555\"); assert (rc == 0); // TCP port 5555 on the local loop-back interface on all platforms rc = zmq_bind(socket, \"tcp://127.0.0.1:5555\"); assert (rc == 0); // TCP port 5555 on the first Ethernet network interface on Linux rc = zmq_bind(socket, \"tcp://eth0:5555\"); assert (rc == 0); // Connecting using an IP address rc = zmq_connect(socket, \"tcp://192.168.1.1:5555\"); assert (rc == 0); // Connecting using a DNS name rc = zmq_connect(socket, \"tcp://server1:5555\"); assert (rc == 0); tcp类型的transport支持disconnected模式. 即server后起起, client先起, 但还是连的上. pgm PGM (Pragmatic General Multicast) is a protocol for reliable multicast transport of data over IP networks. zmq支持两种pgm: pgm: 基于raw IP epgm: 基于UDP pgm的socket只能用于PUB SUB模式 pgm的地址格式有点长: interface名;多播地址;端口// Connecting to the multicast address 239.192.1.1, port 5555, // using the first Ethernet network interface on Linux // and the Encapsulated PGM protocol rc = zmq_connect(socket, \"epgm://eth0;239.192.1.1:5555\"); assert (rc == 0); // Connecting to the multicast address 239.192.1.1, port 5555, // using the network interface with the address 192.168.1.1 // and the standard PGM protocol rc = zmq_connect(socket, \"pgm://192.168.1.1;239.192.1.1:5555\"); assert (rc == 0); zmq socket 一个zmq socket可以同时有多个incoming和outgoing连接 zmq的socket没有accept概念: 一个bind了的socket, 自动会accept连接 背景io线程会处理原始连接, 自动断线重连 app层不应该关心原始socket, 实际上app层也不能直接使用原始socket server和client server代表一个不变的组件. server有well-konwn地址, 用zmq_bind() client更多的是动态的. 用zmq_connect()来连server zmq会缓存message. client和server可以任意顺序启动. server可以bind多中transport类型, 比如tcp和进程间, 线程间都可以混在一起bind到一个zmq的socket上zmq_bind (socket, \"tcp://*:5555\"); zmq_bind (socket, \"tcp://*:9999\"); zmq_bind (socket, \"inproc://somename\"); 但同一个transport一般不能bind两次, 比如tcp, 肯定是端口重复. zmq socket类型 socket的类型决定了交互模式, 路由方式, 缓存策略等等. 是zmq对交互模式的归类和抽象 zmq的send和recv和tcp的不同 zmq的message有边界的, 就像UDP; 而tcp是stream式的. zmq有后台IO线程, message先进local的input queue; 发送的message也是从local 的output queue里面来的. zmq的socket可以1发多, 即1:N的多播. zmq_send()只是把message发送到queue里, 后台的IO线程会异步的发送这个message. 除非特别情况, 这个API不会阻塞 zmq_msg_send()后面还要研究一下 zmq的message framing zmq内部对wire上的数据是做了按size分割的. 是类UDP的设计. 所以recv的API被设计成要传入buffer size的格式: int zmq_send (void *socket, void *buf, size_t len, int flags) int zmq_recv (void *socket, void *buf, size_t len, int flags) 那么zmq_recv会根据用户提供的buf size来\"截断\"消息. 这个用起来就不地道. 所以zmq提供了另外两个API, 这里面就没有size, 每次调用都是一个完整的msg int zmq_msg_send (zmq_msg_t *msg, void *socket, int flags) int zmq_msg_recv (zmq_msg_t *msg, void *socket, int flags) 实际上, 对msg的操作是一系列的API Initialise a message: zmq_msg_init(), zmq_msg_init_size(), zmq_msg_init_data(). Sending and receiving a message: zmq_msg_send(), zmq_msg_recv(). Release a message: zmq_msg_close(). Access message content: zmq_msg_data(), zmq_msg_size(), zmq_msg_more(). Work with message properties: zmq_msg_get(), zmq_msg_set(). Message manipulation: zmq_msg_copy(), zmq_msg_move(). 用户要自己选择合适的\"纯数据\"的表现形式, 比如用gpb/json来序列化等等, 这个zmq不管. 使用msg的要点: You create and pass around zmq_msg_t objects, not blocks of data. To read a message, you use zmq_msg_init() to create an empty message, and then you pass that to zmq_msg_recv(). To write a message from new data, you use zmq_msg_init_size() to create a message and at the same time allocate a block of data of some size. You then fill that data using memcpy, and pass the message to zmq_msg_send(). To release (not destroy) a message, you call zmq_msg_close(). This drops a reference, and eventually ZeroMQ will destroy the message. To access the message content, you use zmq_msg_data(). To know how much data the message contains, use zmq_msg_size(). Do not use zmq_msg_move(), zmq_msg_copy(), or zmq_msg_init_data() unless you read the man pages and know precisely why you need these. After you pass a message to zmq_msg_send(), ØMQ will clear the message, i.e., set the size to zero. You cannot send the same message twice, and you cannot access the message data after sending it. 需要的话, 用zmq_msg_copy()增加引用, 但实际并不拷贝msg内容. 最后一个引用被send成功后, msg会被自动销毁. These rules don’t apply if you use zmq_send() and zmq_recv(), to which you pass byte arrays, not message structures. Frames zmq定义了自己的frame格式. frame是msg的基本承载单元. 一个msg可以包括多个frame. frame的size是确定的 frame的定义在protocol called ZMTP The ZeroMQ Message Transport Protocol (ZMTP) is a transport layer protocol for exchanging messages between two peers over a connected transport layer such as TCP. This document describes ZMTP/2.0. ZMTP delimits the TCP stream as ‘frames’. A message can consist of multiple frames, for purposes of structuring. A frame consists of a flags field, followed by a length field and a frame body of length octets. The length does not include the flags field, nor itself, so an empty frame has a length of zero. Bit 0 (MORE): More frames to follow. A value of 0 indicates that there are no more frames to follow. A value of 1 indicates that more frames will follow. On messages consisting of a single frame the MORE bit MUST be 0. Bit 1 (LONG): Long message. A value of 0 indicates that the message length is encoded as a single octet. A value of 1 indicates that the message length is encoded as a 64-bit unsigned integer in network byte order. Bits 2-7: Reserved. Bits 2-7 are reserved for future use and MUST be zero. The following diagram shows the layout of a final frame with a length of 0 to 255 octets: +-----------------+ Octet 0 | 0 0 0 0 0 0 0 0 | +-----------------+ Octet 1 | Length | +-----------------+- ... -----------------+ Octets 2+ | Body Length octets | +------------------- ... -----------------+ The following diagram shows the layout of a final LONG frame: +-----------------+ Octet 0 | 0 0 0 0 0 0 1 0 | +-----------------+ Octets 1-8 | Length 8 octets | +------------------ ... ------------------+ Octets 9+ | Body Length octets | +------------------ ... ------------------+ 概念点: A message can be one or more parts. These parts are also called “frames”. Each part is a zmq_msg_t object. You send and receive each part separately, in the low-level API. Higher-level APIs provide wrappers to send entire multipart messages. 使用要点: You may send zero-length messages, e.g., for sending a signal from one thread to another. ZeroMQ guarantees to deliver all the parts (one or more) for a message, or none of them. ZeroMQ does not send the message (single or multipart) right away, but at some indeterminate later time. A multipart message must therefore fit in memory. A message (single or multipart) must fit in memory. If you want to send files of arbitrary sizes, you should break them into pieces and send each piece as separate single-part messages. Using multipart data will not reduce memory consumption. You must call zmq_msg_close() when finished with a received message, in languages that don’t automatically destroy objects when a scope closes. You don’t call this method after sending a message. 兼容性 PAIR accepts connections from PAIR. PUB accepts connections from SUB. SUB accepts connections from PUB. REQ accepts connections from REP or ROUTER. REP accepts connections from REQ or DEALER. DEALER accepts connections from REP, DEALER, or ROUTER. ROUTER accepts connections from REQ, DEALER, or ROUTER. PULL accepts connections from PUSH. PUSH accepts connections from PULL. IO thread和context IO thread从属于context. 一个context创建的时候, 就默认启动了一个IO thread. 其后从属于context创建的socket们, 共同使用这个IO thread.有个apizmq_ctx_set (context, ZMQ_IO_THREADS, io_threads);可以更改io thread的个数. zmq_poll()函数 如果一个zmq的socket有多个对端, 我们想同时读怎么办?一般用zmq_poll(), 比如把poll包装在框架里, 框架分发event, 应用侧代码只负责react. 用户需要自己实现基于zmq的事件驱动框架, zmq不提供这个框架. An even better way might be to wrap zmq_poll() in a framework that turns it into a nice event-driven reactor, but it’s significantly more work than we want to cover here. 不用poll // // Reading from multiple sockets // This version uses a simple recv loop // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"time\" ) func main() { context, _ := zmq.NewContext() defer context.Close() // Connect to task ventilator receiver, _ := context.NewSocket(zmq.PULL) defer receiver.Close() receiver.Connect(\"tcp://localhost:5557\") // Connect to weather server subscriber, _ := context.NewSocket(zmq.SUB) defer subscriber.Close() subscriber.Connect(\"tcp://localhost:5556\") subscriber.SetSubscribe(\"10001\") // Process messages from both sockets // We prioritize traffic from the task ventilator for { // ventilator for b, _ := receiver.Recv(zmq.NOBLOCK); b != nil; { // fake process task } // weather server for b, _ := subscriber.Recv(zmq.NOBLOCK); b != nil; { // process task fmt.Printf(\"found weather =%s\\n\", string(b)) } // No activity, so sleep for 1 msec time.Sleep(1e6) } fmt.Println(\"done\") } 在循环里非阻塞读, 每轮傻傻的sleep 1ms. 使用poll // // Reading from multiple sockets // This version uses zmq.Poll() // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" ) func main() { context, _ := zmq.NewContext() defer context.Close() // Connect to task ventilator receiver, _ := context.NewSocket(zmq.PULL) defer receiver.Close() receiver.Connect(\"tcp://localhost:5557\") // Connect to weather server subscriber, _ := context.NewSocket(zmq.SUB) defer subscriber.Close() subscriber.Connect(\"tcp://localhost:5556\") subscriber.SetSubscribe(\"10001\") pi := zmq.PollItems{ zmq.PollItem{Socket: receiver, Events: zmq.POLLIN}, zmq.PollItem{Socket: subscriber, Events: zmq.POLLIN}, } // Process messages from both sockets for { _, _ = zmq.Poll(pi, -1) switch { case pi[0].REvents&zmq.POLLIN != 0: // Process task pi[0].Socket.Recv(0) // eat the incoming message case pi[1].REvents&zmq.POLLIN != 0: // Process weather update pi[1].Socket.Recv(0) // eat the incoming message } } fmt.Println(\"done\") } 添加poll item, 不用傻sleep 分片的msg 没仔细看, 详见: https://zguide.zeromq.org/docs/chapter2/#Multipart-Messages 发送方: zmq_msg_send (&message, socket, ZMQ_SNDMORE); ... zmq_msg_send (&message, socket, ZMQ_SNDMORE); ... zmq_msg_send (&message, socket, 0); 接收方: while (1) { zmq_msg_t message; zmq_msg_init (&message); zmq_msg_recv (&message, socket, 0); // Process the message frame ... zmq_msg_close (&message); if (!zmq_msg_more (&message)) break; // Last message frame } 服务发现 服务发现解决的是网络主体间如何知道对方的问题. hard code或者静态配置式的都不算是服务发现. 因为你在写静态配置的时候, 已经知道你的网络的样子了. 比如下面静态配置的网络: 如果简单的增加subscriber是简单的, 每个subscriber写死192.168.55.210:5556就行了.但如果增加publisher呢? 原来的subscriber也要改吗? 用下面的图来解决这个问题: 增加中间人. 这是典型的M:N到M: 1 :N的解耦 那为什么zmq不退出一个默认的中心式的broker? 让网络一开始就是星形的中心结构? 作者对broker是持有谨慎的态度的, 一方面是担心性能, 一方面担心担心故障 You might wonder, if all networks eventually get large enough to need intermediaries, why don’t we simply have a message broker in place for all applications? For beginners, it’s a fair compromise. Just always use a star topology, forget about performance, and things will usually work. However, message brokers are greedy things; in their role as central intermediaries, they become too complex, too stateful, and eventually a problem. 最后抽象的典型模式是: 多对多的client和server 比如每个client连接所有的server, servers都提供同一个服务, 以至于client和哪个server请求, 都是回一样的数据. 多个server扩展了网络的处理能力, 但这样要求每个client都知道所有的server. 可以演化为这种模式:中间人会记住哪个client来的req, 当某个server给出对这个req的回复的时候, 中间人会找到之前记住的client来沿路返回rep. zmq把中间这个转发逻辑抽象成API: int zmq_proxy (const void *frontend, const void *backend, const void *capture) 其他模型 并发支持 简单来说, zmq做了你能想到的所有对并发的支持: we don’t need mutexes, locks, or any other form of inter-thread communication except messages sent across ZeroMQ sockets 作者的基本思想是: 不要共享任何东西, stateless就是最好的并发模型 -- 因为什么都不需要保护 Isolate data privately within its thread and never share data in multiple threads. The only exception to this are ZeroMQ contexts, which are threadsafe.不要共享数据... Stay away from the classic concurrency mechanisms like as mutexes, critical sections, semaphores, etc. These are an anti-pattern in ZeroMQ applications.不要用锁啥的. 和zmq八字不合. Create one ZeroMQ context at the start of your process, and pass that to all threads that you want to connect via inproc sockets. Use attached threads to create structure within your application, and connect these to their parent threads using PAIR sockets over inproc. The pattern is: bind parent socket, then create child thread which connects its socket.在线程间使用inproc socket pair Use detached threads to simulate independent tasks, with their own contexts. Connect these over tcp. Later you can move these to stand-alone processes without changing the code significantly. All interaction between threads happens as ZeroMQ messages, which you can define more or less formally.和go一样, 使用\"通信\"来代替\"共享\" Don’t share ZeroMQ sockets between threads. ZeroMQ sockets are not threadsafe. Technically it’s possible to migrate a socket from one thread to another but it demands skill. The only place where it’s remotely sane to share sockets between threads are in language bindings that need to do magic like garbage collection on sockets.虽然context是并发安全的. 但socket不是. 不要在线程间共享socket. Do not use or close sockets except in the thread that created them.除了创建socket的线程, 不要在另外的线程使用和关闭socket. zmq的应用代码可以run在线程, 进程, 或node上. 通过zmq的抽象, 应用代码面对的是通信编程, 可以方便的在各种环境中扩展. 比如要实现下面的模块: 虚框内是一个进程 // Multithreaded Hello World server. // Uses Goroutines. We could also use channels (a native form of // inproc), but I stuck to the example. // // Author: Brendan Mc. // Requires: http://github.com/alecthomas/gozmq package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"time\" ) func main() { // Launch pool of worker threads for i := 0; i != 5; i = i + 1 { go worker() } // Prepare our context and sockets context, _ := zmq.NewContext() defer context.Close() // Socket to talk to clients clients, _ := context.NewSocket(zmq.ROUTER) //和clients连 defer clients.Close() clients.Bind(\"tcp://*:5555\") // Socket to talk to workers workers, _ := context.NewSocket(zmq.DEALER) defer workers.Close() workers.Bind(\"ipc://workers.ipc\") //应该是unix socket // connect work threads to client threads via a queue zmq.Device(zmq.QUEUE, clients, workers) //这个后面要看一下 } func worker() { context, _ := zmq.NewContext() defer context.Close() // Socket to talk to dispatcher receiver, _ := context.NewSocket(zmq.REP) defer receiver.Close() receiver.Connect(\"ipc://workers.ipc\") for true { received, _ := receiver.Recv(0) fmt.Printf(\"Received request [%s]\\n\", received) // Do some 'work' time.Sleep(time.Second) // Send reply back to client receiver.Send([]byte(\"World\"), 0) } } 解释 The server starts a set of worker threads. Each worker thread creates a REP socket and then processes requests on this socket. Worker threads are just like single-threaded servers. The only differences are the transport (inproc instead of tcp), and the bind-connect direction. The server creates a ROUTER socket to talk to clients and binds this to its external interface (over tcp). The server creates a DEALER socket to talk to the workers and binds this to its internal interface (over inproc). The server starts a proxy that connects the two sockets. The proxy pulls incoming requests fairly from all clients, and distributes those out to workers. It also routes replies back to their origin. pub到已知数量的subscriber 已知subscriber数量, publisher通过REP类型的socket等待所有订阅者连上来(1,2步), 然后发消息(第3步)发布者代码: // Synchronized publisher // // Author: Brendan Mc. // Requires: http://github.com/alecthomas/gozmq package main import ( zmq \"github.com/alecthomas/gozmq\" ) var subsExpected = 10 func main() { context, _ := zmq.NewContext() defer context.Close() // Socket to talk to clients publisher, _ := context.NewSocket(zmq.PUB) defer publisher.Close() publisher.Bind(\"tcp://*:5561\") // Socket to receive signals syncservice, _ := context.NewSocket(zmq.REP) defer syncservice.Close() syncservice.Bind(\"tcp://*:5562\") // Get synchronization from subscribers for i := 0; i 订阅者代码 // Synchronized subscriber // // Author: Aleksandar Janicijevic // Requires: http://github.com/alecthomas/gozmq package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"time\" ) func main() { context, _ := zmq.NewContext() defer context.Close() subscriber, _ := context.NewSocket(zmq.SUB) defer subscriber.Close() subscriber.Connect(\"tcp://localhost:5561\") subscriber.SetSubscribe(\"\") // 0MQ is so fast, we need to wait a while... time.Sleep(time.Second) // Second, synchronize with publisher syncclient, _ := context.NewSocket(zmq.REQ) defer syncclient.Close() syncclient.Connect(\"tcp://localhost:5562\") // - send a synchronization request fmt.Println(\"Send synchronization request\") syncclient.Send([]byte(\"\"), 0) fmt.Println(\"Wait for synchronization reply\") // - wait for synchronization reply syncclient.Recv(0) fmt.Println(\"Get updates\") // Third, get our updates and report how many we got update_nbr := 0 for { reply, _ := subscriber.Recv(0) if string(reply) == \"END\" { break } update_nbr++ } fmt.Printf(\"Received %d updates\\n\", update_nbr) } 用shell起10个订阅者进程, 然后再起发布者进程. echo \"Starting subscribers...\" for ((a=0; a 结果 Starting subscribers... Starting publisher... Received 1000000 updates Received 1000000 updates ... Received 1000000 updates Received 1000000 updates 零拷贝 这里的拷贝是指应用层的发送buffer到zmq的socket之间的拷贝. 零拷贝指发送的时候, 应用层的buffer直接用于zmq的发送, 而不是先拷贝到zmq的buffer, 再发送. To do zero-copy, you use zmq_msg_init_data() to create a message that refers to a block of data already allocated with malloc() or some other allocator, and then you pass that to zmq_msg_send(). When you create the message, you also pass a function that ZeroMQ will call to free the block of data, when it has finished sending the message. This is the simplest example, assuming buffer is a block of 1,000 bytes allocated on the heap: 代码如下: void my_free (void *data, void *hint) { free (data); } // Send message from buffer, which we allocate and ZeroMQ will free for us zmq_msg_t message; zmq_msg_init_data (&message, buffer, 1000, my_free, NULL); zmq_msg_send (&message, socket, 0); Note that you don’t call zmq_msg_close() after sending a message–libzmq will do this automatically when it’s actually done sending the message. pub-sub实际上是msg filter 订阅系统实际上做的是string的prefix match: 在数据里搜索prefix, 匹配到了就是命中订阅. 订阅的filter发生在发送侧.但问题是, 比如prefix是xyz, 如果\"纯数据\"种也有xyz怎么办? -- 用zmq的key和data分离 因为这个prefix的匹配会在msg内搜索, 但不会跨frame. --那么第一个frame来发key, 后面的frame发data就可以了. 这样prefix只会在第一个frame种匹配发布方: // // Pubsub envelope publisher // package main import ( zmq \"github.com/alecthomas/gozmq\" \"time\" ) func main() { context, _ := zmq.NewContext() defer context.Close() publisher, _ := context.NewSocket(zmq.PUB) defer publisher.Close() publisher.Bind(\"tcp://*:5563\") for { publisher.SendMultipart([][]byte{[]byte(\"A\"), []byte(\"We don't want to see this\")}, 0) publisher.SendMultipart([][]byte{[]byte(\"B\"), []byte(\"We would like to see this\")}, 0) time.Sleep(time.Second) } } 订阅方: // // Pubsub envelope subscriber // package main import ( zmq \"github.com/alecthomas/gozmq\" ) func main() { context, _ := zmq.NewContext() defer context.Close() subscriber, _ := context.NewSocket(zmq.SUB) defer subscriber.Close() subscriber.Connect(\"tcp://localhost:5563\") subscriber.SetSubscribe(\"B\") for { address, _ := subscriber.Recv(0) content, _ := subscriber.Recv(0) print(\"[\" + string(address) + \"] \" + string(content) + \"\\n\") } } 如果此时加新需求: 增加多个发布者, 那么可以设计交互格式为: 高水位 作者对于flow control有经典的描述, 详见: https://zguide.zeromq.org/docs/chapter2/#High-Water-Marks作者认为高水线后, 向app发送反压\"stop\"是不好的. 如果A高频率给B发, B由于gc或者CPU load高处理不了, 消息会在这个通信系统上堆积, A的应用侧也会有消息堆积. ZeroMQ uses the concept of HWM (high-water mark) to define the capacity of its internal pipes. Each connection out of a socket or into a socket has its own pipe, and HWM for sending, and/or receiving, depending on the socket type. Some sockets (PUB, PUSH) only have send buffers. Some (SUB, PULL, REQ, REP) only have receive buffers. Some (DEALER, ROUTER, PAIR) have both send and receive buffers.In ZeroMQ v3.x, it’s set to 1,000 by defaultzmq内部也使用高水线定义buffer的容量. 默认是10000个msg. When your socket reaches its HWM, it will either block or drop data depending on the socket type. PUB and ROUTER sockets will drop data if they reach their HWM, while other socket types will block. Over the inproc transport, the sender and receiver share the same buffers, so the real HWM is the sum of the HWM set by both sides.到达高水线后, 不同的socket type的策略不同: PUB和ROUTER类型的是丢弃; 而其他类型是阻塞. 丢包怎么定位? 见这里: https://zguide.zeromq.org/docs/chapter2/#Missing-Message-Problem-Solver要点: On SUB sockets, set a subscription using zmq_setsockopt()ZMQ_SUBSCRIBE, or you won’t get messages. Because you subscribe to messages by prefix, if you subscribe to \"” (an empty subscription), you will get everything. If you start the SUB socket (i.e., establish a connection to a PUB socket) after the PUB socket has started sending out data, you will lose whatever it published before the connection was made. If this is a problem, set up your architecture so the SUB socket starts first, then the PUB socket starts publishing. Even if you synchronize a SUB and PUB socket, you may still lose messages. It’s due to the fact that internal queues aren’t created until a connection is actually created. If you can switch the bind/connect direction so the SUB socket binds, and the PUB socket connects, you may find it works more as you’d expect. If you’re using REP and REQ sockets, and you’re not sticking to the synchronous send/recv/send/recv order, ZeroMQ will report errors, which you might ignore. Then, it would look like you’re losing messages. If you use REQ or REP, stick to the send/recv order, and always, in real code, check for errors on ZeroMQ calls. If you’re using PUSH sockets, you’ll find that the first PULL socket to connect will grab an unfair share of messages. The accurate rotation of messages only happens when all PULL sockets are successfully connected, which can take some milliseconds. As an alternative to PUSH/PULL, for lower data rates, consider using ROUTER/DEALER and the load balancing pattern. If you’re sharing sockets across threads, don’t. It will lead to random weirdness, and crashes. If you’re using inproc, make sure both sockets are in the same context. Otherwise the connecting side will in fact fail. Also, bind first, then connect. inproc is not a disconnected transport like tcp. If you’re using ROUTER sockets, it’s remarkably easy to lose messages by accident, by sending malformed identity frames (or forgetting to send an identity frame). In general setting the ZMQ_ROUTER_MANDATORY option on ROUTER sockets is a good idea, but do also check the return code on every send call. Lastly, if you really can’t figure out what’s going wrong, make a minimal test case that reproduces the problem, and ask for help from the ZeroMQ community. 消息交互模式 zmq文档中有详细解释 核心的交互模式有4种: Request-reply, which connects a set of clients to a set of services. This is a remote procedure call and task distribution pattern. rpc或通常的client server Pub-sub, which connects a set of publishers to a set of subscribers. This is a data distribution pattern. 订阅-发布 Pipeline, which connects nodes in a fan-out/fan-in pattern that can have multiple steps and loops. This is a parallel task distribution and collection pattern. 这就是例子push-pull Exclusive pair, which connects two sockets exclusively. This is a pattern for connecting two threads in a process, not to be confused with “normal” pairs of sockets. 线程间的socket对. socket类型配对: PUB and SUB REQ and REP REQ and ROUTER (take care, REQ inserts an extra null frame) DEALER and REP (take care, REP assumes a null frame) DEALER and ROUTER DEALER and DEALER ROUTER and ROUTER PUSH and PULL PAIR and PAIR PAIR类型 对于线程间的\"共享\", zmq的策略是不要用锁啥的, 用zmq的通信机制. use PAIR sockets over the inproc transport 比如下面这个模型: // Multithreaded relay. // Uses Goroutines. We could also use channels (a native form of // inproc), but I stuck to the example. // // Author: Brendan Mc. // Requires: http://github.com/alecthomas/gozmq package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" ) func main() { // Prepare our context and sockets context, _ := zmq.NewContext() defer context.Close() // Bind inproc socket before starting step2 receiver, _ := context.NewSocket(zmq.PAIR) defer receiver.Close() receiver.Bind(\"ipc://step3.ipc\") go step2() // Wait for signal receiver.Recv(0) fmt.Println(\"Test successful!\") } func step1() { // Connect to step2 and tell it we're ready context, _ := zmq.NewContext() defer context.Close() xmitter, _ := context.NewSocket(zmq.PAIR) defer xmitter.Close() xmitter.Connect(\"ipc://step2.ipc\") fmt.Println(\"Step 1 ready, signaling step 2\") xmitter.Send([]byte(\"READY\"), 0) } func step2() { context, _ := zmq.NewContext() defer context.Close() // Bind inproc before starting step 1 receiver, _ := context.NewSocket(zmq.PAIR) defer receiver.Close() receiver.Bind(\"ipc://step2.ipc\") go step1() // wait for signal and pass it on receiver.Recv(0) // Connect to step3 and tell it we're ready xmitter, _ := context.NewSocket(zmq.PAIR) defer xmitter.Close() xmitter.Connect(\"ipc://step3.ipc\") fmt.Println(\"Step 2 ready, singaling step 3\") xmitter.Send([]byte(\"READY\"), 0) } 注意这里的go的例子, 用的是ipc的transport类型, 而实际上, 这里要表达的是inproc的transport类型, 似乎这个文档的当时还不支持go的inproc transport? 下面是c版本: // Multithreaded relay #include \"zhelpers.h\" #include static void * step1 (void *context) { // Connect to step2 and tell it we're ready void *xmitter = zmq_socket (context, ZMQ_PAIR); zmq_connect (xmitter, \"inproc://step2\"); printf (\"Step 1 ready, signaling step 2\\n\"); s_send (xmitter, \"READY\"); zmq_close (xmitter); return NULL; } static void * step2 (void *context) { // Bind inproc socket before starting step1 void *receiver = zmq_socket (context, ZMQ_PAIR); zmq_bind (receiver, \"inproc://step2\"); pthread_t thread; pthread_create (&thread, NULL, step1, context); // Wait for signal and pass it on char *string = s_recv (receiver); free (string); zmq_close (receiver); // Connect to step3 and tell it we're ready void *xmitter = zmq_socket (context, ZMQ_PAIR); zmq_connect (xmitter, \"inproc://step3\"); printf (\"Step 2 ready, signaling step 3\\n\"); s_send (xmitter, \"READY\"); zmq_close (xmitter); return NULL; } int main (void) { void *context = zmq_ctx_new (); // Bind inproc socket before starting step2 void *receiver = zmq_socket (context, ZMQ_PAIR); zmq_bind (receiver, \"inproc://step3\"); pthread_t thread; pthread_create (&thread, NULL, step2, context); // Wait for signal char *string = s_recv (receiver); free (string); zmq_close (receiver); printf (\"Test successful!\\n\"); zmq_ctx_destroy (context); return 0; } 注意用inproc类型的transport的主要场景是低延迟, 高性能. 但这个类型的transport扩展性不好. 如果是用tcp或者是ipc, 多线程的模型可以很轻易的breakdown到多进程. 为社么要用pair You can use PUSH for the sender and PULL for the receiver. This looks simple and will work, but remember that PUSH will distribute messages to all available receivers. If you by accident start two receivers (e.g., you already have one running and you start a second), you’ll “lose” half of your signals. PAIR has the advantage of refusing more than one connection; the pair is exclusive. You can use DEALER for the sender and ROUTER for the receiver. ROUTER, however, wraps your message in an “envelope”, meaning your zero-size signal turns into a multipart message. If you don’t care about the data and treat anything as a valid signal, and if you don’t read more than once from the socket, that won’t matter. If, however, you decide to send real data, you will suddenly find ROUTER providing you with “wrong” messages. DEALER also distributes outgoing messages, giving the same risk as PUSH. You can use PUB for the sender and SUB for the receiver. This will correctly deliver your messages exactly as you sent them and PUB does not distribute as PUSH or DEALER do. However, you need to configure the subscriber with an empty subscription, which is annoying. req-rep 典型的rpc模式或最常用的client-server模式, 同步的. 按作者的说法是, lock-step:从示例代码来看 server端 // // Hello World Zeromq server // // Author: Aaron Raddon github.com/araddon // Requires: http://github.com/alecthomas/gozmq // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"time\" ) func main() { context, _ := zmq.NewContext() socket, _ := context.NewSocket(zmq.REP) //REP类型, server侧 defer context.Close() defer socket.Close() socket.Bind(\"tcp://*:5555\") // Wait for messages for { msg, _ := socket.Recv(0) println(\"Received \", string(msg)) // do some fake \"work\" time.Sleep(time.Second) // send reply back to client reply := fmt.Sprintf(\"World\") socket.Send([]byte(reply), 0) } } 从server的app角度看: server只\"监听\"tcp://*:5555的数据 zmq对app屏蔽了listen, accept过程, app看不到数据通道的socket. 那么在app看来, 它不知道对端client是谁. 很可能这次recv的数据, 和下次recv的数据, 都不是一个client发过来的.-- 所以作者提到lockstep: 本次recv的东西要马上处理, 马上send, 才能保证两次操作是对一个client. 这里zmq的示例代码是典型的同步结构.或者说是框架代码和app代码混在一起的.作者专门提到: Now this looks too simple to be realistic, but ZeroMQ sockets have, as we already learned, superpowers. You could throw thousands of clients at this server, all at once, and it would continue to work happily and quickly. For fun, try starting the client and then starting the server, see how it all still works, then think for a second what this means. 这个看起来简单的server代码, 可以同时支持上千个client发起请求; 即使先启动client, 再启动server, 还能正常工作. -- 似乎暗示了zmq为client侧提供的lib库有缓存msg功能. client端 // // Hello World Zeromq Client // // Author: Aaron Raddon github.com/araddon // Requires: http://github.com/alecthomas/gozmq // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" ) func main() { context, _ := zmq.NewContext() socket, _ := context.NewSocket(zmq.REQ) //REQ类型, client侧 defer context.Close() defer socket.Close() fmt.Printf(\"Connecting to hello world server...\") socket.Connect(\"tcp://localhost:5555\") for i := 0; i client的关键操作是socket.Connect(\"tcp://localhost:5555\"), 从此这个socket代表了和server的连接.说明从app的角度看: client是持有server连接信息的 client send后马上recv, 虽然不是rcp的call形式, 但这里必须是\"lockstep\"式的同步等. 因为过了这个点, app就不知道reply对应的是那个request. pub-sub 一个pub, 多个sub. 复制消息, 异步模式 subcriber需要用zmq_setsockopt()来订阅一个string关键词.sub的\"socket\"是只读的, pub的\"socket\"是只写的.虽然理论上client也可以pub, 但一般都是server pub, client来订阅.从下面作者的描述来看: A subscriber can connect to more than one publisher, using one connect call each time. Data will then arrive and be interleaved (“fair-queued”) so that no single publisher drowns out the others.一个subscriber可以从多个publisher来订阅. 从这个sub的\"socket\"读能读到所有订阅的消息. 从多个publisher来的消息交叉放置在socket里 If a publisher has no connected subscribers, then it will simply drop all messages.如果没有人订阅, publisher会丢弃所有消息. -- 说明订阅者是直接和发布者有真正的socket连接的, publisher都知道. If you’re using TCP and a subscriber is slow, messages will queue up on the publisher. We’ll look at how to protect publishers against this using the “high-water mark” later.如果是远端机器上的慢速subscriber, 那消息会在publisher侧拥塞. -- 更加说明了没有\"中间人\"来转发. From ZeroMQ v3.x, filtering happens at the publisher side when using a connected protocol (tcp或ipc). Using the epgm protocol, filtering happens at the subscriber side. In ZeroMQ v2.x, all filtering happened at the subscriber side.zmq的2.0版本, filtering发生在接收方; 而zmq3.0版本, filtering发生在发送方. server侧 // // Weather update server // Binds PUB socket to tcp://*:5556 // Publishes random weather updates // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"math/rand\" \"time\" ) func main() { context, _ := zmq.NewContext() socket, _ := context.NewSocket(zmq.PUB) //注意这里指明了PUB类型 defer context.Close() defer socket.Close() socket.Bind(\"tcp://*:5556\") socket.Bind(\"ipc://weather.ipc\") //似乎PUB类型能绑定多个地址? // Seed the random number generator rand.Seed(time.Now().UnixNano()) // loop for a while aparently for { // make values that will fool the boss zipcode := rand.Intn(100000) temperature := rand.Intn(215) - 80 relhumidity := rand.Intn(50) + 10 msg := fmt.Sprintf(\"%d %d %d\", zipcode, temperature, relhumidity) // Send message to all subscribers socket.Send([]byte(msg), 0) } } client侧 // // Weather proxy listens to weather server which is constantly // emitting weather data // Binds SUB socket to tcp://*:5556 // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"os\" \"strconv\" \"strings\" ) func main() { context, _ := zmq.NewContext() socket, _ := context.NewSocket(zmq.SUB) //SUB类型 defer context.Close() defer socket.Close() var temps []string var err error var temp int64 total_temp := 0 filter := \"59937\" // find zipcode if len(os.Args) > 1 { // ./wuclient 85678 filter = string(os.Args[1]) } // Subscribe to just one zipcode (whitefish MT 59937) fmt.Printf(\"Collecting updates from weather server for %s…\\n\", filter) socket.SetSubscribe(filter) //订阅实际就是filter socket.Connect(\"tcp://localhost:5556\") for i := 0; i push-pull zmq的push和pull模式用于生产 消费系统push生产, pull来消费. 可以一个push, 多个pull. 但很显然, push的消息不会复制广播给每个puller A ventilator that produces tasks that can be done in parallel A set of workers that process tasks A sink that collects results back from the worker processes push和pull没有固定的server client角色, client也可以pull, 也可以push. 这里的所有框框都是独立的进程. 生产者 ventilator // // Task ventilator // Binds PUSH socket to tcp://localhost:5557 // Sends batch of tasks to workers via that socket // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"math/rand\" \"time\" ) func main() { context, _ := zmq.NewContext() defer context.Close() // Socket to send messages On sender, _ := context.NewSocket(zmq.PUSH) defer sender.Close() sender.Bind(\"tcp://*:5557\") // Socket to send start of batch message on sink, _ := context.NewSocket(zmq.PUSH) defer sink.Close() sink.Connect(\"tcp://localhost:5558\") fmt.Print(\"Press Enter when the workers are ready: \") var line string fmt.Scanln(&line) fmt.Println(\"Sending tasks to workers…\") sink.Send([]byte(\"0\"), 0) // Seed the random number generator rand.Seed(time.Now().UnixNano()) total_msec := 0 for i := 0; i 消费者worker // // Task Wroker // Connects PULL socket to tcp://localhost:5557 // Collects workloads from ventilator via that socket // Connects PUSH socket to tcp://localhost:5558 // Sends results to sink via that socket // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"strconv\" \"time\" ) func main() { context, _ := zmq.NewContext() defer context.Close() // Socket to receive messages on receiver, _ := context.NewSocket(zmq.PULL) defer receiver.Close() receiver.Connect(\"tcp://localhost:5557\") // Socket to send messages to task sink sender, _ := context.NewSocket(zmq.PUSH) defer sender.Close() sender.Connect(\"tcp://localhost:5558\") // Process tasks forever for { msgbytes, _ := receiver.Recv(0) fmt.Printf(\"%s.\\n\", string(msgbytes)) // Do the work msec, _ := strconv.ParseInt(string(msgbytes), 10, 64) time.Sleep(time.Duration(msec) * 1e6) // Send results to sink sender.Send([]byte(\"\"), 0) } } sink // // Task sink // Binds PULL socket to tcp://localhost:5558 // Collects results from workers via that socket // package main import ( \"fmt\" zmq \"github.com/alecthomas/gozmq\" \"time\" ) func main() { context, _ := zmq.NewContext() defer context.Close() // Socket to receive messages on receiver, _ := context.NewSocket(zmq.PULL) defer receiver.Close() receiver.Bind(\"tcp://*:5558\") // Wait for start of batch msgbytes, _ := receiver.Recv(0) fmt.Println(\"Received Start Msg \", string(msgbytes)) // Start our clock now start_time := time.Now().UnixNano() // Process 100 confirmations for i := 0; i 总结 The workers connect upstream to the ventilator, and downstream to the sink. This means you can add workers arbitrarily. If the workers bound to their endpoints, you would need (a) more endpoints and (b) to modify the ventilator and/or the sink each time you added a worker. We say that the ventilator and sink are stable parts of our architecture and the workers are dynamic parts of it.worker对上对下都是client, 所以worker能够动态的添加. 而上下两层都是server, 架构上是稳定的. We have to synchronize the start of the batch with all workers being up and running. This is a fairly common gotcha in ZeroMQ and there is no easy solution. The zmq_connect method takes a certain time. So when a set of workers connect to the ventilator, the first one to successfully connect will get a whole load of messages in that short time while the others are also connecting. If you don’t synchronize the start of the batch somehow, the system won’t run in parallel at all. Try removing the wait in the ventilator, and see what happens.如果生产者ventilator不等所有worker连接成功, 那在第一个连上的worker会收到大量的work. 而其他的worker还在连接中. zmq的经验是, 建立连接需要ms级的时间, 这个时间内足够大量的消息交互了. -- 可能说的是跨机器跨网络的场景. The ventilator’s PUSH socket distributes tasks to workers (assuming they are all connected before the batch starts going out) evenly. This is called load balancing and it’s something we’ll look at again in more detail.load balance发生在生产者即ventilator里面 The sink’s PULL socket collects results from workers evenly. This is called fair-queuing.sinker收集结果的时候, 从每个worker公平的收集. 公平队列 context 上面go代码中, 首先要建个context, socket是从属于context的. zmq建议: 每个进程一个context. You should create and use exactly one context in your process. Technically, the context is the container for all sockets in a single process, and acts as the transport for inproc sockets, which are the fastest way to connect threads in one process. pipeline fanout 灵魂几问 https://zguide.zeromq.org/docs/chapter1/#Why-We-Needed-ZeroMQ摘要如下: 阻塞还是异步? 阻塞性能不佳, 异步很难搞对. How do we handle I/O? Does our application block, or do we handle I/O in the background? This is a key design decision. Blocking I/O creates architectures that do not scale well. But background I/O can be very hard to do right.How do we handle I/O? Does our application block, or do we handle I/O in the background? This is a key design decision. Blocking I/O creates architectures that do not scale well. But background I/O can be very hard to do right. 谁当server谁当client? 需要server永远在吗? 需要断线重连吗? 怎么在wire上表示一个message? 我觉得用结构体, 或者tengo中的Object抽象 如果对端没准备好的时候, 要发送的数据缓存在哪里? 拥塞控制策略? 消息丢失怎么办? 要保证送达吗? 如果有新的transport方法怎么办? app要改吗? 比如增加支持yipc? What if we need to use a different network transport. Say, multicast instead of TCP unicast? Or IPv6? Do we need to rewrite the applications, or is the transport abstracted in some layer? 消息怎么路由? How do we route messages? Can we send the same message to multiple peers? Can we send replies back to an original requester? 多语言怎么适配? encoding怎么选择? 网络错误怎么处理? 作者观点 消息处理框架不好搞. 很多人越高越复杂, 故障率越高. 这包括了broker概念的发明和广泛使用. 在带来方便的同时, broker也有很多问题, 比如broker系统本身需要人维护, 只适合大型系统.对中小型的系统开发来说, 通常的结局作者已经预言了: Either they avoid network programming and make monolithic applications that do not scale. Or they jump into network programming and make brittle, complex applications that are hard to maintain. Or they bet on a messaging product, and end up with scalable applications that depend on expensive, easily broken technology.不是自己瞎搞, 就是赌上一个新技术来瞎搞. 作者认为的理想消息系统: What we need is something that does the job of messaging, but does it in such a simple and cheap way that it can work in any application, with close to zero cost. It should be a library which you just link, without any other dependencies. No additional moving pieces, so no additional risk. It should run on any OS and work with any programming language. 只是个库, 简单, 高效, 没有其他的比如broker, 消息中心等独立实体. It handles I/O asynchronously, in background threads. These communicate with application threads using lock-free data structures, so concurrent ZeroMQ applications need no locks, semaphores, or other wait states.背景线程处理异步IO, 无锁设计. app层不关心锁 Components can come and go dynamically and ZeroMQ will automatically reconnect. This means you can start components in any order. You can create “service-oriented architectures” (SOAs) where services can join and leave the network at any time.自动重连 It queues messages automatically when needed. It does this intelligently, pushing messages as close as possible to the receiver before queuing them.自动缓存消息 It has ways of dealing with over-full queues (called “high water mark”). When a queue is full, ZeroMQ automatically blocks senders, or throws away messages, depending on the kind of messaging you are doing (the so-called “pattern”).拥塞时有考虑 It lets your applications talk to each other over arbitrary transports: TCP, multicast, in-process, inter-process. You don’t need to change your code to use a different transport.多transport支持: tcp, 多播, 进程间, 线程间 It handles slow/blocked readers safely, using different strategies that depend on the messaging pattern.还是关于拥塞的 It lets you route messages using a variety of patterns such as request-reply and pub-sub. These patterns are how you create the topology, the structure of your network.支持多模式, 比如req-rep, pub-sub等等 It lets you create proxies to queue, forward, or capture messages with a single call. Proxies can reduce the interconnection complexity of a network.支持proxy来做转发, 转存, 抓包等 It delivers whole messages exactly as they were sent, using a simple framing on the wire. If you write a 10k message, you will receive a 10k message.消息级别传输. 而不是报文级别 It does not impose any format on messages. They are blobs from zero to gigabytes large. When you want to represent data you choose some other product on top, such as msgpack, Google’s protocol buffers, and others.从zmq级别来看, zmq不对传输的数据做包装. It handles network errors intelligently, by retrying automatically in cases where it makes sense.对网络错误有一套处理 zmq底层是背景线程管理的\"连接\" In the ZeroMQ universe, sockets are doorways to fast little background communications engines that manage a whole set of connections automagically for you. You can’t see, work with, open, close, or attach state to these connections. Whether you use blocking send or receive, or poll, all you can talk to is the socket, not the connections it manages for you. The connections are private and invisible, and this is the key to ZeroMQ’s scalability. This is because your code, talking to a socket, can then handle any number of connections across whatever network protocols are around, without change. A messaging pattern sitting in ZeroMQ scales more cheaply than a messaging pattern sitting in your application code. zmq对app提供的socket不是原始的os级别socket, 而是os.socket+os.thread组成的一套系统. zero mq(zmq, 0mq) https://zeromq.org/get-started/ https://lwn.net/Articles/466304/ 概念: http://zguide.zeromq.org/page:all http://zguide.zeromq.org/page:chapter1 纯go实现zero mq https://github.com/zeromq/gomq https://github.com/go-zeromq/zmq4 cgo版本: https://github.com/zeromq/goczmq https://github.com/pebbe/zmq4 zero mq api: http://api.zeromq.org/2-1:zmq-connect 问题: REQ-REP模式下的RPC, 怎么把异步同步化的? 比如socket send后, 怎么等待socket recv? 最简单的send, 原地recv可以, 但并发场景呢? 我记得在哪里看到过, 思路有点像channel in channel. 但client方需要有个守护routine, 收到REP后, 给对应的goroutine发channel zmq使用最基本的模式, send后马上recv client代码 // Hello World client #include #include #include #include int main (void) { printf (\"Connecting to hello world server...\\n\"); void *context = zmq_ctx_new (); void *requester = zmq_socket (context, ZMQ_REQ); zmq_connect (requester, \"tcp://localhost:5555\"); int request_nbr; for (request_nbr = 0; request_nbr != 10; request_nbr++) { char buffer [10]; printf (\"Sending Hello %d...\\n\", request_nbr); //先send zmq_send (requester, \"Hello\", 5, 0); //原地recv zmq_recv (requester, buffer, 10, 0); printf (\"Received World %d\\n\", request_nbr); } zmq_close (requester); zmq_ctx_destroy (context); return 0; } server侧代码 // Hello World server #include #include #include #include #include int main (void) { // Socket to talk to clients void *context = zmq_ctx_new (); void *responder = zmq_socket (context, ZMQ_REP); int rc = zmq_bind (responder, \"tcp://*:5555\"); assert (rc == 0); while (1) { char buffer [10]; //先接收 zmq_recv (responder, buffer, 10, 0); printf (\"Received Hello\\n\"); sleep (1); // Do some 'work' //再发送 zmq_send (responder, \"World\", 5, 0); } return 0; } 在这个基本模式下, zmq不允许同时发REQ: The REQ-REP socket pair is in lockstep. The client issues zmq_send() and then zmq_recv(), in a loop (or once if that’s all it needs). Doing any other sequence (e.g., sending two messages in a row) will result in a return code of -1 from the send or recv call. go标准库的rpc 在src/net/rpc/client.go中, client支持同时多个REQ并发发送到server, 怎么做到的? // Client represents an RPC Client. // There may be multiple outstanding Calls associated // with a single Client, and a Client may be used by // multiple goroutines simultaneously. type Client struct { codec ClientCodec reqMutex sync.Mutex // protects following request Request mutex sync.Mutex // protects following //秘诀在seq和后面的pending map seq uint64 pending map[uint64]*Call closing bool // user has called Close shutdown bool // server has told us to stop } 在send的时候 func (client *Client) send(call *Call) { client.mutex.Lock() //在加锁的情况下, 把seq++并加到map中 seq := client.seq client.seq++ client.pending[seq] = call client.mutex.Unlock() //发送 client.request.Seq = seq client.codec.WriteRequest(&client.request, call.Args) } 这里面的Call是这次RPC调用的抽象: // Call represents an active RPC. type Call struct { ServiceMethod string // The name of the service and method to call. Args interface{} // The argument to the function (*struct). Reply interface{} // The reply from the function (*struct). Error error // After completion, the error status. Done chan *Call // Strobes when call is complete. } 好了, 下面看看client.Call // Call invokes the named function, waits for it to complete, and returns its error status. func (client *Client) Call(serviceMethod string, args interface{}, reply interface{}) error { call := Go函数里面, 实例化一个Call, 和一个channel, send这个Call给server, 返回这个channel. 注意, Go函数并不等待server的REP过来. // Go invokes the function asynchronously. It returns the Call structure representing // the invocation. The done channel will signal when the call is complete by returning // the same Call object. If done is nil, Go will allocate a new channel. // If non-nil, done must be buffered or Go will deliberately crash. func (client *Client) Go(serviceMethod string, args interface{}, reply interface{}, done chan *Call) *Call { call := new(Call) call.ServiceMethod = serviceMethod call.Args = args call.Reply = reply if done == nil { done = make(chan *Call, 10) // buffered. } else { // If caller passes done != nil, it must arrange that // done has enough buffer for the number of simultaneous // RPCs that will be using that channel. If the channel // is totally unbuffered, it's best not to run at all. if cap(done) == 0 { log.Panic(\"rpc: done channel is unbuffered\") } } call.Done = done client.send(call) return call } 等待发生在Go函数里面, 等待这个返回的channel. 好了, 但是哪里写这个channel呢? 每个client都启动了一个守护goroutine, 叫input // NewClientWithCodec is like NewClient but uses the specified // codec to encode requests and decode responses. func NewClientWithCodec(codec ClientCodec) *Client { client := &Client{ codec: codec, pending: make(map[uint64]*Call), } go client.input() return client } 前面说过了, client.Call函数只是调用了send, 然后等待在chennel上. 没有调用recv 是这个input routine不断的调用recv来收包的. func (client *Client) input() { for { response = Response{} //recv收header err = client.codec.ReadResponseHeader(&response) //包头里面由seq号 seq := response.Seq client.mutex.Lock() //根据seq号找到call channel call := client.pending[seq] delete(client.pending, seq) client.mutex.Unlock() //读Reply client.codec.ReadResponseBody(call.Reply) //这个函数写channel, 通知client.Call停止等待 call.done() } } 总结: send之前实例化一个call结构体, 用来传递信息. 同时实例化一个channel(每个call一个). 发送的时候, 每个req都有个序号, 这个序号和call是一一对应的, 把这个对应关系保存在map里. client等待在这个channel. server端把seq原封不动的搬到reply的header里面: resp.Seq = req.Seq 有个input goroutine不断的读socket, 把header里面包含的seq序号, 通过map查到pending的call结构体. 读reply到这个call结构体, 写channel通知client本次call完成. 问题和改进思路 个人感觉这个RPC写的一般, 很啰嗦, 用seq和map来记录每个call, server会原封不动的把seq返回来, client再根据seq号查找到本次call实例. 这里面有map的hash计算, 扩容等等性能不好的地方.我的想法是, 既然server要把client发送的seq号原封不动的返回, 何不在client直接\"封装\"chennel信息, server端还是原封不动返回, 这样client的input守护routine在收到回复报文的时候, 就可以直接用这个channel了, 省掉了hash map.但input守护routine是没有办法省掉的. 这和routine之间channel in channel的RPC不同, 因为socket RPC跨了进程. "},"notes/golang_mango.html":{"url":"notes/golang_mango.html","title":"消息中间件mango","keywords":"","body":" 库地址 代码走读 接口抽象 socket.go socket context options.go pipe.go message.go message pool msg对象的buffer管理 protocol.go 离用户最近的NewSocket transport.go TranPipe本质上是连接conn transport是NewDialer和NewListener和命名方式的组合 TranDialer TranListener 总结 顶层listener.go和dialer.go device.go transport transport/transport.go transport/handshaker.go transport/conn.go recv和send写的很有水平 发送接收总结 option函数就是字符串版本的ioctl header和同步协议握手 后台握手接口 transport/tcp/tcp.go NewDialer和NewListener dialer listener adress和close Set和Get Option 总结 transport小节 transport/inproc/inproc.go 全局listener表 Listen Accept Dial Send和Recv 总结 transport/ipc internal/core internal/core/socket.go NewDialer和NewListener socket具体定义 核心动作: addPipe send和recv internal/core/pipe.go pipe定义 pipeList addPipe和ID生成 pipe的send和recv 为什么底层send或者recv失败要close掉pipe? 携带额外数据的典型模式: SetPrivate internal/core/dialer.go internal/core/listener.go 总结 protocol protocol/req/req.go 核心结构体定义 为什么要有context? req的SendMsg 发送小节 NewSocket AddPipe方法 RecvMsg receiver()函数 接收小结 REQ小结 protocol/rep/rep.go NewSocket AddPipe 每个连接一个sender 每个连接一个receiver RecvMsg SendMsg REP小节 req rep疑问 xreq和xrep xreq xrep 再看核心层的核心价值 总结 库地址 https://github.com/nanomsg/mangos.git mangos是nanomsg的纯go版本实现. Mangos™ is an implementation in pure Go of the SP (“Scalability Protocols”) messaging system. These are colloquially known as a “nanomsg”. The design is intended to make it easy to add new transports with almost trivial effort, as well as new topologies (“protocols” in SP parlance.) At present, all of the Req/Rep, Pub/Sub, Pair, Bus, Push/Pull, and Surveyor/Respondent patterns are supported. This project also supports an experimental protocol called Star. 代码走读 mangos的顶层go文件, 是典型的抽象定义模式: 先在顶层定义好抽象, 子文件夹管实现. 这是自顶向下的设计方法. 接口抽象 socket.go socket.go提供了socket的抽象. 这里的socket是zeroMQ的socket概念, 是对更底层\"socket\"的场景化抽象. socket是应用侧访问这个SP system的接口. socket.go是对internal/core/socket.go的对外呈现, core的socket.go是实现者. socket 接口设计的很简洁, 符合zeroMQ类似的抽象. // Socket is the main access handle applications use to access the SP // system. It is an abstraction of an application's \"connection\" to a // messaging topology. Applications can have more than one Socket open // at a time. type Socket interface { // Info returns information about the protocol (numbers and names) // and peer protocol. Info() ProtocolInfo // Close closes the open Socket. Further operations on the socket // will return ErrClosed. Close() error // Send puts the message on the outbound send queue. It blocks // until the message can be queued, or the send deadline expires. // If a queued message is later dropped for any reason, // there will be no notification back to the application. Send([]byte) error // Recv receives a complete message. The entire message is received. Recv() ([]byte, error) // SendMsg puts the message on the outbound send. It works like Send, // but allows the caller to supply message headers. AGAIN, the Socket // ASSUMES OWNERSHIP OF THE MESSAGE. SendMsg(*Message) error // RecvMsg receives a complete message, including the message header, // which is useful for protocols in raw mode. RecvMsg() (*Message, error) // Dial connects a remote endpoint to the Socket. The function // returns immediately, and an asynchronous goroutine is started to // establish and maintain the connection, reconnecting as needed. // If the address is invalid, then an error is returned. Dial(addr string) error DialOptions(addr string, options map[string]interface{}) error // NewDialer returns a Dialer object which can be used to get // access to the underlying configuration for dialing. NewDialer(addr string, options map[string]interface{}) (Dialer, error) // Listen connects a local endpoint to the Socket. Remote peers // may connect (e.g. with Dial) and will each be \"connected\" to // the Socket. The accepter logic is run in a separate goroutine. // The only error possible is if the address is invalid. Listen(addr string) error ListenOptions(addr string, options map[string]interface{}) error NewListener(addr string, options map[string]interface{}) (Listener, error) // GetOption is used to retrieve an option for a socket. GetOption(name string) (interface{}, error) // SetOption is used to set an option for a socket. SetOption(name string, value interface{}) error // OpenContext creates a new Context. If a protocol does not // support separate contexts, this will return an error. OpenContext() (Context, error) // SetPipeEventHook sets a PipeEventHook function to be called when a // Pipe is added or removed from this socket (connect/disconnect). // The previous hook is returned (nil if none.) (Only one hook can // be used at a time.) SetPipeEventHook(PipeEventHook) PipeEventHook } Send和Recv是不带header的, 而SendMsg和RecvMsg带header; 发送都是发往outbound Q. Q满了会阻塞 Dial表达的是connect, Listen是bind GetOption和SetOption用来设置底层socket context 每个protocol都有个默认的context. 只有部分protocol允许自建context context的抽象和socket一样? 只是socket的子集? -- context是建立在socket之上的, 顾名思义, 是带上下文的socket. // Context is a protocol context, and represents the upper side operations // that applications will want to use. Every socket has a default context, // but only a certain protocols will allow the creation of additional // Context instances (only if separate stateful contexts make sense for // a given protocol). type Context interface { // Close closes the open Socket. Further operations on the socket // will return ErrClosed. Close() error // GetOption is used to retrieve an option for a socket. GetOption(name string) (interface{}, error) // SetOption is used to set an option for a socket. SetOption(name string, value interface{}) error // Send puts the message on the outbound send queue. It blocks // until the message can be queued, or the send deadline expires. // If a queued message is later dropped for any reason, // there will be no notification back to the application. Send([]byte) error // Recv receives a complete message. The entire message is received. Recv() ([]byte, error) // SendMsg puts the message on the outbound send. It works like Send, // but allows the caller to supply message headers. AGAIN, the Socket // ASSUMES OWNERSHIP OF THE MESSAGE. SendMsg(*Message) error // RecvMsg receives a complete message, including the message header, // which is useful for protocols in raw mode. RecvMsg() (*Message, error) } options.go 上文提到的option, 都在这里定义, 注释写的非常确切. options大大小小包括很多, 比如raw mode, 比如超时时间, Q的size 举几个例子: const ( // OptionRaw is used to test if the socket in RAW mod. The details of // how this varies from normal mode vary from protocol to protocol, // but RAW mode is generally minimal protocol processing, and // stateless. RAW mode sockets are constructed with different // protocol constructor. Raw mode is generally used with Device() // or similar proxy configurations. OptionRaw = \"RAW\" // OptionRecvDeadline is the time until the next Recv times out. The // value is a time.Duration. Zero value may be passed to indicate that // no timeout should be applied. A negative value indicates a // non-blocking operation. By default there is no timeout. OptionRecvDeadline = \"RECV-DEADLINE\" //默认不超时, 永远等 // OptionRetryTime is used by REQ. The argument is a time.Duration. // When a request has not been replied to within the given duration, // the request will automatically be resent to an available peer. // This value should be longer than the maximum possible processing // and transport time. The value zero indicates that no automatic // retries should be sent. The default value is one minute. // // Note that changing this option is only guaranteed to affect requests // sent after the option is set. Changing the value while a request // is outstanding may not have the desired effect. OptionRetryTime = \"RETRY-TIME\" //默认一分钟 // OptionSubscribe is used by SUB/XSUB. The argument is a []byte. // The application will receive messages that start with this prefix. // Multiple subscriptions may be in effect on a given socket. The // application will not receive messages that do not match any current // subscriptions. (If there are no subscriptions for a SUB/XSUB // socket, then the application will not receive any messages. An // empty prefix can be used to subscribe to all messages.) OptionSubscribe = \"SUBSCRIBE\" //subscribe通过option来操作 // OptionWriteQLen is used to set the size, in messages, of the write // queue channel. By default, it's 128. This option cannot be set if // Dial or Listen has been called on the socket. OptionWriteQLen = \"WRITEQ-LEN\" //发送q的大小, 默认128个message // OptionLinger is used to set the linger property. This is the amount // of time to wait for send queues to drain when Close() is called. // Close() may block for up to this long if there is unsent data, but // will return as soon as all data is delivered to the transport. // Value is a time.Duration. Default is one second. OptionLinger = \"LINGER\" // close默认等待1秒, 好让还没发送出去的msg发出去. // OptionMaxRecvSize supplies the maximum receive size for inbound // messages. This option exists because the wire protocol allows // the sender to specify the size of the incoming message, and // if the size were overly large, a bad remote actor could perform a // remote Denial-Of-Service by requesting ridiculously large message // sizes and then stalling on send. The default value is 1MB. // // A value of 0 removes the limit, but should not be used unless // absolutely sure that the peer is trustworthy. // // Not all transports honor this limit. For example, this limit // makes no sense when used with inproc. // // Note that the size includes any Protocol specific header. It is // better to pick a value that is a little too big, than too small. // // This option is only intended to prevent gross abuse of the system, // and not a substitute for proper application message verification. // // This option is type int64. OptionMaxRecvSize = \"MAX-RCV-SIZE\" //这个厉害了, DDOS, 默认最大收1MB, 防止对端饱和攻击. // OptionReconnectTime is the initial interval used for connection // attempts. If a connection attempt does not succeed, then ths socket // will wait this long before trying again. An optional exponential // backoff may cause this value to grow. See OptionMaxReconnectTime // for more details. This is a time.Duration whose default value is // 100msec. This option must be set before starting any dialers. OptionReconnectTime = \"RECONNECT-TIME\" //重连间隔, 默认100ms // OptionBestEffort enables non-blocking send operations on the // socket. Normally (for some socket types), a socket will block if // there are no receivers, or the receivers are unable to keep up // with the sender. (Multicast sockets types like Bus or Star do not // behave this way.) If this option is set, instead of blocking, the // message will be silently discarded. The value is a boolean, and // defaults to False. OptionBestEffort = \"BEST-EFFORT\" //这个厉害了, 有这个标记的socket, 如果遇到发送时对端没准备好等情况, 直接丢弃msg // OptionLocalAddr expresses a local address. For dialers, this is // the (often random) address that was locally bound. For listeners, // it is usually the service address. The value is a net.Addr. This // is generally a read-only value for pipes, though it might sometimes // be available on dialers or listeners. OptionLocalAddr = \"LOCAL-ADDR\" //获取本地地址 // OptionRemoteAddr expresses a remote address. For dialers, this is // the service address. For listeners, its the address of the far // end dialer. The value is a net.Addr. It is generally read-only // and available only on pipes and dialers. OptionRemoteAddr = \"REMOTE-ADDR\" //获取对端地址, 对pipe和dialer有效. 等等 pipe.go pipe似乎很重要, 但接口定义很简单: // Pipe represents the high level interface to a low level communications // channel. There is one of these associated with a given TCP connection, // for example. This interface is intended for application use. // // Note that applications cannot send or receive data on a Pipe directly. type Pipe interface { // ID returns the numeric ID for this Pipe. This will be a // 31 bit (bit 32 is clear) value for the Pipe, which is unique // across all other Pipe instances in the application, while // this Pipe exists. (IDs are recycled on Close, but only after // all other Pipe values are used.) ID() uint32 // Address returns the address (URL form) associated with the Pipe. // This matches the string passed to Dial() or Listen(). Address() string // GetOption returns an arbitrary option. The details will vary // for different transport types. GetOption(name string) (interface{}, error) // Listener returns the Listener for this Pipe, or nil if none. Listener() Listener // Dialer returns the Dialer for this Pipe, or nil if none. Dialer() Dialer // Close closes the Pipe. This does a disconnect, or something similar. // Note that if a dialer is present and active, it will redial. Close() error } message.go message是对数据的承载载体, 包括header 对mesage的分包因protocol而异. 这里的message不包括比如tcp/ip头. 对transport来说, 这个message就是个大的payload. // Message encapsulates the messages that we exchange back and forth. The // meaning of the Header and Body fields, and where the splits occur, will // vary depending on the protocol. Note however that any headers applied by // transport layers (including TCP/ethernet headers, and SP protocol // independent length headers), are *not* included in the Header. type Message struct { // Header carries any protocol (SP) specific header. Applications // should not modify or use this unless they are using Raw mode. // No user data may be placed here. Header []byte // Body carries the body of the message. This can also be thought // of as the message \"payload\". Body []byte // Pipe may be set on message receipt, to indicate the Pipe from // which the Message was received. There are no guarantees that the // Pipe is still active, and applications should only use this for // informational purposes. Pipe Pipe //这个有点像channel in channel的意思. bbuf []byte hbuf []byte bsize int refcnt int32 } message pool 和标准库fmt包一样, msg也用了pool模式, 按照msg的size进行了分块. 这样做是为了减小GC的压力. type msgCacheInfo struct { maxbody int pool *sync.Pool } func newMsg(sz int) *Message { m := &Message{} m.bbuf = make([]byte, 0, sz) //实际的size全部在body中分配 m.hbuf = make([]byte, 0, 32) //这里很清楚, 默认的header大小为32字节 m.bsize = sz return m } // We can tweak these! var messageCache = []msgCacheInfo{ { maxbody: 64, pool: &sync.Pool{ New: func() interface{} { return newMsg(64) }, }, }, { maxbody: 128, pool: &sync.Pool{ New: func() interface{} { return newMsg(128) }, }, }, { ... 一直翻倍到65536 补充: sync.Pool可以传入一个New函数, 在pool里面没有对象的时候, 默认new一个. 有了这个Pool, 可以手动free msg, 进一步减轻gc压力. // Free releases the message to the pool from which it was allocated. // While this is not strictly necessary thanks to GC, doing so allows // for the resources to be recycled without engaging GC. This can have // rather substantial benefits for performance. func (m *Message) Free() { if m != nil { if atomic.AddInt32(&m.refcnt, -1) == 0 { for i := range messageCache { if m.bsize == messageCache[i].maxbody { messageCache[i].pool.Put(m) //free其实就是返还到pool return } } } } } msg对象的buffer管理 msg对象有如下方法: 上面说的Free() Clone() 只是把引用计数加一, 表示这个msg是共享的. 调用Clone()的人不要修改msg, 因为这个msg还是别人的. Dup() 深拷贝这个msg, 可以修改. MakeUnique() 一般用m = m.MakeUnique()来保证这个msg是自己独享的: 如果msg的引用计数为1, 返回msg本身; 否则Dup()一份, 删掉原msg, 返回Dup的, 即MakeUnique()后, 原msg也不能使用了. 最后是NewMessage()函数: 从pool中new一个msg // NewMessage is the supported way to obtain a new Message. This makes // use of a \"cache\" which greatly reduces the load on the garbage collector. func NewMessage(sz int) *Message { var m *Message for i := range messageCache { if sz protocol.go protocol就是场景化的套路类型: 比如Req/Rep Pub/Sub // Useful constants for protocol numbers. Note that the major protocol number // is stored in the upper 12 bits, and the minor (subprotocol) is located in // the bottom 4 bits. const ( ProtoPair = (1 * 16) ProtoPub = (2 * 16) ProtoSub = (2 * 16) + 1 ProtoReq = (3 * 16) ProtoRep = (3 * 16) + 1 ProtoPush = (5 * 16) ProtoPull = (5 * 16) + 1 ProtoSurveyor = (6 * 16) + 2 ProtoRespondent = (6 * 16) + 3 ProtoBus = (7 * 16) ProtoStar = (100 * 16) // Experimental! ) protocol就是下面的接口: // ProtocolPipe represents the handle that a Protocol implementation has // to the underlying stream transport. It can be thought of as one side // of a TCP, IPC, or other type of connection. type ProtocolPipe interface { // ID returns a unique 31-bit value associated with this. // The value is unique for a given socket, at a given time. ID() uint32 // Close does what you think. Close() error // SendMsg sends a message. On success it returns nil. This is a // blocking call. SendMsg(*Message) error // RecvMsg receives a message. It blocks until the message is // received. On error, the pipe is closed and nil is returned. RecvMsg() *Message // SetPrivate is used to set protocol private data. SetPrivate(interface{}) // GetPrivate returns the previously stored protocol private data. GetPrivate() interface{} } context就是protocol+使用上下文; 所有context都是stateful的. 奇怪的是RecvMsg() (*Message, error)的签名, 只有它和protocol长得不一样 // ProtocolContext is a \"context\" for a protocol, which contains the // various stateful operations such as timers, etc. necessary for // running the protocol. This is separable from the protocol itself // as the protocol may permit the creation of multiple contexts. type ProtocolContext interface { // Close closes the context. Close() error // SendMsg sends the message. The message may be queued, or // may be delivered immediately, depending on the nature of // the protocol. On success, the context assumes ownership // of the message. On error, the caller retains ownership, // and may either resend the message or dispose of it otherwise. SendMsg(*Message) error // RecvMsg receives a complete message, including the message header, // which is useful for protocols in raw mode. RecvMsg() (*Message, error) // GetOption is used to retrieve the current value of an option. // If the protocol doesn't recognize the option, EBadOption should // be returned. GetOption(string) (interface{}, error) // SetOption is used to set an option. EBadOption is returned if // the option name is not recognized, EBadValue if the value is // invalid. SetOption(string, interface{}) error } ProtocolBase匿名包含了ProtocolContext // ProtocolBase provides the protocol-specific handling for sockets. // This is the new style API for sockets, and is how protocols provide // their specific handling. type ProtocolBase interface { ProtocolContext // Info returns the information describing this protocol. Info() ProtocolInfo // XXX: Revisit these when we can use Pipe natively. // AddPipe is called when a new Pipe is added to the socket. // Typically this is as a result of connect or accept completing. // The pipe ID will be unique for the socket at this time. // The implementation must not call back into the socket, but it // may reject the pipe by returning a non-nil result. AddPipe(ProtocolPipe) error // RemovePipe is called when a Pipe is removed from the socket. // Typically this indicates a disconnected or closed connection. // This is called exactly once, after the underlying transport pipe // is closed. The Pipe ID will still be valid. RemovePipe(ProtocolPipe) // OpenContext is a request to create a unique instance of the // protocol state machine, allowing concurrent use of states on // a given protocol socket. Protocols that don't support this // should return ErrProtoOp. OpenContext() (ProtocolContext, error) } ProtocolInfo定义: // ProtocolInfo is a description of the protocol. type ProtocolInfo struct { Self uint16 Peer uint16 SelfName string PeerName string } 离用户最近的NewSocket 用户使用的时候, 一般会调用NewSocket, 这个API就是对下面protocol的MakeSocket的调用. // MakeSocket creates a Socket on top of a Protocol. func MakeSocket(proto Protocol) Socket { return core.MakeSocket(proto) } 核心层的实现如下 func newSocket(proto mangos.ProtocolBase) *socket { s := &socket{ proto: proto, reconnMinTime: defaultReconnMinTime, reconnMaxTime: defaultReconnMaxTime, maxRxSize: defaultMaxRxSize, } return s } // MakeSocket is intended for use by Protocol implementations. The intention // is that they can wrap this to provide a \"proto.NewSocket()\" implementation. func MakeSocket(proto mangos.ProtocolBase) mangos.Socket { return newSocket(proto) } transport.go 提供给transport类型实现方使用的统一的transport的抽象. 这是root下的tansport.go文件, transport下面还有自己的transport.go 这两个文件以后估计会merge到一起. TranPipe本质上是连接conn 注意, ransport的实现方才关心pipe, 应用侧不要用pipe. // TranPipe behaves like a full-duplex message-oriented connection between two // peers. Callers may call operations on a Pipe simultaneously from // different goroutines. (These are different from net.Conn because they // provide message oriented semantics.) // // Pipe is only intended for use by transport implementors, and should // not be directly used in applications. type TranPipe interface { // Send sends a complete message. In the event of a partial send, // the Pipe will be closed, and an error is returned. For reasons // of efficiency, we allow the message to be sent in a scatter/gather // list. Send(*Message) error // Recv receives a complete message. In the event that either a // complete message could not be received, an error is returned // to the caller and the Pipe is closed. // // To mitigate Denial-of-Service attacks, we limit the max message // size to 1M. Recv() (*Message, error) // Close closes the underlying transport. Further operations on // the Pipe will result in errors. Note that messages that are // queued in transport buffers may still be received by the remote // peer. Close() error // GetOption returns an arbitrary transport specific option on a // pipe. Options for pipes are read-only and specific to that // particular connection. If the property doesn't exist, then // ErrBadOption should be returned. GetOption(string) (interface{}, error) } transport是NewDialer和NewListener和命名方式的组合 下面会看到很清楚, 这个文件旨在统一transport的抽象, 我们从上到下看: // Transport is the interface for transport suppliers to implement. type Transport interface { // Scheme returns a string used as the prefix for SP \"addresses\". // This is similar to a URI scheme. For example, schemes can be // \"tcp\" (for \"tcp://xxx...\"), \"ipc\", \"inproc\", etc. Scheme() string // NewDialer creates a new Dialer for this Transport. NewDialer(url string, sock Socket) (TranDialer, error) // NewListener creates a new PipeListener for this Transport. // This generally also arranges for an OS-level file descriptor to be // opened, and bound to the the given address, as well as establishing // any \"listen\" backlog. NewListener(url string, sock Socket) (TranListener, error) } 一个transport本质要提供 一个命名规则: 比如\"tcp://xxx...\" 一个NewDialer方法 一个NewListener方法 TranDialer TranListener TranDialer TranListener都分别有别名Dialer和Listener // TranDialer represents the client side of a connection. Clients initiate // the connection. // // TranDialer is only intended for use by transport implementors, and should // not be directly used in applications. type TranDialer interface { // Dial is used to initiate a connection to a remote peer. Dial() (TranPipe, error) // SetOption sets a local option on the dialer. // ErrBadOption can be returned for unrecognized options. // ErrBadValue can be returned for incorrect value types. SetOption(name string, value interface{}) error // GetOption gets a local option from the dialer. // ErrBadOption can be returned for unrecognized options. GetOption(name string) (value interface{}, err error) } // TranListener represents the server side of a connection. Servers respond // to a connection request from clients. // // TranListener is only intended for use by transport implementors, and should // not be directly used in applications. type TranListener interface { // Listen actually begins listening on the interface. It is // called just prior to the Accept() routine normally. It is // the socket equivalent of bind()+listen(). Listen() error // Accept completes the server side of a connection. Once the // connection is established and initial handshaking is complete, // the resulting connection is returned to the client. Accept() (TranPipe, error) // Close ceases any listening activity, and will specifically close // any underlying file descriptor. Once this is done, the only way // to resume listening is to create a new Server instance. Presumably // this function is only called when the last reference to the server // is about to go away. Established connections are unaffected. Close() error // SetOption sets a local option on the listener. // ErrBadOption can be returned for unrecognized options. // ErrBadValue can be returned for incorrect value types. SetOption(name string, value interface{}) error // GetOption gets a local option from the listener. // ErrBadOption can be returned for unrecognized options. GetOption(name string) (value interface{}, err error) // Address gets the local address. The value may not be meaningful // until Listen() has been called. Address() string } 以上的\"二级\"接口依然不是最终的接口, 最终的接口是TranPipe, 就是本节最开头的接口定义. 总结 这里对transport的抽象是\"分级\"的. 接口的函数返回接口, 是对\"另一件事\"的抽象.比如Transport的NewDialer()方法, 返回TranDialer抽象; 再由其的Dial()方法返回TranPipe抽象, 后者才有Send(*Message) error和Recv() (*Message, error)方法. 抽象是有层次的, 抽象返回抽象. 对比我模糊的对transport的抽象认知: transport似乎应该是包括了send recv等多个方法的单一接口.mangos的抽象更有层次, 更清晰. 顶层listener.go和dialer.go 和transport的listener和dialer完全不一样, 顶层的定义是面向用户api的. // Listener is an interface to the underlying listener for a transport // and address. type Listener interface { // Close closes the listener, and removes it from any active socket. // Further operations on the Listener will return ErrClosed. Close() error // Listen starts listening for new connectons on the address. Listen() error // Address returns the string (full URL) of the Listener. Address() string // SetOption sets an option on the Listener. Setting options // can only be done before Listen() has been called. SetOption(name string, value interface{}) error // GetOption gets an option value from the Listener. GetOption(name string) (interface{}, error) } // Dialer is an interface to the underlying dialer for a transport // and address. type Dialer interface { // Close closes the dialer, and removes it from any active socket. // Further operations on the Dialer will return ErrClosed. Close() error // Dial starts connecting on the address. If a connection fails, // it will restart. Dial() error // Address returns the string (full URL) of the Listener. Address() string // SetOption sets an option on the Dialer. Setting options // can only be done before Dial() has been called. SetOption(name string, value interface{}) error // GetOption gets an option value from the Listener. GetOption(name string) (interface{}, error) } device.go 用于在socket之间转发, 这两个socket必须都是raw模式 func Device(s1 Socket, s2 Socket) error { go forwarder(s1, s2) if s2 != s1 { go forwarder(s2, s1) } return nil } // Forwarder takes messages from one socket, and sends them to the other. // The sockets must be of compatible types, and must be in Raw mode. func forwarder(fromSock Socket, toSock Socket) { for { m, err := fromSock.RecvMsg() if err != nil { // Probably closed socket, nothing else we can do. return } err = toSock.SendMsg(m) if err != nil { return } } } transport transport/transport.go 提供注册transport实现到全局变量表的方法 var transports = map[string]Transport{} // RegisterTransport is used to register the transport globally, // after which it will be available for all sockets. The // transport will override any others registered for the same // scheme. func RegisterTransport(t Transport) { lock.Lock() transports[t.Scheme()] = t lock.Unlock() } // GetTransport is used by a socket to lookup the transport // for a given scheme. func GetTransport(scheme string) Transport { lock.RLock() defer lock.RUnlock() if t, ok := transports[scheme]; ok { return t } return nil } 多出使用了技巧: type和等号实际上是alias // Pipe is a transport pipe. type Pipe = mangos.TranPipe // Dialer is a factory that creates Pipes by connecting to remote listeners. type Dialer = mangos.TranDialer // Listener is a factory that creates Pipes by listening to inbound dialers. type Listener = mangos.TranListener // Transport is our transport operations. type Transport = mangos.Transport transport/handshaker.go 这里handshaker是对异步握手的抽象. 所谓异步握手就是握手会在后台进行, 不block当前流程. // Handshaker is used to support dealing with asynchronous // handshaking used for some transports. This allows the // initial handshaking to be done in the background, without // stalling the server's accept queue. This is important to // ensure that a slow remote peer cannot bog down the server // or effect a denial-of-service for new connections. type Handshaker interface { // Start injects a pipe into the handshaker. The // handshaking is done asynchronously on a Go routine. Start(Pipe) // Waits for until a pipe has completely finished the // handshaking and returns it. Wait() (Pipe, error) // Close is used to close the handshaker. Any existing // negotiations will be canceled, and the underlying // transport sockets will be closed. Any new attempts // to start will return mangos.ErrClosed. Close() } transport/conn.go 这个文件实现了基于net.conn的TranPipe. 这个TranPipe也有别名connPipe, 其他的stream式的transport实现可以被被包装成connPipe, 使用这里通用的分包方法和握手协议. // conn implements the Pipe interface on top of net.Conn. The // assumption is that transports using this have similar wire protocols, // and conn is meant to be used as a building block. type conn struct { c net.Conn proto ProtocolInfo open bool options map[string]interface{} //对于option, 一把get set操作不频繁, 用string类的map再合适不过了. maxrx int sync.Mutex } recv和send写的很有水平 对net库, encoding/binary库的使用很到位: // Recv implements the TranPipe Recv method. The message received is expected // as a 64-bit size (network byte order) followed by the message itself. func (p *conn) Recv() (*Message, error) { var sz int64 var err error var msg *Message //先读size //binary标准库解析字节序列到size, 到结构体都行. 需要指明大小端. if err = binary.Read(p.c, binary.BigEndian, &sz); err != nil { return nil, err } // Limit messages to the maximum receive value, if not // unlimited. This avoids a potential denial of service. if sz 0 && sz > int64(p.maxrx)) { return nil, mangos.ErrTooLong } //根据size准备buffer //这里是从pool里new msg msg = mangos.NewMessage(int(sz)) msg.Body = msg.Body[0:sz] //用io.ReadFull读完整的msg, 也就是size长度的msg. //很明显, 这里的前提是stream方式的数据报. if _, err = io.ReadFull(p.c, msg.Body); err != nil { msg.Free() return nil, err } return msg, nil } // Send implements the Pipe Send method. The message is sent as a 64-bit // size (network byte order) followed by the message itself. func (p *conn) Send(msg *Message) error { //使用net包的Buffer对象发送 //net的Buffer是[][]byte, 是典型scatter模式, 即离散buffer模式. var buff = net.Buffers{} //这里size一定是大端格式的, 而且这个size是header和body的和. // Serialize the length header l := uint64(len(msg.Header) + len(msg.Body)) lbyte := make([]byte, 8) binary.BigEndian.PutUint64(lbyte, l) //lbyte是大端, msg.Header, msg.Body还是原始字节序 //用append来组包, 因为是二维byte, append只拷贝`[]byte`头, 不存在额外数据拷贝. // Attach the length header along with the actual header and body buff = append(buff, lbyte, msg.Header, msg.Body) //buffer自带的writeTo函数, 一把写完. if _, err := buff.WriteTo(p.c); err != nil { return err } msg.Free() return nil } 注意, 以上的函数中的header部分是[]byte, 对其具体的header格式没有认知. 发送接收总结 发送接收都是先有个size, 再根据size取出\"payload\". size是header+body的总大小 因为共享size, 所以接收方没有办法分别知道Header和Body的大小. 所以只能都用Body来接收 即发送方发的是Header+Body, 到接收方结构体中, Header为空, Body是原Header+Body 使用了encoding/binary对字节序列进行\"解释\", 比如size就是按大端字节序解析的. 发送的是net.Buffers, 是个二维byte [][]byte, 本质上是scatter的指针数组, 不拷贝数据 接收用的是sync.Pool的内存池化方式, 减小gc压力. 对header和body不做假设, 也没有知识. 发送接收认为他们都是[]byte option函数就是字符串版本的ioctl func (p *conn) GetOption(n string) (interface{}, error) { switch n { case mangos.OptionMaxRecvSize: return p.maxrx, nil } if v, ok := p.options[n]; ok { return v, nil } return nil, mangos.ErrBadProperty } func (p *conn) SetOption(n string, v interface{}) { switch n { case mangos.OptionMaxRecvSize: p.maxrx = v.(int) } p.options[n] = v } header和同步协议握手 // connHeader is exchanged during the initial handshake. type connHeader struct { Zero byte // must be zero S byte // 'S' P byte // 'P' Version byte // only zero at present Proto uint16 Reserved uint16 // always zero at present } header里面固定的字段意义 握手的过程是交换header的过程. // handshake establishes an SP connection between peers. Both sides must // send the header, then both sides must wait for the peer's header. // As a side effect, the peer's protocol number is stored in the conn. // Also, various properties are initialized. func (p *conn) handshake() error { var err error h := connHeader{S: 'S', P: 'P', Proto: p.proto.Self} //这里的binary.Write就是直接对底层conn p.c 做send操作. if err = binary.Write(p.c, binary.BigEndian, &h); err != nil { return err } //再从对端读出handshake信息 if err = binary.Read(p.c, binary.BigEndian, &h); err != nil { _ = p.c.Close() return err } if h.Zero != 0 || h.S != 'S' || h.P != 'P' || h.Reserved != 0 { _ = p.c.Close() return mangos.ErrBadHeader } // The only version number we support at present is \"0\", at offset 3. if h.Version != 0 { _ = p.c.Close() return mangos.ErrBadVersion } // The protocol number lives as 16-bits (big-endian) at offset 4. if h.Proto != p.proto.Peer { _ = p.c.Close() return mangos.ErrBadProto } p.open = true return nil } 后台握手接口 上面是同步的握手函数, 是具体实现. 同步的接口可以被异步框架异步化, 来满足handshaker的要求 基本上, 握手是底层建立连接后要做的第一件事. 下面是conn.go提供的异步包装框架, 是承上(transport/handshaker.go要求的后台握手)启下(各个transport类型实现的handshake) type connHandshakerPipe interface { handshake() error //被包装的对象要实现handshake函数 Pipe //和send recv等tranpipe接口, 这个接口是对所有transport类型的统一抽象. } type connHandshakerItem struct { c connHandshakerPipe e error } type connHandshaker struct { workq map[connHandshakerPipe]bool //interface可以当map的key doneq []*connHandshakerItem //注意这里, handshaker是个汇聚者, 这里包括了listen后accept的所有conn(被包装为connHandshakerPipe) closed bool cv *sync.Cond sync.Mutex } 先看start: 一个connHandshaker可以和多个pipe后台握手, 每个pipe都启动一个go routine; 一个pipe代表了一个conn func (h *connHandshaker) Start(p Pipe) { // If the following type assertion fails, then its a software bug. conn := p.(connHandshakerPipe) h.Lock() h.workq[conn] = true //标记这个conn正在握手中 h.Unlock() go h.worker(conn) } func (h *connHandshaker) worker(conn connHandshakerPipe) { item := &connHandshakerItem{c: conn} item.e = conn.handshake() //这里直接调用同步版本的handshake函数. 此时必然已经建立连接, 否则也不会有conn对象. h.Lock() defer h.Unlock() delete(h.workq, conn) //有错误就close掉这个conn if item.e != nil { _ = item.c.Close() item.c = nil } else if h.closed { item.e = mangos.ErrClosed _ = item.c.Close() } //把结果放到doneq里 h.doneq = append(h.doneq, item) h.cv.Broadcast() //广播会唤醒所有等待的goroutine, 但实际上wait路径上有锁, 结果是只能有一个routine在cv上wait. 其他人在锁上wait. } wait是等待返回任意一个已经完成握手的conn func (h *connHandshaker) Wait() (Pipe, error) { h.Lock() defer h.Unlock() for len(h.doneq) == 0 && !h.closed { h.cv.Wait() //持有锁的wait } if h.closed { return nil, mangos.ErrClosed } item := h.doneq[0] //按完成顺序取 h.doneq = h.doneq[1:] //整个过程被锁保护 return item.c, item.e } close func (h *connHandshaker) Close() { h.Lock() h.closed = true h.cv.Broadcast() for conn := range h.workq { _ = conn.Close() } for len(h.doneq) != 0 { item := h.doneq[0] h.doneq = h.doneq[1:] if item.c != nil { _ = item.c.Close() } } h.Unlock() } transport/tcp/tcp.go tcp实现了Scheme NewDialer NewListener方法, 满足了transport接口. type tcpTran int func (t tcpTran) Scheme() string { return \"tcp\" } 有意思的是, tcpTran只是个int类型的重命名. 在init里, 注册这个tcpTran类型 const ( // Transport is a transport.Transport for TCP. Transport = tcpTran(0) ) //本质上是注册接口类型: transports[t.Scheme()] = t func init() { transport.RegisterTransport(Transport) } NewDialer和NewListener 这两个接口类似工厂类, 其产品transport.Dialer和transport.Listener才是重点. func (t tcpTran) NewDialer(addr string, sock mangos.Socket) (transport.Dialer, error) { var err error //StripScheme把tcp://192.168.0.1:1234中的192.168.0.1:1234部分提取出来 if addr, err = transport.StripScheme(t, addr); err != nil { return nil, err } // check to ensure the provided addr resolves correctly. //实际上是调用net.ResolveTCPAddr if _, err = transport.ResolveTCPAddr(addr); err != nil { return nil, err } d := &dialer{ //返回重点, 就是这个dialer addr: addr, proto: sock.Info(), //这里transport也关心应用socket类型了? hs: transport.NewConnHandshaker(), //handshake是应用侧socket交换信息的开始 } return d, nil } func (t tcpTran) NewListener(addr string, sock mangos.Socket) (transport.Listener, error) { var err error l := &listener{ //返回重点, 就是这个listener proto: sock.Info(), closeq: make(chan struct{}), } if addr, err = transport.StripScheme(t, addr); err != nil { return nil, err } l.addr = addr l.handshaker = transport.NewConnHandshaker() return l, nil } dialer dialer要实现Dial GetOption和SetOption接口 type dialer struct { addr string proto transport.ProtocolInfo hs transport.Handshaker maxRecvSize int d net.Dialer lock sync.Mutex } func (d *dialer) Dial() (_ transport.Pipe, err error) { conn, err := d.d.Dial(\"tcp\", d.addr) // 先dail得到conn, 也就是pipe if err != nil { return nil, err } p := transport.NewConnPipe(conn, d.proto) //包装已有的conn得到connPipe, stream方式的Conn都可以用它来实现transport. 但为什么这里要知道proto? proto是应用侧socket的类型, 包括自己和对端. d.lock.Lock() p.SetOption(mangos.OptionMaxRecvSize, d.maxRecvSize) //似乎默认maxRecvSize为0 d.lock.Unlock() d.hs.Start(p) //里面会断言p是connHandshakerPipe, 因为NewConnPipe()返回的p, 实现了handshake()函数. 注意这里的handshake()函数是小写开头, 外部不能直接调用. 但go编译器的类型系统还是会判定p实现了不对外的handshake()函数. return d.hs.Wait() //先start再wait, 这就是同步化了, 实际就像是调了p.handshake()函数. 但实际上handshake()是不对外的, 只能用异步再同步化操作. } 注: 这里把接口的隐含属性用的出神入化. p是个interface, 把p传入给d.hs.Start(p)时, p的方法集变成了TranPipe接口的方法集; 在transport/conn.go里的start实现中, 断言入参TranPipe是conn := p.(connHandshakerPipe) 因为p是transport/conn.go里NewConnPipe()函数返回的, 保证能被同一个文件里的start函数断言成功. 而且其签名函数handshake()可以小写. 这些都是内部的事情. listener listener要复杂点, 要实现Accept Listen Address Close和Set/Get Option的方法 type listener struct { addr string bound net.Addr proto transport.ProtocolInfo l net.Listener lc net.ListenConfig maxRecvSize int handshaker transport.Handshaker closeq chan struct{} once sync.Once lock sync.Mutex } Accept()一般紧跟着Listen()之后, 等待并返回协商成功的pipe func (l *listener) Accept() (transport.Pipe, error) { if l.l == nil { return nil, mangos.ErrClosed } return l.handshaker.Wait() } Listen启动了一个goroutine来做accept func (l *listener) Listen() (err error) { select { case adress和close address返回全称, 比如tcp://192.168.1.1:1111 close先用close channel的方式改变listener的状态, 这样其他例程可以安全的检查listener是否已经被close掉了. -- 似乎比bool的方式先进些? close的意思是停止listen和accept, 但之前已经建立的连接不受影响. func (l *listener) Address() string { if b := l.bound; b != nil { return \"tcp://\" + b.String() } return \"tcp://\" + l.addr } func (l *listener) Close() error { l.once.Do(func() { close(l.closeq) if l.l != nil { _ = l.l.Close() } l.handshaker.Close() }) return nil } Set和Get Option 和dialer类似, 有 OptionMaxRecvSize OptionKeepAliveTime 总结 tcp是stream方式的transport, 底层使用net标准库的方法, 最主要的是把net.Conn包装成了connPipe, 后者是mangos的统一化的流式pipe的实现, 有最简单的根据header+body的size定界方法, 和协议握手的方法. 可以说, tcp使用了connPipe的\"helper\"方法, 实现了流式transport, 满足了transport的所有接口要求. 需要注意的是, mangos做为nanomsg的纯go版本实现, 并没有follow其前身zeroMQ对报文格式的规定, 即分frame的规定. -- 有待确认. transport小节 transport是知道顶层socket信息的, 因为transport先于protocol动作, 比如在client dial的时候, 就要握手交换protocol的信息做验证. server listen的时候也如是. transport/inproc/inproc.go 前面我们知道, transport首先要实现NewDialer, NewListener, 后续要实现Dial和Listen方法, 最后一层要实现Send和Recv. 注意Send和Recv都是规定好的m *mangos.Message -- 为什么不规定个interface来做msg抽象? 全局listener表 var listeners struct { // Who is listening, on which \"address\"? byAddr map[string]*listener //记录了全局的名字 cv sync.Cond mx sync.Mutex } Listen NewListener就不看了, 就是返回一个带Listen功能和Accept功能的对象. Listen很简单, 如果要Listen的名字没有人用, 表示可以listen, 就记录到全局表中. func (l *listener) Listen() error { listeners.mx.Lock() if l.closed { listeners.mx.Unlock() return mangos.ErrClosed } if _, ok := listeners.byAddr[l.addr]; ok { listeners.mx.Unlock() return mangos.ErrAddrInUse } l.active = true listeners.byAddr[l.addr] = l listeners.cv.Broadcast() //这里要广播给谁呢? listeners.mx.Unlock() return nil } Accept 每次Accept生成一个新的inproc类型的server func (l *listener) Accept() (mangos.TranPipe, error) { server := &inproc{ selfProto: l.selfProto, peerProto: l.peerProto, addr: addr(l.addr), } server.readyq = make(chan struct{}) server.closeq = make(chan struct{}) listeners.mx.Lock() if !l.active || l.closed { listeners.mx.Unlock() return nil, mangos.ErrClosed } l.accepters = append(l.accepters, server) //这里这么早就add了, 不好吧? 要不放到case里 -- 根据下文逻辑, 一定要先在l.accepters里有server listeners.cv.Broadcast() listeners.mx.Unlock() select { case Dial func (d *dialer) Dial() (transport.Pipe, error) { var server *inproc client := &inproc{ selfProto: d.selfProto, peerProto: d.peerProto, addr: addr(d.addr), //client也直接用dial的addr, 不合适吧? } client.readyq = make(chan struct{}) client.closeq = make(chan struct{}) listeners.mx.Lock() // NB: No timeouts here! for { var l *listener var ok bool if l, ok = listeners.byAddr[d.addr]; !ok || l == nil { //一定要先有listenr才能Dial, 不支持先有client, 再有server. -- 其实没必要 listeners.mx.Unlock() return nil, mangos.ErrConnRefused } if (client.selfProto != l.peerProto) || //这是个简单的协商机制 (client.peerProto != l.selfProto) { listeners.mx.Unlock() return nil, mangos.ErrBadProto } if len(l.accepters) != 0 { server = l.accepters[len(l.accepters)-1] //取最后一个server l.accepters = l.accepters[:len(l.accepters)-1] break } listeners.cv.Wait() continue } listeners.mx.Unlock() //到这里client和server已经配对. server.wq = make(chan *transport.Message) //才建立server的通道, 注意都是unbuffer类型的 server.rq = make(chan *transport.Message) client.rq = server.wq //client和server共用一个channel, 只是方向不一样 client.wq = server.rq server.peer = client client.peer = server close(server.readyq) close(client.readyq) return client, nil } Send和Recv func (p *inproc) Send(m *mangos.Message) error { // Upper protocols expect to have to pick header and body part. // Also we need to have a fresh copy of the message for receiver, to // break ownership. nmsg := mangos.NewMessage(len(m.Header) + len(m.Body)) nmsg.Body = append(nmsg.Body, m.Header...) nmsg.Body = append(nmsg.Body, m.Body...) //复制报文, 但是不free? select { case p.wq func (p *inproc) Recv() (*transport.Message, error) { select { case m := 总结 server和client都是*inproc类型 Send msg是拷贝式的. 基于名字查找和共享channel的消息传递, 开销比较小. transport/ipc internal/core internal/core/socket.go NewDialer和NewListener transport提供了GetTransport()函数从全局map表var transports = map[string]Transport{}中获取已经注册的transport工厂对象, 该对象的NewDialer和NewListener方法会新建连接. 而这里internal core里面, 就调用了GetTransport(), 根据传入的string, 返回mangos.Dialer. 注意这个Dialer已经不是transport的Dialer了. func (s *socket) NewDialer(addr string, options map[string]interface{}) (mangos.Dialer, error) { t := s.getTransport(addr) td, err := t.NewDialer(addr, s) //调用transport层的NewDialer d := &dialer{ d: td, s: s, //很重要, dialer保持了这个socket的信息 reconnMinTime: s.reconnMinTime, reconnMaxTime: s.reconnMaxTime, asynch: s.dialAsynch, addr: addr, } for n, v := range options { //处理options d.SetOption(n, v) 或td.SetOption(n, v) } s.dialers = append(s.dialers, d) //把刚才New的dialer关联到socket; socket可以有多个dialer return d, nil } 上面的NewDialer就是顶层API func (s *socket) Dial(addr string) error或func (s *socket) DialOptions(addr string, opts map[string]interface{}) error调用下来的. func (s *socket) Dial(addr string) error s.DialOptions(addr, nil) d, err := s.NewDialer(addr, opts) return d.Dial() //注意这里并不是transport的Dial, 而是应用侧的dial; 应用侧的dial签名不同, 它没有返回conn 同样的listen也是类似的过程 socket.ListenOptions 或socket.Listen l := socket.NewListener() l.Listen() //注意这里开始Listen, 这也是应用侧的listen, 只返回error 后面会看到, 应用侧Dial和Listen后, conn的信息是通过socket的addPipe方法来保存的. socket具体定义 在core看起来, socket持有多个listener, 多个dialer, 以及真正的conn(也就是这里的pipeList). // socket is the meaty part of the core information. type socket struct { proto mangos.ProtocolBase sync.Mutex closed bool // true if Socket was closed at API level reconnMinTime time.Duration // reconnect time after error or disconnect reconnMaxTime time.Duration // max reconnect interval maxRxSize int // max recv size dialAsynch bool // asynchronous dialing? listeners []*listener dialers []*dialer pipes pipeList //定义于pipe.go pipehook mangos.PipeEventHook //提供给app层的hook函数入口 } 最后一个hook再详细看一下: 在socket的pipe中枢系统发生变化时, 调用这个hook. const ( // PipeEventAttaching is called before the Pipe is registered with the // socket. The intention is to permit the application to reject // a pipe before it is attached. PipeEventAttaching = iota // PipeEventAttached occurs after the Pipe is attached. // Consequently, it is possible to use the Pipe for delivering // events to sockets, etc. PipeEventAttached // PipeEventDetached occurs after the Pipe has been detached // from the socket. PipeEventDetached ) 换算成y的说法就是. onConnect, onDisconnect. 核心动作: addPipe // AddPipe is called when a new Pipe is added to the socket. // Typically this is as a result of connect or accept completing. // The pipe ID will be unique for the socket at this time. // The implementation must not call back into the socket, but it // may reject the pipe by returning a non-nil result. 就是说每当有连接的时候, 就会addPipe func (s *socket) addPipe(tp transport.Pipe, d *dialer, l *listener) p := newPipe(tp, s, d, l) s.pipes.Add(p) ph(mangos.PipeEventAttaching, p) //调用hook s.proto.AddPipe(p) //这个pipe也会被add到ProtocolBase ph(mangos.PipeEventAttached, p) //调用hook 注意这里的这句代码有学问: s.proto.AddPipe(p) p是个*core.pipe类型, 本身是小写的, 不能直接被外部使用. 但这里调用protocol的接口函数: AddPipe(p), 把p当作ProtocolPipe来看, 就把pipe的\"能力\"输出了. 这里的s.proto是mangos.ProtocolBase, 实际上是每个protocol的实现者的socket对象. 比如req.socket 要输出能力, 不一定要自己声明; 调用别人的接口, 把自己包装出去也可以. send和recv socket的重头戏, 这是核心层的对用户层API的具体实现的地方: func (s *socket) SendMsg(msg *Message) error { return s.proto.SendMsg(msg) //不要困惑 这里还没到transport. 这里的send需要protocol来决定发送给谁, 怎么发. 所以要用s.proto对象来send. } func (s *socket) Send(b []byte) error { msg := mangos.NewMessage(len(b)) msg.Body = append(msg.Body, b...) //这里打散再拼接性能会不会有问题? 不用个bytes.Buffer啥的? 看这里的情况是拷贝发送. 这里https://gist.github.com/xogeny/b819af6a0cf8ba1caaef似乎是说copy和append性能差不多 return s.SendMsg(msg) } func (s *socket) RecvMsg() (*Message, error) { return s.proto.RecvMsg() } func (s *socket) Recv() ([]byte, error) { msg, err := s.RecvMsg() if err != nil { return nil, err } b := make([]byte, 0, len(msg.Body)) //返回新buffer而不是transport侧底层的buffer; 而且是去了头的; 而且这个buffer是直接make出来的, 不是用的pool的buffer. b = append(b, msg.Body...) msg.Free() //注意这里的Free是返还给pool return b, nil } internal/core/pipe.go 这里的pipe实现了多个接口: pipe.go.Pipe接口, 暂时不知道哪里用了 // Pipe represents the high level interface to a low level communications // channel. There is one of these associated with a given TCP connection, // for example. This interface is intended for application use. // // Note that applications cannot send or receive data on a Pipe directly. type Pipe interface { // ID returns the numeric ID for this Pipe. This will be a // 31 bit (bit 32 is clear) value for the Pipe, which is unique // across all other Pipe instances in the application, while // this Pipe exists. (IDs are recycled on Close, but only after // all other Pipe values are used.) ID() uint32 // Address returns the address (URL form) associated with the Pipe. // This matches the string passed to Dial() or Listen(). Address() string // GetOption returns an arbitrary option. The details will vary // for different transport types. GetOption(name string) (interface{}, error) // Listener returns the Listener for this Pipe, or nil if none. Listener() Listener // Dialer returns the Dialer for this Pipe, or nil if none. Dialer() Dialer // Close closes the Pipe. This does a disconnect, or something similar. // Note that if a dialer is present and active, it will redial. Close() error } protocol.go.ProtocolPipe接口, 这个接口被protocol的实现者使用. 具体来讲, 在protocol的实现实例的AddPipe()被调用的时候, ProtocolPipe这个接口会被传入. // ProtocolPipe represents the handle that a Protocol implementation has // to the underlying stream transport. It can be thought of as one side // of a TCP, IPC, or other type of connection. type ProtocolPipe interface { // ID returns a unique 31-bit value associated with this. // The value is unique for a given socket, at a given time. ID() uint32 // Close does what you think. Close() error // SendMsg sends a message. On success it returns nil. This is a // blocking call. SendMsg(*Message) error // RecvMsg receives a message. It blocks until the message is // received. On error, the pipe is closed and nil is returned. RecvMsg() *Message // SetPrivate is used to set protocol private data. SetPrivate(interface{}) // GetPrivate returns the previously stored protocol private data. GetPrivate() interface{} } pipe定义 在core看起来, pipe的信息很丰富: 它毫无疑问的持有底层transport的pipe对象, 同时它还知道这个pipe的listender和dialer, 即知道对端和自己的信息. 它还持有socket对象, 知道app层的想法.总结: pipe是个中枢. // pipe wraps the Pipe data structure with the stuff we need to keep // for the core. It implements the Pipe interface. type pipe struct { id uint32 p transport.Pipe l *listener d *dialer s *socket closeOnce sync.Once data interface{} // Protocol private added bool closing bool lock sync.Mutex // held across calls to remPipe and addPipe } pipeList pipeList是个map, 保存了所有的pipe实例, 每个pipe实例有个独特的uint32 ID type pipeList struct { pipes map[uint32]*pipe lock sync.Mutex } addPipe和ID生成 socket中调用addPipe会新生成一个uint32 ID, 从随机数开始, 全局唯一. 原理是在全局表map[uint32]struct{}中, 依次找个没被使用的ID. 是的, ID被使用完了还会还到这个map里. 真正的add是调用Add把p按照p.id func (l *pipeList) Add(p *pipe) { l.lock.Lock() if l.pipes == nil { l.pipes = make(map[uint32]*pipe) } l.pipes[p.id] = p l.lock.Unlock() } pipe的send和recv pipe是原始conn的封装, 所以send recv都是直接调用底层transport的接口; 所以到这里就不是应用侧的发送接收了. func (p *pipe) SendMsg(msg *mangos.Message) error { if err := p.p.Send(msg); err != nil { _ = p.Close() return err } return nil } func (p *pipe) RecvMsg() *mangos.Message { msg, err := p.p.Recv() if err != nil { _ = p.Close() //API定义的很明白, 如果底层Recv返回错误, 就close掉这个pipe return nil } msg.Pipe = p //这里值得注意, 从msg里能够拿到pipe的信息, 进而能拿到所有信息. 但不保证到msg的处理的时候, pipe还在. return msg } 为什么底层send或者recv失败要close掉pipe? 那后面还怎么整?而且在close中, 会彻底关闭transport层的连接 func (p *pipe) Close() error { p.closeOnce.Do(func() { // Close the underlying transport pipe first. _ = p.p.Close() // Deregister it from the socket. This will also arrange // for asynchronously running the event callback, and // releasing the pipe ID for reuse. p.lock.Lock() p.closing = true if p.added { p.s.remPipe(p) } p.lock.Unlock() if p.d != nil { // Inform the dialer so that it will redial. go p.d.pipeClosed() //这里值得关注, 里面会延迟调用redial函数重新建立连接, redial会不断重试. } }) return nil } 所以回答上面的问题, 发送或者接收失败都会close掉连接. 但close的流程最后启动了redial流程.也就是说, 比如发送失败的情况下, mangos并不是直接重新再send一次, 而是把整个连接都关掉, 再重新建立新连接来重发. 携带额外数据的典型模式: SetPrivate 使用万能的interface做为输入 func (p *pipe) SetPrivate(i interface{}) { p.data = i } func (p *pipe) GetPrivate() interface{} { return p.data } internal/core/dialer.go 在socket.go里, NewDialer就是new的下面的dialer type dialer struct { sync.Mutex d transport.Dialer s *socket addr string closed bool active bool asynch bool redialer *time.Timer reconnTime time.Duration reconnMinTime time.Duration reconnMaxTime time.Duration closeq chan struct{} } dial流程 func (d *dialer) Dial() error { d.Lock() if d.active { d.Unlock() return mangos.ErrAddrInUse } if d.closed { d.Unlock() return mangos.ErrClosed } d.closeq = make(chan struct{}) d.active = true d.reconnTime = d.reconnMinTime if d.asynch { go d.redial() d.Unlock() return nil } d.Unlock() return d.dial(false) //就是下面的dial函数 } func (d *dialer) dial(redial bool) error { d.Lock() if d.closed { d.Unlock() return errors.ErrClosed } if d.asynch { redial = true } d.Unlock() p, err := d.d.Dial() if err == nil { d.s.addPipe(p, d, nil) //dial成功了就addPipe到socket return nil } //到这里就是不成功, 需要重试 d.Lock() defer d.Unlock() // We're no longer dialing, so let another reschedule happen, if // appropriate. This is quite possibly paranoia. We should only // be in this routine in the following circumstances: // // 1. Initial dialing (via Dial()) // 2. After a previously created pipe fails and is closed due to error. // 3. After timing out from a failed connection attempt. //没让你重试就返回 if !redial { return err } switch err { case mangos.ErrClosed: //对端不给连 // Stop redialing, no further action. default: //缓启动逻辑 // Exponential backoff, and jitter. Our backoff grows at // about 1.3x on average, so we don't penalize a failed // connection too badly. minfact := float64(1.1) maxfact := float64(1.5) actfact := rand.Float64()*(maxfact-minfact) + minfact rtime := d.reconnTime if d.reconnMaxTime != 0 { d.reconnTime = time.Duration(actfact * float64(d.reconnTime)) if d.reconnTime > d.reconnMaxTime { d.reconnTime = d.reconnMaxTime } } d.redialer = time.AfterFunc(rtime, d.redial) //AfterFunc会启动一个goroutine来延迟执行d.redial; 这里还用了经典的 方法转函数的编译器黑科技 } return err } func (d *dialer) redial() { _ = d.dial(true) } 总的来说, dialer实现了对底层transport的dail, 支持后台redial, 支持dail失败重试(不是马上, 而是带缓启动的重试); 支持多种option: OptionReconnectTime OptionMaxReconnectTime OptionDialAsynch internal/core/listener.go listener是对transport的listener的简单包装 type listener struct { sync.Mutex l transport.Listener s *socket addr string closed bool active bool } Listen先查看状态, 再调用transport的Listen. func (l *listener) Listen() error { // This function sets up a goroutine to accept inbound connections. // The accepted connection will be added to a list of accepted // connections. The Listener just needs to listen continuously, // as we assume that we want to continue to receive inbound // connections without limit. l.Lock() if l.closed { l.Unlock() return mangos.ErrClosed } if l.active { l.Unlock() return mangos.ErrAddrInUse } l.active = true l.Unlock() if err := l.l.Listen(); err != nil { l.Lock() l.active = false l.Unlock() return err } go l.serve() //serve()函数才是不断的在循环里Accept连接的主体 return nil } server()函数是被go的 // serve spins in a loop, calling the accepter's Accept routine. func (l *listener) serve() { for { l.Lock() if l.closed { l.Unlock() break } l.Unlock() // If the underlying PipeListener is closed, or not // listening, we expect to return back with an error. if tp, err := l.l.Accept(); err == mangos.ErrClosed { return } else if err == nil { l.s.addPipe(tp, nil, l) //把握手后的tranPipe加入socket. } else { // Debounce a little bit, to avoid thrashing the CPU. time.Sleep(time.Second / 100) } } } 总结 core的核心是pipe. 它承上启下 上面的socket负责符合app的想象, socket的send和recv都要按照protocol的套路来整, 比如多播啥的 下面的dialer和listener对下对接transport的dialer和listener, 提供了额外的重试连接, 后台连接, routine化accpet等功能. dialer和listener成功返回的连接, 被包装成pipe, add到socket里. 核心是 func (s *socket) addPipe(tp transport.Pipe, d *dialer, l *listener) 对dialer来说, 会这样调用d.s.addPipe(p, d, nil) 对listener来说, 会这样调用l.s.addPipe(tp, nil, l) 最终都是add到socket的pipes中, 这是个按ID索引的pipe表 map[uint32]*pipe protocol protocol是高于socket层的抽象, 是socket的场景化模式的归纳, 继承自zeroMQ. 目前支持 bus pair pub/sub pull/push req/rep star 以及以上带x版本的. 下面从最基本的req/rep开始, 看看protocol怎么实现的 protocol/req/req.go 每个protocol都有统一的协议编号, req/rep的大协议号一样, 后四位小协议号不同 // Protocol identity information. const ( Self = protocol.ProtoReq //48 Peer = protocol.ProtoRep //49 SelfName = \"req\" PeerName = \"rep\" ) req.go只依赖protocol包, 它对core包是无感知的.所以虽然core.pipe是唯一的mangos.ProtocolPipe实现者, 但core.pipe也是小写, 而且req.go也不引用core, 那怎么实例化mangos.ProtocolPipe的? 核心结构体定义 req的socket核心当然是socket的具体实例: type socket struct { sync.Mutex defCtx *context // default context ctxs map[*context]struct{} // all contexts (set) ctxByID map[uint32]*context // contexts by request ID nextID uint32 // next request ID closed bool // true if we are closed sendq []*context // contexts waiting to send readyq []*pipe // pipes available for sending } socket持有的context和pipe在本文件定义: type pipe struct { p protocol.Pipe s *socket //反向持有socket closed bool } type context struct { s *socket //反向持有socket cond *sync.Cond resendTime time.Duration // tunable resend time sendExpire time.Duration // how long to wait in send recvExpire time.Duration // how long to wait in recv sendTimer *time.Timer // send timer recvTimer *time.Timer // recv timer resender *time.Timer // resend timeout reqMsg *protocol.Message // message for transmit repMsg *protocol.Message // received reply sendMsg *protocol.Message // messaging waiting for send lastPipe *pipe // last pipe used for transmit reqID uint32 // request ID recvWait bool // true if a thread is blocked in RecvMsg bestEffort bool // if true, don't block waiting in send queued bool // true if we need to send a message closed bool // true if we are closed } 为什么要有context? 因为socket的SendMsg, RecvMsg都依赖context发送 func (s *socket) SendMsg(m *protocol.Message) error { return s.defCtx.SendMsg(m) } req的SendMsg 最顶层是context send func (c *context) SendMsg(m *protocol.Message) error { s := c.s id := atomic.AddUint32(&s.nextID, 1) id |= 0x80000000 // cooked mode, we stash the header m.Header = append([]byte{}, //按照大端字节序发id byte(id>>24), byte(id>>16), byte(id>>8), byte(id)) s.Lock() //每个context只能同时发送一个msg defer s.Unlock() if s.closed || c.closed { return protocol.ErrClosed } //上一次还没发送出去, 哪里有问题了... //因为req/rep是同步模式, 取消上次的发送: 上次的msg会被free掉, 延迟的timer被stop; 并把这个context从socket的sendq里面删掉: s.sendq = append(s.sendq[:i], s.sendq[i+1:]...); 最后广播c.cond.Broadcast() c.cancel() // this cancels any pending send or recv calls c.unscheduleSend() //似乎这个函数多余了, 在c.cancel()中调用过了 c.reqID = id c.queued = true c.sendMsg = m s.sendq = append(s.sendq, c) //本次的context加入socket的sendq if c.bestEffort { // for best effort case, we just immediately go the // reqMsg, and schedule it as a send. No waiting. // This means that if the message cannot be delivered // immediately, it will still get a chance later. s.send() //没看懂上面的注释, 但这里明显是调用socket的原始send, 不管错误马上返回 return nil //这里返回了, 那上面sendq怎么办? 不删掉吗? } expired := false if c.sendExpire > 0 { c.sendTimer = time.AfterFunc(c.sendExpire, func() { //超时就取消的函数 s.Lock() if c.sendMsg == m { expired = true c.cancel() // also does a wake up } s.Unlock() }) } s.send() //触发后台send // This sleeps until someone picks us up for scheduling. // It is responsible for providing the blocking semantic and // ultimately back-pressure. Note that we will \"continue\" if // the send is canceled by a subsequent send. for c.sendMsg == m && !expired && !c.closed { //sync.Cond.Wait接口需要在for里面调用 c.cond.Wait() //典型的场景是上面的s.send把c.sendMsg置为nil, 解除这里的for循环, 退出wait往下走. 但如果readyq中没有可用的pipe, 就是说没有有效的连接, SendMsg会block在这里, 直到有可用的连接为止. } if c.sendMsg == m { //到这里, 要么就是超时了, 要么就是close了;这个m现在是发不了了 c.unscheduleSend() //删除m对应的context c.sendMsg = nil //就是说这个m不发了 c.reqID = 0 if c.closed { return protocol.ErrClosed } return protocol.ErrSendTimeout //返回错误, m并没有发送. } return nil } context send里面调用了socket send 注意这里的send没有任何参数和返回值: 需要知道的已经全部知道, 它要做的就是调度发送. func (s *socket) send() { for len(s.sendq) != 0 && len(s.readyq) != 0 { //有待发送的context 并且有ready的pipe c := s.sendq[0] //取出第一个待发context s.sendq = s.sendq[1:] c.queued = false var m *protocol.Message if m = c.sendMsg; m != nil { //从sendMsg转移到reqMsg c.reqMsg = m c.sendMsg = nil s.ctxByID[c.reqID] = c //按ID记录context c.cond.Broadcast() } else { m = c.reqMsg } m.Clone() //增加msg的引用计数, 该msg是共享模式. p := s.readyq[0] //取第一个ready 的pipe s.readyq = s.readyq[1:] // Schedule a retransmit for the future. c.lastPipe = p //ready的pipe保存到lastPipe里 if c.resendTime > 0 { id := c.reqID c.resender = time.AfterFunc(c.resendTime, func() { //默认一分钟重发 c.resendMessage(id) }) } go p.sendCtx(c, m) //后台发送, 注意go了以后, 就脱离了锁的保护了 } } sendCtx()会调用底层的pipe来发送, 一般发送完还会调度自己继续发送. func (p *pipe) sendCtx(c *context, m *protocol.Message) { s := p.s // Send this message. If an error occurs, we examine the // error. If it is ErrClosed, we don't schedule our self. if err := p.p.SendMsg(m); err != nil { m.Free() if err == protocol.ErrClosed { return } } s.Lock() //重新调度发送还是需要获取锁. if !s.closed && !p.closed { s.readyq = append(s.readyq, p) s.send() //重新调度 } s.Unlock() } 发送小节 req/rep本质上是同步模式, 其实并没有同时发送的说法 但即使是简单的同步发送, 这里也要走三步: 第一步, 要send的message会和context来结合, 被放到socket的sendq中等待发送 第二步, 调用socket.send()来触发调度. 所谓调度就是从sendq取context, 从readyq取pipe 第三步, 用选定的pipe来发送这个context中的msg. 每个待发送的context都会在独立的goroutine中发送. 在本步骤会触发接下来的调度send, 即第二三步是反复互相调用的, 直到所有待发的msg都发送完成. 综上, 这里的设计是典型的同步异步化, 再同步化的过程. 同步异步化实际上是先缓存(指针)再调度的模式, 为的是快速从发送返回; 再次同步化是为了模式简单? SendMsg一般情况下会快速返回, 但在后台发送 发送失败会重传 NewSocket 看了send, 我们再回过头来从最开始的NewSocket看起.NewSocket是对上的顶层API 在实现上, req用了core的MakeSocket接口, 传入一个req.socket实例当作protocolBase // NewSocket allocates a new Socket using the REQ protocol. func NewSocket() (protocol.Socket, error) { return protocol.MakeSocket(NewProtocol()), nil } // NewProtocol allocates a new protocol implementation. func NewProtocol() protocol.Protocol { s := &socket{ nextID: uint32(time.Now().UnixNano()), // quasi-random ctxs: make(map[*context]struct{}), ctxByID: make(map[uint32]*context), } s.defCtx = &context{ s: s, cond: sync.NewCond(s), //NewCond需要传入一个locker, 而s匿名包含了sync.Mutex, 就是个locker resendTime: time.Minute, } s.ctxs[s.defCtx] = struct{}{} return s } AddPipe方法 req.socket的AddPipe方法会被core.socket在顶层dial和listen的时候有新的conn产生时调用, 会把transport层的conn\"通道\"化, 保存在req.socket的readyq里. 对REQ类型的socket来说, 所谓的readyq就是所有对端能提供REP类型的socket. func (s *socket) AddPipe(pp protocol.Pipe) error { p := &pipe{ p: pp, s: s, } pp.SetPrivate(p) s.Lock() defer s.Unlock() if s.closed { return protocol.ErrClosed } s.readyq = append(s.readyq, p) s.send() //有新的rep连接了, 触发一次调度send go p.receiver() //后面会讲 return nil } RecvMsg 无论是send还是recv msg, 都要在context上下文中进行. 因为不同于linux概念上的socket只管\"通道\"; 这里的socket的概念是应用场景下的如何使用socket的抽象, 是有状态的, 必须在状态上下文中进行. func (s *socket) SendMsg(m *protocol.Message) error { return s.defCtx.SendMsg(m) } func (s *socket) RecvMsg() (*protocol.Message, error) { return s.defCtx.RecvMsg() } ok, 从RecvMsg开始: RecvMsg是要看状态的, 而且这个函数并不是调用原始接口recv, 而是\"等待\"msg到位 -- 应该有个后台的routine一直在收包. func (c *context) RecvMsg() (*protocol.Message, error) { s := c.s s.Lock() defer s.Unlock() if s.closed || c.closed { return nil, protocol.ErrClosed } if c.recvWait || c.reqID == 0 { return nil, protocol.ErrProtoState } c.recvWait = true id := c.reqID expired := false if c.recvExpire > 0 { c.recvTimer = time.AfterFunc(c.recvExpire, func() { s.Lock() if c.reqID == id { expired = true c.cancel() } s.Unlock() }) } for id == c.reqID && c.repMsg == nil { c.cond.Wait() //在这里等待其他routine收包完成. 注意, cond.Wait会自动unlock s的mutex锁, 这是c.cond初始化时指明的. wait返回这个tmux锁会自动lock. 也就是说wait期间, s的mutex锁是开放的. 另外, go的mutex锁和routine没有关系, 可以在一个routine里lock, 但安排其他routine去unlock } //另外, cond.Wait是在for里循环的, 是有条件退出的. 即满足条件还是继续wait, 即使中间被唤醒过. m := c.repMsg c.reqID = 0 c.repMsg = nil c.recvWait = false c.cond.Broadcast() //到这里是broadcast给谁呢? if m == nil { if expired { return nil, protocol.ErrRecvTimeout } if c.closed { return nil, protocol.ErrClosed } return nil, protocol.ErrCanceled } return m, nil } 注: context.RecvMsg是一处cond.Wait()点, 另外一个点发生在context.SendMsg. 这两个地方的wait()都是在条件循环里, 即使广播式的c.cond.Broadcast()也没关系. 不是自己想要的退出条件, 还是会继续wait. receiver()函数 前面提到, 当顶层比如dial()成功了之后, 系统会把一个pipe实例AddPipe()到protocol, 也就是add到这里. 这个add流程的最后, 会 go p.receiver() 也就是说一个握了手的连接, 都对应一个go routine, 来receive msg, 可以想象, 这个receiver一定是个for循环, 从底层收了msg之后来缓存到哪里. func (p *pipe) receiver() { s := p.s for { m := p.p.RecvMsg() //从底层收完整的msg if m == nil { //m为nil的时候, 说明底层recv出现了错误, 系统会关闭错误连接, 启动redial流程新建连接. break } if len(m.Body) 接收小结 每个成功握手协商的连接都会被add到protocol里, 被protocol知道. protocol会启动一个receiver routine, 这个receiver负责收包. 然后从headder还原发送时的ID, 查表得到当时的context. 然后把接收到的msg放到c.repMsg = m, 最后返回这个msg REQ小结 从用户侧看来, req的使用很简单: sock, err = req.NewSocket() //新建req的socket, 同时也新建了默认的context; 标准API的send recv都走默认的context err = sock.Dial(url) //核心层会调用url代表的transport类型的Dial, 成功就AddPipe(); 失败会重试, 知道返回errClosed sock.Send([]byte(\"DATE\")) msg, err = sock.Recv() req.NewSocket()调用核心层的MakeSocket(REQ的接口实例)来创建socket. 特别的, 默认的最大rx size是1M REQ的socket有 默认的context sendq 代表的要发送的context readq 代表了所有可用的连接 nextID 用于给每个req分配一个唯一的ID, 这个ID用来反查ctxByID表得到context req.socket.AddPipe()会被核心层框架在新连接建立成功后调用. 这里的AddPipe()启动了receiver routine来不断的收报文. 每个新连接都会AddPipe(), 所以一个连接一个receiver routine 从收到的报文的Header部分恢复request ID, 这个ID是发送的时候填的, 代表了一个req的唯一存在. 用这个ID查到当时发送的context, 并给这个context的等待routine发广播唤醒 socket的接收必须结合context来接收, 默认使用default context来接收 context和具体的连接(pipe)没有绑定关系 RecvMsg()是用户行为, 没有msg的时候会阻塞. 某个连接的receiver routine收到报文后, 查到是这个context的报文, 其发送的广播唤醒会解除本RecvMsg路径上的wait. RecvMsg()还负责唤醒本context上等待的SendMsg() SendMsg也是要结合context, 默认也是默认的context来发送 得到requestID, 和context一起记录到socket的sendq中 真正的发送实际上有点像softirq, 是个触发点: 调用send()的时候, 实际上是trigger了一次后台发送序列, 该序列中会把所有之前的报文都发出去. 真正的\"通道\"级发送是每个报文都在后台发送go p.sendCtx(c, m); 在没有真正发送完成之前, 就\"广播\"到该context可以继续往下走了(见下一条), 每个待发的msg都\"广播\"一次. 如果本context还是在发本msg, 或者没有超时, 就一直wait(一般这个条件不成立) 综上, 发送是softirq式的触发一波异步发送(go p.sendCtx(c, m)), 但不用等真正的报文从\"通道\"socket发送完毕. 用户侧发送和接收都有超时设定 发送有重发机制, 默认1分钟重发. 但似乎性能很堪忧, 因为每个sendq里面的context, 在真正发送之前, 都无条件起一个1分钟定时器来重发. 就不能发送失败了再起定时器重发吗? protocol/rep/rep.go NewSocket NewSocket的套路和REQ一样: 调用核心层protocol的函数MakeSocket, 传入自己的接口实现 // NewSocket allocates a new Socket using the REP protocol. func NewSocket() (protocol.Socket, error) { //传入的参数是REP类型的protocol的实现的实例 return protocol.MakeSocket(NewProtocol()), nil } REP类型的protocol的实现的实例: // NewProtocol allocates a protocol state for the REP protocol. func NewProtocol() protocol.Protocol { s := &socket{ ttl: 8, //默认最大支持8跳, 即中间有7个\"router\"存在 contexts: make(map[*context]struct{}), recvQ: make(chan recvQEntry), // unbuffered! 注释写的很清楚, 非缓冲的channel master: &context{ closeQ: make(chan struct{}), }, } s.master.s = s s.contexts[s.master] = struct{}{} return s } 其核心实现结构体: type socket struct { closed bool ttl int sendQLen int recvQ chan recvQEntry //这个socket只有recvQ, 没有sendQ? 而且这个recvQ可以大约认为只有1个槽位. contexts map[*context]struct{} master *context sync.Mutex } AddPipe 在AddPipe()的时候, 同时起了sender routine和receiver routine. 这里的protocol.Pipe是对底层transport.Pipe的封装, 实现了核心层ProtocolPipe的接口要求. func (s *socket) AddPipe(pp protocol.Pipe) error { p := &pipe{ p: pp, s: s, sendQ: make(chan *protocol.Message, s.sendQLen), //sendQlen是有Q size的 closeQ: make(chan struct{}), } pp.SetPrivate(p) //各种回调函数变身成了名正言顺的接口的使用 s.Lock() if s.closed { s.Unlock() return protocol.ErrClosed } go p.sender() //每个连接一个sender go p.receiver() //每个连接一个receiver s.Unlock() return nil } func (s *socket) RemovePipe(pp protocol.Pipe) { p := pp.GetPrivate().(*pipe) //这里对应了AddPipe的时候SetPrivate(存钱), 这里就来取钱了. close(p.closeQ) } 注意, 这里的sendQLen默认为0; 但一般会调用SetOption()接口把protocol.OptionWriteQLen设为1, 需要手动设置. protocol/rep/rep_test.go-101- p := GetSocket(t, xreq.NewSocket) protocol/rep/rep_test.go:102: MustSucceed(t, s.SetOption(mangos.OptionWriteQLen, 1)) protocol/rep/rep_test.go-103- MustSucceed(t, p.SetOption(mangos.OptionReadQLen, 1)) 本文语境下的 socket有recvQ pipe有sendQ 每个连接一个sender 从sendQ里面拿一个msg, 发送; 发送失败就关闭这个pipe. func (p *pipe) sender() { for { select { case m := 每个连接一个receiver 从底层通道收包, 检查hops是否超过ttl 最后放到socket的recvQ中 func (p *pipe) receiver() { s := p.s outer: for { m := p.p.RecvMsg() //从protocol pipe收msg, 其具体实现在core/pipe.go; 最终是transport层收包 if m == nil { break } // Move backtrace from body to header. // 每4个字节表示一跳, 最开始都会有一跳; 最高位不是0就不是一跳 // hop信息是保存在body里面的, 因为header好像是固定字节的, 不可能把所有hop都放进去. hops := 0 for { if hops >= s.ttl { m.Free() // ErrTooManyHops continue outer } hops++ if len(m.Body) RecvMsg 用户调用Recv, core会调用到这里 func (c *context) RecvMsg() (*protocol.Message, error) { s := c.s s.Lock() //先是检查些状态 if c.closed { s.Unlock() return nil, protocol.ErrClosed } //在这个recv没有结束之前, 不能再次recv; 就是说recv路径下不能有并发 if c.recvWait { s.Unlock() return nil, protocol.ErrProtoState } c.recvWait = true cq := c.closeQ wq := nilQ expireTime := c.recvExpire s.Unlock() if expireTime > 0 { wq = time.After(expireTime) } var err error var m *protocol.Message var p *pipe select { case entry := SendMsg 用户调用send, 核心层会调用到这里 rep服务器的逻辑是, 必须先recv, 再send, 这两者必须成对出现. 而且中间不能有其他的recv或send. func (c *context) SendMsg(m *protocol.Message) error { r := c.s r.Lock() if r.closed || c.closed { r.Unlock() return protocol.ErrClosed } if c.backtrace == nil { //没有recv过, 状态错误 r.Unlock() return protocol.ErrProtoState } p := c.recvPipe //发送来的通道 c.recvPipe = nil bestEffort := c.bestEffort timeQ := nilQ if bestEffort { timeQ = closedQ } else if c.sendExpire > 0 { timeQ = time.After(c.sendExpire) } m.Header = c.backtrace //回复给client的header就是client当时发过来的, 一模一样. c.backtrace = nil cq := c.closeQ r.Unlock() select { //这个是经典的select结构:带timeout, 带close的选择器 case REP小节 从用户侧看来, 使用rep很简单: sock, err = rep.NewSocket() //新建一个rep socket实例 err = sock.Listen(url) //接受连接. 核心层有个goroutine会循环等待新连接. 每个成功建立的连接都会调用protocol的AddPipe()动作, 后者会起后台的sender和receiver msg, err = sock.Recv() //用户发起一次recv, 会从socket.recvQ中接收一个msg, 这样\"众多\"的连接的receiver routine的写recvQ的动作才能往下走. 这个recvQ是阻塞的. 用户在recv和send之间做业务逻辑 err = sock.Send([]byte(d)) //因为recv记录了来的路径到上下文, send的reply会根据上下文沿路返回到client 所有连接背后的receiver收包后, 都会往s.recvQ中写一个entry(也就是msg), 这个recvQ是unbuffered.即所有写都会阻塞, 同时只有一个能写; 在用户没有调用最顶层的recv时, 全部连接的收包都不能继续. 当用户调用了顶层recv处理了一个req后, 下一个req的msg会被送入s.recvQ receiver收包后, 会把msg和代表连接信息的pipe一起, 发给s.recvQ 用户recv并进行了业务逻辑的处理后, 调用send, 把相应msg沿路返回发送回client. recv和send时严格的lock step模式. 即一个context同时只能处理一个请求; 我认为如果业务逻辑复杂, 要么调用顶层APIOpenContext()创建并使用多context来并发处理 -- 可行. 要么就用当前context, 但用go的方式处理业务逻辑? -- 不行, 因为recv和send之间用context来传递上下文, 比如recv后, 用c.backtrace保存req的header; send的时候又要用这个header. 如果产生并发, c.backtrace会被后续的新的recv覆盖. rep支持多跳(hops), hops信息被保存在msg.Body里面; 在receiver里面, hops信息被搬到msg.Header里 在socket级别只有一个unbuffered的recvQ, 没有sendQ; sendQ是代表连接的pipe的属性. 在发送的时候, rep msg会被先放到对应连接的sendQ里, 在sender routine里, 调用protocol pipe来真正发送. 不要被protocol pipe的名字迷惑, 它实际上是核心层core对transport的pipe的包装. 不要使用bestEffort选项, rep会随机\"不发\"响应. 发送 reply失败没有重传, 超时了会返回protocol.ErrSendTimeout req rep疑问 为什么req的设计思路是sync.cond + softirq式的延迟执行, 而rep的设计思路是简单明了的channel? 在req和rep的receiver实现中, 都有下面的操作: 把body移动4个字节给header, 如果说body的前四个字节有特殊意义, 但为什么没有找到对应的发送时候填入的这四个字节? 他们是在哪里填入的?m.Header = append(m.Header, m.Body[:4]...) m.Body = m.Body[4:] 答: 他们是在transport层填入的. 在transport层看来, header和body是一体的, transport先计算总的size(int64), 然后把(size, header, body)写入通道; 而接收的时候, 先用一次系统调用得出size, 再把剩下的内容全部放到body中. 这就解释了为什么上面的protocol层的代码中, 要从body中还原header了... xreq和xrep xreq和xrep是简化版的req和rep, 他们都是同一个协议族. 他们都没有context的概念 实例代码在examples/raw xreq xreq似乎改用了channel, 更简单, 功能似乎也删减了... xreq的socket有recvQ和sendQ的channel, 里面是msg, 默认都是128个长度 在AddPipe()阶段改成了和上面rep一样的每个连接都有后台的sender和receiver 在receiver中, 还是会把对端发来的body的头四个字节当作header, 但这次是直接当header, 而不是appendm.Header = m.Body[:4] m.Body = m.Body[4:] 竟然还是有timeQ的bug???? xreq没有失败自动重传 xrep 默认的recvQ有128的长度, 而rep是unbuffered AddPipe()依然有receiver和sender后台routine 取消了context, 在接收msg的时候, 把pipe信息直接加到msg header中. 发送msg的时候, 看header就知道pipe ID, 就能原路返回 这样搞就必须要求用户直接使用RecvMsg和SendMsg 再看核心层的核心价值 提供了socket对象的实现, 这个对象被protocol层传递给用户, 它就是用户看到的socket. 这个socket对象持有的数据都是私有的, 对外只提供接口 这个socket是client和server的结合体, 既有listener列表, 又有dialer列表 持有代表连接的核心对象pipes, 被组织成按ID(unit32)查询的map 有一把全局socket锁 有其他的属性和标记, 比如是否异步dial, 最大接收size等. protocol层会调用core的MakeSocket()来创建socket, 核心层只是新建了一个socket结构体返回, 没有任何其他的routine的创建. 核心动作Dial和DialOptions最终由用户调用. 注意, 此时socket就已经知道transport的具体类型了. 实际动作包括两步: 先调用和transport约定好的的NewDialer()接口td, err := t.NewDialer(addr, s), 并包装这个td, 构成核心层的dialer结构体. 最后把这个结构体放到s.dialers中. 调用同是核心层的dialer.go中的Dial 检查状态, 调用transport层的Dial方法; 连接成功会调用核心层的addPipe()方法, 后面会讲. redial模式下会后台dial, 这里提前返回nil; 后台redial有重试的避让策略, 避免短时间大量不断重试 非redial模式下, dial不成功马上返回err 核心动作Listen是server端的用户动作 第一步也是处理完option后, 包装transport层的tl, err := t.NewListener(addr, s)对象tl, 做为核心层的listener实例. 第二步是调用transport的listen, 这个listen实际上是bind+listen的原始socket的动作 第三步是起个后台routine来做循环l.l.Accept(), 并把建立好的pipe加到socket: l.s.addPipe(tp, nil, l) 核心动作在Dial有重试, Listen有后台routine做循环accept, 这些\"附加\"的功能是核心层提供的. 核心层的Send/SendMsg, Recv/RecvMsg都是用户调用触发的, 都是直接调用protocol层的SendMsg/RecvMsg 核心层的核心是pipe, 每个连接都会addPipe(); 核心层的pipe是个混合体, 有dialer和listener, 但一般场景下, 不是同时生效. 每个连接, 不管是client dial的来的, 还是server listen得来的, 都对应一个核心层的pipe; 虽然实现在核心层, 但对外是以protocol.Pipe示人的, 即核心层的pipe实现了protocol层(更确切的说法是全局层, 对外层)Pipe规定的方法. addPipe()会新生成一个pipe实例, 并分配唯一的ID(unit32); 这个pipe实例被socket保存在map表里, 方便后面根据ID查询 addPipe()会调用与protocol层约定好的AddPipe()方法s.proto.AddPipe(p); 后者会保存p以便发送, 接收 核心层的pipe的Send和Recv以及Close, 提供失败重连的功能. 因为在addPipe阶段, 会把核心层的pipe实例以protocol.Pipe接口的形式传递给protocol, protocol想真正发送 接收msg的时候, 会调用到核心层pipe提供的SendMsg()和RecvMsg()方法, 这两个方法都是调用transport层的发送接收方法. 特别的, 从核心层收到的msg, 都会记录pipe信息到msg.Pipe域. 任何transport层的send recv失败发生时, 都会调用p.Close(); 这个close的动作会触发重建连接(redial)动作, 说明系统任务这个transport通道已然不行了, 要重建. 同时, 也会调用protocol层的RemovePipe()接口, 来让protocol层知道这个pipe已经失效. 这是个很好的策略, 即发送失败不要简单重试, 而是重新建链. 核心层的Send()实现是拷贝式send, 即用户提供的buffer会被拷贝到新buffer后再send; Recv()也是拷贝式的,即接收的msg.Body会被拷贝到新buffer后返回新buffer; 题外: 作者特别喜欢用append来拷贝buffer msg.Body = append(msg.Body, b...) 但对应的SendMsg()和RecvMsg()就都不会有新buffer产生. 总结 核心层对上直接承接了用户api的接口, 它既规定了protocol要实现的接口: AddPipe() SendMsg() RecvMsg(), 又规定了transport必须实现的接口: NewListener(), NewDialer(), Dial(), Listen(), Accept(). 通过这些规定的接口, 核心层把protocol层和transport层解耦了.核心层对连接的抽象是combine的, 即把client和server的dial和listen过程独立抽象, 但他们的结果都是新建一个pipe, 而这个pipe就承载了后面的发送, 接收功能. 核心层在建立连接后, 会把自己对protocol的承诺(也就是protocol.pipe)的实例, 传递addPipe(p)给protocol层, 这个p就代表了transport的抽象. 同时, 核心层还提供如下功能: 核心层会在redial模式下重试dial 核心层会启动后台routine帮助server来accept连接 核心层在调用transport层的发送接收api失败时, 会断开当前连接, 自动重建新连接. 核心层并没有提供以下机制: 队列 反压 统计 调度 即核心层负责抽象和解耦, 只提供最基本的功能. "},"notes/golang_libp2p.html":{"url":"notes/golang_libp2p.html","title":"Golang p2p网络","keywords":"","body":" libp2p文档 入门 transport QUIC 特性 libp2p里的quic 地址格式 同时支持多transport Nat穿透 circuit relay 协议和stream 内置协议 peer ID 内容路由和peer发现 安全 发布和订阅 发现peer full message和meta message 多路复用 知识点 go的开发状态 transport Stream muxers Crypto channels Connection & Connection Upgrades Peer routing NAT Traversal Discovery 其他 例子 echo makeBasicHost runListener runSender 总结 host routed echo 总结 chat 使用 既然要IP, 那后面那一大串p2p的地址是干啥的? 代码 chat mdns 使用 代码 chat with peer发现, 基于dht 用到的package 代码 总结 pub sub chat room 代码 relay 代码 multipro 代码 总结 总结的总结 扩展 host的option 默认值 规范 如何建立连接 协议协商 连接升级 新建stream 具体实现 隧道 relay Addressing STUN-like Coordination 节点发现机制 几种场景 问答 将来work Peer Ids and Keys key的使用场景 identify协议 mplex协议 Rendezvous集结协议 大概原理 Protocols汇总 基础知识 公钥与私钥 讨论 对称加密（Symmetric Cryptography） 非对称加密（Asymmetric Cryptography） 总结 libp2p文档 https://docs.libp2p.io/introduction/ libp2p是个开发p2p应用的框架. libp2p起源于IPFS, 以模块化的方式把基础的p2p网络分离为独立的libp2p. 那么libp2p有哪些功能? 解决了哪些通用问题? Transport: transport是p2p网络的基础, 负责peer间的实际数据的发送/接收. 为了支持现有的和将来的协议, libp2p定义了一组统一的interface Identity: 身份标识是安全和可靠网络的保证. libp2p用非对称加密的公钥做为peer的标识(PeerId), 这个PeerId就是每个peer的全球唯一名字; 因为p2p的\"open\"本质, PeerId既是名字又是公钥, 其他任何peer都可以通过PeerId得到其公钥. 这样在peer之间的通信是安全的. Security: libp2p支持TLS1.3(就是ssh用的那个, 前身是ssl)和Noise协议在transport上面加密传输数据. Peer Routing: 知道了PeerId, 还要知道网络上怎么访问到它. 经常我们只知道PeerId不知道网络位置, 这就需要peer路由. libp2p用DHT(distributed hash table, 通常每个node都预分配key空间, 所有的node组成一个大的hash表)来查询peer routing, DHT也可以提供其他metadata的key value的查询服务. go支持Kademlia的DHT算法(UDP based, O(log(n))的搜索效率) Content Discovery: 有时候我们不关心对方peer在哪, 只关心我要的内容能不能被其他peer提供. libp2p提供了内容发现的接口, 底层基于同样的Kademlia的DHT技术. Messaging / PubSub: peer发送消息给其他peer是p2p网络的核心, 其中pub/sub模式最为实用.libp2p定义了pub/sub的接口, 用于在给定topic下面发送消息给所有的peer. 目前支持简单粗暴的floodsub和比较高端的gossipsub. 后者的升级版本episub正在开发中. 入门 listen的地址是0号端口, 自动分配. 这里关闭了内置的ping(还有内置的ping?) func main() { ... // start a libp2p node that listens on a random local TCP port, // but without running the built-in ping protocol node, err := libp2p.New(ctx, libp2p.ListenAddrStrings(\"/ip4/127.0.0.1/tcp/0\"), libp2p.Ping(false), ) // configure our own ping protocol pingService := &ping.PingService{Host: node} node.SetStreamHandler(ping.ID, pingService.PingHandler) ... } transport 最常见的transport是TCP, 另外也提到了QUIC(Quick UDP Internet Connections, google发布的基于UDP的协议), 其目标是替代TCP libp2p的设计初衷是transport透明化, 由开发者决定使用哪种transport, 或者同时支持多种transport. QUIC QUIC 与现有 TCP + TLS + HTTP/2 方案相比，有以下几点主要特征： 1）利用缓存，显著减少连接建立时间； 2）改善拥塞控制，拥塞控制从内核空间到用户空间； 3）没有 head of line 阻塞的多路复用； 4）前向纠错，减少重传； 5）连接平滑迁移，网络状态的变更不会影响连接断线。 特性 采用 多路复用 思想，一个连接可以同时承载多个 流 ( stream )，同时发起多个请求。 请求间完全 独立 ，某个请求阻塞甚至报文出错均不影响其他请求。 对比 HTTP 长连接，由于 TCP 是只实现一个字节流，如果请求阻塞，新请求无法发起。 新的安全机制比 TLS 性能更好，而且具有各种攻击防御策略。 前向纠错 TCP 采用 重传 机制，而 QUIC 采用 纠错 机制。 TCP 发生丢包时，需要一个等待延时判断发生了丢包，然后再启动重传机制，这个过程会造成一定的阻塞，影响传输时间。 而 QUIC 则采用一种更主动的方案，有点类似 RAID5 ，每 n 个包额外发一个 校验和包 。 如果这 n 个包中丢 应用程序内实现 QUIC 直接基于客户端(应用进程)实现，而非基于内核，可以快速迭代更新，不需要操作系统层面的改造，部署灵活。 这也是不使用基于迭代升级 TCP 方案的原因 —— TCP 在操作系统内核中实现，很难进行大规模调整以及推广。 连接保持 QUIC 在客户端保存连接标识，当客户端 IP 或者端口发生变化时，可以快速恢复连接 —— 客户端以标识请求服务端，服务端验证标识后感知客户端新地址端口并重新关联，继续通讯。 这对于改善移动端应用连接体验意义重大(从 WiFi 切换到流量)。 libp2p里的quic go-libp2p-quic-transport uses quic-go to provide QUIC support for libp2p. 地址格式 libp2p使用一种叫multiaddr的东西来描述地址, 这个描述兼容所有的transport类型.比如 /ip4/7.7.7.7/tcp/6543 libp2p的通信单元叫peer, 用peerId来唯一标识; 必须指定peerId来建立安全信道来通信. /ip4/1.2.3.4/tcp/4321/p2p/QmcEPrat8ShnCph8WjkREzt5CPXF2RwhYxYBALDcLC1iV6 同时支持多transport 一般libp2p是支持多transport, 比如一个服务可以同时对使用TCP的一个守护进程和使用websocket的浏览器服务. 能实现对多transport的多路复用, 是因为libp2p有个switch层(也叫swarm层), 提供了 协议协商: 当客户端dialing一个新的stream的时候, 它会发送protocol id来和服务端协商, 如果服务端也支持该协议, 就会返回这个协议ID, 之后的通信会使用这个协商好的协议来进行. 如果不支持, 返回不支持, 那么客户端要么停止, 要么发起另外一个protocol ID来进行下一轮协商. -- 想法: encoding也要协商 stream多路复用: 就是使用一个物理的transport(比如一个TCP连接)来支持多个逻辑上的stream的通信. 一个stream有个很短的header来标记stream. libp2p使用mplex multiplexing实现, 同时也提供了yamux和spdy实现. mplex: 每次通信都会加个header, payload是固定长度的data segment. header是个base128的变长数, 最长9字节. mplex基于可靠的, 保序的传输层协议比如TCP或UDS. 这个应该是默认的.| header | length | data | | uvarint | uvarint | 'length' bytes | Yamux: 另外一种multiplexing, 支持双向stream, 支持流控, 同时支持大量stream消耗比较小. spdy: chrome浏览器项目, 面向web协议的 建立安全连接 Nat穿透 代码在go-libp2p-nat 有的路由器支持UPnP或nat-pmp协议, libp2p会尝试自动配置端口映射. STUN打洞: 内网能dial出去, 是因为内网路由器会map这个内部端口到对外可见的端口. 有的路由会把incoming的到这个端口的连接也转发给内网. libp2pyong SO_REUSEPORT选项复用这个IP的端口号 内网出去方向的连接会在内网路由器分配一个对外的端口号, 这个端口号外部可见, 但内部是不知道的. 那么可以让外部的peer告诉我们我们的外部地址是什么, 然后我们就可以告诉p2p网络, 这个地址可以到达我们. 那么谁来当这个外部的观察者呢?传统的STUN实现需要固定的外部观察者, 而libp2p使用了identify protocol在peer间交换信息:https://github.com/libp2p/specs/tree/master/identify, 基本上是个/ipfs/id/1.0.0这个协议ID, 交换的信息如下, 其中就有observedAddrmessage Identify { optional string protocolVersion = 5; optional string agentVersion = 6; optional bytes publicKey = 1; repeated bytes listenAddrs = 2; optional bytes observedAddr = 4; repeated string protocols = 3; } AutoNAT服务: 在identify告知我们的外部观察port后, 还可以让它peer回拨我们的port, 这需要使能AutoNAT服务, 让对方回拨. 回拨成功就知道这个端口可以允许穿透内网. Circuit Relay (TURN): 有时候内网就是出不去, 这个协议可以先在内网找个跳板节点再访问. 比如内网的QmAlice想让QmBob访问, 她可以找个relay的节点:/ip4/7.7.7.7/tcp/55555/p2p/QmRelay/p2p-circuit/p2p/QmAlice告知QmBob, 这样QmBob就可以通过这个relay的节点访问QmAlice了. circuit relay 打开了autorelay的话, libp2p会使用内容路由的接口, 自动发现提供relay服务的peer. 当autoNAT服务发现我们在受限的NAT网络里面后, autorelay就起作用了 discovering relay nodes around the world, establishing long-lived connections to them, and advertising relay-enabled addresses for ourselves to our peers, thus making ourselves routable through delegated routing. When AutoNAT service detects we’re behind a NAT that blocks inbound connections, Autorelay jumps into action, and the following happens: We locate candidate relays by running a DHT provider search for the /libp2p/relay namespace. We select three results at random, and establish a long-lived connection to them (/libp2p/circuit/relay/0.1.0 protocol). Support for using latency as a selection heuristic will be added soon. We enhance our local address list with our newly acquired relay-enabled multiaddrs, with format: /ip4/1.2.3.4/tcp/4001/p2p/QmRelay/p2p-circuit, where: 1.2.3.4 is the relay’s public IP address, 4001 is the libp2p port, and QmRelay is the peer ID of the relay. Elements in the multiaddr can change based on the actual transports at use. We announce our new relay-enabled addresses to the peers we’re already connected to via the IdentifyPush protocol. The last step is crucial, as it enables peers to learn our updated addresses, and in turn return them when another peer looks us up. 协议和stream 这里说的协议是应用定义的协议, 协议id是任意字符串, 但按惯例是类似这样的: /my-app/amazing-protocol/1.0.1 两端在协议对接的时候, 有个协商过程. 协议id使用全匹配方式 每个协议都可以set一个handler. 也可以set一个match handler, 自己进行协议匹配. 可以注册多个协议匹配handler, 当一个协议没有确切的handler的时候, 会逐个调用匹配handler来匹配. 可以使用MultistreamSemverMatcher来匹配版本号 dial的时候要传入协议id, 可以是多个, 通常是一个协议的多个版本. stream可以是半关闭的, 即以把写关了, 只保留读. stream地下的switch层是加密的, 但stream看到的是解密后的数据 内置协议 内置协议都使用protobuf编码 /ipfs/ping/1.0.0: 一个peer来ping, 另外一个响应. 记录lattency /ipfs/id/1.0.0: 交换peerId的协议, 告知对方自己的信息. 特别的, identify协议的响应消息里有observedAddr, 用于告知对方它的外部观测的地址. /ipfs/id/push/1.0.0: 用于通知别人自己的网络有变化 /secio/1.0.0: secure IO. 用于加密通信. 因为peerId实际上是从公钥得出的, 所以可以验证签名是否正确. 这样就不需有像TLS中Certificate Authority的过程. -- 注意, secio是默认的加密协议, 但以后的默认加密协议是TLS1.3 /ipfs/kad/1.0.0: dht协议 /libp2p/circuit/relay/0.1.0: realy协议 peer ID 每个peer都有自己知道的私钥, 并把公钥公开到p2p网络中, 公钥的hash就是peer ID. 所以 连接上的peer都知道每个人的公钥 peer ID其实不是字符串, 只是hash值用base58编码后, 字符串化了. peer info结构包括peerID和这个peer监听的multiaddr地址. 每个peer都有个\"电话本\", 记录它知道的peer info 内容路由和peer发现 待续 安全 libp2p的底层通信是加密, 但只是在传输层. 一个p2p网络通常还需要某种\"权限控制\"策略, 来决定谁有权限去做什么. libp2p没有提供内置的\"权限控制\"功能. 发布和订阅 基于gossipsub协议的消息扩散策略. Reliability: All messages get delivered to all peers subscribed to the topic. Speed: Messages are delivered quickly. Efficiency: The network is not flooded with excess copies of messages. Resilience: Peers can join and leave the network without disrupting it. There is no central point of failure. Scale: Topics can have enormous numbers of subscribers and handle a large throughput of messages. Simplicity: The system is simple to understand and implement. Each peer only needs to remember a small amount of state. 发现peer 这里的发现指发现同一个topic的peer, 如果gossipsub已经基于一个建立好的p2p网络, 怎么知道谁订阅了某个topic呢? 应用需要自己发现peer, 用下面的方法: Distributed hash tables Local network broadcasts Exchanging peer lists with existing peers Centralized trackers or rendezvous points Lists of bootstrap peers full message和meta message 比如上图中, 有连接的节点都是一个topic下面的, 但只有粗实线的连接被用来传递\"gossip\". 上图配置成3个peer来做粗线, 通常配置成6, 一般在4-12之间. meta data的连接用来维护gossip网络 粗线和细线可以互相转换: https://docs.libp2p.io/concepts/publish-subscribe/#grafting-and-pruning subscribe和unsubscribe信息交换: 每个peer都和它相连的peer交换各自的topic订阅信息. 发送消息: 消息会被粗线连接扩散. 谣言八卦: 每秒钟, 细线连接间都互相八卦说我看到了什么gossip. 目的是补充实线网络中的遗漏, 如果一个peer发现它得到的八卦总是少点, 它就需要和别人多增加实线连接. 不需要订阅就可以发送某个topic下面的消息: 随机选择6个连接来发送 信息聚合: 不同类型的信息可以聚合在一起, 用一个报文发送 多路复用 多路复用的目的是使用一个底层transport来服务多个stream. 这个switch(aka swarm)是在第一次建立transport连接的时候协商的. libp2p提供: mplex:libp2p自己写的, 比较简单 yamux: 复杂点, 支持一些高级特性比如流控 quic: 基于UDP的用户态TCP. 它实际上是一个transport, 但有native multiplexing能力. libp2p会在支持quic协议的节点直接使用这个能力. spdy: 目标是http2.0 muxado: 可能比较小众... 知识点 https://docs.libp2p.io/reference/glossary/ Circuit Relay: 在两个不能直接连接的节点之间, 通过第三个willing node来连接. 比如两个不同内网的节点通过共同的外网节点relay; 或者一个说tcp, 一个说websocket的节点, 通过第三个说双语的节点来talk. DHT: 分布式的hash表. peer routing和content routing依赖DHT. 可以用来做内容发现和服务广播. Multihash: 用来产生peerID和其他libp2p系统内的hash值的.multihash在原hash基础上加了两个字节的头.这两个字节表示了hash算法和长度. 比如用base58编码后的QmYyQSo1c1Ym7orWxLYvCrM2EmxFTANf8wXmmE7DWjhx5N, Qm就是加的两个字节, 表示SHA-256, 256bit长度. go的开发状态 https://libp2p.io/implementations transport libp2p-tcp libp2p-quic libp2p-websockets 支持比较好 Stream muxers libp2p-multiplex libp2p-yamux 已ready Crypto channels libp2p-secio libp2p-tls libp2p-noise Connection & Connection Upgrades libp2p-conn Peer routing libp2p-kad-dht NAT Traversal libp2p-nat还在开发中, 不稳定 Discovery bootstrap random-walk mdns-discovery都OK 其他 libp2p-floodsub libp2p-gossipsub crypto libp2p-ping libp2p-peer-info libp2p-peer-book libp2p-swarm都OK. 例子 echo 运行两个echo实例, 一个是listener, 一个是sender 它们都是个basichost, 它是更底层的go-libp2p swarms的封装. swarm处理更细节的流, 连接, 多协议多路复用等. makeBasicHost // makeBasicHost creates a LibP2P host with a random peer ID listening on the // given multiaddress. It won't encrypt the connection if insecure is true. func makeBasicHost(listenPort int, insecure bool, randseed int64) (host.Host, error) { var r io.Reader if randseed == 0 { r = rand.Reader } else { r = mrand.New(mrand.NewSource(randseed)) } // Generate a key pair for this host. We will use it at least // to obtain a valid host ID. priv, _, err := crypto.GenerateKeyPairWithReader(crypto.RSA, 2048, r) if err != nil { return nil, err } opts := []libp2p.Option{ libp2p.ListenAddrStrings(fmt.Sprintf(\"/ip4/127.0.0.1/tcp/%d\", listenPort)), libp2p.Identity(priv), libp2p.DisableRelay(), } if insecure { opts = append(opts, libp2p.NoSecurity) } return libp2p.New(context.Background(), opts...) } crypto.GenerateKeyPairWithReader: 先用RSA生成私钥, 私钥可以推导出公钥 libp2p.ListenAddrStrings: 使用写死的地址: /ip4/127.0.0.1/tcp/%d libp2p.Identity: 使用上面的priv key生成PeerId libp2p.DisableRelay(): 禁止relay transport libp2p.NoSecurity: libp2p默认都是加密的, 初非用这个显式声明非加密 libp2p.New: 新建libp2p node的核心接口, 支持很多配置选项, 如果没有显式的指定, 有合理的默认值: 默认监听/ip4/0.0.0.0/tcp/0 和 /ip6/::/tcp/0 默认使用TCP和websocket传输协议 默认使用多路复用协议yamux和mplux, 多路复用协议会在peer间协商? 默认加密, 使用noise协议或者TLS 默认使用RSA 2048产生随机的PeerId 默认用空的peerstore(用于peer routing) 有个API NewWithoutDefaults没有任何default配置, 但不建议使用. Host是p2p网络的一个实体, 它既是server, 可以提供服务; 又是client, 可以发送请求. 所以叫host, 是二合一. host有一系列聚合的抽象: type Host interface { // ID returns the (local) peer.ID associated with this Host ID() peer.ID // Peerstore returns the Host's repository of Peer Addresses and Keys. Peerstore() peerstore.Peerstore // Returns the listen addresses of the Host Addrs() []ma.Multiaddr // Networks returns the Network interface of the Host Network() network.Network // Mux returns the Mux multiplexing incoming streams to protocol handlers Mux() protocol.Switch // Connect ensures there is a connection between this host and the peer with // given peer.ID. Connect will absorb the addresses in pi into its internal // peerstore. If there is not an active connection, Connect will issue a // h.Network.Dial, and block until a connection is open, or an error is // returned. // TODO: Relay + NAT. Connect(ctx context.Context, pi peer.AddrInfo) error // SetStreamHandler sets the protocol handler on the Host's Mux. // This is equivalent to: // host.Mux().SetHandler(proto, handler) // (Threadsafe) SetStreamHandler(pid protocol.ID, handler network.StreamHandler) // SetStreamHandlerMatch sets the protocol handler on the Host's Mux // using a matching function for protocol selection. SetStreamHandlerMatch(protocol.ID, func(string) bool, network.StreamHandler) // RemoveStreamHandler removes a handler on the mux that was set by // SetStreamHandler RemoveStreamHandler(pid protocol.ID) // NewStream opens a new stream to given peer p, and writes a p2p/protocol // header with given ProtocolID. If there is no connection to p, attempts // to create one. If ProtocolID is \"\", writes no header. // (Threadsafe) NewStream(ctx context.Context, p peer.ID, pids ...protocol.ID) (network.Stream, error) // Close shuts down the host, its Network, and services. Close() error // ConnManager returns this hosts connection manager ConnManager() connmgr.ConnManager // EventBus returns the hosts eventbus EventBus() event.Bus } runListener func runListener(ctx context.Context, ha host.Host, listenPort int, insecure bool) { fullAddr := getHostAddress(ha) log.Printf(\"I am %s\\n\", fullAddr) // Set a stream handler on host A. /echo/1.0.0 is // a user-defined protocol name. ha.SetStreamHandler(\"/echo/1.0.0\", func(s network.Stream) { log.Println(\"listener received new stream\") if err := doEcho(s); err != nil { log.Println(err) s.Reset() } else { s.Close() } }) log.Println(\"listening for connections\") if insecure { log.Printf(\"Now run \\\"./echo -l %d -d %s -insecure\\\" on a different terminal\\n\", listenPort+1, fullAddr) } else { log.Printf(\"Now run \\\"./echo -l %d -d %s\\\" on a different terminal\\n\", listenPort+1, fullAddr) } // Wait until canceled getHostAddress: 使用mutliaddr的Encapsulate函数, 把peerId包装进network地址中:I am /ip4/127.0.0.1/tcp/10000/p2p/QmW1Ze4AbEtWtTg5ibcnsLgUPJsZ3wh1VuGzhqHtrvAp2e SetStreamHandler(pid protocol.ID, handler network.StreamHandler): 给mux层设定一个handler. 第一个参数是个string, 描述协议的, 第二个参数handler的签名是type StreamHandler func(Stream), 有点像OnConnect(), 同样的是会被框架传入Stream, 这是个在mux层之上的概念, 提供逻辑上的2个agent之间的双向通信, 是io reader和io writer, 它的下面是个multiplexer. 这里这个handler很简单, 就是doEcho. 因为s network.Stream是框架传入的, 能直接做io读写操做. // doEcho reads a line of data a stream and writes it back func doEcho(s network.Stream) error { buf := bufio.NewReader(s) str, err := buf.ReadString('\\n') if err != nil { return err } log.Printf(\"read: %s\", str) _, err = s.Write([]byte(str)) return err } runSender sender通过用于输入-d的地址, 比如/ip4/127.0.0.1/tcp/10000/p2p/QmW1Ze4AbEtWtTg5ibcnsLgUPJsZ3wh1VuGzhqHtrvAp2e来dial listener. 通过multiaddr的Decapsulate操做, 把targetPeerAddr提取出来:/ip4/127.0.0.1/tcp/10000 然后add到peerstore中: ha.Peerstore().AddAddr(peerid, targetAddr, peerstore.PermanentAddrTTL) s, err := ha.NewStream(context.Background(), peerid, \"/echo/1.0.0\"): NewStream连listener, 使用相同的echo1.0.0协议. 然后直接调用s.Write([]byte(\"Hello, world!\\n\"))来发送. 总结 listener和sender都是host, host是p2p网络的一个node host可以定制, 列表在https://pkg.go.dev/github.com/libp2p/go-libp2p listener指定stream的处理函数, 有新的stream连接的时候会调用. RSA等非对称算法生成的key是peerId multiaddr能够处理多地址协议, 统一地址的表达方式 空的peerstore不能\"发现\"peer, 所以本例的连接信息还是要用户输入. host.Newstream(ctx, peerId, ...protocolId)用于向指定peerId发起一个stream连接, protocolId是个[]string, 指定要走的用户(多个)协议, 比如\"/echo/1.0.0\" stream的抽象是io reader writer, 面向字节流的. sender类似client, client的逻辑是建立连接然后直接write host 之前的echo的例子只是用了简单的option. 我们在这个例子里会用一些常用的option, 使能了routing, 使nat网络也能发现. // To construct a simple host with all the default settings, just use `New` h, err := libp2p.New(ctx) //h2是个定制的host h2, err := libp2p.New(ctx, // Use the keypair we generated, 使用自定义key libp2p.Identity(priv), // Multiple listen addresses, 指定多个listen地址 libp2p.ListenAddrStrings( \"/ip4/0.0.0.0/tcp/9000\", // regular tcp connections \"/ip4/0.0.0.0/udp/9000/quic\", // a UDP endpoint for the QUIC transport ), // support TLS connections libp2p.Security(libp2ptls.ID, libp2ptls.New), // support secio connections libp2p.Security(secio.ID, secio.New), // support QUIC - experimental libp2p.Transport(libp2pquic.NewTransport), // support any other default transports (TCP) libp2p.DefaultTransports, // DefaultTransports是tcp和websocket // Let's prevent our peer from having too many // connections by attaching a connection manager. libp2p.ConnectionManager(connmgr.NewConnManager( 100, // Lowwater 400, // HighWater, time.Minute, // GracePeriod )), // 下面这几个配置了NAT可达 // Attempt to open ports using uPNP for NATed hosts. libp2p.NATPortMap(), // Let this host use the DHT to find other hosts libp2p.Routing(func(h host.Host) (routing.PeerRouting, error) { idht, err = dht.New(ctx, h) return idht, err }), // Let this host use relays and advertise itself on relays if // it finds it is behind NAT. Use libp2p.Relay(options...) to // enable active relays and more. libp2p.EnableAutoRelay(), // If you want to help other peers to figure out if they are behind // NATs, you can launch the server-side of AutoNAT too (AutoRelay // already runs the client) // // This service is highly rate-limited and should not cause any // performance issues. libp2p.EnableNATService(), ) 配置好了host, 为了让这个host高可达, 还要连接预配置的bootstrap nodes. // This connects to public bootstrappers for _, addr := range dht.DefaultBootstrapPeers { pi, _ := peer.AddrInfoFromP2pAddr(addr) // We ignore errors as some bootstrap peers may be down // and that is fine. h2.Connect(ctx, *pi) } routed echo 在简单echo和host的基础上, routed echo配置一个host使用DHT,连接bootstrap节点, 使其能够被peer发现. 其他peer可以只根据peerId来访问它. makeRoutedHost函数首先生成私钥, 然后配置options如下, 此时它还是个basic host opts := []libp2p.Option{ libp2p.ListenAddrStrings(fmt.Sprintf(\"/ip4/0.0.0.0/tcp/%d\", listenPort)), libp2p.Identity(priv), libp2p.DefaultTransports, libp2p.DefaultMuxers, libp2p.DefaultSecurity, libp2p.NATPortMap(), } ctx := context.Background() basicHost, err := libp2p.New(ctx, opts...) 然后新建datastroe // Construct a datastore (needed by the DHT). This is just a simple, in-memory thread-safe datastore. dstore := dsync.MutexWrap(ds.NewMapDatastore()) // Make the DHT dht := dht.NewDHT(ctx, basicHost, dstore) // Make the routed host -- 这步就是把basic host包装成routed host // 可以看出, routed host需要basic host和dht routedHost := rhost.Wrap(basicHost, dht) //这里的rhost就是go-libp2p/p2p/host/routed // connect to the chosen ipfs nodes // for每个bootstrap peer, 调用host.Connect // 奇怪的是在connect之前就把每个bootstrap peer的地址加到自己的peerstore中了: ph.Peerstore().AddAddrs(p.ID, p.Addrs, peerstore.PermanentAddrTTL) // -- 不奇怪, 要先知道地址信息, 再connect. 因为connect只需要peer ID err = bootstrapConnect(ctx, routedHost, bootstrapPeers) // Bootstrap the host err = dht.Bootstrap(ctx) bootstrap peer如何确定的? 可以是全局写死的, 比如: IPFS_PEERS = convertPeers([]string{ \"/ip4/104.131.131.82/tcp/4001/p2p/QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ\", \"/ip4/104.236.179.241/tcp/4001/p2p/QmSoLPppuBtQSGwKDZT2M73ULpjvfd3aZ6ha4oFGL1KrGM\", \"/ip4/128.199.219.111/tcp/4001/p2p/QmSoLSafTMBsPKadTEgaXctDQVcqN88CNLHXMkTNwMKPnu\", \"/ip4/104.236.76.40/tcp/4001/p2p/QmSoLV4Bbm51jM9C4gDYZQ9Cy3U6aXMJDAbzgu2fzaDs64\", \"/ip4/178.62.158.247/tcp/4001/p2p/QmSoLer265NRgSp2LA3dPaeykiS1J6DifTC88f5uVQKNAd\", }) 或者从本地http server获取LOCAL_PEER_ENDPOINT = \"http://localhost:5001/api/v0/id\" resp, err := http.Get(LOCAL_PEER_ENDPOINT) 总结 client和server都连接了bootstrap节点, 都有dht 用到的包: go-libp2p basichost 基本的host, 没有peer发现的能力 go-libp2p-kad-dht peer ID查找 go-libp2p/p2p/host/routed 包装上述两个对象成为routed host routed host创建后, 就可以调用NewStream打开一个双向的stream了, 这个stream就可以read write了. host也可以调用SetStreamHandle来listen incoming的连接. chat 一个简单的聊天应用. 假设 A在内网, B在外网(有公网IP) 或者A和B都在局域网 使用 在138机器上开启A:根据提示, 把127的地址改成大网IP. 在190服务器上连接A. 190服务器的这个chat叫做B.我又在138的docker环境里开了C, 同样连接A A能看到B和C发的消息 A的回复只能B看到 C不会收到回复 必须显式指定ip才能连上 所以chat只是两个人的chat, 第一次连上的两个. 既然要IP, 那后面那一大串p2p的地址是干啥的? 答: 校验用的. 如果填错p2p地址, 比如B错填了A的地址, 会报错, 不让连接 代码 打印颜色 // Green console colour: \\x1b[32m // Reset console colour: \\x1b[0m fmt.Printf(\"\\x1b[32m%s\\x1b[0m> \", str) 如果初始随机值一样, 得出的key每次都一样的: if *debug { // Use the port number as the randomness source. // This will always generate the same host ID on multiple executions, if the same port number is used. // Never do this in production code. r = mrand.New(mrand.NewSource(int64(*sourcePort))) } else { r = rand.Reader } 全0的地址会listen所有的网络接口// 0.0.0.0 will listen on any interface device. sourceMultiAddr, _ := multiaddr.NewMultiaddr(fmt.Sprintf(\"/ip4/0.0.0.0/tcp/%d\", port)) \"服务端\"要SetStreamHandler 可以让系统自动分配端口 // Let's get the actual TCP port from our listen multiaddr, in case we're using 0 (default; random available port). var port string for _, la := range h.Network().ListenAddresses() { if p, err := la.ValueForProtocol(multiaddr.P_TCP); err == nil { port = p break } } if port == \"\" { log.Println(\"was not able to find actual local port\") return } \"客户端\"不仅要建立自己的host, 还要connect\"服务端\". 要先从-d选项的字符串里提取地址信息 // Turn the destination into a multiaddr. maddr, err := multiaddr.NewMultiaddr(destination) if err != nil { log.Println(err) return nil, err } // Extract the peer ID from the multiaddr. info, err := peer.AddrInfoFromP2pAddr(maddr) if err != nil { log.Println(err) return nil, err } 先在peerstore里面add对端的地址, 然后就可以发起连接了. // Add the destination's peer multiaddress in the peerstore. // This will be used during connection and stream creation by libp2p. h.Peerstore().AddAddrs(info.ID, info.Addrs, peerstore.PermanentAddrTTL) // Start a stream with the destination. // Multiaddress of the destination peer is fetched from the peerstore using 'peerId'. s, err := h.NewStream(context.Background(), info.ID, \"/chat/1.0.0\") 使用bufio可以把阻塞的io读写变成非阻塞的: // Start a stream with the destination. // Multiaddress of the destination peer is fetched from the peerstore using 'peerId'. s, err := h.NewStream(context.Background(), info.ID, \"/chat/1.0.0\") // Create a buffered stream so that read and writes are non blocking. rw := bufio.NewReadWriter(bufio.NewReader(s), bufio.NewWriter(s)) chat mdns 使能里mdns发现的chat. 其他的和chat一样 使用 ./chat-with-mdns -port 6666 ./chat-with-mdns -port 6668 代码 先New一个基本的host, 这个和普通的chat一样; SetStreamHandler也一样 // 0.0.0.0 will listen on any interface device. sourceMultiAddr, _ := multiaddr.NewMultiaddr(fmt.Sprintf(\"/ip4/%s/tcp/%d\", cfg.listenHost, cfg.listenPort)) // libp2p.New constructs a new libp2p Host. // Other options can be added here. host, err := libp2p.New( ctx, libp2p.ListenAddrs(sourceMultiAddr), libp2p.Identity(prvKey), ) // Set a function as stream handler. // This function is called when a peer initiates a connection and starts a stream with this peer. host.SetStreamHandler(protocol.ID(cfg.ProtocolID), handleStream) 使能mDNS, 等待并连接peer. peerChan := initMDNS(ctx, host, cfg.RendezvousString) peer := go-libp2p/p2p/discovery提供了mDNS发现功能. 这个discovery包目前只有mdns发现方式. discoveryNotifee是被通知的一方. mdns发现新的peer, 就会发送到PeerChan, 这样就接上前面的从peerChan读peer信息, 再connect, 再NewStream的操做了. //Initialize the MDNS service func initMDNS(ctx context.Context, peerhost host.Host, rendezvous string) chan peer.AddrInfo { // An hour might be a long long period in practical applications. But this is fine for us ser, err := discovery.NewMdnsService(ctx, peerhost, time.Hour, rendezvous) //register with service so that we get notified about peer discovery n := &discoveryNotifee{} n.PeerChan = make(chan peer.AddrInfo) ser.RegisterNotifee(n) return n.PeerChan } \"github.com/whyrusleeping/mdns\"是底层提供mdns的包, mdns的query返回ServiceEntry // ServiceEntry is returned after we query for a service type ServiceEntry struct { Name string Host string AddrV4 net.IP AddrV6 net.IP Port int Info string // peerID就是用这个field来传递的. InfoFields []string Addr net.IP // @Deprecated hasTXT bool } p2p/discovery/mdns.go的NewMdnsService函数中, 起了个mdns.NewMDNSService(), 自己的ID就会被当作Info传进去 info := []string{myid} if serviceTag == \"\" { serviceTag = ServiceTag } service, err := mdns.NewMDNSService(myid, serviceTag, \"\", \"\", port, ipaddrs, info) chat with peer发现, 基于dht 用到的package \"github.com/libp2p/go-libp2p\" \"github.com/libp2p/go-libp2p-core/network\" \"github.com/libp2p/go-libp2p-core/peer\" \"github.com/libp2p/go-libp2p-core/protocol\" \"github.com/libp2p/go-libp2p-discovery\" dht \"github.com/libp2p/go-libp2p-kad-dht\" multiaddr \"github.com/multiformats/go-multiaddr\" 代码 还是先New一个host // libp2p.New constructs a new libp2p Host. Other options can be added // here. host, err := libp2p.New(ctx, libp2p.ListenAddrs([]multiaddr.Multiaddr(config.ListenAddresses)...), ) // Set a function as stream handler. This function is called when a peer // initiates a connection and starts a stream with this peer. host.SetStreamHandler(protocol.ID(config.ProtocolID), handleStream) 后面这里厉害了, 注意看注释: New了一个dht还不够, 还需要Bootstrap, 这样每个node都有个dht的拷贝, 即使初始节点down了也不影响后续的peer发现. Bootstrap这个函数会每5分钟后台刷新这个dht // Start a DHT, for use in peer discovery. We can't just make a new DHT // client because we want each peer to maintain its own local copy of the // DHT, so that the bootstrapping node of the DHT can go down without // inhibiting future peer discovery. kademliaDHT, err := dht.New(ctx, host) // Bootstrap the DHT. In the default configuration, this spawns a Background // thread that will refresh the peer table every five minutes. err = kademliaDHT.Bootstrap(ctx) 接下来就要连接bootstrap节点了for _, peerAddr := range config.BootstrapPeers { peerinfo, _ := peer.AddrInfoFromP2pAddr(peerAddr) go func() { host.Connect(ctx, *peerinfo) } } 接下来要声明\"meet me here\"服务, 然后通过FindPeer功能找到\"同类\"discovery这块在go-libp2p-discovery/routing.go // We use a rendezvous point \"meet me here\" to announce our location. // This is like telling your friends to meet you at the Eiffel Tower. logger.Info(\"Announcing ourselves...\") routingDiscovery := discovery.NewRoutingDiscovery(kademliaDHT) discovery.Advertise(ctx, routingDiscovery, config.RendezvousString) logger.Debug(\"Successfully announced!\") // Now, look for others who have announced // This is like your friend telling you the location to meet you. logger.Debug(\"Searching for other peers...\") peerChan, err := routingDiscovery.FindPeers(ctx, config.RendezvousString) RoutingDiscovery是个子类, 继承了go-libp2p-core/routing的ContentRoutingContentRouting是对内容的routing, 那么内容由一个hash值标识(Cid) // RoutingDiscovery is an implementation of discovery using ContentRouting. // Namespaces are translated to Cids using the SHA256 hash. type RoutingDiscovery struct { routing.ContentRouting } ContentRouting在go-libp2p-core/routing/routing.go // ContentRouting is a value provider layer of indirection. It is used to find // information about who has what content. // // Content is identified by CID (content identifier), which encodes a hash // of the identified content in a future-proof manner. type ContentRouting interface { // Provide adds the given cid to the content routing system. If 'true' is // passed, it also announces it, otherwise it is just kept in the local // accounting of which objects are being provided. Provide(context.Context, cid.Cid, bool) error // Search for peers who are able to provide a given key // // When count is 0, this method will return an unbounded number of // results. FindProvidersAsync(context.Context, cid.Cid, int) 得到了peerChan, 那么最后就range这个peerChan, 对每个peer都Newstream一把. 总结 和上个chat不同的是, 这里使用dht来发现peer. 上一个是用mdns协议在局域网发现. pub sub chat room 代码 还是首先建立一个基本的host, 使用系统分配的端口 ctx := context.Background() // create a new libp2p Host that listens on a random TCP port h, err := libp2p.New(ctx, libp2p.ListenAddrStrings(\"/ip4/0.0.0.0/tcp/0\")) 用gossipsub协议建立一个pubsub服务 // create a new PubSub service using the GossipSub router ps, err := pubsub.NewGossipSub(ctx, h) 用到了libp2p/go-libp2p-pubsub包, 这个包重度使用了map, 有点复杂. gossip网络扩散\"gossip\", 但每个peer只扩散大约4个周边的peer. node对topic进行订阅, 发布者在某个topic上发布消息, 消息扩散到所有订阅了topic的node. 这个行为又叫overlay multicast. pubsub对象基于topic, 提供基础方法; 在此基础上提供了三个router: 暴力广播式的Floodsub, 随机选择下线peer的Randomsub, 基于gossip协议的Gossipsub 用topic的Publish APi来发送消息, 不用订阅topic就可以发送消息 用topic的Subscribe来订阅返回sub对象, sub对象的Next方法返回下一条消息. 消息是用protocol buf来封装的 配合官方pubsub说明来理解: https://docs.libp2p.io/concepts/publish-subscribe/ 使用mdns协议内网发现, 默认1个小时更新一次列表 // setup local mDNS discovery err = setupDiscovery(ctx, h) 接下来就是加入聊天室开始ui // join the chat room cr, err := JoinChatRoom(ctx, ps, h.ID(), nick, room) // draw the UI ui := NewChatUI(cr) ui.Run() join的过程就是pubsub Join一个topic的过程, 底层是个Subscribe的过程.最后开始后台routine, 读这个topic.Next, 发送给ui注意虽然pubsub使用gpb编码的, 但里面的msg.Data是json编码的 ```go // join the pubsub topic topic, err := ps.Join(topicName(roomName)) // and subscribe to it sub, err := topic.Subscribe() cr := &ChatRoom{ ctx: ctx, ps: ps, topic: topic, sub: sub, self: selfID, nick: nickname, roomName: roomName, Messages: make(chan *ChatMessage, ChatRoomBufSize), } // start reading messages from the subscription in a loop go cr.readLoop() * ui负责打印收到的message. ## relay ### 代码 * h1显式使能了relay功能, 这里指使用realy功能的功能 ```go // Tell the host use relays h1, err := libp2p.New(context.Background(), libp2p.EnableRelay()) h2能给其他node提供relay功能 // Tell the host to relay connections for other peers (The ability to *use* // a relay vs the ability to *be* a relay) h2, err := libp2p.New(context.Background(), libp2p.EnableRelay(circuit.OptHop)) h3清空了listen地址(默认是全部interface监听), 让h3只能通过上面的circuit relay来通信 // Zero out the listen addresses for the host, so it can only communicate // via p2p-circuit for our example h3, err := libp2p.New(context.Background(), libp2p.ListenAddrs(), libp2p.EnableRelay()) 分别连接h1和h3到h2, 但h1和h3不直接连接 h2info := peer.AddrInfo{ ID: h2.ID(), Addrs: h2.Addrs(), } err := h1.Connect(context.Background(), h2info) err := h3.Connect(context.Background(), h2info) 测试h1到h3的连通性 // Now, to test things, let's set up a protocol handler on h3 h3.SetStreamHandler(\"/cats\", func(s network.Stream) { log.Println(\"Meow! It worked!\") s.Close() }) //下面这块演示了直接从h1连接到h3是不行的. _, err = h1.NewStream(context.Background(), h3.ID(), \"/cats\") if err == nil { log.Println(\"Didnt actually expect to get a stream here. What happened?\") return } log.Printf(\"Okay, no connection from h1 to h3: %v\", err) log.Println(\"Just as we suspected\") // Creates a relay address to h3 using h2 as the relay relayaddr, err := ma.NewMultiaddr(\"/p2p/\" + h2.ID().Pretty() + \"/p2p-circuit/ipfs/\" + h3.ID().Pretty()) //先清理h3, 因为刚才连接失败了, 默认不让马上连 // Since we just tried and failed to dial, the dialer system will, by default // prevent us from redialing again so quickly. Since we know what we're doing, we // can use this ugly hack (it's on our TODO list to make it a little cleaner) // to tell the dialer \"no, its okay, let's try this again\" h1.Network().(*swarm.Swarm).Backoff().Clear(h3.ID()) //使用h3的relay地址连接 h3relayInfo := peer.AddrInfo{ ID: h3.ID(), Addrs: []ma.Multiaddr{relayaddr}, } //h1可以连接到h3了 h1.Connect(context.Background(), h3relayInfo) s, err := h1.NewStream(context.Background(), h3.ID(), \"/cats\") s.Read(make([]byte, 1)) // block until the handler closes the stream multipro 创建两个host, 同时支持自定义的ping协议和echo协议. 每个自定的协议都是用protobuf来request和response 这样的好处是, 通过协议号先分流不同的行为, 而不是像一般的应用, 只有一个protobuf的结构体, 里面有大量的可选fields. 这个例子可以处理异步的response, 并匹配到其对应的request. 在message级别使用libp2p protocol multiplexing 代码 Node是个多协议的组合体// Node type - a p2p host implementing one or more p2p protocols type Node struct { host.Host // lib-p2p host *PingProtocol // ping protocol impl *EchoProtocol // echo protocol impl // add other protocols here... } new一个Node就是new ping协议和echo协议// Create a new node with its implemented protocols func NewNode(host host.Host, done chan bool) *Node { node := &Node{Host: host} node.PingProtocol = NewPingProtocol(node, done) node.EchoProtocol = NewEchoProtocol(node, done) return node } ping协议有两级的protocol名// pattern: /protocol-name/request-or-response-message/version const pingRequest = \"/ping/pingreq/0.0.1\" const pingResponse = \"/ping/pingresp/0.0.1\" ping使用一个map来记录request和response的关系// PingProtocol type type PingProtocol struct { node *Node // local host requests map[string]*p2p.PingRequest // used to access request data from response handlers done chan bool // only for demo purposes to stop main from terminating } NewPingProtocol的时候要set两个handler, pingreq和pingresp分别一个. 另外, 这里还用到了高级技巧: 方法当作函数func NewPingProtocol(node *Node, done chan bool) *PingProtocol { p := &PingProtocol{node: node, requests: make(map[string]*p2p.PingRequest), done: done} node.SetStreamHandler(pingRequest, p.onPingRequest) node.SetStreamHandler(pingResponse, p.onPingResponse) return p } pingreq的handler// remote peer requests handler func (p *PingProtocol) onPingRequest(s network.Stream) { //大概流程是从s读request, 然后马上关闭s. 因为s只包括一次request //读到data先proto.Unmarshal到&p2p.PingRequest{}结构体 data := &p2p.PingRequest{} buf, err := ioutil.ReadAll(s) s.Close() proto.Unmarshal(buf, data) //这部就关键了: 每个request都要验证签名 valid := p.node.authenticateMessage(data, data.MessageData) //然后组一个response, 并签名 resp := &p2p.PingResponse{MessageData: p.node.NewMessageData(data.MessageData.Id, false), Message: fmt.Sprintf(\"Ping response from %s\", p.node.ID())} signature, err := p.node.signProtoMessage(resp) // add the signature to the message resp.MessageData.Sign = signature //最后发送这个response, 这个函数里面每次都会新建一个stream来发送. 对Lip2p来说, 因为用了多路复用, 一个stream好像没有那么重. p.node.sendProtoMessage(s.Conn().RemotePeer(), pingResponse, resp) } pingrep的handler类似的, 逻辑很简单, 就是收到对方的response, 说明对方收到了我们的request, 并且回复了, 那我们就删除之前的reqeust map里面记录的id. // locate request data and remove it if found _, ok := p.requests[data.MessageData.Id] if ok { // remove request from map as we have processed it here delete(p.requests, data.MessageData.Id) } else { log.Println(\"Failed to locate request data boject for response\") return } echo协议也非常类似 main流程如下: 建立两个对等的host, 比较普通的那种 priv, _, _ := crypto.GenerateKeyPair(crypto.Secp256k1, 256) listen, _ := ma.NewMultiaddr(fmt.Sprintf(\"/ip4/127.0.0.1/tcp/%d\", port)) host, _ := libp2p.New( context.Background(), libp2p.ListenAddrs(listen), libp2p.Identity(priv), ) 最后run 注意这里也是先Peerstore添加peer的信息, 但没有马上connect. 而是等到后面NewStream的时候再连接. // connect peers h1.Peerstore().AddAddrs(h2.ID(), h2.Addrs(), peerstore.PermanentAddrTTL) h2.Peerstore().AddAddrs(h1.ID(), h1.Addrs(), peerstore.PermanentAddrTTL) // send messages using the protocols h1.Ping(h2.Host) h2.Ping(h1.Host) h1.Echo(h2.Host) h2.Echo(h1.Host) 总结 使用了host的SetStreamHandler()函数注册协议的handler, 这里使用了多个协议(ping和echo), 大协议里面还有小协议(req和rep). 可见一个host可以set多个协议的handler. 总结的总结 host是个基础设施实体, 类似IP协议的地位, 上面可以运行不同的protocol(可以类比为端口号) 比如pubsub.NewGossipSub(ctx, h)的入参是一个ctx和一个host. 并不是说\"注册\"一个gossipsub给这个host, 相反的, host只提供基础能力, gossipsub主动使用host的能力, 自主的维护基于这个host节点的gossip网络. 多个不同的\"功能\"可以在一个host节点上同时运行. 比如同时运行一个mdns服务 SetStreamHandler是每个protocol一个, 这里的protocol不是指tcp啥的, 而是一个string, 比如/chat/1.1.0, 或者/echo/1.0.0 对应的, \"client\"在NewStream的时候, 也要传入一个protocol.ID 扩展 host的底层是swarm(蜂巢), 新建一个swarm需要5个参数:swarm, err := NewSwarm(ctx, laddrs, pid, pstore, bwc) ctx: ctx laddrs: an array of multiaddrs that the swarm will open up listeners for pid: peer id. 通常是RSA的key生成的ID pstore: 存peer id用的 bwc: 统计用的. swarm是基于多路复用的. stream可以set handlerswrm.SetStreamHandler(func(s inet.Stream) { defer s.Close() fmt.Println(\"Got a stream from: \", s.SwarmConn().RemotePeer()) fmt.Fprintln(s, \"Hello Friend!\") }) 也可以直接连接NewStreamWithPeers, err := swrm.NewStreamWithPeer(ctx, rpid) defer s.Close() io.Copy(os.Stdout, s) // pipe the stream to stdout 在swarm这一层, 没有protocol ID; 后者是host层加的. host的option libp2p.New: 新建libp2p node的核心接口, 支持很多配置选项, 如果没有显式的指定, 有合理的默认值: 默认监听/ip4/0.0.0.0/tcp/0 和 /ip6/::/tcp/0 默认使用TCP和websocket传输协议 默认使用多路复用协议yamux和mplux, 多路复用协议会在peer间协商? 默认加密, 使用noise协议或者TLS 默认使用RSA 2048产生随机的PeerId 默认用空的peerstore(用于peer routing) 有个API NewWithoutDefaults没有任何default配置, 但不建议使用. 可选配置如下: ListenAddrStrings: 指定(多个)listen地址, 使用原始字符串表达libp2p.ListenAddrStrings(\"/ip4/0.0.0.0/tcp/0\")) libp2p.ListenAddrStrings( \"/ip4/0.0.0.0/tcp/9000\", // regular tcp connections \"/ip4/0.0.0.0/udp/9000/quic\", // a UDP endpoint for the QUIC transport ) ListenAddrs: 指定(多个)listen地址, 使用Multiaddr表达listen, _ := ma.NewMultiaddr(fmt.Sprintf(\"/ip4/127.0.0.1/tcp/%d\", port)) libp2p.ListenAddrs(listen), Security: 配置transport加密// support TLS connections libp2p.Security(libp2ptls.ID, libp2ptls.New), // support secio connections libp2p.Security(secio.ID, secio.New), NoSecurity: 不加密 Muxer: 配置multiplexing Transport: 配置支持的transport// support QUIC - experimental libp2p.Transport(libp2pquic.NewTransport), libp2p.DefaultTransports, // DefaultTransports是tcp和websocket Peerstore: 配置使用peerstore PrivateNetwork: 保护私有网络, 只有同样的私有网络标识才能连接 BandwidthReporter: 配置带宽报告 Identity: 配置keypriv, _, err := crypto.GenerateKeyPairWithReader(crypto.RSA, 2048, r) libp2p.Identity(priv), ConnectionManager: 配置连接管理libp2p.ConnectionManager(connmgr.NewConnManager( 100, // Lowwater 400, // HighWater, time.Minute, // GracePeriod )), AddrsFactory: 地址工厂 EnableRelay: 传入OptHop会让这个host 广告自己可以是relay hoplibp2p.EnableRelay(circuit.OptHop) DisableRelay: 禁用relay. 默认使能 EnableAutoRelay: 使能relay服务(对别人提供relay功能)libp2p.EnableAutoRelay(), StaticRelays: 配置静态relay DefaultStaticRelays: 使用内置的代码写死的relay节点// These are the known PL-operated relays var DefaultRelays = []string{ \"/ip4/147.75.80.110/tcp/4001/p2p/QmbFgm5zan8P6eWWmeyfncR5feYEMPbht5b1FW1C37aQ7y\", \"/ip4/147.75.195.153/tcp/4001/p2p/QmW9m57aiBDHAkKj9nmFSEn7ZqrcF1fZS4bipsTCHburei\", \"/ip4/147.75.70.221/tcp/4001/p2p/Qme8g49gm3q4Acp7xWBKg3nAa9fxZ1YmyDJdyGgoG6LsXh\", } ForceReachabilityPublic: 在autoNAT时, 强制让这个host相信自己public可达 ForceReachabilityPrivate: 在autoNAT时, 强制让这个host相信自己在内部NAT网络 EnableNATService: 使能nat观测服务. 尝试用新连接回连到peer, 并告知对端是否成功, 帮助对端判断自己是否外网可访问. AutoNATServiceRateLimit: 限制回连服务的次数 ConnectionGater: 配置一个连接看护人来拒绝特定的inbound和outbound连接 NATPortMap: 尝试使用路由器的UPnP功能来穿透NAT NATManager: 上面的api使用默认的nat manager, 这个api是自己配置一个. Ping: 使能ping服务. 默认使能 Routing: // Let this host use the DHT to find other hosts libp2p.Routing(func(h host.Host) (routing.PeerRouting, error) { idht, err = dht.New(ctx, h) return idht, err }), NoListenAddrs: 配置这个node不listen UserAgent: user-agent sent MultiaddrResolver: dns resolver 默认值 这里提供默认值的目的是让用户扩展选项, 同时也有这些默认值 var DefaultSecurity = ChainOptions( Security(noise.ID, noise.New), Security(tls.ID, tls.New), ) var DefaultMuxers = ChainOptions( Muxer(\"/yamux/1.0.0\", yamux.DefaultTransport), Muxer(\"/mplex/6.7.0\", mplex.DefaultTransport), ) var DefaultTransports = ChainOptions( Transport(tcp.NewTCPTransport), Transport(ws.New), ) // DefaultPeerstore configures libp2p to use the default peerstore. var DefaultPeerstore Option = func(cfg *Config) error { return cfg.Apply(Peerstore(pstoremem.NewPeerstore())) } // RandomIdentity generates a random identity. (default behaviour) var RandomIdentity = func(cfg *Config) error { priv, _, err := crypto.GenerateKeyPairWithReader(crypto.RSA, 2048, rand.Reader) return cfg.Apply(Identity(priv)) } // DefaultListenAddrs configures libp2p to use default listen address. var DefaultListenAddrs = func(cfg *Config) error { defaultIP4ListenAddr, err := multiaddr.NewMultiaddr(\"/ip4/0.0.0.0/tcp/0\") defaultIP6ListenAddr, err := multiaddr.NewMultiaddr(\"/ip6/::/tcp/0\") return cfg.Apply(ListenAddrs( defaultIP4ListenAddr, defaultIP6ListenAddr, )) } // DefaultEnableRelay enables relay dialing and listening by default. var DefaultEnableRelay = func(cfg *Config) error { return cfg.Apply(EnableRelay()) } 规范 在 https://github.com/libp2p/specs 如何建立连接 先是建立raw transport连接, 比如tcp连接. 这部分很标准. 在raw连接基础上, 规范描述了如何协商安全特性和多路复用等能力的过程, 包括了从最初的传输建立后, 到打开应用层的stream, 识别应用层的protocol ID, 根据ID找到合适的handler 有连个核心的特性: 安全: 最初的握手之后, 数据都是加密的, 用签名的方式 stream是可靠的, 双向的, 通信channel, 是基于在底层transport多路复用的基础上的. stream必须支持反压, 支持半连接, 比如可以关了写, 但是可以读. stream的好处是可以多个逻辑的流共享一个底层transport, 节约建立通道的开销. 有的transport协议比如QUIC, 本身就有内置的安全和多路复用. 其他的协议比如TCP, 就必须在原始的transport之上建立安全和多路复用. 对于这种本身没有安全和多路复用支持的情况, 就需要\"连接升级\" 协议协商 协议号是个字符串, 通常带版本号. 比如/multistream/1.0.0 multistream也叫multistream-select, 使用utf8编码的字符串来传递消息. 协商不上返回na\\n 连接升级 连接升级也是使用/multistream/1.0.0来协商的过程:比如上图的过程, 在原始连接基础上: 一定是先协商安全特性, 这里发起方先发送/tls/1.0.0, 但对方不支持, 发送方再协商/noise, 这回对方支持了 接下来协商多路复用, 一下子就协商成了mplex/1.0.0 新建stream 上面的过程是升级一个底层连接, 在连接上面, 可以新建多个逻辑上的stream. 新建stream的过程也是要使用/multistream/1.0.0协议来协商\"应用层\"的协议: 比如发送方发送/echo/1.0.1, 接收方查询自己是否有/echo/1.0.1协议的handler(或能match到这个协议的match handler), 没有就返回na\\n, 那发送方就停止交互, 或者再换一个应用层协议. 如果接收方支持echo协议, 双方就可以按照echo协议开始干活了. 具体实现 推荐使用Noise安全特性(又有说法是回切换到tls) 推荐使用mplex多路复用 推荐提供peer metadata storage, see go-libp2p-peerstore 推荐进行连接限制 连接状态通知类型: Event Description Connected A new connection has been opened Disconnected A connection has closed OpenedStream A new stream has opened over a connection ClosedStream A stream has closed Listen We've started listening on a new address ListenClose We've stopped listening on an address 隧道 底层transport有两种情况 non-browser: 通常是tcp或quic browser: 不能直接dial, 不能直接控制底层socket 能\"穿透\"的协议有 relay TURN: Traversal Using Relays around NAT (TURN), 基于中间节点 circuit relay v1和circuit relay v2: 双方都知晓这个relay server的存在 relay的地址格式: []/p2p-circuit/ Addressing /p2p-circuit multiaddrs don't carry any meaning of their own. They need to encapsulate a /p2p address, or be encapsulated in a /p2p address, or both. As with all other multiaddrs, encapsulation of different protocols determines which metaphorical tubes to connect to each other. A /p2p-circuit circuit address, is formated following: []/p2p-circuit/ Examples: /p2p-circuit/p2p/QmVT6GYwjeeAF5TR485Yc58S3xRF5EFsZ5YAF4VcP3URHt - Arbitrary relay node /ip4/127.0.0.1/tcp/5002/p2p/QmdPU7PfRyKehdrP5A3WqmjyD6bhVpU1mLGKppa2FjGDjZ/p2p-circuit/p2p/QmVT6GYwjeeAF5TR485Yc58S3xRF5EFsZ5YAF4VcP3URHt - Specific relay node This opens the room for multiple hop relay, where the second relay is encapsulated in the first relay multiaddr, such as: /p2p-circuit//p2p-circuit/ A few examples: Using any relay available: /p2p-circuit/p2p/QmTwo Dial QmTwo, through any available relay node (or find one node that can relay). The relay node will use peer routing to find an address for QmTwo if it doesn't have a direct connection. /p2p-circuit/ip4/../tcp/../p2p/QmTwo Dial QmTwo, through any available relay node, but force the relay node to use the encapsulated /ip4 multiaddr for connecting to QmTwo. Specify a relay: /p2p/QmRelay/p2p-circuit/p2p/QmTwo Dial QmTwo, through QmRelay. Use peer routing to find an address for QmRelay. The relay node will also use peer routing, to find an address for QmTwo. /ip4/../tcp/../p2p/QmRelay/p2p-circuit/p2p/QmTwo Dial QmTwo, through QmRelay. Includes info for connecting to QmRelay. The relay node will use peer routing to find an address for QmTwo. Double relay: /p2p-circuit/p2p/QmTwo/p2p-circuit/p2p/QmThree Dial QmThree, through a relayed connection to QmTwo. The relay nodes will use peer routing to find an address for QmTwo and QmThree. We'll not support nested relayed connections for now, see Future Work section. STUN-like 让另外一个host来观测自己是否是在NAT里面, 并帮助自己发现端口映射. libp2p使用AutoNAT和identify来实现 Coordination 在relay协议基础上, 让两个host来互相写作进行打洞. 比如Session Description Protocol (SDP) and Direct Connection Upgrade through Relay. 节点发现机制 这里主要是libp2p Kademlia and Gossipsub protocol 几种场景 Public Non-Browser (A) to Public Non-Browser (B): 不需要打洞, 直接连. Private Non-Browser (A) to Public Non-Browser (B): 不需要打洞, A可以直接连B Private Browser (A) to Public Non-Browser (B): B需要支持Websocket, 因为之后websocket才能直接连. B也必须支持TLS Public or Private Non-Browser (A) to Private Non-Browser (B): 可以有几种方式连接: B使用了AutoNAT和Identify得到\"对外\"的ip和端口, 这样A就能够访问到. 但A或B是symmeritc, NAT这招就不好用了. B使用 circuit relay v2 protocol协议, 先主动连接某个relay server, B广播自己的relay地址让. A通过访问这个relay地址来访问B, A和B连接上之后, 执行升级协议Direct Connection Upgrade through Relay, 进最大可能做打洞建立直接连接. 问答 打洞失败会怎么样?确实有可能失败, 没办法, 上层的协议要接受这一点. 但两个节点间还是可以用relay节点连接, 虽然这个连接是比较绕远的, 但没办法. 为什么同时使用AutoNAT and STUN, 为什么不用其中一个就好了?browser场景下只能用STUN, 不能用AutoNAT. TCP或QUIC场景下可以用STUN取代AutoNAT. --但上文中看到, AutoNAT配合identify机制, 是libp2p对SATUN的实现. 将来work 优化协商协议multistream-select 优化connection manager 支持quic(好像已经支持了) event bus Peer Ids and Keys libp2p使用使用密钥来给message签名, 以及生成peer ID. 下面是key的protobuf定义 syntax = \"proto2\"; enum KeyType { RSA = 0; Ed25519 = 1; Secp256k1 = 2; ECDSA = 3; } message PublicKey { required KeyType Type = 1; required bytes Data = 2; } message PrivateKey { required KeyType Type = 1; required bytes Data = 2; } 其中data域可以有其他的编码方式 PrivateKey不会在线路中传输, 但可以以protobuf的编码保存在本地存储上, 从而可以被load来重用这个key. key的使用场景 给message签名 IPNS records PubSub messages SECIO handshake 生成peerID identify协议 用/ipfs/id/1.0.0和/ipfs/id/push/1.0.0来查询remote peer和广播自己的peerID.里面包含public key message Identify { optional string protocolVersion = 5; optional string agentVersion = 6; optional bytes publicKey = 1; repeated bytes listenAddrs = 2; optional bytes observedAddr = 4; repeated string protocols = 3; } mplex协议 mplex是个简单的多路复用协议, 比如简单到没有流控. open一个streamd时候要先生成个stream ID, stream名只是用来debug 写stream是个message, 用MessageReceiver (1) or MessageInitiator (2)标记是否是发起者, data域是你要写的内容, 最大1MB 还有大小限制?! 可以半关 reset操做会同时关闭读和写 Rendezvous集结协议 一个去中心化的, 轻量化的, 通用的发现协议. 我的理解是加入集结就能发现周边的peer. During bootstrap, a node can use known rendezvous points to discover peers that provide critical services. For instance, rendezvous can be used to discover circuit relays for connectivity restricted nodes. During initialization, a node can use rendezvous to discover peers to connect with the rest of the application. For instance, rendezvous can be used to discover pubsub peers within a topic. In a real time setting, applications can poll rendezvous points in order to discover new peers in a timely fashion. In an application specific routing setting, rendezvous points can be used to progressively discover peers that can answer specific queries or host shards of content. 大概原理 实现了/rendezvous/1.0.0的每个node都可以是集结点, 其他节点连接到集结点时, 会注册自己的名字空间. 其他节点可以向集结点发请求查询某个namespace, 返回其包括的节点. 也支持cookie来进行增量查询 注册的时候可以指定TTL, 设置老化时间. 默认时2个小时, 最大72小时. namespace最大255, 最大注册数1000 Protocols汇总 These specs define wire protocols that are used by libp2p for connectivity, security, multiplexing, and other purposes. The protocols described below all use protocol buffers (aka protobuf) to define message schemas. Version proto2 is used unless stated otherwise. identify - Exchange keys and addresses with other peers mplex - The friendly stream multiplexer plaintext - An insecure transport for non-production usage pnet - Private networking in libp2p using pre-shared keys pubsub - PubSub interface for libp2p gossipsub - An extensible baseline PubSub protocol episub - Proximity Aware Epidemic PubSub for libp2p relay - Circuit Switching for libp2p (similar to TURN) rendezvous - Rendezvous Protocol for generalized peer discovery secio - SECIO, a transport security protocol for libp2p tls - The libp2p TLS Handshake (TLS 1.3+) 基础知识 公钥与私钥 数字签名是什么？ 鲍勃有两把钥匙，一把是公钥，另一把是私钥。 鲍勃把公钥送给他的朋友们----帕蒂、道格、苏珊----每人一把。 苏珊要给鲍勃写一封保密的信。她写完后用鲍勃的公钥加密，就可以达到保密的效果。 鲍勃收信后，用私钥解密，就看到了信件内容。这里要强调的是，只要鲍勃的私钥不泄露，这封信就是安全的，即使落在别人手里，也无法解密。 鲍勃给苏珊回信，决定采用\"数字签名\"。他写完后先用Hash函数，生成信件的摘要（digest）。 然后，鲍勃使用私钥，对这个摘要加密，生成\"数字签名\"（signature）。 鲍勃将这个签名，附在信件下面，一起发给苏珊。 苏珊再对信件本身使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。如果两者一致，就证明这封信未被修改过。 复杂的情况出现了。道格想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。此时，苏珊实际拥有的是道格的公钥，但是还以为这是鲍勃的公钥。因此，道格就可以冒充鲍勃，用自己的私钥做成\"数字签名\"，写信给苏珊，让苏珊用假的鲍勃公钥进行解密。 后来，苏珊感觉不对劲，发现自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找\"证书中心\"（certificate authority，简称CA），为公钥做认证。证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成\"数字证书\"（Digital Certificate）。 鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。 苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明\"数字签名\"是否真的是鲍勃签的。 下面，我们看一个应用\"数字证书\"的实例：https协议。这个协议主要用于网页加密。 首先，客户端向服务器发出加密请求。 服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。 客户端（浏览器）的\"证书管理器\"，有\"受信任的根证书颁发机构\"列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。 如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。 如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。 讨论 那么鲍勃发给苏珊的信件，帕蒂不是也能打开了么??(既然大家都有鲍勃的公钥)?这个问题其实被作者简化了（如果我没记错的话），实际上bob和Susan的通信过程应该是：1.他俩用公钥和私钥通信达成一个临时的协议，包括一个对称加密的密钥，密码因为是通过非对称加密的方式（就像此文第一个加密传输过程)传输所以能在第一次传输的时候也保证安全和正确。2.然后他俩用这个对称加密的密钥通信，帕蒂就没办法解密密文了。 所以实际上真正的通信过程应该是大部分基于对称加密，和小部分（对称加密密钥)基于非对称加密。 你好，本人非密码学专业出身。可否请教一下：既然 Bob 公钥是公开的，那 Susan 用 Bob 公钥加密的内容为什么无法被其他有 Bob 公钥的人解密呢？公钥加密得到的密文只能由私钥进行解密，如果不知道私钥，仅有公钥和密文是无法计算出明文的 对称加密（Symmetric Cryptography） 对称加密是最快速、最简单的一种加密方式，加密（encryption）与解密（decryption）用的是同样的密钥（secret key）。对称加密有很多种算法，由于它效率很高，所以被广泛使用在很多加密协议的核心当中。 对称加密通常使用的是相对较小的密钥，一般小于256 bit。因为密钥越大，加密越强，但加密与解密的过程越慢。如果你只用1 bit来做这个密钥，那黑客们可以先试着用0来解密，不行的话就再用1解；但如果你的密钥有1 MB大，黑客们可能永远也无法破解，但加密和解密的过程要花费很长的时间。密钥的大小既要照顾到安全性，也要照顾到效率，是一个trade-off。 2000年10月2日，美国国家标准与技术研究所（NIST--American National Institute of Standards and Technology）选择了Rijndael算法作为新的高级加密标准（AES--Advanced Encryption Standard）。.NET中包含了Rijndael算法，类名叫RijndaelManaged，下面举个例子。 对称加密的一大缺点是密钥的管理与分配，换句话说，如何把密钥发送到需要解密你的消息的人的手里是一个问题。在发送密钥的过程中，密钥有很大的风险会被黑客们拦截。现实中通常的做法是将对称加密的密钥进行非对称加密，然后传送给需要它的人。 非对称加密（Asymmetric Cryptography） 非对称加密为数据的加密与解密提供了一个非常安全的方法，它使用了一对密钥，公钥（public key）和私钥（private key）。私钥只能由一方安全保管，不能外泄，而公钥则可以发给任何请求它的人。非对称加密使用这对密钥中的一个进行加密，而解密则需要另一个密钥。比如，你向银行请求公钥，银行将公钥发给你，你使用公钥对消息加密，那么只有私钥的持有人--银行才能对你的消息解密。与对称加密不同的是，银行不需要将私钥通过网络发送出去，因此安全性大大提高。 目前最常用的非对称加密算法是RSA算法，是Rivest, Shamir, 和Adleman于1978年发明，他们那时都是在MIT。.NET中也有RSA算法，请看下面的例子： 虽然非对称加密很安全，但是和对称加密比起来，它非常的慢，所以我们还是要用对称加密来传送消息，但对称加密所使用的密钥我们可以通过非对称加密的方式发送出去。为了解释这个过程，请看下面的例子： （1） Alice需要在银行的网站做一笔交易，她的浏览器首先生成了一个随机数作为对称密钥。 （2） Alice的浏览器向银行的网站请求公钥。 （3） 银行将公钥发送给Alice。 （4） Alice的浏览器使用银行的公钥将自己的对称密钥加密。 （5） Alice的浏览器将加密后的对称密钥发送给银行。 （6） 银行使用私钥解密得到Alice浏览器的对称密钥。 （7） Alice与银行可以使用对称密钥来对沟通的内容进行加密与解密了。 总结 （1） 对称加密加密与解密使用的是同样的密钥，所以速度快，但由于需要将密钥在网络传输，所以安全性不高。 （2） 非对称加密使用了一对密钥，公钥与私钥，所以安全性高，但加密与解密速度慢。 （3） 解决的办法是将对称加密的密钥使用非对称加密的公钥进行加密，然后发送出去，接收方使用私钥进行解密得到对称加密的密钥，然后双方可以使用对称加密来进行沟通。 "},"notes/as_title_rust.html":{"url":"notes/as_title_rust.html","title":"Rust","keywords":"","body":"如题 "},"notes/rust_入门_brief.html":{"url":"notes/rust_入门_brief.html","title":"Rust 入门系列","keywords":"","body":"我在学习rust语法过程中的笔记, 大部分来自于网上摘录, 略有更改. "},"notes/rust_books.html":{"url":"notes/rust_books.html","title":"Rust reference books","keywords":"","body":"参考书目: 文档汇总: https://www.rust-lang.org/learn 语法: https://doc.rust-lang.org/book 语法参考: https://doc.rust-lang.org/reference 标准库: https://doc.rust-lang.org/std/index.html 例子: https://doc.rust-lang.org/stable/rust-by-example cargo: https://doc.rust-lang.org/stable/cargo rustup: https://rust-lang.github.io/rustup 交叉编译: https://rust-lang.github.io/rustup/cross-compilation.html "},"notes/rust_入门1.html":{"url":"notes/rust_入门1.html","title":"Rust 安装和基础语法","keywords":"","body":" 安装 组件 rustup hello world 基础语法 格式化输出 语法糖 变量 类型推导 默认只读 变量遮蔽 类型别名 全局变量 基本数据类型 指针 什么是Box 什么是Rc 类型转换 字符串 复合数据类型 tuple 元组 数组 数组切片 字符串切片 非字符串切片 索引和边界检查 struct 部分初始化 输出结构体 tuple struct 元组结构体 举例 结构体方法 结构体关联函数 普遍方法 静态方法 enum enum和match 经常出现的Option就是一种enum if let代替match 表达式 if else if let和while let 循环 函数 发散函数Diverging functions main函数 const_fn trait 默认trait trait做参数 匿名trait Box impl trait for trait 为别人的类型实现trait trait不能做为参数, 返回值, 实例变量 调用trait 方法和函数没有本质不同? trait约束 trait继承 derive 常见trait 带关联类型的trait 模式解构 match ref和mut 知晓变量类型 方案一: 利用编译错误来获取变量类型 方案二: 使用标准库 方案三: 不需要nightly版本 问号操作符 问号操作背后 安装 使用rustup安装: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh 安装了如下的文件: $ ls /home/yingjieb/.rustup downloads settings.toml tmp toolchains update-hashes $ ls /home/yingjieb/.cargo bin env $ ls /home/yingjieb/.cargo/bin cargo cargo-fmt clippy-driver rustc rustfmt rust-lldb cargo-clippy cargo-miri rls rustdoc rust-gdb rustup 在/home/yingjieb/.profile增加了 . \"$HOME/.cargo/env\" 在/home/yingjieb/.bashrc增加了 . \"$HOME/.cargo/env\" 这个env主要就是干了一件事: export PATH=\"$HOME/.cargo/bin:$PATH\" 组件 默认安装了如下组件: cargo 5.7 MiB clippy rust-docs rust-std 34.9 MiB rustc 74.2 MiB rustfmt 如果是在你的CI环境下只想用rustc来编译, 可以指定profile为minimal 比如 ## install RUST ARG RUST_TOOLCHAIN=\"1.60.0\" RUN mkdir $CARGO_HOME && chmod 777 $CARGO_HOME RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --profile minimal --default-toolchain \"$RUST_TOOLCHAIN\" \\ && rustup target add x86_64-unknown-linux-musl --toolchain \"$RUST_TOOLCHAIN\" \\ && rustup component add rustfmt \\ && rustup component add clippy rustup rustup是管理rust工具链的工具: $ rustup -V rustup 1.24.3 (ce5817a94 2021-05-31) info: This is the version for the rustup toolchain manager, not the rustc compiler. info: The currently active `rustc` version is `rustc 1.59.0 (9d1b2106e 2022-02-23) hello world $ cat hello.rs fn main() { println!(\"hello world!\"); } $ rustc hello.rs yingjieb@godev-server /repo/yingjieb/rust/practice $ ls hello hello.rs yingjieb@godev-server /repo/yingjieb/rust/practice $ ./hello hello world! yingjieb@godev-server /repo/yingjieb/rust/practice $ file hello hello: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 3.2.0, BuildID[sha1]=ae53c8a4def1de8266d96cfe6dc8d8074ffa1d2b, with debug_info, not stripped $ llh hello -rwxr-xr-x 1 yingjieb platform 3.5M Mar 23 02:59 hello yingjieb@godev-server /repo/yingjieb/rust/practice $ ldd hello linux-vdso.so.1 (0x00007ffce327e000) libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fe769517000) librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fe76930f000) libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fe7690f0000) libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fe768eec000) libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fe768afb000) /lib64/ld-linux-x86-64.so.2 (0x00007fe769979000) $ strip hello yingjieb@godev-server /repo/yingjieb/rust/practice $ llh total 300K -rwxr-xr-x 1 yingjieb platform 295K Mar 23 03:05 hello -rw-r--r-- 1 yingjieb platform 44 Mar 23 02:59 hello.rs 可以看到编译出来的hello可执行程序达到3.5M, 而且还动态链接了c库. strip后是295K. 这个大小正常 $ size -A hello hello : section size addr .interp 28 624 .note.ABI-tag 32 652 .note.gnu.build-id 36 684 .gnu.hash 28 720 .dynsym 1800 752 .dynstr 1232 2552 .gnu.version 150 3784 .gnu.version_r 224 3936 .rela.dyn 16776 4160 .rela.plt 96 20936 .init 23 21032 .plt 80 21056 .plt.got 8 21136 .text 217583 21152 .fini 9 238736 .rodata 20335 238752 .eh_frame_hdr 3772 259088 .eh_frame 21392 262864 .gcc_except_table 2972 284256 .tdata 40 2384528 .tbss 57 2384576 .init_array 16 2384576 .fini_array 8 2384592 .data.rel.ro 9280 2384600 .dynamic 576 2393880 .got 1696 2394456 .data 56 2396160 .bss 456 2396216 .comment 41 0 .debug_aranges 37632 0 .debug_pubnames 401408 0 .debug_info 785107 0 .debug_abbrev 3086 0 .debug_line 413191 0 .debug_frame 96 0 .debug_str 1057897 0 .debug_pubtypes 144 0 .debug_ranges 514768 0 Total 3512131 实际上是debug信息占了绝大部分size. 基础语法 用//或/**/来注释 函数声明fn Foo( input1 : i32, input2 : u32) -> i32 { ... } 局部变量声明使用let关键字开头，用双引号包含起来的部分是字符串常量 分号结尾 最简单的标准输出是使用println！宏来完成. println后面的感叹号，它代表这是一个宏，而不是一个函数。 代码组织: crate: 类似项目概念 mod: 类似namespace概念 std: 标准库. 编译器会为用户写的每个crate自动插入一句话 use std::prelude::*; 函数可以在使用的位置后面声明 格式化输出 fn main() { println!(\"{}\", 1); // 打印变量的默认格式 println!(\"{:o}\", 9); // 八进制 println!(\"{:x}\", 255); // 十六进制 小写 println!(\"{:X}\", 255); // 十六进制 大写 println!(\"{:p}\", &0); // 指针 println!(\"{:b}\", 15); // 二进制 println!(\"{:e}\", 10000f32); // 科学计数(小写) println!(\"{:E}\", 10000f32); // 科学计数(大写) println!(\"{:?}\", \"test\"); // 打印Debug println!(\"{:#?}\", (\"test1\", \"test2\")); // 带换行和缩进的Debug打印 println!(\"{a} {b} {b}\", a = \"x\", b = \"y\"); // 命名参数 } 从属于std::fmt模块, 这些是宏可以做编译时检查, 最终是调用std::io里面的函数输出. 语法糖 ..表示rangefn main() { let r = 1..10; // r是一个Range,中间是两个点,代表[1,10)这个区间 for i in r { print!(\"{:?}\\t\", i); } } 两个小数点的语法仅仅是一个“语法糖”而已，用它构造出来的变量 是Range类型use std::ops::Range; fn main() { let r = Range { start: 1, end: 10 }; // r是一个Range for i in r { print!(\"{:?}\\t\", i); } } 这个类型本身实现了Iterator trait，因此它可以直接应用到循环语句中。Range具有迭代器的全部功能，因此它能调用迭代器的成员方法。fn main() { use std::iter::Iterator; // 先用rev方法把这个区间反过来,然后用map方法把每个元素乘以10 let r = (1i32..11).rev().map(|i| i * 10); for i in r { print!(\"{:?}\\t\", i); } } 左闭右开: start..end 左闭右闭: start..=end 变量 Rust的变量必须先声明后使用, 变量必须初始化, 不初始化会报错。对于局部变量，最常见的声明语法为： let variable : i32 = 100; 类型推导 变量类型可以推导: let x = 5; 而且类型推导比较强大: fn main() { // 没有明确标出变量的类型,但是通过字面量的后缀, // 编译器知道elem的类型为u8 let elem = 5u8; // 创建一个动态数组,数组内包含的是什么元素类型可以不写 let mut vec = Vec::new(); vec.push(elem); // 到后面调用了push函数,通过elem变量的类型, // 编译器可以推导出vec的实际类型是 Vec println!(\"{:?}\", vec); } 我们甚至还可以只写一部分类型，剩下的部分让编译器去推导，比如下面的这个程序，我们只知道players变量是Vec动态数组类型，但是里面包含什么元素类型并不清楚，可以在尖括号中用下划线来代替： fn main() { let player_scores = [ (\"Jack\", 20), (\"Jane\", 23), (\"Jill\", 18), (\"John\", 19), ]; // players 是动态数组,内部成员的类型没有指定,交给编译器自动推导 let players : Vec = player_scores .iter() .map(|&(player, _score)| { player }) .collect(); println!(\"{:?}\", players); } 默认只读 默认变量是只读的, 重新赋值会出错: fn main() { let x = 5; x = 10; //编译错误: re-assignment of immutable variable`x` } 加mut关键字才能可写: let mut x = 5; // mut x: i32 x = 10; 按照我的理解, 第一次赋值叫变量绑定, 后面再修改需要加mut fn test(condition: bool) { let x: i32; // 声明 x,不必使用 mut 修饰 if condition { x = 1; // 初始化 x,不需要 x 是 mut 的,因为这是初始化,不是修改 println!(\"{}\", x); } // 如果条件不满足,x 没有被初始化 // 但是没关系,只要这里不使用 x 就没事 } 类型没有“默认构造函数”，变量没有“默认值”。对于let x：i32；如果没有显式赋值，它就没有被初始化，不要想当然地以为它的值是0。编译器会做变量检查, 没有\"绑定\"的使用会报错. 变量遮蔽 比如 fn main() { let x = \"hello\"; println!(\"x is {}\", x); let x = 5; println!(\"x is {}\", x); } 第二个let x中的x把前面的x遮蔽了, 这两个x是两个变量, 类型和在内存里的空间都不一样; 前面的x实际上从此不能再次被访问到 变量遮蔽在类型转换, 改变变量读写属性时很有用: // 注意：这段代码只是演示变量遮蔽功能,并不是Vec类型的最佳初始化方法 fn main() { let mut v = Vec::new(); // v 必须是mut修饰,因为我们需要对它写入数据 v.push(1); v.push(2); v.push(3); let v = v; // 从这里往下,v成了只读变量,可读写变量v已经被遮蔽,无法再访问 for i in &v { println!(\"{}\", i); } } 反过来也行 fn main() { let v = Vec::new(); //v是不可变的 let mut v = v; //这个v可变, 这个v和上面的v已经不是一个v了 v.push(1); println!(\"{:?}\", v); } 类型别名 type Age = u32; fn grow(age: Age, year: u32) -> Age { age + year } fn main() { let x : Age = 20; println!(\"20 years later: {}\", grow(x, 20)); } 或者用在泛型场景里: type Double = (T, Vec); // 小括号包围的是一个 tuple,请参见后文中的复合数据类型 // 那么以后使用Double的时候，就等同于（i32，Vec） 全局变量 比如: static GLOBAL: i32 = 0; 全局变量必须是静态变量 全局变量必须在声明的时候马上初始化 全局变量的初始化必须是编译期可确定的常量，不能包括执行期才能确定的表达式、语句和函数调用// 这样是允许的 static array : [i32; 3] = [1,2,3]; // 这样是不允许的 static vec : Vec = { let mut v = Vec::new(); v.push(1); v }; 但使用const fn是可以的:#![feature(const_fn)] fn main() { use std::sync::atomic::AtomicBool; static FLAG: AtomicBool = AtomicBool::new(true); } 带有mut修饰的全局变量，在使用的时候必须使用unsafe关键字 fn main() { //局部变量声明,可以留待后面初始化,只要保证使用前已经初始化即可 let x; let y = 1_i32; x = 2_i32; println!(\"{} {}\", x, y); //全局变量必须声明的时候初始化,因为全局变量可以写到函数外面,被任意一个函数使用 static G1 : i32 = 3; println!(\"{}\", G1); //可变全局变量无论读写都必须用 unsafe修饰 static mut G2 : i32 = 4; unsafe { G2 = 5; println!(\"{}\", G2); } //全局变量的内存不是分配在当前函数栈上,函数退出的时候,并不会销毁全局变量占用的内存空间,程序退出才会回收 } 基本数据类型 boolfn main() { let x = true; let y: bool = !x; // 取反运算 let z = x && y; // 逻辑与,带短路功能 println!(\"{}\", z); let z = x || y; // 逻辑或,带短路功能 println!(\"{}\", z); let z = x & y; // 按位与,不带短路功能 println!(\"{}\", z); let z = x | y; // 按位或,不带短路功能 println!(\"{}\", z); let z = x ^ y; // 按位异或,不带短路功能 println!(\"{}\", z); } charlet love = '❤'; // 可以直接嵌入任何 unicode 字符 let c1 = '\\n'; // 换行符 let c2 = '\\x7f'; // 8 bit 字符变量 let c3 = '\\u{7FFF}'; // unicode字符 因为char类型的设计目的是描述任意一个unicode字符，因此它占据的内存空间不是1个字节，而是4个字节 用u8或者前面加b前缀来表示1个字节的ASCII字符let x :u8 = 1; let y :u8 = b'A'; let s :&[u8;5] = b\"hello\"; let r :&[u8;14] = br#\"hello \\n world\"#; //这里表示byte的raw string, 好像去掉前后的#也行 注意这里的b\"hello\"和\"hello\"不一样:fn print_type_of(t: &T) { println!(\"{:#?}: {}\", t, std::any::type_name::()) } fn main() { print_type_of(&\"hello\"); print_type_of(&b\"hello\"); } //结果 \"hello\": &str [ 104, 101, 108, 108, 111, ]: &[u8; 5] 整型 用i或者u加位位宽表示, 比如i32 u64 i128等. 指针用isize或者usize表示, 在32位机器上是32位, 在64位机器上是64位.let var1 : i32 = 32; // 十进制表示 let var2 : i32 = 0xFF; // 以0x开头代表十六进制表示 let var3 : i32 = 0o55; // 以0o开头代表八进制表示 let var4 : i32 = 0b1001; // 以0b开头代表二进制表示 let var5 = 0x_1234_ABCD; //使用下划线分割数字,不影响语义,但是极大地提升了阅读体验。 let var6 = 123usize; // i6变量是usize类型 let var7 = 0x_ff_u8; // i7变量是u8类型 let var8 = 32; // 不写类型,默认为 i32 类型 rust可以在整型溢出时panic, 但需要\"debug\"版本编译, 比如rustc test.rs, 而rustc -O test.rs产生的\"release\"版本则不会panic, 而是自动截断. 通过开关rustc -C overflow-checks=no test.rs可以控制这个行为. 浮点let f1 = 123.0f64; // type f64 let f2 = 0.1f64; // type f64 let f3 = 0.1f32; // type f32 let f4 = 12E+99_f64; // type f64 科学计数法 let f5 : f64 = 2.; // type f64 指针 类型名 简介 Box 指向类型T的, 具有所有权的指针, 有权释放内存; T在堆中分配 &T 指向类型T的借用指针, 也称为引用, 无权释放内存, 无权写数据 &mut T 指向类型T的mut借用指针, 无权释放内存, 有权写数据 *const T 指向类型T的只读指针, 没有生命周期信息, 无权写数据 *mut T 指向类型T的读写指针, 没有生命周期信息, 有权写数据 注: &T是借用指针, 而*T实际上也存在的, 叫raw pointer. 但是必须以*mut T或*const T存在. 一般raw pointer不常用. 除此之外，在标准库中还有一种封装起来的可以当作指针使用的类型，叫“智能指针”（smart pointer） 类型名 简介 Rc 指向类型T的引用计数指针, 共享所有权, 线程不安全 Arc 指向类型T的原子引用计数指针, 共享所有权, 线程安全 Cow clone on write, 写时复制指针. 可能是借用指针, 也可能是具有所有权的指针 什么是Box Box是指向堆中类型为T的变量的指针. 这个T可以是unsized的. All values in Rust are stack allocated by default. Values can be boxed (allocated on the heap) by creating a Box. A box is a smart pointer to a heap allocated value of type T. When a box goes out of scope, its destructor is called, the inner object is destroyed, and the memory on the heap is freed. 默认变量是分配在栈上的, 用Box可以指定分配到堆上. 除了在堆上分配内存, Box没有其他的性能损失. 比如下面的代码: fn main() { let b = Box::new(5); //在堆中分配int32类型的5, Box这个\"指针\"是在栈上. 当b离开scope的时候, 堆上的数字5会被回收. println!(\"b = {}\", b); } 这个例子在堆里分配一个i32并没有实际意义, 只是演示Box的分配原理. Box的使用情况: When you have a type whose size can’t be known at compile time and you want to use a value of that type in a context that requires an exact size When you have a large amount of data and you want to transfer ownership but ensure the data won’t be copied when you do so 在堆里分配的数据在转移所有权的时候不拷贝. When you want to own a value and you care only that it’s a type that implements a particular trait rather than being of a specific type 什么是Rc To enable multiple ownership, Rust has a type called Rc, which is an abbreviation for reference counting. The Rc type keeps track of the number of references to a value to determine whether or not the value is still in use. If there are zero references to a value, the value can be cleaned up without any references becoming invalid. We use the Rc type when we want to allocate some data on the heap for multiple parts of our program to read and we can’t determine at compile time which part will finish using the data last. 比如b和c都\"拥有\"a这个链表 如果用Box是不行的: enum List { Cons(i32, Box), Nil, } use crate::List::{Cons, Nil}; fn main() { let a = Cons(5, Box::new(Cons(10, Box::new(Nil)))); let b = Cons(3, Box::new(a)); let c = Cons(4, Box::new(a)); // 当a move进b的时候, 所有权已经转给b了. a就不能再访问了. } The Cons variants own the data they hold, so when we create the b list, a is moved into b and b owns a. Then, when we try to use a again when creating c, we’re not allowed to because a has been moved. 下面的代码把Box换成了Rc Instead, we’ll change our definition of List to use Rc in place of Box, as shown in Listing 15-18. Each Cons variant will now hold a value and an Rc pointing to a List. When we create b, instead of taking ownership of a, we’ll clone the Rc that a is holding, thereby increasing the number of references from one to two and letting a and b share ownership of the data in that Rc. We’ll also clone a when creating c, increasing the number of references from two to three. Every time we call Rc::clone, the reference count to the data within the Rc will increase, and the data won’t be cleaned up unless there are zero references to it. enum List { Cons(i32, Rc), Nil, } use crate::List::{Cons, Nil}; use std::rc::Rc; fn main() { let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil))))); let b = Cons(3, Rc::clone(&a)); //这里是clone应该只是clone包装, 而不是clone里面的数据, 但增加引用计数. let c = Cons(4, Rc::clone(&a)); } 注: 用Rc::clone(&a)和a.clone()是一样的. 前者更隐含了是浅拷贝的意思, 开销非常小. 类型转换 Rust对不同类型之间的转换控制得非常严格。即便是下面这样的程序，也会出现编译错误 fn main() { let var1 : i8 = 41; let var2 : i16 = var1; } Rust提供了一个关键字as，专门用于这样的类型转换 fn main() { let var1 : i8 = 41; let var2 : i16 = var1 as i16; } 类型不能随便转换: let a = \"some string\"; let b = a as u32; // 编译错误 有时必须用多个as转换 fn main() { let i = 42; // 先转为 *const i32,再转为 *mut i32 let p = &i as *const i32 as *mut i32; println!(\"{:p}\", p); } 字符串 Rust的字符串涉及两种类型，一种是&str，另外一种是String.str是Rust的内置类型。&str是对str的借用。Rust的字符串内部默认是使用utf-8编码格式的。而内置的char类型是4字节长度的，存储的内容是Unicode Scalar Value。所以，Rust里面的字符串不能视为char类型的数组，而更接近u8类型的数组. [T]是DST类型，对应的str是DST类型。 &[T]是数组切片类型，对应的&str是字符串切片类型.下面的代码能编过 &str是个胖指针, 它对指向的字符串没有所有权. let greeting : &str = \"Hello\"; fn main() { let greeting: &str = \"Hello\"; let substr: &str = &greeting[2..]; println!(\"{}\", substr); } &greeting[2..]去掉&就编译不过 我们没办法扩大greeting所引用的范围，在它后面增加内容。但是String类型可以: fn main() { let mut s = String::from(\"Hello\"); s.push(' '); s.push_str(\"World.\"); println!(\"{}\", s); } &String类型可以被编译器自动转换为&str类型: fn capitalize(substr: &mut str) { substr.make_ascii_uppercase(); } fn main() { let mut s = String::from(\"Hello World\"); capitalize(&mut s); println!(\"{}\", s); } 在这个例子中，capitalize函数调用的时候，形式参数要求是&mut str类型，而实际参数是&mut String类型，这里编译器给我们做了自动类型转换。在capitalize函数内部，它有权修改&mut str所指向的内容，但是无权给这个字符串扩容或者释放内存。 复合数据类型 tuple 元组 用一对 () 包括的一组数据，可以包含不同种类的数据 let a = (1i32, false); // 元组中包含两个元素,第一个是i32类型,第二个是bool类型 let b = (\"a\", (1i32, 2i32)); // 元组中包含两个元素,第二个元素本身也是元组,它又包含了两个元素 只有一个元素要加逗号 let a = (0,); // a是一个元组,它有一个元素 let b = (0); // b是一个括号表达式,它是i32类型 访问tuple可以用模式匹配 let p = (1i32, 2i32); let (a, b) = p; //模式匹配 let x = p.0; //数字索引 let y = p.1; //数字索引 println!(\"{} {} {} {}\", a, b, x, y); 空元组占用0内存空间: let empty : () = (); fn main() { println!(\"size of i8 {}\" , std::mem::size_of::()); println!(\"size of char {}\" , std::mem::size_of::()); println!(\"size of '()' {}\" , std::mem::size_of::()); } 注: size_of的原型是 pub const fn size_of() -> usize 这个函数没有入参, 但需要实例化类型参数T 本例中这样调用: std::mem::size_of::() 用了双冒号实例化类型参数的方式. 数组 用一对 [] 包括的同类型数据数组是一个容器，它在一块连续空间内存中，存储了一系列的同样类型的数据。数组中元素的占用空间大小必须是编译期确定的。数组本身所容纳的元素个数也必须是编译期确定的，执行阶段不可变。如果需要使用变长的容器，可以使用标准库中的Vec/LinkedList等。数组类型的表示方式为[T;n]。其中T代表元素类型；n代表元素个数；它必须是编译期常量整数；中间用分号隔开。 let a = [1, 2, 3, 4, 5]; // a 是一个长度为 5 的整型数组 let b = [\"January\", \"February\", \"March\"]; // b 是一个长度为 3 的字符串数组 let c: [i32; 5] = [1, 2, 3, 4, 5]; // c 是一个长度为 5 的 i32 数组 let d = [3; 5]; // 等同于 let d = [3, 3, 3, 3, 3]; let first = a[0]; let second = a[1]; // 数组访问 a[0] = 123; // 错误：数组 a 不可变 let mut a = [1, 2, 3]; a[0] = 4; // 正确 只有元素类型和元素个数都完全相同，这两个数组才是同类型的。数组与指针之间不能隐式转换。同类型的数组之间可以互相赋值 fn main() { let mut xs: [i32; 5] = [1, 2, 3, 4, 5]; let ys: [i32; 5] = [6, 7, 8, 9, 10]; xs = ys; xs[0] = 111; println!(\"new array {:?}\", xs); println!(\"old array {:?}\", ys); } //output new array [111, 7, 8, 9, 10] old array [6, 7, 8, 9, 10] 把数组xs作为参数传给一个函数，这个数组并不会退化成一个指针。而是会将这个数组完整复制进这个函数。函数体内对数组的改动不会影响到外面的数组。 这里再解释一下, rust的数组赋值(或者说是move), 是拷贝完整数组. rust默认在栈上分配变量, 如果想在退出作用域的时候继续使用数据, 用Box来声明变量. 数组可以直接比较: fn main() { let v1 = [1, 2, 3]; let v2 = [1, 2, 4]; println!(\"{:?}\", v1 数组支持for in fn main() { let v = [0_i32; 10]; for i in &v { println!(\"{:?}\", i); } } 数组切片 对数组取借用borrow操作，可以生成一个“数组切片”（Slice）。数组切片对数组没有“所有权”，我们可以把数组切片看作专门用于指向数组的指针，是对数组的另外一个“视图”。比如，我们有一个数组[T; n]，它的借用指针的类型就是&[T; n]。它可以通过编译器内部魔法转换为数组切片类型&[T]。数组切片实质上还是指针，它不过是在类型系统中丢弃了编译阶段定长数组类型的长度信息，而将此长度信息存储为运行期的值。示例如下: fn main() { fn mut_array(a: &mut [i32]) { a[2] = 5; } println!(\"size of &[i32; 3] : {:?}\", std::mem::size_of::()); println!(\"size of &[i32] : {:?}\", std::mem::size_of::()); let mut v: [i32; 3] = [1, 2, 3]; { let s: &mut [i32; 3] = &mut v; mut_array(s); } println!(\"{:?}\", v); } 变量v是[i32; 3]类型；变量s是&mut[i32; 3]类型，占用的空间大小与指针相同。它可以自动转换为&mut[i32]数组切片类型传入函数 mut_array，占用的空间大小等于两个指针的空间大小。通过这个指针，在函数内部，修改了外部的数组v的值。 数组切片是指向一个数组的指针，而它比指针又多了一点东西——它不止包含有一个指向数组的指针，切片本身还含带长度信息。又叫胖指针.由于不定长数组类型[T]在编译阶段是无法判断该类型占用空间的大小的，目前我们不能在栈上声明一个不定长大小数组的变量实例，也不能用它作为函数的参数、返回值。但是，指向不定长数组的胖指针的大小是确定的，&[T]类型可以用做变量实例、函数参数、返回值。 字符串切片 fn main() { let s = String::from(\"broadcast\"); let part1 = &s[0..5]; let part2 = &s[5..9]; println!(\"{}={}+{}\", s, part1, part2); } // broadcast=broad+cast 非字符串切片 fn main() { let arr = [1, 3, 5, 7, 9]; let part = &arr[0..3]; for i in part.iter() { println!(\"{}\", i); } } 索引和边界检查 rust的索引是会边界检查的. 如果越界会panic 数组的index操做执行的是下面的代码: impl ops::Index for [T] { type Output = T; fn index(&self, index: usize) -> &T { assert!(index 实际上, 自己也可以定义index操做, 只要满足 std::ops::Index trait //索引读操做 std::ops::IndexMut trait //索引写操做 一般情况下，Rust不鼓励大量使用“索引”操作。正常的“索引”操作 都会执行一次“边界检查”。从执行效率上来说，Rust比C/C++的数组索引效率低一点，因为C/C++的索引操作是不执行任何安全性检查的，它们对应的Rust代码相当于调用get_unchecked()函数更推荐使用迭代器 fn main() { use std::iter::Iterator; let v = &[10i32, 20, 30, 40, 50]; // 如果我们同时需要index和内部元素的值,调用enumerate()方法 for (index, value) in v.iter().enumerate() { println!(\"{} {}\", index, value); } // filter方法可以执行过滤,nth函数可以获取第n个元素 let item = v.iter().filter(|&x| *x % 2 == 0).nth(2); println!(\"{:?}\", item); } 注: filter的原型如下, 可以看到闭包的入参是引用&Self::Item fn filter(self, predicate: P) -> Filter where Self: Sized, P: FnMut(&Self::Item) -> bool, { Filter::new(self, predicate) } FnMut原型如下, 闭包都会自动实现FnMut. pub trait FnMut: FnOnce { extern \"rust-call\" fn call_mut( &mut self, args: Args ) -> Self::Output; } struct 普通结构体定义: struct Site { domain: String, name: String, nation: String, found: u32 } 普通结构体实例化: let runoob = Site { domain: String::from(\"www.runoob.com\"), name: String::from(\"RUNOOB\"), nation: String::from(\"China\"), found: 2013 }; 举例: struct Point { x: i32, y: i32, } fn main() { let p = Point { x: 0, y: 0}; println!(\"Point is at {} {}\", p.x, p.y); } fn main() { // 刚好局部变量名字和结构体成员名字一致 let x = 10; let y = 20; // 下面是简略写法,等同于 Point { x: x, y: y },同名字的相对应 let p = Point { x, y }; println!(\"Point is at {} {}\", p.x, p.y); } 访问struct内部的元素: fn main() { let p = Point { x: 0, y: 0}; // 声明了px 和 py,分别绑定到成员 x 和成员 y let Point { x : px, y : py } = p; println!(\"Point is at {} {}\", px, py); // 同理,在模式匹配的时候,如果新的变量名刚好和成员名字相同,可以使用简写方式 let Point { x, y } = p; println!(\"Point is at {} {}\", x, y); } 部分初始化 Rust设计了一个语法糖，允许用一种简化的语法赋值使用另外一个 struct的部分成员。比如： struct Point3d { x: i32, y: i32, z: i32, } fn default() -> Point3d { Point3d { x: 0, y: 0, z: 0 } } // 可以使用default()函数初始化其他的元素 // ..expr 这样的语法,只能放在初始化表达式中,所有成员的最后最多只能有一个 let origin = Point3d { x: 5, ..default()}; let point = Point3d { z: 1, x: 2, ..origin }; 输出结构体 调试中，完整地显示出一个结构体实例是非常有用的。但如果我们手动的书写一个格式会非常的不方便。所以 Rust 提供了一个方便地输出一整个结构体的方法： #[derive(Debug)] struct Rectangle { width: u32, height: u32, } fn main() { let rect1 = Rectangle { width: 30, height: 50 }; println!(\"rect1 is {:?}\", rect1); } 如第一行所示：一定要导入调试库 #[derive(Debug)] ，之后在 println 和 print 宏中就可以用 {:?} 占位符输出一整个结构体： rect1 is Rectangle { width: 30, height: 50 } 如果属性较多的话可以使用另一个占位符 {:#?} rect1 is Rectangle { width: 30, height: 50 } tuple struct 元组结构体 有一种更简单的定义和使用结构体的方式：元组结构体。 元组结构体是一种形式是元组的结构体。 与元组的区别是它有名字和固定的类型格式。它存在的意义是为了处理那些需要定义类型（经常使用）又不想太复杂的简单数据： struct Color(u8, u8, u8); struct Point(f64, f64); let black = Color(0, 0, 0); let origin = Point(0.0, 0.0); \"颜色\"和\"点坐标\"是常用的两种数据类型，但如果实例化时写个大括号再写上两个名字就为了可读性牺牲了便捷性，Rust不会遗留这个问题。元组结构体对象的使用方式和元组一样，通过. 和下标来进行访问： fn main() { struct Color(u8, u8, u8); struct Point(f64, f64); let black = Color(0, 0, 0); let origin = Point(0.0, 0.0); println!(\"black = ({}, {}, {})\", black.0, black.1, black.2); println!(\"origin = ({}, {})\", origin.0, origin.1); } 举例 //以下三种都可以,内部可以没有成员: 空struct struct Foo1; struct Foo2(); struct Foo3{} // struct有名字, 但成员不用名字, 这种类型的叫tuple struct struct Color(i32, i32, i32); struct Point(i32, i32, i32); // 可以类似下面的结构体: struct Color{ 0: i32, 1: i32, 2: i32, } struct Point { 0: i32, 1: i32, 2: i32, } 举例: // define struct struct T1 { v: i32 } // define tuple struct struct T2(i32); fn main() { let v1 = T1 { v: 1 }; let v2 = T2(1); // init tuple struct let v3 = T2 { 0: 1 }; // init tuple struct let i1 = v1.v; let i2 = v2.0; let i3 = v3.0; } fn main() { struct Inches(i32); fn f1(value : Inches) {} fn f2(value : i32) {} let v : i32 = 0; f1(v); // 编译不通过,'mismatched types' f2(v); } fn type_alias() { type I = i32; fn f1(v : I) {} fn f2(v : i32) {} let v : i32 = 0; f1(v); //可以编译过, 因为type只是别名 f2(v); } 结构体方法 第一个入参是&self的函数就是这个结构体方法, 用impl块包住: struct Rectangle { width: u32, height: u32, } impl Rectangle { fn area(&self) -> u32 { self.width * self.height } } fn main() { let rect1 = Rectangle { width: 30, height: 50 }; println!(\"rect1's area is {}\", rect1.area()); } //结果 //rect1's area is 1500 请注意，在调用结构体方法的时候不需要填写 self ，这是出于对使用方便性的考虑。 结构体关联函数 impl块里面的没有用&self的函数就是关联函数, 使用的时候要加这个结构体的前缀:: #[derive(Debug)] struct Rectangle { width: u32, height: u32, } impl Rectangle { fn create(width: u32, height: u32) -> Rectangle { Rectangle { width, height } } } fn main() { let rect = Rectangle::create(30, 50); println!(\"{:?}\", rect); } //结果 //Rectangle { width: 30, height: 50 } 普遍方法 在Rust中，我们可以为任何一个类型添加方法，整型也不例外。比如在标准库中，整数类型有一个方法是pow，它可以计算n次幂，于是我们可以这么使用： let x : i32 = 9; println!(\"9 power 3 = {}\", x.pow(3)); println!(\"9 power 3 = {}\", 9_i32.pow(3)); //也可以直接使用字面量来调用方法 静态方法 没有receiver参数的方法（第一个参数不是self参数的方法）称作“静态方法”。静态方法可以通过Type::FunctionName()的方式调用。 需要注意的是，即便我们的第一个参数是Self相关类型，只要变量名字不是self，就不能使用小数点的语法调用函数。 struct T(i32); impl T { // 这是一个静态方法 fn func(this: &Self) { println!{\"value {}\", this.0}; } } fn main() { let x = T(42); // x.func(); 小数点方式调用是不合法的 T::func(&x); } trait也可以定义静态函数: pub trait Default { fn default() -> Self; } Default trait实际上可以看作一个针对无参数构造函数的统一抽象 impl Default for Vec { fn default() -> Vec { Vec::new() } } enum 枚举类在 Rust 中并不像其他编程语言中的概念那样简单，但依然可以十分简单的使用： #[derive(Debug)] enum Book { Papery, Electronic } fn main() { let book = Book::Papery; println!(\"{:?}\", book); } // 结果 // Papery 书分为纸质书（Papery book）和电子书（Electronic book）。 如果你现在正在开发一个图书管理系统，你需要描述两种书的不同属性（纸质书有索书号，电子书只有 URL），你可以为枚举类成员添加元组属性描述： enum Book { Papery(u32), Electronic(String), } let book = Book::Papery(1001); let ebook = Book::Electronic(String::from(\"url://...\")); 如果你想为属性命名，可以用结构体语法： enum Book { Papery { index: u32 }, Electronic { url: String }, } let book = Book::Papery{index: 1001}; 虽然可以如此命名，但请注意，并不能像访问结构体字段一样访问枚举类绑定的属性。访问的方法在 match 语法中。 enum有或的意思, 下面的Number就是或者是Int, 或者是Float enum Number { Int(i32), Float(f32), } fn read_num(num: &Number) { match num { // 如果匹配到了 Number::Int 这个成员,那么value的类型就是 i32 &Number::Int(value) => println!(\"Integer {}\", value), // 如果匹配到了 Number::Float 这个成员,那么value的类型就是 f32 &Number::Float(value) => println!(\"Float {}\", value), } } fn main() { let n: Number = Number::Int(10); read_num(&n); } 在Rust中，enum和struct为内部成员创建了新的名字空间。如果要访问内部成员，可以使用::符号 enum Message { Quit, ChangeColor(i32, i32, i32), Move { x: i32, y: i32 }, Write(String), } let x: Message = Message::Move { x: 3, y: 4 }; enum BoardGameTurn { Move { squares: i32 }, Pass, } let y: BoardGameTurn = BoardGameTurn::Move { squares: 1 }; enum和match Rust 通过 match 语句来实现分支结构。先认识一下如何用 match 处理枚举类： fn main() { enum Book { Papery {index: u32}, Electronic {url: String}, } let book = Book::Papery{index: 1001}; let ebook = Book::Electronic{url: String::from(\"url...\")}; match book { Book::Papery { index } => { println!(\"Papery book {}\", index); }, Book::Electronic { url } => { println!(\"E-book {}\", url); } } } // 结果 //Papery book 1001 match 块也可以当作函数表达式来对待，它也是可以有返回值的： match 枚举类实例 { 分类1 => 返回值表达式, 分类2 => 返回值表达式, ... } 但是所有返回值表达式的类型必须一样！ 如果把枚举类附加属性定义成元组，在 match 块中需要临时指定一个名字： enum Book { Papery(u32), Electronic {url: String}, } let book = Book::Papery(1001); match book { Book::Papery(i) => { println!(\"{}\", i); }, Book::Electronic { url } => { println!(\"{}\", url); } } 经常出现的Option就是一种enum enum Option是个标准库里经常用到的类型, 已经被preclude了, 不用use能直接用: enum Option { None, Some(T), } 它表示的含义是“要么存在、要么不存在”。比如Option表达的意思 就是“可以是一个i32类型的值，或者没有任何值”。 Rust引入Option是为了解决Null指针问题 如果你想定义一个可以为空值的类，你可以这样： let opt = Option::Some(\"Hello\"); // 这个opt的类型是core::option::Option 如果你想针对 opt 执行某些操作，你必须先判断它是否是 Option::None： fn main() { let opt = Option::Some(\"Hello\"); match opt { Option::Some(something) => { println!(\"{}\", something); }, Option::None => { println!(\"opt is nothing\"); } } } //结果 //Hello 由于 Option 是 Rust 编译器默认引入的，在使用时可以省略 Option:: 直接写 None 或者 Some()。 fn main() { let t = Some(64); match t { Some(64) => println!(\"Yes\"), _ => println!(\"No\"), } } 总结一下: 如果从逻辑上说，我们需要一个变量确实是可空的，那么就应该显式标明其类型为Option，否则应该直接声明为T类型。从类型系 统的角度来说，这二者有本质区别，切不可混为一谈 不要轻易使用unwrap方法。这个方法可能会导致程序发生 panic。对于小工具来说无所谓，在正式项目中，最好是使用lint工具强制禁止调用这个方法 相对于裸指针，使用Option包装的指针类型的执行效率不会降低，这是“零开销抽象” 不必担心这样的设计会导致大量的match语句，使得程序可读性变差。因为Option类型有许多方便的成员函数，再配合上闭包功能，实际上在表达能力和可读性上要更胜一筹 if let代替match match需要强制做全匹配, 否则会编译报错. 用下划线可以解决编译错误, 但有点笨. 用if let可以更好的表达只关心一部分匹配的情况: if let 匹配值 = 源变量 { 语句块 } 比如 let i = 0; if let 0 = i { println!(\"zero\"); } 更完整的例子: fn main() { enum Book { Papery(u32), Electronic(String) } let book = Book::Electronic(String::from(\"url\")); if let Book::Papery(index) = book { println!(\"Papery {}\", index); } else { println!(\"Not papery book\"); } } 表达式 小知识: ! 按位取反或者逻辑取反, 按照operand类型决定Rust不支持++、--运算符，请使用+=1、-=1替代 rust里面每个表达式都是有类型的, 比如赋值表达式的类型为空的tuple (), 也叫unit fn main() { let x = 1; let mut y = 2; // 注意这里专门用括号括起来了 let z = (y = x); println!(\"{:?}\", z); //编译有警告, 但能运行, 结果为() } 连续语句块的类型是最后一个表达式的类型: // 语句块可以是表达式,注意后面有分号结尾,x的类型是() let x : () = { println!(\"Hello.\"); }; // Rust将按顺序执行语句块内的语句,并将最后一个表达式类型返回,y的类型是 i32 let y : i32 = { println!(\"Hello.\"); 5 }; 利用这个特点, 函数的返回可以不写return fn my_func() -> i32 { // ... blablabla 各种语句 100 //最后一个表达式是100, 是i32. 注意没有分号 } if else fn func(i : i32) { if n 0 { print!(\"{} is positive\", n); } else { print!(\"{} is zero\", n); } } if else也可以当作表达式: let x : i32 = if condition { 1 } else { 10 }; //注意1和10后面不加分号. 因为加了分号表达式的整体类型就变了 if let和while let while-let与if-let一样，提供了在while语句中使用“模式解构”的能力 if let Some(x) = optVal { doSomethingWith(x); } 相当于: match optVal { Some(x) => { doSomethingWith(x); } _ => {} } 也相当于: if optVal.is_some() { // 首先判断它一定是 Some(_) let x = optVal.unwrap(); // 然后取出内部的数据 doSomethingWith(x); } 循环 loop 没条件注意break和continue都可以跳转到外层 我们可以在loop while for循环前面加上“生命周期标识符”。该标识符以单引号开头，在内部的循环中可以使用break语句选择跳出到哪一层。 fn main() { // A counter variable let mut m = 1; let n = 1; 'a: loop { if m 50 { println!(\"break\"); break 'a; } else { continue 'a; } } } } } loop表达式也可以做右值: fn main() { let v = loop { break 10; }; println!(\"{}\", v); } 在loop内部break的后面可以跟一个表达式，这个表达式就是最终的 loop表达式的值。如果一个loop永远不返回，那么它的类型就是“发散类型”。示例如下： fn main() { let v = loop {}; println!(\"{}\", v); //永远到不了这里 } while是带条件的循环 for in是迭代器循环. 没有三段式的语法 函数 fn add1(t : (i32,i32)) -> i32 { t.0 + t.1 } // 模式解构传参 fn add2((x,y) : (i32,i32)) -> i32 { x + y //也可以用return x+y } 函数不写返回类型默认是unit () 函数是一等公民 函数内部可以定义函数, 类型, trait等 虽然add1和add2的入参和出参都一样, 但每个函数都有自己的类型fn main() { // 先让 func 指向 add1 let mut func = add1; // 再重新赋值,让 func 指向 add2 func = add2; //这样不行, 会编译报错. 原因就是函数名也是类型的一部分 } 上面的func声明改成下面的写法就不会出错了:// 写法一,用 as 类型转换 let mut func = add1 as fn((i32,i32))->i32; // 写法二,用显式类型标记 let mut func : fn((i32,i32))->i32 = add1; 但只有add1和add2形式一样的时候才行. 形式不同不能转换. 发散函数Diverging functions 返回类型是!的函数是发散函数, 比如: fn diverges() -> ! { panic!(\"This function never returns!\"); } !可以转换为任何类型 let x : i32 = diverges(); let y : String = diverges(); 比如下面的情况就很有用: let p = if x { panic!(\"error\"); } else { 100 }; 上面的代码能编译通过, 因为!也可以赋值给p 内置发散函数: panic! 以及基于它实现的各种函数/宏，比如unimplemented! unreachable! 死循环loop{} 进程退出函数std::process::exit以及类似的libc中的exec一类函数 main函数 main函数没有入参和返回值, 命令行参数用std::env::args() fn main() { for arg in std::env::args() { println!(\"Arg: {}\", arg); } std::process::exit(0); } const_fn 函数可以用const关键字修饰，这样的函数可以在编译阶段被编译器执行，返回值也被视为编译期常量 #![feature(const_fn)] const fn cube(num: usize) -> usize { num * num * num } fn main() { const DIM : usize = cube(2); const ARR : [i32; DIM] = [0; DIM]; println!(\"{:?}\", ARR); } trait trait的意思是特性 初看和go的interface差不多. trait Shape { fn area(&self) -> f64; } Rust中Self（大写S）和self（小写s）都是关键字，大写S的是类型名，小写s的是变量名 所有的trait中都有一个隐藏的类型Self（大写S），代表当前这个实现了此trait的具体类型 trait T { fn method1(self: Self); fn method2(self: &Self); fn method3(self: &mut Self); } // 上下两种写法是完全一样的 trait T { fn method1(self); fn method2(&self); fn method3(&mut self); } 上面定义的这个area方法的参数的名字为self，它的类型是&Self类型。我们可以把上面这个方法的声明看成： trait Shape { fn area(self: &Self) -> f64; } 假如我们有一个结构体类型Circle，它实现了这个trait，代码如下: struct Circle { radius: f64, } impl Shape for Circle { // Self 类型就是 Circle // self 的类型是 &Self,即 &Circle fn area(&self) -> f64 { // 访问成员变量,需要用 self.radius std::f64::consts::PI * self.radius * self.radius } } fn main() { let c = Circle { radius : 2f64}; // 第一个参数名字是 self,可以使用小数点语法调用 println!(\"The area is {}\", c.area()); } 默认trait 这是特性与接口的不同点：接口只能规范方法而不能定义方法，但特性可以定义方法作为默认方法，因为是\"默认\"，所以对象既可以重新定义方法，也可以不重新定义方法使用默认的方法 trait中可以包含方法的默认实现。如果这个方法在trait中已经有了方法体，那么在针对具体类型实现的时候，就可以选择不用重写 trait Descriptive { fn describe(&self) -> String { String::from(\"[Object]\") } } struct Person { name: String, age: u8 } impl Descriptive for Person { fn describe(&self) -> String { format!(\"{} {}\", self.name, self.age) } } fn main() { let cali = Person { name: String::from(\"Cali\"), age: 24 }; println!(\"{}\", cali.describe()); } //结果 //Cali 24 如果Person不实现describe, 则最后打印 [Object] trait做参数 fn output(object: impl Descriptive) { println!(\"{}\", object.describe()); } 任何实现了 Descriptive 特性的对象都可以作为这个函数的参数，这个函数没必要了解传入对象有没有其他属性或方法，只需要了解它一定有 Descriptive 特性规范的方法就可以了。当然，此函数内也无法使用其他的属性与方法。 特性参数还可以用这种等效语法实现： fn output(object: T) { println!(\"{}\", object.describe()); } 这是一种风格类似泛型的语法糖，这种语法糖在有多个参数类型均是特性的情况下十分实用： fn output_two(arg1: T, arg2: T) { println!(\"{}\", arg1.describe()); println!(\"{}\", arg2.describe()); } 特性作类型表示时如果涉及多个特性，可以用 + 符号表示，例如： fn notify(item: impl Summary + Display) fn notify(item: T) 复杂的实现关系可以使用 where 关键字简化，例如： fn some_function(t: T, u: U) 可以简化成: fn some_function(t: T, u: U) -> i32 where T: Display + Clone, U: Clone + Debug 匿名trait 针对一个类型，我们可以直接对它impl来增加成员方法，无须trait名字。比如： impl Circle { fn get_radius(&self) -> f64 { self.radius } } 我们可以把这段代码看作是为Circle类型impl了一个匿名的trait。用这种方式定义的方法叫作这个类型的“内在方法”（inherent methods）。 Box trait Shape { fn area(self: Box) -> f64; } struct Circle { radius: f64, } impl Shape for Circle { // Self 类型就是 Circle // self 的类型是 Box,即 Box fn area(self : Box) -> f64 { // 访问成员变量,需要用 self.radius std::f64::consts::PI * self.radius * self.radius } } fn main() { let c = Circle { radius : 2f64}; // 编译错误 // c.area(); let b = Box::new(Circle {radius : 4f64}); // 编译正确 b.area(); } impl trait for trait 语法: impl for Rust 同一个类可以实现多个特性，每个 impl 块只能实现一个。 trait Shape { fn area(&self) -> f64; } trait Round { fn get_radius(&self) -> f64; } struct Circle { radius: f64, } impl Round for Circle { fn get_radius(&self) -> f64 { self.radius } } // 注意这里是 impl Trait for Trait impl Shape for dyn Round { fn area(&self) -> f64 { std::f64::consts::PI * self.get_radius() * self.get_radius() } } fn main() { let c = Circle { radius : 2f64}; // 编译错误 // c.area(); // let b = Circle { radius : 2f64} as dyn Round; //这样也不行, error[E0620]: cast to unsized type: `Circle` as `dyn Round` let b = Box::new(Circle {radius : 4f64}) as Box; // 编译正确 b.area(); } 注: 以上代码在edition2021编译不过, 需要在Round前面加dyn关键词(已加)加了以后上面代码能编过, 但有个变量c没有使用的警告. 为别人的类型实现trait 比如下面的代码就给内置类型i32实现了Double方法: trait Double { fn double(&self) -> Self; } impl Double for i32 { fn double(&self) -> i32 { *self * 2 } } fn main() { // 可以像成员方法一样调用 let x : i32 = 10.double(); println!(\"{}\", x); } 要给别人的类型添加方法, 需要满足下面的条件:impl块要么与trait的声明在同一个的crate中，要么与类型的声明在同一个crate中 trait不能做为参数, 返回值, 实例变量 参数, 返回值, 实例变量等需要明确知道size的地方不能直接用trait.Rust是一种用户可以对内存有精确控制能力的强类型语言。我们可以自由指定一个变量是在栈里面，还是在堆里面，变量和指针也是不同的类型。类型是有大小（Size）的。有些类型的大小是在编译阶段可以确定的，有些类型的大小是编译阶段无法确定的。目前版本的Rust规定，在函数参数传递、返回值传递等地方，都要求这个类型在编译阶段有确定的大小。否则，编译器就不知道该如何生成代码了。 而trait本身既不是具体类型，也不是指针类型，它只是定义了针对类型的、抽象的“约束”。不同的类型可以实现同一个trait，满足同一个trait的类型可能具有不同的大小。因此，trait在编译阶段没有固定大小，目前我们不能直接使用trait作为实例变量、参数、返回值。 下面的代码是不对的: let x: Shape = Circle::new(); // Shape 不能做局部变量的类型 fn use_shape(arg : Shape) {} // Shape 不能直接做参数的类型 fn ret_shape() -> Shape {} // Shape 不能直接做返回值的类型 调用trait trait Cook { fn start(&self); } trait Wash { fn start(&self); } struct Chef; impl Cook for Chef { fn start(&self) { println!(\"Cook::start\"); } } impl Wash for Chef { fn start(&self) { println!(\"Wash::start\"); } } fn main() { let me = Chef; me.start(); //这里编不过, 因为两个trait都有start方法 } //应该用下面的格式来调用: fn main() { let me = Chef; // 函数名字使用更完整的path来指定,同时,self参数需要显式传递 // 下面两种格式都可以 ::start(&me); ::start(&me); } 方法的点引用是语法糖: 和go一样, 通过小数点语法调用方法调用，有一个“隐藏着”的“取引用”步骤。虽然我们看起来源代码长的是这个样子 me.start()，但是大家心里要清楚，真正传递给start()方法的参数是 &me而不是me，这一步是编译器自动帮我们做的。不论这个方法接受的self参数究竟是Self、&Self还是&mut Self，最终在源码上，我们都是统一的写法：variable.method() 方法和函数没有本质不同? struct T(usize); impl T { fn get1(&self) -> usize { self.0 } fn get2(&self) -> usize { self.0 } } fn get3(t: &T) -> usize { t.0 } fn check_type(_: fn(&T) -> usize) {} fn main() { check_type(T::get1); check_type(T::get2); check_type(get3); } 可以看到，get1、get2和get3都可以自动转成fn（&T）-> usize类型 trait约束 use std::fmt::Debug; fn my_print(x: T) { println!(\"The value is {:?}.\", x); } fn main() { my_print(\"China\"); my_print(41_i32); my_print(true); my_print(['a', 'b', 'c']) } 上面的代码能编过, 是因为my_print需要泛型T满足Debug约束(因为使用了{:?}), 而字符串, i32, bool, 数组都实现了Debug trait. 如果一个自定义类型没有实现Debug trait, 编译就会报错. 上面的约束还可以写成: fn my_print(x: T) where T: Debug { println!(\"The value is {:?}.\", x); } trait继承 实现Derived trait的struct也被要求实现Base trait trait Base {} trait Derived : Base {} //等同于trait Derived where Self：Base{} struct T; impl Derived for T {} impl Base for T {} //需要再加上这句 fn main() { ... } derive derive是个特殊的编译指示: #[derive(Copy, Clone, Default, Debug, Hash, PartialEq, Eq, PartialOrd, Ord)] struct Foo { data: i32, } fn main() { let v1 = Foo { data: 0 }; let v2 = v1; println!(\"{:?}\", v2); } 意思是编译器会自动生成如下的代码: impl Copy for Foo { ... } impl Clone for Foo { ... } impl Default for Foo { ... } impl Debug for Foo { ... } impl Hash for Foo { ... } impl PartialEq for Foo { ... } ... 从而让Foo能够继承列表里的trait. 能够被derive的trait有: Debug Clone Copy Hash RustcEncodable RustcDecodable PartialEq Eq ParialOrd Ord Default FromPrimitive Send Sync 常见trait 只有实现了Display trait的类型，才能用{}格式控制打印出来；只有实现了Debug trait的类型，才能用{:?} {:#?}格式控制打印出来 // std::fmt::Display pub trait Display { fn fmt(&self, f: &mut Formatter) -> Result; } // std::fmt::Debug pub trait Debug { fn fmt(&self, f: &mut Formatter) -> Result; } 带关联类型的trait 标准库中有一个trait叫FromStr，它有一个关联类型代表错误： pub trait FromStr { type Err; fn from_str(s: &str) -> Result; } 如果某些类型调用from_str方法永远不会出错，那么这个Err类型可以指定为! use std::mem::{size_of, size_of_val}; use std::str::FromStr; struct T(String); impl FromStr for T { type Err = !; fn from_str(s: &str) -> Result { Ok(T(String::from(s))) } } fn main() { let r: Result = T::from_str(\"hello\"); println!(\"Size of T: {}\", size_of::()); println!(\"Size of Result: {}\", size_of_val(&r)); // 将来甚至应该可以直接用 let 语句进行模式匹配而不发生编译错误 // 因为编译器有能力推理出 Err 分支没必要存在 // let Ok(T(ref s)) = r; // println!(\"{}\", s); } 模式解构 struct T1(i32, char); struct T2 { item1: T1, item2: bool, } fn main() { let x = T2 { item1: T1(0, 'A'), item2: false, }; let T2 { item1: T1(value1, value2), item2: value3, } = x; //从x解构出value1, value2, value3; 后者直接就当变量用了 println!(\"{} {} {}\", value1, value2, value3); } 下划线用来占位: struct P(f32, f32, f32); fn calc(arg: P) -> f32 { // 匹配 tuple struct,但是忽略第二个成员的值 let P(x, _, y) = arg; x * x + y * y } fn main() { let t = P(1.0, 2.0, 3.0); println!(\"{}\", calc(t)); } match rust的match初看和switch case意思差不多: enum Direction { East, West, South, North, } fn print(x: Direction) { match x { Direction::East => { println!(\"East\"); } Direction::West => { println!(\"West\"); } Direction::South => { println!(\"South\"); } Direction::North => { println!(\"North\"); } } } fn main() { let x = Direction::East; print(x); } 但match更严格, 比如我们删除North分支, 编译会报错. 解决办法就是把不需要的分支用_覆盖: fn print(x: Direction) { match x { Direction::East => { println!(\"East\"); } Direction::West => { println!(\"West\"); } Direction::South => { println!(\"South\"); } _ => { println!(\"Other\"); } } } 类似的, 下面的match如果没有下划线那个分支, 也是编译不过的: fn category(x: i32) { match x { -1 => println!(\"negative\"), 0 => println!(\"zero\"), 1 => println!(\"positive\"), _ => println!(\"error\"), } } fn main() { let x = 1; category(x); } match还支持|操做和..范围操做: fn category(x: i32) { match x { -1 | 1 => println!(\"true\"), 0 => println!(\"false\"), _ => println!(\"error\"), } } fn main() { let x = 1; category(x); } let x = 'X'; match x { 'a' ..= 'z' => println!(\"lowercase\"), 'A' ..= 'Z' => println!(\"uppercase\"), _ => println!(\"something else\"), } match还支持在分支内加if判断: match x { OptionalInt::Value(i) if i > 5 => println!(\"Got an int bigger than five!\"), OptionalInt::Value(..) => println!(\"Got an int!\"), OptionalInt::Missing => println!(\"No such luck.\"), } fn intersect(arg: i32) { match arg { i if i println!(\"case 1\"), i if i println!(\"case 2\"), i if i * i println!(\"case 3\"), _ => println!(\"default case\"), } } 还可以用@绑定变量. @符号前面是新声明的变量，后面是需要匹配的模式： let x = 1; match x { e @ 1 ..= 5 => println!(\"got a range element {}\", e), _ => println!(\"anything\"), } ref和mut ref: 引用, 避免出现所有权转移 mut: 借用 let mut x: &mut i32; 以上两处的mut含义是不同的。第1处mut，代表这个变量x本身可变，因此它能够重新绑定到另外一个变量上去，具体到这个示例来说，就是指针的指向可以变化。第2处mut，修饰的是指针，代表这个指针对于所指向的内存具有修改能力，因此我们可以用*x=1;这样的语句，改变它所指向的内存的值。 知晓变量类型 方案一: 利用编译错误来获取变量类型 // 这个函数接受一个 unit 类型作为参数 fn type_id(_: ()) {} fn main() { let ref x = 5_i32; // 实际参数的类型肯定不是 unit,此处必定有编译错误,通过编译错误,我们可以看到实参的具体类型 type_id(x); } 方案二: 使用标准库 #![feature(core_intrinsics)] fn print_type_name(_arg: &T) { unsafe { println!(\"{}\", std::intrinsics::type_name::()); } } fn main() { let ref x = 5_i32; print_type_name(&x); } 比如要知道下面的变量类型: let x = 5_i32; // i32 let x = &5_i32; // &i32 let ref x = 5_i32; // ??? let ref x = &5_i32; // ??? 代码: #![feature(core_intrinsics)] fn print_type_name(_arg: &T) { println!(\"{}\", std::intrinsics::type_name::()); } fn main() { let x = 5_i32; print_type_name(&x); //i32 let x = &5_i32; print_type_name(&x); //&i32 let ref x = 5_i32; print_type_name(&x); //&i32 let ref x = &5_i32; print_type_name(&x); //&&i32 } 注: 上面代码只能在debug优化模式和nightly channel下编译通过 方案三: 不需要nightly版本 fn print_type_of(_: &T) { println!(\"{}\", std::any::type_name::()) } 问号操作符 用在Result或Option后面, 用于提前返回或者unwrap value用在Result上, 可以提前返回Err(From::from(e)); 没有Error时则unwrap返回T fn try_to_parse() -> Result { let x: i32 = \"123\".parse()?; // x = 123 let y: i32 = \"24a\".parse()?; // returns an Err() immediately Ok(x + y) // Doesn't run. } let res = try_to_parse(); println!(\"{:?}\", res); 用在Option上, 可以提前返回None; 有值则unwrap返回T: fn try_option_some() -> Option { let val = Some(1)?; Some(val) } assert_eq!(try_option_some(), Some(1)); fn try_option_none() -> Option { let val = None?; Some(val) } assert_eq!(try_option_none(), None); 问号操作背后 代码rust/library/core/src/ops/try_trait.rs 问号operator背后是Try这个trait pub trait Try: FromResidual { type Output; type Residual; fn from_output(output: Self::Output) -> Self; fn branch(self) -> ControlFlow; } "},"notes/rust_入门2.html":{"url":"notes/rust_入门2.html","title":"Rust 泛型和内存所有权","keywords":"","body":" 宏 泛型 结构体的泛型 结构体泛型的具化 枚举的泛型 泛型参数可以有默认值 函数中的泛型 泛型实现重载 重载需要判断类型 impl块中的泛型 泛型的约束 关联类型 trait for trait 泛型的方法 内存安全 生命周期 生命周期标记 类型的生命周期标记 省略生命周期标记 所有权 所有权规则 数据赋值和拷贝 所有权与函数 返回值与作用域 println不会发生所有权转移(move) Rc允许多个所有权拥有者 引用和借用 copy和clone 析构函数 mut和&mut 借用指针 为什么rust是内存安全的? 解引用 自定义解引用 常见指针类型 自动解引用 Rc的自动解引用 引用计数 cow 智能指针 其他 在方法里定义struct是可以的 文件打开 unsafe unsafe可以操作裸指针 不用unsafe的swap例子 标准库的swap Vec代码 宏 比如打印当前文件和行号: fn main() { println!(\"file {} line {} \", file!(), line!()); } 比如避免重复: add_impl! { usize u8 u16 u32 u64 isize i8 i16 i32 i64 f32 f64 } 比如初始化一个动态数组: let v = vec![1, 2, 3, 4, 5]; 泛型 max函数的入参是泛型T的数组 fn max(array: &[T]) -> T { let mut max_index = 0; let mut i = 1; while i array[max_index] { max_index = i; } i += 1; } array[max_index] } 结构体的泛型 struct Point { x: T, y: T } 可以这样用: let p1 = Point {x: 1, y: 2}; let p2 = Point {x: 1.0, y: 2.0}; 但这样不行: let p = Point {x: 1, y: 2.0}; x 与 1 绑定时就已经将 T 设定为 i32，所以不允许再出现 f64 的类型。如果我们想让 x 与 y 用不同的数据类型表示，可以使用两个泛型标识符： struct Point { x: T1, y: T2 } 结构体泛型的具化 可以让编译器自动推断, 也可以指定类型, 在左侧和右侧都可以, 但在右侧不支持S的语法: fn print_type_of(_: &T) { println!(\"{}\", std::any::type_name::()) } struct S { data: T, } fn main() { //let four: u32 = \"4\".parse(); //println!(\"{:?}\", four); let x = S { data: 6 }; //编译器自动推断 let y = S { data: 5.5 }; //编译器自动推断 let y1: S = S { data: 5.5 }; //声明y1是S let y2: S:: = S { data: 5.5 }; //声明y2是S:: //let y3 = S {data: 5.5}; //NOK, 编译不过, 编译器认为<>是大于小于号. 据说是编译器图简单 let y3 = S:: { data: 5.5 }; //显式实例化y3, 在右侧只能用双冒号 print_type_of(&x); print_type_of(&y); print_type_of(&y1); print_type_of(&y2); print_type_of(&y3); print_type_of(&\"4\".parse::()); } //结果 playground::S playground::S playground::S playground::S playground::S core::result::Result 枚举的泛型 比如 enum Option { Some(T), None, } 这里的实际上是声明了一个“类型”参数。在这个Option内部，Some(T)是一个tuple struct，包含一个元素类型为T。这个泛型参数类型T，可以在使用时指定具体类型。 使用的时候: let x: Option = Some(42); let y: Option = None; 泛型参数可以有默认值 比如下面的泛型T默认是i32 struct S { data: T } fn main() { let v1 = S { data: 0}; let v2 = S:: { data: false}; println!(\"{} {}\", v1.data, v2.data); } 函数中的泛型 比如: fn compare_option(first: Option, second: Option) -> bool { match(first, second) { (Some(..), Some(..)) => true, (None, None) => true, _ => false } } 注:Some(..)中的双点表示ignore全部, Some(_)表示ignore第一个参数. 再举例: fn largest(list: &[T]) -> &T { let mut largest = &list[0]; for item in list.iter() { if item > largest { largest = item; } } largest } fn main() { let number_list = vec![34, 50, 25, 100, 65]; let result = largest(&number_list); println!(\"The largest number is {}\", result); let char_list = vec!['y', 'm', 'a', 'q']; let result = largest(&char_list); println!(\"The largest char is {}\", result); } //结果 The largest number is 100 The largest char is y 泛型实现重载 比如str的contains方法就接受不同类型的参数: fn main() { let s = \"hello\"; println!(\"{}\", s.contains('a')); println!(\"{}\", s.contains(\"abc\")); println!(\"{}\", s.contains(&['H'] as &[char])); println!(\"{}\", s.contains(|c : char| c.len_utf8() > 2)); } 这个contains的签名是: fn contains>(&'a self, pat: P) -> bool 第二个参数pat是个泛型P, 只要满足Pattern trait, 就能被contains所用. 重载需要判断类型 下面的代码编译不过, 因为let f = i.convert();中, 编译器无法知道f的类型, 于是无法推断出应该调用哪个. trait ConvertTo { fn convert(&self) -> T; } impl ConvertTo for i32 { fn convert(&self) -> f32 { *self as f32 } } impl ConvertTo for i32 { fn convert(&self) -> f64 { *self as f64 } } fn main() { let i = 1_i32; let f = i.convert(); // 这里编译不过 println!(\"{:?}\", f); } 要这样改: let f : f32 = i.convert(); // 或者 let f = ConvertTo::::convert(&i); impl块中的泛型 可以impl 某个trait for type, 也可以为trait impl trait. 即trait for trait. 这个impl trait for trait比如: impl Into for T where U: From, { fn into(self) -> U { U::from(self) } } 泛型的约束 下面的代码编译不通过: fn max(a: T, b: T) -> T { if a 因为并不是所有类型都实现了比较操作, 那么泛型T没有约束的话, 是编译不过的. 加约束有两个写法: 冒号方式 use std::cmp::PartialOrd; // 第一种写法：在泛型参数后面用冒号约束 fn max(a: T, b: T) -> T { where语法 // 第二种写法,在后面单独用 where 子句指定 fn max(a: T, b: T) -> T where T: PartialOrd where语法灵活性更好: trait Iterator { type Item; // Item 是一个关联类型 // 此处的where子句没办法在声明泛型参数的时候写出来 fn max(self) -> Option where Self: Sized, Self::Item: Ord, { ... } ... } 它要求Self类型满足Sized约束，同时关联类型Self::Item要满足Ord约束，这是用冒号语法写不出来的。 比较泛型的完整代码: use std::cmp::Ordering; use std::cmp::PartialOrd; fn max(a: T, b: T) -> T where T: PartialOrd, { if a Option { self.value.partial_cmp(&other.value) } } impl PartialEq for T { fn eq(&self, other: &T) -> bool { self.value == other.value } } fn main() { let t1 = T { value: 1 }; let t2 = T { value: 2 }; let m = max(t1, t2); } 注意由于标准库中的PartialOrd继承了PartialEq，因此单独实现PartialOrd 还是会产生编译错误，必须同时实现PartialEq才能编译通过。 关联类型 trait中不仅可以包含方法（包括静态方法）、常量，还可以包含“类型”。 比如迭代器中的Item就是个关联类型, 关联类型也必须指定才能实例化. pub trait Iterator { type Item; ... } 可以看到，我们希望参数是一个泛型迭代器，可以在约束条件中写Iterator。跟普通泛型参数比起来，关联类型参数必须使用名字赋值的方式。 use std::fmt::Debug; use std::iter::Iterator; fn use_iter(mut iter: ITER) where ITER: Iterator, ITEM: Debug, { while let Some(i) = iter.next() { println!(\"{:?}\", i); } } fn main() { let v: Vec = vec![1, 2, 3, 4, 5]; use_iter(v.iter()); } 也可以将ITEM ITER简化为一个, 因为满足ITER: Iterator, 就可以继续声明其关联类型需要满足的约束ITER::Item: Debug use std::fmt::Debug; use std::iter::Iterator; fn use_iter(mut iter: ITER) where ITER: Iterator, ITER::Item: Debug, { while let Some(i) = iter.next() { println!(\"{:?}\", i); } } fn main() { let v: Vec = vec![1, 2, 3, 4, 5]; use_iter(v.iter()); } trait for trait 下面的例子中, 为一个泛型T, 实现了ToString trait pub trait ToString { fn to_string(&self) -> String; } impl ToString for T { #[inline] fn to_string(&self) -> String { use core::fmt::Write; let mut buf = String::new(); let _ = buf.write_fmt(format_args!(\"{}\", self)); buf.shrink_to_fit(); buf } } 凡是实现了这个trait的类型，都可以调用to_string来得到一个String 类型的结果。同时，标准库中还存在一个std::fmt::Display trait， 也可以做到类似的事情。而且Display是可以通过#[derive（Display）]由 编译器自动实现的。所以，我们可以想到，针对所有满足T: Display的类型，我们可以为它们提供一个统一的实现. 泛型的方法 struct Point { x: T, y: T, } //注意下面的第一个是类型声明, 第二个是\"实例化\"这个Point, 虽然是用泛型来\"实例化\" //所以写成这样也可以的: impl Point:: { impl Point { fn x(&self) -> &T { &self.x } } fn main() { let p = Point { x: 1, y: 2 }; println!(\"p.x = {}\", p.x()); } 内存安全 生命周期 fn main() { let v = vec![1, 2, 3, 4, 5]; // --> v 的生命周期开始 { let center = v[2]; // --> center 的生命周期开始 println!(\"{}\", center); } // 生命周期标记 引用往往导致极其复杂的资源管理问题，首先认识一下垂悬引用： { let r; { let x = 5; r = &x; } println!(\"r: {}\", r); } 这段代码是不会通过 Rust 编译器的，原因是 r 所引用的值已经在使用之前被释放。上图中的绿色范围 'a 表示 r 的生命周期，蓝色范围 'b 表示 x 的生命周期。很显然，'b 比 'a 小得多，引用必须在值的生命周期以内才有效 下面的例子中 longer函数不能编译通过 fn longer(s1: &str, s2: &str) -> &str { if s2.len() > s1.len() { s2 } else { s1 } } 原因是返回值引用可能会返回过期的引用, 一般情况下, 编译器可以推导出返回值的生命周期标记, 比如函数只有唯一入参的时候; 但这里编译器不知道到底返回的引用是s1还是s2 fn main() { let r; { let s1 = \"rust\"; let s2 = \"ecmascript\"; r = longer(s1, s2); } println!(\"{} is longer\", r); } 把longer函数改成带生命周期声明的方式, 就可以成功运行了: fn longer(s1: &'a str, s2: &'a str) -> &'a str { if s2.len() > s1.len() { s2 } else { s1 } } fn main() { let r; { let s1 = \"rust\"; let s2 = \"ecmascript\"; r = longer(s1, s2); } println!(\"{} is longer\", r); //println!(\"s2: {}\", s2); } //ecmascript is longer 可以看到, r实际引用的s2, s2的内容在出了scope后还能被r引用. 但直接打印s2是不行的, 会报错误:cannot find value s2 in this scope 生命周期注释用单引号开头，跟着一个小写字母单词： &i32 // 常规引用 &'a i32 // 含有生命周期注释的引用 &'a mut i32 // 可变型含有生命周期注释的引用 'static // 特殊的生命周期标记, 表示静态, 好像是全局的意思 下面的写法是一样的: fn test(arg: &'a T) -> &'a i32 fn test(arg: &'a T) -> &'b i32 where 'a:'b //'a:'b表示'a比'b“活”得长 类型的生命周期标记 如果自定义类型中有成员包含生命周期参数，那么这个自定义类型 也必须有生命周期参数: struct Test { member: &'a str } 在使用impl的时候，也需要先声明再使用: impl Test { fn test(&self, s: &'a str) { } } 如果在泛型约束中有where T: 'a之类的条件，其意思是，类型T的所有生命周期参数必须大于等于'a。要特别说明的是，若是有where T: 'static的约束，意思则是，类型T里面不包含任何指向短生命周期的借用指针，意思是要么完全不包含任何借用，要么可以有指向'static的借用指针。 省略生命周期标记 fn get_str(s: &String) -> &str { s.as_ref() } 等同于 fn get_str(s: &'a String) -> &'a str { s.as_ref() } 所有权 下面的代码编译不过: fn main() { let s = String::from(\"hello\"); let s1 = s; println!(\"{}\", s); } 出现错误的原因是let s1 = s;导致了所有权转移, 转移后s就不能再访问了. 每个值只有一个所有者。变量s的生命周期从声明开始，到move给s1就结束了。变量s1的生命周期则是从它声明开始， 到函数结束。而字符串本身，由String::from函数创建出来，到函数结束的时候就会销毁。中间所有权的转换，并不会将这个字符串本身重新销毁再创建。在任意时刻，这个字符串只有一个所有者，要么是s， 要么是s1。 一个变量可以把它拥有的值转移给另外一个变量，称为“所有权转移”。赋值语句、函数调用、函数返回等，都有可能导致所有权转移。 Rust中的变量绑定操作，默认是move语义，执行了新的变量绑定后，原来的变量就不能再被使用！一定要记住！ 就是说rust里面的赋值语句实际上是移动语义.但是有例外: 比如下面的代码就可以编译通过: fn main() { let v1 : isize = 0; let v2 = v1; println!(\"{}\", v1); } 因为在Rust中有一部分“特殊照顾”的类型，其变量绑定操作是copy语义。实现了copy trait的类型就会在assign的时候使用copy. 所谓的copy语义，是指在执行变量绑定操作的时候，v2是对v1所属数据的一份复制。v1所管理的这块内存依然存在，并未失效，而v2是新开辟了一块内存，它的内容是从v1管理的内存中复制而来的。和手动调用clone方法效果一样，let v2=v1；等效于let v2=v1.clone() Rust中，在普通变量绑定、函数传参、模式匹配等场景下，凡是实 现了std::marker::Copy trait的类型，都会执行copy语义。基本类型，比如数字、字符、bool等，都实现了Copy trait，因此具备copy语 义。 对于自定义类型，默认是没有实现Copy trait的，但是我们可以手动添上。 要实现这个copy trait: #[derive(Copy, Clone)] struct Foo { data : i32 } fn main() { let v1 = Foo { data : 0 }; let v2 = v1; println!(\"{:?}\", v1.data); } Rust中的copy语义就是浅复制 所有权规则 Rust中的每一个值都有一个对应的变量作为它的所有者 。 在同一时间内，值有且仅有一个所有者。 Rc允许多个所有权的owner 当所有者离开自己的作用域时，它持有的值就会被释放掉。 数据赋值和拷贝 Rust永远不会自动地创 建数据的深度拷贝。因此在Rust中，任何自动的赋值操作都可以被视为高效的。 let s1 = String::from(\"hello\"); //s1在堆里 let s2 = s1; // s2是s1的浅拷贝, 同时s1失效 println!(\"{}, world!\", s1); //这里会报错, 因为所有权已经move了 //后面s2拥有对应的数据, s2退出作用域的时候, drop函数被自动调用以释放空间 但下面的代码也是对的, 因为i32有Copy trait let x = 5; let y = x; println!(\"x = {}, y = {}\", x, y); 有copy trait的类型有: 所有的整数类型，诸如u32 仅拥有两种值（true和false）的布尔类型：bool 字符类型：char 所有的浮点类型，诸如f64 如果元组包含的所有字段的类型都是Copy的，那么这个元组也 是Copy的。例如，(i32, i32)是Copy的，但(i32, String)则不是 所有权与函数 将值传递给函数在语义上类似于对变量进行赋值。将变量传递给 函数将会触发移动或复制，就像是赋值语句一样。 fn main() { let s = String::from(\"hello\"); // 变量s进入作用域 take_ownership(s); // s的值被移动进了函数 // 所以它从这里开始不再有效 let x = 5; // 变量x进入作用域 make_copy(x); // 变量x同样被传递进了函数 //但由于i32是copy的, 所以我们依然可以在这之后使用x } // x首先离开作用域, 随后是s. 但由于s的值已经发生了移动, 所以没什么特别的事情发生. fn take_ownership(some_string: String) { // some_string进入作用域 println!(\"{}\", some_string); } // some_string在这里离开作用域, 它的drop函数被自动调用, 其所占的内存也随之被释放了 fn make_copy(some_integer: i32) { // some_integer进入作用域 println!(\"{}\", some_integer); } // some_integer在这里离开了作用域, 没有什么特别的事情发生 返回值与作用域 函数在返回值的过程中也会发生所有权的转移。 fn main() { let s1 = give_ownership(); // 返回值移动至s1中 let s2 = String::from(\"hello\"); // s2进入作用域 let s3 = take_and_giveback(s2); // s2被移动进函数, 而这个函数的返回值又被移动到了s3上 } // s3在这里离开作用域并被销毁. 由于s2已经移动了, 所以它不会在离开作用域的时候发生任何事情. s1最后离开作用域并被销毁. // give_ownership 会将它的返回值移动至调用它的函数内 fn give_ownership() -> String { let some_string = String::from(\"hello\"); // some_string进入作用域 some_string // some_string做为返回值移动到调用函数 } // take_and_giveback 将取得一个String的所有权并将它做为结果返回 fn take_and_giveback(a_string: String) -> String { // a_string进入作用域 a_string // a_string做为返回值移动至调用函数 } println不会发生所有权转移(move) 因为println是宏, 比如下面的代码: fn main() { let x = 5; println!(\"{}\", x); } 用这个命令rustc -Z unstable-options --pretty expanded得到下面宏展开的代码: #![feature(prelude_import)] #[prelude_import] use std::prelude::v1::*; #[macro_use] extern crate std; fn main() { let x = 5; { ::std::io::_print(::core::fmt::Arguments::new_v1( &[\"\", \"\\n\"], &match (&x,) { (arg0,) => [::core::fmt::ArgumentV1::new( arg0, ::core::fmt::Display::fmt, )], }, )); }; } 化简后是借用传参的. use std::{fmt, io}; fn main() { let x = 5; io::_print(fmt::Arguments::new_v1( &[\"\", \"\\n\"], &[fmt::ArgumentV1::new(&x, fmt::Display::fmt)], // ^^ )); } Rc允许多个所有权拥有者 To enable multiple ownership, Rust has a type called Rc, which is an abbreviation for reference counting. The Rc type keeps track of the number of references to a value to determine whether or not the value is still in use. enum List { Cons(i32, Box), Nil, } use crate::List::{Cons, Nil}; fn main() { let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil))))); println!(\"count after creating a = {}\", Rc::strong_count(&a)); let b = Cons(3, Rc::clone(&a)); println!(\"count after creating b = {}\", Rc::strong_count(&a)); { let c = Cons(4, Rc::clone(&a)); println!(\"count after creating c = {}\", Rc::strong_count(&a)); } println!(\"count after c goes out of scope = {}\", Rc::strong_count(&a)); } //结果 $ cargo run Compiling cons-list v0.1.0 (file:///projects/cons-list) Finished dev [unoptimized + debuginfo] target(s) in 0.45s Running `target/debug/cons-list` count after creating a = 1 count after creating b = 2 count after creating c = 3 count after c goes out of scope = 2 注: 用Rc::clone(&a)和a.clone()是一样的. 前者更隐含了是浅拷贝的意思, 开销非常小. 引用和借用 引用方式传递也叫借用, 所有权不转移. 在任何一段给定的时间里，你要么只能拥有一个可变引用，要么只能拥有任意数量的不可变引用。 引用总是有效的。 fn dangle() -> &String{ // dangle会返回一个指向String的引用 let s = String::from(\"hello\"); // s被绑定到新的String上 &s // 我们将指向s的引用返回给调用者 } // 变量s在这里离开作用域并随之被销毁, 它指向的内存自然也不再有效. // 危险! copy和clone copy是std::marker::Copy 带marker的都是特殊的trait, 编译器会有特殊处理一旦一个类型实现了Copy trait，那么它在变量绑定、函数参数传递、函数返回值传递等场景下，都是copy语义，而不再是默认的move语义。并不是所有的类型都可以实现Copy trait。Rust规定，对于自定义类型，只有所有成员都实现了Copy trait，这个类型才有资格实现Copy trait。常见的数字类型、bool类型、共享借用指针&，都是具有Copy属性的类型。而Box、Vec、可写借用指针&mut等类型都是不具备Copy属性的类型。 对于数组类型，如果它内部的元素类型是Copy，那么这个数组也是 Copy类型。 对于元组tuple类型，如果它的每一个元素都是Copy类型，那么这个 tuple也是Copy类型。 clone是std::clone::Clone pub trait Clone: Sized { fn clone(&self) -> Self; fn clone_from(&mut self, source: &Self) { *self = source.clone() } } 即使实现了clone trait的对象, 在赋值的时候也是move语义. 举例: // A unit struct without resources #[derive(Debug, Clone, Copy)] struct Unit; // A tuple struct with resources that implements the `Clone` trait #[derive(Clone, Debug)] struct Pair(Box, Box); fn main() { // Instantiate `Unit` let unit = Unit; // Copy `Unit`, there are no resources to move let copied_unit = unit; // Both `Unit`s can be used independently println!(\"original: {:?}\", unit); println!(\"copy: {:?}\", copied_unit); // Instantiate `Pair` let pair = Pair(Box::new(1), Box::new(2)); println!(\"original: {:?}\", pair); // Move `pair` into `moved_pair`, moves resources let moved_pair = pair; println!(\"moved: {:?}\", moved_pair); // Error! `pair` has lost its resources //println!(\"original: {:?}\", pair); // TODO ^ Try uncommenting this line // Clone `moved_pair` into `cloned_pair` (resources are included) let cloned_pair = moved_pair.clone(); // Drop the original pair using std::mem::drop drop(moved_pair); // Error! `moved_pair` has been dropped //println!(\"copy: {:?}\", moved_pair); // TODO ^ Try uncommenting this line // The result from .clone() can still be used! println!(\"clone: {:?}\", cloned_pair); } 总结: Copy内部没有方法，Clone内部有两个方法。 Copy trait是给编译器用的，告诉编译器这个类型默认采用copy语 义，而不是move语义。 Clone trait是给程序员用的，赋值操作还是move语义. 我们必须手动调用 clone方法，它才能发挥作用。 Copy trait不是想实现就能实现的，它对类型是有要求的，有些类型不可能impl Copy。而Clone trait则没有什么前提条件，任何类型都可以实现（unsized类型除外，因为无法使用unsized类型作为返回值） Copy trait规定了这个类型在执行变量绑定、函数参数传递、函数返回等场景下的操作方式。即这个类型在这种场景下，必然执行的 是“简单内存复制”操作，这是由编译器保证的，程序员无法控制。 Clone trait里面的clone方法究竟会执行什么操作，则是取决于程序员自己写的逻辑。一般情况下，clone方法应该执行一个“深复制”操作，但这不是强制性的，如果你愿意，在里面启动一个人工智能程序都是有可能的。 析构函数 rust没有构造函数, 但允许析构函数: 用户可以自己写满足std::ops::Drop的trait trait Drop { fn drop(&mut self); } 这个Drop会在变量声明周期结束的时候被调用. 自己调用Drop是非法的, 但可以间接调用标准库的drop函数: use std::mem::drop; fn main() { let mut v = vec![1, 2, 3]; // v的生命周期结束 v.push(4); // 错误的调用 } 其实, 标准库的drop就是入参是值传递的空函数: #[inline] pub fn drop(_x: T) { } 这里入参是值传递非常重要, 将对象的所有权移入函数中，什么都不用做，编译器就会自动释放掉这个对象了。 因为这个drop函数的关键在于使用move语义把参数传进来，使得变量的所有权从调用方移动到drop函数体内，参数类型一定要是T，而不是&T或者其他引用类型。函数体本身其实根本不重要，重要的是把变量的所有权move进入这个函数体中，函数调用结束的时候该变量的生命周期结束，变量的析构函数会自动调用，管理的内存空间也会自然释放。这个过程完全符合前面讲的生命周期、move语义，无须编译器做特殊处理。事实上，我们完全可以自己写一个类似的函数来实现同样的效果，只要保证参数传递是move语义即可。 因此, 有copy()语义的变量, 对其drop()是没有作用的, 因为这些变量是复制不是move. mut和&mut mut可以出现在绑定(=)的左右两侧 fn main() { let mut var = 0_i32; { let p1 = &mut var; // p1 指针本身不能被重新绑定,我们可以通过p1改变变量var的值 *p1 = 1; } { let temp = 2_i32; let mut p2 = &var; // 我们不能通过p2改变变量var的值,但p2指针本身指向的位置可以被改变 p2 = &temp; } { let mut temp = 3_i32; let mut p3 = &mut var; // 我们既可以通过p3改变变量var的值,而且p3指针本身指向的位置也可以改变 *p3 = 3; p3 = &mut temp; } } 借用指针 借用指针不能比它指向的变量存在的时间更长 &mut型借用只能指向本身具有mut修饰的变量，对于只读变量，不可以有&mut型借用 &mut型借用指针存在的时候，被借用的变量本身会处于“冻结”状态 如果只有&型借用指针，那么能同时存在多个；如果存在&mut型借用指针，那么只能存在一个；如果同时有其他的&或者&mut型借用指针存在，那么会出现编译错误 // 这里的参数采用的“引用传递”,意味着实参本身并未丢失对内存的管理权 fn borrow_semantics(v: &Vec) { // 打印参数占用空间的大小,在64位系统上,结果为8,表明该指针与普通裸指针的内部表示方法相同 println!(\"size of param: {}\", std::mem::size_of::>()); for item in v { print!(\"{} \", item); } println!(\"\"); } // 这里的参数采用的“值传递”,而Vec没有实现Copy trait,意味着它将执行move语义 fn move_semantics(v: Vec) { // 打印参数占用空间的大小,结果为24,表明实参中栈上分配的内存空间复制到了函数的形参中 println!(\"size of param: {}\", std::mem::size_of::>()); for item in v { print!(\"{} \", item); } println!(\"\"); } fn main() { let array = vec![1, 2, 3]; // 需要注意的是,如果使用引用传递,不仅在函数声明的地方需要使用&标记 // 函数调用的地方同样需要使用&标记,否则会出现语法错误 // 这样设计主要是为了显眼,不用去阅读该函数的签名就知道这个函数调用的时候发生了什么 // 而小数点方式的成员函数调用,对于self参数,会“自动转换”,不必显式借用,这里有个区别 borrow_semantics(&array); // 在使用引用传递给上面的函数后,array本身依然有效,我们还能在下面的函数中使用 move_semantics(array); // 在使用move语义传递后,array在这个函数调用后,它的生命周期已经完结 } 任何借用指针的存在，都会导致原来的变量被“冻结”（Frozen）: fn main() { let mut x = 1_i32; let p = &mut x; x = 2; // 因为p的存在，此时对x的改变被认为是非法的 println!(\"value of pointed : {}\", p); } 为什么rust是内存安全的? 比如c语言在迭代vector的时候, 改变vector自身, 编译的时候没问题, 但运行时会崩溃; 而rust在编译时就会报错, 因为rust的原则是: 共享不可变，可变不共享 首先我们介绍一下这两个概念Alias和Mutation。 Alias的意思是“别名”。如果一个变量可以通过多种Path来访问，那它们就可以互相看作alias。Alias意味着“共享”，我们可以通过多个入口访问同一块内存。 Mutation的意思是“改变”。如果我们通过某个变量修改了一块内存，就是发生了mutation。Mutation意味着拥有“修改”权限，我们可以写入数据。 Rust保证内存安全的一个重要原则就是，如果能保证alias和 mutation不同时出现，那么代码就一定是安全的. 为什么在Rust中永远不会出现迭代器失效这样的错误？因为通 过“mutation+alias”规则，就可以完全杜绝这样的现象，这个规则是Rust 内存安全的根，是解决内存安全问题的灵魂。 Rust防范“内存不安全”代码的原则极其清晰明了。如果你对同一块内存存在多个引用，就不要试图对这块内存做修改；如果你需要对一块内存做修改，就不要同时保留多个引用。只要保证了这个原则，我们就可以保证内存安全。它在实践中发挥了强大的作用，可以帮助我们尽早发现问题。这个原则是Rust的立身之本、生命之基、活力之源。 注: std::cell::Cell可以\"突破\"这个“唯一修改权”的原则. 但实际上, Cell被小心的设计成一个包裹, 支持多个共享引用, 可以内部可变. 解引用 和c一样, 用*解引用 自定义解引用 实现std::ops::Deref或者std::ops::DerefMut这两个trait就能自定义解引用 pub trait Deref { type Target: ?Sized; fn deref(&self) -> &Self::Target; } pub trait DerefMut: Deref { fn deref_mut(&mut self) -> &mut Self::Target; } *expr的类型是Target，而 deref()方法返回的类型却是&Target 常见指针类型 Box是“指针”，指向一个在堆上分配的对象 Vec是“指针”，指向一组同类型的顺序排列的堆上分配的对象，且携带有当前缓存空间总大小和元素个数大小的元数据 String是“指针”，指向的是一个堆上分配的字节数组，其中保存的内容是合法的utf8字符序列。且携带有当前缓存空间总大小和字符串实际长度的元数据 以上几个类型都对所指向的内容拥有所有权，管理着它们所指向的内存空间的分配和释放 Rc和Arc也是某种形式的、携带了额外元数据的“指针”，它们提供的是一种“共享”的所有权，当所有的引用计数指针都销毁之后，它们所指向的内存空间才会被释放 自动解引用 len的函数签名是:fn len(&self) -> usize 按理说只有形式是&str的参数才行, 但下面的代码都正确: fn main() { let s = \"hello\"; println!(\"length: {}\", str::len(&s)); println!(\"length: {}\", str::len(s)); println!(\"length: {}\", s.len()); println!(\"length: {}\", (&s).len()); println!(\"length: {}\", (&&&&&&&&&&&&&s).len()); } 这是因为Rust编译器帮我们做了隐式的deref调用，当它找不到这个成员方法的时候，会自动尝试使用deref方法后再找该方法，一 直循环下去。所以&&&&&&&&&&str会被正确解引用 自动deref的规则是，如果类型T可以解引用为U，即T：Deref， 则&T可以转为&U Rc的自动解引用 Rc是带引用计数的智能指针, 它实现了Deref trait impl Deref for Rc { type Target = T; #[inline(always)] fn deref(&self) -> &T { &self.inner().value } } 它的Target类型是它的泛型参数T。这么设计有什么好处呢？我们 看下面的用法： use std::rc::Rc; fn main() { let s = Rc::new(String::from(\"hello\")); println!(\"{:?}\", s.bytes()); } 我们创建了一个指向String类型的Rc指针，并调用了bytes()方 法。这里是不是有点奇怪？这里的机制是这样的：Rc类型本身并没有bytes()方法，所以编译器会尝试自动deref，试试s.deref().bytes()。 String类型其实也没有bytes()方法，但是String可以继续deref，于是再试试s.deref().deref().bytes()。 这次在str类型中找到了bytes()方法，于是编译通过。 我们实际上通过Rc类型的变量调用了str类型的方法，让这个智能指针透明。这就是自动Deref的意义。 这就是为什么String需要实现Deref trait，是为了让&String类型的变量可以在必要的时候自动转换为&str类型。所以String类型的变量可以直接调用str类型的方法。比如： let s = String::from(\"hello\"); let len = s.bytes(); 虽然s的类型是String，但它在调用bytes()方法的时候，编译器会自动查找并转换为s.deref().bytes()调用。所以String类型的变量就可以直接调用str类型的方法了。 同理：Vec类型也实现了Deref trait，目标类型是[T]，&Vec类型的变量就可以在必要的时候自动转换为&[T]数组切片类型；Rc类型也实现了Deref trait，目标类型是T，Rc类型的变量就可以直接调用T类型的方法。 引用计数 普通变量绑定自身消亡的时候，这块内存就会被释放。引用计数智能指针给我们提供了另外一种选择：一块不可变内存可以有多个所有者，当所有的所有者消亡后，这块内存才会被释放。 std：：rc：：Rc: Rc是普通的引用计数, 只能单线程使用 std：：sync：：Arc: Arc是atomic Rc, 多线程安全 一般Rust不允许多owner, 但Rc可以突破这个限制: use std::rc::Rc; struct SharedValue { value: i32, } fn main() { let shared_value: Rc = Rc::new(SharedValue { value: 42 }); let owner1 = shared_value.clone(); let owner2 = shared_value.clone(); //shared_value.value = 88; 这句编译不过, 因为Rc就被设计成引用计数, 而不是共享变量. 用Cell来解决变量共享问题. println!(\"value : {} {}\", owner1.value, owner2.value); println!(\"address : {:p} {:p}\", &owner1.value, &owner2.value); } 运行结果: $ ./test value : 42 42 address : 0x13958abdf20 0x13958abdf20 这说明，owner1 owner2里面包含的数据不仅值是相同的，而且地址也是相同的。这正是Rc的意义所在。 如果要创建指向同样 内存区域的多个Rc指针，需要显式调用clone函数。请注意，Rc指针是 没有实现Copy trait的。如果使用直接赋值方式，会执行move语义，导致前一个指针失效，后一个指针开始起作用，而且引用计数值不变。如果需要创造新的Rc指针，必须手工调用clone()函数，此时引用计数值才会加1。当某个Rc指针失效，会导致引用计数值减1。当引用计数值减到0的时候，共享内存空间才会被释放。 cow 写时拷贝. 它对指向的数据可能“拥有所有权”，或者 可能“不拥有所有权”。 当它只需要对所指向的数据进行只读访问的时候，它就只是一个借用指针；当它需要写数据功能时，它会先分配内存，执行复制操作，再对自己拥有所有权的内存进行写入操作。 在标准库里: pub enum Cow where B: ToOwned { /// Borrowed data. Borrowed(&'a B), /// Owned data. Owned(::Owned) } 它可以是Borrowed或者Owned两种状态。如果是Borrowed状态，可以通过调用to_mut()函数获取所有权。在这个过程中，它实际上会分配一块新的内存，并将原来Borrowed状态的数据通过调用to_owned()方法 构造出一个新的拥有所有权的对象，然后对这块拥有所有权的内存执行操作。 比如下面的remove_spaces()函数, 如果入参没有空格, 就只返回借用; 如果有空格, 就返回一个新申请的有所有权的对象buf. use std::borrow::Cow; fn remove_spaces(input: &'a str) -> Cow { if input.contains(' ') { let mut buf = String::with_capacity(input.len()); for c in input.chars() { if c != ' ' { buf.push(c); } } return Cow::Owned(buf); } return Cow::Borrowed(input); } fn main() { let s1 = \"no_spaces_in_string\"; let result1 = remove_spaces(s1); let s2 = \"spaces in string\"; let result2 = remove_spaces(s2); println!(\"{}\\n{}\", result1, result2); } 为什么这里要用Cow呢? 因为这个函数的返回值类型用&str类 型和String类型都不大合适。 如果返回类型指定为&str类型，那么需要新分配内存的时候，会出现生命周期编译错误。因为函数内部新分配的字符串的引用不能在函数调用结束后继续存在。 如果返回类型指定为String类型，那么对于那种不需要对输入参数做修改的情况，有一些性能损失。因为输入参数&str类型转为String类 型需要分配新的内存空间并执行复制，性能开销较大。这种时候使用Cow类型就是不二之选。既能满足编译器的生命周期要求，也避免了无谓的数据复制。Cow类型，就是优秀的“零性能损失抽象”的设计范例。 Cow类型还实现了Deref trait，所以当我们需要调用类型T的成员函数的时候，可以直接调用，完全无须考虑后面具体是“借用指针”还是“拥有所有权的指针”。所以我们也可以把它当成是一种“智能指针”。 智能指针 上面提到的Rc和Cow都是智能指针.Rust中允许一部分运算符可以由用户自定义行为，即“操作符重载”。其中“解引用”是一个非常重要的操作符，它允许重载。 而需要提醒大家注意的是，“取引用”操作符，如&、&mut，是不允许重载的。因此，“取引用”和“解引用”并非对称互补关系。*&T的类型 一定是T，而&*T的类型未必就是T。 更重要的是，读者需要理解，在某些情况下，编译器帮我们插入了自动deref的调用，简化代码。 在Deref的基础上，我们可以封装出一种自定义类型，它可以直接调用其内部的其他类型的成员方法，我们可以把这种类型称为智能指针类型 其他 在方法里定义struct是可以的 impl Clone for Box { fn clone(&self) -> Self { let mut new = BoxBuilder { data: RawVec::with_capacity(self.len()), len: 0, }; let mut target = new.data.ptr(); for item in self.iter() { unsafe { ptr::write(target, item.clone()); target = target.offset(1); }; new.len += 1; } return unsafe { new.into_box() }; // Helper type for responding to panics correctly. struct BoxBuilder { data: RawVec, len: usize, } impl BoxBuilder { unsafe fn into_box(self) -> Box { let raw = ptr::read(&self.data); mem::forget(self); raw.into_box() } } impl Drop for BoxBuilder { fn drop(&mut self) { let mut data = self.data.ptr(); let max = unsafe { data.offset(self.len as isize) }; while data != max { unsafe { ptr::read(data); data = data.offset(1); } } } } } } 为什么明明可以直接在一个方法里写完的代码，还要引入一个新的类型呢？原因就在于panic safety问题。注意我们这里调用了T类型的 clone方法。T是一个泛型参数，谁能保证clone方法不会产生panic？没有谁能保证，我们只能尽可能让clone发生panic的时候，RawVec的状态不会乱掉。 所以，标准库的实现利用了RAII机制，即便在clone的时候发生了 panic，这个BoxBuilder类型的局部变量的析构函数依然会正确执行，并在析构函数中做好清理工作。上面这段代码之所以搞这么复杂，就是为了保证在发生panic的时候逻辑依然是正确的。大家可以去翻一下标准库中的代码，有大量类似的模式存在，都是因为需要考虑panic safety问题。Rust的标准库在编写的时候有这样一个目标：即便发生了panic，也不会产生“内存不安全”和“线程不安全”的情况。 文件打开 use std::fs::File; use std::io::Read; fn main() { let f = File::open(\"/target/file/path\"); if f.is_err() { println!(\"file is not exist.\"); return; } let mut f = f.unwrap(); let mut content = String::new(); let result = f.read_to_string(&mut content); if result.is_err() { println!(\"read file error.\"); return; } println!(\"{}\", result.unwrap()); } unsafe unsafe可以修饰fn, 代码块, trait, impl等 unsafe有传递性, 比如使用unsafe的fn的代码块也必须用unsafe来修饰. unsafe可以操作裸指针 fn main() { let x = 1_i32; let mut y: u32 = 1; let raw_mut = &mut y as *mut u32 as *mut i32 as *mut i64; // 这是安全的 unsafe { *raw_mut = -1; // 这是不安全的,必须在 unsafe 块中才能通过编译 } println!(\"{:X} {:X}\", x, y); } 上面的例子中: 首先raw_mut必须经过三个as as as的转换才能从u32的指针转为i64的指针 对raw_mut的直接修改是unsafe的 修改了raw_mut, 也就是y, 但也\"一起\"修改了x, 因为x和y两个变量地址是挨着的. 又比如下面的例子: fn raw_to_ref(p: *const i32) -> &'a i32 { unsafe { &*p } } fn main() { let p: &i32 = raw_to_ref(std::ptr::null::()); println!(\"{}\", p); } 这个例子会运行错误, 因为传入了一个空指针, 而在unsafe里面对空指针解引用出现错误. 不加unsafe是不能对裸指针解引用的 在unsafe里, 用户要自己避免空指针问题, 编译器是不管的 要修复这个错误, 改成这样就好了: fn raw_to_ref(p: *const i32) -> Option { if p.is_null() { None } else { unsafe { Some(&*p) } } } fn main() { let p: Option = raw_to_ref(std::ptr::null::()); println!(\"{:?}\", p); } 不用unsafe的swap例子 下面的例子是我自己写的, 能正常工作, 没用unsafe. fn swap(x: &mut i32, y: &mut i32) { let z = *x; *x = *y; *y = z; } fn main() { let mut a = 5; let mut b = 8; swap(&mut a, &mut b); println!(\"a: {}, b: {}\", a, b) } 但如果用泛型, 就会出现编译错误: fn swap(x: &mut T, y: &mut T) { let z = *x; *x = *y; *y = z; } fn main() { let mut a = 5; let mut b = 8; swap(&mut a, &mut b); println!(\"a: {}, b: {}\", a, b) } 错误是:cannot move out of *x which is behind a mutable reference 编译器还提示move occurs because *x has type T, which does not implement the Copy trait 意思是assign的时候, 默认执行的是move语义, 但这里因为x只是借用, 但没有权限解引用. 但如果实现了Copy trait, 也可以调用Copy 那么添加T的约束为Copy, 也就好了: fn swap(x: &mut T, y: &mut T) { let z = *x; //这里用了copy语义 *x = *y; *y = z; } fn main() { let mut a = 5; let mut b = 8; swap(&mut a, &mut b); println!(\"a: {}, b: {}\", a, b) } 标准库的swap 标准的swap并没有要求copy, 而是直接操作指针 fn swap(x: &mut T, y: &mut T) { unsafe { let mut t: T = mem::uninitialized(); ptr::copy_nonoverlapping(&*x, &mut t, 1); ptr::copy_nonoverlapping(&*y, x, 1); ptr::copy_nonoverlapping(&t, y, 1); mem::forget(t); } } 首先，我们依然需要一个作为中转的局部变量。这个局部变量该怎么初始化呢？其实我们不希望它执行初始化，因为我们只需要这部分内存空间而已，它里面的内容马上就会被覆盖掉，做初始化是浪费性能。况且，我们也不知道用什么通用的办法初始化一个泛型类型，它连Default约束都未必满足。所以我们要用mem::uninitialized函数。接下来，我们可以直接通过内存复制来交换两个变量。因为在Rust中，所有的类型、所有的move操作，都是简单的内存复制，不涉及其他的语义。Rust语言已经假定任何一个类型的实例，随时都可以被move到另外的地方，不会产生任何问题。所以，我们可以直接使用ptr::copy系列函数来完成。再加上在safe代码中，&mut型指针具有排他性， 我们可以确信，x和y一定指向不同的变量。所以可以使用ptr::copy_nonoverlapping函数，比ptr::copy要快一点。 最后，一定要记得，要阻止临时的局部变量t执行析构函数。因为t本身并未被合理地初始化，它内部的值是直接通过内存复制获得的。在复制完成后，它内部的指针（如果有的话）会和y指向的变量是相同的。如果我们不阻止它，那么在函数结束的时候它的析构函数就会被自动调用，这样y指向的变量就变成非法的了。 这样我们才能正确地完成这个功能。虽然源代码看起来比较长，但是实际生成的代码并不多，就是3次内存块的复制。 Vec代码 下面的代码中, 用Vec的new创建出来的变量v1, 开始的capacity是0, length也是0, 增长的倍数也是2倍速. fn main() { let mut v1 = Vec::::new(); println!(\"Start: length {} capacity {}\", v1.len(), v1.capacity()); for i in 1..10 { v1.push(i); println!( \"[Pushed {}] length {} capacity {}\", i, v1.len(), v1.capacity() ); } let mut v2 = Vec::::with_capacity(1); println!(\"Start: length {} capacity {}\", v2.len(), v2.capacity()); v2.reserve(10); for i in 1..10 { v2.push(i); println!( \"[Pushed {}] length {} capacity {}\", i, v2.len(), v2.capacity() ); } } //结果 Start: length 0 capacity 0 [Pushed 1] length 1 capacity 4 [Pushed 2] length 2 capacity 4 [Pushed 3] length 3 capacity 4 [Pushed 4] length 4 capacity 4 [Pushed 5] length 5 capacity 8 [Pushed 6] length 6 capacity 8 [Pushed 7] length 7 capacity 8 [Pushed 8] length 8 capacity 8 [Pushed 9] length 9 capacity 16 Start: length 0 capacity 1 [Pushed 1] length 1 capacity 10 [Pushed 2] length 2 capacity 10 [Pushed 3] length 3 capacity 10 [Pushed 4] length 4 capacity 10 [Pushed 5] length 5 capacity 10 [Pushed 6] length 6 capacity 10 [Pushed 7] length 7 capacity 10 [Pushed 8] length 8 capacity 10 [Pushed 9] length 9 capacity 10 用with_capacity()创建的容量一开始就分配了. Vec的特点是空间会自动扩展, 并且当变量生命周 期结束的时候，它会自动释放它管理的内存空间 为什么Vec能够自动回收空间呢? 因为Vec实现了Drop: unsafe impl Drop for Vec { fn drop(&mut self) { unsafe { // use drop for [T] ptr::drop_in_place(&mut self[..]); } // RawVec handles deallocation } } "},"notes/rust_入门3.html":{"url":"notes/rust_入门3.html","title":"Rust 闭包 容器 迭代器 生成器 线程","keywords":"","body":" 静态分派和动态分派 trait不能用作入参或者返回值 impl Trait做为入参 impl trait只是语法糖 trait object impl trait 闭包 普通的函数不能捕获局部变量 闭包和函数 闭包如何捕获变量? move关键字 闭包和泛型 可以用Box封装闭包 容器 Vec VecDeque HashMap BTreeMap 迭代器 从容器创造迭代器 迭代器组合 for in 生成器 协程 标准库 类型转换 AsRef/AsMut borrow From/Into ToOwned ToString/FromStr 运算符重载 complex加法重载 IO OsString和OsStr 文件和路径 标准输入输出 进程启动参数 Any和反射 线程安全 创建线程 更多线程参数 rust怎么保证线程安全 Send & Sync 什么是Send类型 什么是Sync类型 保证线程安全的类型 Arc Mutex RwLock Atomic Barrier Condvar 全局变量 线程局部存储 异步管道 同步管道 相当于go channel 第三方线程库 静态分派和动态分派 trait不能用作入参或者返回值 比如下面的Bird这个trait, 有两个实现, Duck和Swan trait Bird { fn fly(&self); } struct Duck; struct Swan; impl Bird for Duck { fn fly(&self) { println!(\"duck duck\"); } } impl Bird for Swan { fn fly(&self) { println!(\"swan swan\"); } } 但Bird不能直接用作入参和出参, 因为trait是一种DST类型，它的大小在编译阶段是不固定的. 这点和go的interface是不同的. // 以下代码不能编译 fn test(arg: Bird) {} fn test() -> Bird {} 有两个办法: 用泛型传参, 即静态分派fn test(arg: T) { arg.fly(); } 用trait object, 即自己给trait穿个Box的马甲, 做到动态分派 // 根据不同需求,可以用不同的指针类型,如 Box/&/&mut 等 fn test(arg: Box) { arg.fly(); } 似乎还有个办法是加dyn关键词, dyn是比较新的关键词 trait Bird { fn fly(&self); } struct Duck; struct Swan; impl Bird for Duck { fn fly(&self) { println!(\"duck duck\"); } } impl Bird for Swan { fn fly(&self) { println!(\"swan swan\"); } } fn call_fly(f: &dyn Bird) { f.fly() } fn main() { let duck = Duck; call_fly(&duck); call_fly(&Swan{}); } //输出 duck duck swan swan impl Trait做为入参 比如下面的函数 fn parse_csv_document(src: R) -> std::io::Result>> { src.lines() .map(|line| { // For each line in the source line.map(|line| { // If the line was read successfully, process it, if not, return the error line.split(',') // Split the line separated by commas .map(|entry| String::from(entry.trim())) // Remove leading and trailing whitespace .collect() // Collect all strings in a row into a Vec }) }) .collect() // Collect all lines into a Vec> } 可以由泛型约束改成impl trait, 即声明src必须实现std::io::BufRead这个trait. 这点和golang传入interface有点像. fn parse_csv_document(src: impl std::io::BufRead) -> std::io::Result>> { src.lines() .map(|line| { // For each line in the source line.map(|line| { // If the line was read successfully, process it, if not, return the error line.split(',') // Split the line separated by commas .map(|entry| String::from(entry.trim())) // Remove leading and trailing whitespace .collect() // Collect all strings in a row into a Vec }) }) .collect() // Collect all lines into a Vec> } impl trait只是语法糖 比如下面的代码用了impl trait形式的入参: pub fn notify(item: &impl Summary) { println!(\"Breaking news! {}\", item.summarize()); } 它实际上是下面显式声明泛型约束的方法一样: pub fn notify(item: &T) { println!(\"Breaking news! {}\", item.summarize()); } trait object 指向trait的指针就是trait object。假如Bird是一个trait的名称，那么dyn Bird就是一个DST动态大小类型。&dyn Bird、&mut dyn Bird、Box、*const dyn Bird、*mut dyn Bird以及Rc等等都是Trait Object。 A trait object points to both an instance of a type implementing our specified trait as well as a table used to look up trait methods on that type at runtime. We create a trait object by specifying some sort of pointer, such as a & reference or a Box smart pointer, then the dyn keyword, and then specifying the relevant trait. impl trait 还有个impl trait语法, 比如: fn foo(n: u32) -> impl Iterator { (0..n).map(|x| x * 100) } 返回一个函数也可以用类似的语法: fn multiply(m: i32) -> impl Fn(i32) -> i32 { move |x| x * m } fn main() { let f = multiply(5); println!(\"{}\", f(2)); } //结果 10 闭包 闭包(closure)是一种匿名函数，具有“捕获”外部变量的能力。闭包有时候也被称作lambda表达式。它有两个特点: 可以像函数一 样被调用； 可以捕获当前环境中的变量。 语法如下: fn main() { let add = |a: i32, b: i32| -> i32 { return a + b; }; let x = add(1, 2); println!(\"result is {}\", x); } 以上闭包有两个参数，以两个|包围。执行语句包含在{}中。闭包的参数和返回值类型的指定与普通函数的语法相同。闭包的参 数和返回值类型都是可以省略的，因此以上闭包可省略为: let add = |a, b| a + b; //省略了类型, 括号, 和return 普通的函数不能捕获局部变量 rust支持函数中定义函数, 但不支持内部函数引用外部函数的局部变量. 比如下面的代码编译不过 fn main() { let x = 1_i32; fn inner_add() -> i32 { x + 1 } let x2 = inner_add(); println!(\"result is {}\", x2); } 要改为闭包: fn main() { let x = 1_i32; let inner_add = || x + 1; let x2 = inner_add(); println!(\"result is {}\", x2); } 闭包和函数 闭包的实现: fn main() { let x = 1_i32; let add_x = | a | x + a; let result = add_x( 5 ); println!(\"result is {}\", result); } 如果改为非闭包的普通实现: struct Closure { inner1: i32, } impl Closure { fn call(&self, a: i32) -> i32 { self.inner1 + a } } fn main() { let x = 1_i32; let add_x = Closure { inner1: x }; let result = add_x.call(5); println!(\"result is {}\", result); } 可以看到闭包的方式更简洁. 闭包如何捕获变量? Rust主要是通过分析外部变量在闭包中的使用方式，通过一系列的规则自动推导出来的。主要规则如下: 如果一个外部变量在闭包中，只通过借用指针&使用，那么这个变量就可通过引用&的方式捕获； 如果一个外部变量在闭包中，通过&mut指针使用过，那么这个变量就需要使用&mut的方式捕获； 如果一个外部变量在闭包中，通过所有权转移的方式使用过，那么这个变量就需要使用“by value”的方式捕获。 简单点总结规则是，在保证能编译通过的情况下，编译器会自动选择一种对外部影响最小的类型存储。对于被捕获的类型为T的外部变量，在匿名结构体中的存储方式选择为: 尽可能先选择&T类型，其次选择&mut T类型，最后选择T类型。 move关键字 闭包的捕获默认是引用捕获, 即捕获&T. 下面的例子编译不通过 fn make_adder(x: i32) -> Box i32> { Box::new(|y| x + y) } fn main() { let f = make_adder(3); println!(\"{}\", f(1)); // 4 println!(\"{}\", f(10)); // 13 } 函数make_adder中有一个局部变量x，按照前面所述的规则，它被闭包所捕获，而且可以使用引用&的方式完成闭包内部的逻辑，因此它是被引用捕获的。而闭包则作为函数返回值被传递出去了。于是，闭包被调用的时候，它内部的引用所指向的内容已经被释放了。 这个时候就要用move关键字了, 表示后面语句块的所有变量都强制使用\"值传递\": fn make_adder(x: i32) -> Box i32> { Box::new(move |y| x + y) // 使用move来强制使用值传递 } 闭包和泛型 下面的例子是传入一个闭包函数到泛型函数: fn call_with_closure(some_closure: F) -> i32 where F: Fn(i32) -> i32, { some_closure(1) } fn main() { let answer = call_with_closure(|x| x + 2); println!(\"{}\", answer); } 每个闭包，编译器都会为它生成一个匿名结构体类型；即使两个闭包的参数和返回值一致，它们也是完全不同的两个类 型，只是都实现了同一个trait而已。 可以用Box封装闭包 fn test() -> Box i32> { let c = |i: i32| i * 2; Box::new(c) } fn main() { let closure = test(); let r = closure(2); println!(\"{}\", r); } 容器 容器 描述 Vec 可变长数组, 连续存储 VecDeque 双向队列, 适用于从头部和尾部插入删除数据 LinkedList 双向链表, 非连续存储 HashMap 基于Hash算法存储key value HashSet 只有key没有value的HashMap BTreeMap 基于B树存储key value BTreeSet 只有key没有value的BtreeMap BinaryHeap 基于二叉堆的优先级队列 Vec 它就是一个可以自动扩展容量的动态数组。它重载了Index运算符，可以通过中括号取下标的形式访问内部成员。它还重载了Deref/DerefMut运算符，因此可 以自动被解引用为数组切片。 用法示例: fn main() { // 常见的几种构造Vec的方式 // 1. new() 方法与 default() 方法一样,构造一个空的Vec let v1 = Vec::::new(); // 2. with_capacity() 方法可以预先分配一个较大空间,避免插入数据的时候动态扩容 let v2: Vec = Vec::with_capacity(1000); // 3. 利用宏来初始化,语法跟数组初始化类似 let v3 = vec![1, 2, 3]; // 插入数据 let mut v4 = Vec::new(); // 多种插入数据的方式 v4.push(1); v4.extend_from_slice(&[10, 20, 30, 40, 50]); v4.insert(2, 100); println!(\"capacity: {} length: {}\", v4.capacity(), v4.len()); // 访问数据 // 调用 IndexMut 运算符,可以写入数据 v4[5] = 5; let i = v4[5]; println!(\"{}\", i); // Index 运算符直接访问,如果越界则会造成 panic,而 get 方法不会,因为它返回一个 Option if let Some(i) = v4.get(6) { println!(\"{}\", i); } // Index 运算符支持使用各种 Range 作为索引 let slice = &v4[4..]; println!(\"{:?}\", slice); } VecDeque A double-ended queue implemented with a growable ring buffer VecDeque是一个双向队列。在它的头部或者尾部执行添加或者删除操作，都是效率很高的。它的用法和Vec非常相似，主要是多了pop_front()和push_front()等方法 use std::collections::VecDeque; fn main() { let mut queue = VecDeque::with_capacity(64); // 向尾部按顺序插入一堆数据 for i in 1..10 { queue.push_back(i); } // 从头部按顺序一个个取出来 while let Some(i) = queue.pop_front() { println!(\"{}\", i); } } HashMap HashMap泛型参数K是键的类型，V是值的类型，S是哈希算法的类型。S这个泛型参数有一个默认值. Hash trait就是这个算法: trait Hash { fn hash(&self, state: &mut H); ... } trait Hasher { fn finish(&self) -> u64; fn write(&mut self, bytes: &[u8]); ... } 如果一个类型，实现了Hash，给定了一种哈希算法Hasher，就能计算出一个u64类型的哈希值, 比如类似下面的: struct Person { first_name: String, last_name: String, } impl Hash for Person { fn hash(&self, state: &mut H) { self.first_name.hash(state); self.last_name.hash(state); } } 但通常写作: #[derive(Hash)] struct Person { first_name: String, last_name: String, } 完整的写法如下: use std::collections::HashMap; #[derive(Hash, Eq, PartialEq, Debug)] struct Person { first_name: String, last_name: String, } impl Person { fn new(first: &str, last: &str) -> Self { Person { first_name: first.to_string(), last_name: last.to_string(), } } } fn main() { let mut book = HashMap::new(); book.insert(Person::new(\"John\", \"Smith\"), \"521-8976\"); book.insert(Person::new(\"Sandra\", \"Dee\"), \"521-9655\"); book.insert(Person::new(\"Ted\", \"Baker\"), \"418-4165\"); let p = Person::new(\"John\", \"Smith\"); // 查找键对应的值 if let Some(phone) = book.get(&p) { println!(\"Phone number found: {}\", phone); } // 删除 book.remove(&p); // 查询是否存在 println!(\"Find key: {}\", book.contains_key(&p)); } HashMap对查询和insert/delete这种组合, 提供了entry API, 可以节省一次hash运算. 比如下面的代码: if map.contains_key(key) { // 执行了一遍hash查找的工作 map.insert(key, value); // 又执行了一遍hash查找的工作 } 使用entry API可以写成: map.entry(key).or_insert(value); BTreeMap 和HashMap用起来类似, 但使用B树来存储key value. 和hash不同, 这个解构是有序的. BTreeMap对key的要求是满足Ord约束，即具备“全序”特征. BTreeMap使用起来和Hashap类似, 但多了一个range功能如下: use std::collections::BTreeMap; fn main() { let mut map = BTreeMap::new(); map.insert(3, \"a\"); map.insert(5, \"b\"); map.insert(8, \"c\"); for (k, v) in map.range(2..6) { println!(\"{} : {}\", k, v); } } 迭代器 迭代器的trait: trait Iterator { type Item; fn next(&mut self) -> Option; } 它最主要的一个方法就是next()，返回一个Option。一般情况返回Some(Item)；如果迭代完成，就返回None。 一个迭代器需要自己记录内部状态, 比如下面Seq结构体里的current: use std::iter::Iterator; struct Seq { current: i32, } impl Seq { fn new() -> Self { Seq { current: 0 } } } impl Iterator for Seq { type Item = i32; //指定关联类型 fn next(&mut self) -> Option { if self.current 从容器创造迭代器 从容器创造迭代器: iter()创造一个Item是&T类型的迭代器； iter_mut()创造一个Item是&mut T类型的迭代器； into_iter()创造一个Item是T类型的迭代器。--这里有问题, 不一定是T 比如Vec等容器创造迭代器: fn main() { let v = vec![1, 2, 3, 4, 5]; let mut iter = v.iter(); while let Some(i) = iter.next() { println!(\"{}\", i); } } 这个和用for in是一样的: fn main() { let v = vec![1, 2, 3, 4, 5]; for i in v { println!(\"{}\", i) } } 迭代器组合 比如下面的例子: fn main() { let v = vec![1, 2, 3, 4, 5, 6, 7, 8, 9]; let mut iter = v .iter() .take(5) .filter(|&x| x % 2 == 0) .map(|&x| x * x) .enumerate(); while let Some((i, v)) = iter.next() { println!(\"{} {}\", i, v); } } 直到执行iter.next()前, 创造iter使用的\"组合\"模式的代价很小, 它只是按照用户定义的组合, 初始化了一个迭代器对象, 并没有真正干活. 比如下面的代码只是构造了一个迭代器示例, 不会打印, 因为没有调用next()方法. let v = vec![1, 2, 3, 4, 5]; v.iter().map(|x| println!(\"{}\", x)); for in for in就是给迭代器设计的语法糖 use std::collections::HashMap; fn main() { let v = vec![1, 2, 3, 4, 5, 6, 7, 8, 9]; for i in v { println!(\"{}\", i); } //从array生成一个hashmap let map: HashMap = [(1, 'a'), (2, 'b'), (3, 'c')].iter().cloned().collect(); //比较新的版本>1.56, 可以用from函数 let map = HashMap::from([(1, 'a'), (2, 'b'), (3, 'c')]); for (k, v) in &map { println!(\"{} : {}\", k, v); } } 因为for调用的是 trait IntoIterator { type Item; type IntoIter: Iterator; //感觉这行是约束的意思 fn into_iter(self) -> Self::IntoIter; } 只要某个类型实现了IntoIterator，那么调用into_iter()方法就可以得到对应的迭代器。这个into_iter()方法的receiver是self，而不是&self，执行的是move语义。这么做，可以同时支持Item类型为T、&T 或者&mut T，用户有选择的权力。 那么如何实现这个trait呢? 需要三个版本一起实现: impl IntoIterator for BTreeMap { type Item = (K, V); type IntoIter = IntoIter; } impl IntoIterator for &'a BTreeMap { type Item = (&'a K, &'a V); type IntoIter = Iter; } impl IntoIterator for &'a mut BTreeMap { type Item = (&'a K, &'a mut V); type IntoIter = IterMut; } 对于一个容器类型，标准库里面对它impl了三次IntoIterator。当Self类型为BTreeMap的时候，Item类型为(K，V)，这意味着，每次next()方法都是把内部的元素move出来了；当Self类型为&BTreeMap的时候，Item类型为(&K，&V)，每次next()方法返回的是借用； 当Self类型为&mut BTreeMap的时候，Item类型为(&K，&mut V)，每次next()方法返回的key是只读的，value是可读写的。 所以，如果有个变量m，其类型为BTreeMap，那么用户可以选择使用m.into_iter()或者(&m).into_iter()或者(&mut m).into_iter()，分别达到不同的目的。 for in循环用了into_iter()方法, 对应上面三种实现, 使用方法如下: // container在循环之后生命周期就结束了,循环过程中的每个item是从container中move出来的 for item in container {} // 迭代器中只包含container的&型引用,循环过程中的每个item都是container中元素的借用 for item in &container {} // 迭代器中包含container的&mut型引用,循环过程中的每个item都是指向container中元素的可变借用 for item in &mut container {} 可以看到, 实现了IntoIterator, 就可以被for in所使用. 生成器 生成器的语法像闭包, 区别是在语句块中有yield关键词, 比如: // 方案一 #![feature(generators, generator_trait)] use std::ops::{Generator, GeneratorState}; fn main() { let mut g = || { let mut curr: u64 = 1; let mut next: u64 = 1; loop { let new_next = curr.checked_add(next); if let Some(new_next) = new_next { curr = next; next = new_next; yield curr; // println!(\"{}\", v), GeneratorState::Complete(_) => return, } } } } 注意以上代码编译不过, 但可以看看其中原理: 生成器最大的特点就是，程序的执行流程可以在生成器和调用者之间来回切换。当我们需要暂时从生成器中返回的时候，就使用yield关键字；当调用者希望再次进入生成器的时候，就调用resume()方法，这时程序执行的流程是从上次yield返回的那个点继续执行。 回想迭代器, next方法就很像resume. 迭代器需要自己维护内部状态, 生成器也是类似的, 感觉生成器能自动维护其内部状态. 协程 用户态调度的协程.Rust的协程设计，核心是async和await两个关键字，以及Future这个 trait: pub trait Future { type Output; fn poll(self: PinMut, cx: &mut Context) -> Poll; ...... } 比如下面的代码: async fn async_fn(x: u8) -> u8 { let msg = await!(read_from_network()); let result = await!(calculate(msg, x)); result } 在这个示例中，假设read_from_network()以及calculate()函数都是异步的。最外层的async_fn()函数当然也是异步的。当代码执行到 await！(read_from_network())里面的时候，发现异步操作还没有完成，它会直接退出当前这个函数，把CPU让给其他任务执行。当这个数据从网络上传输完成了，调度器会再次调用这个函数，它会从上次中断的地方恢复执行。所以用async/await的语法写代码，异步代码的逻辑在源码组织上跟同步代码的逻辑差别并不大。这里面状态保存和恢复这些琐碎的事情，都由编译器帮我们完成了。 async关键字可以修饰函数、闭包以及代码块。对于函数: async fn f1(arg: u8) -> u8 {} 实际上等同于: fn f1(arg: u8) -> impl Future {} rust和go不同的, rust需要显式的手动指定调度点, 比如上面的await宏就埋了调度的代码; 而go的调度代码是\"隐藏\"的, 表面上让人感觉不到发生了用户态调度. 标准库 类型转换 常用的类型转换已经被preclude 包括AsRef、AsMut、Into、From、ToOwned等 AsRef/AsMut AsRef这个trait代表的意思是，这个类型可以通过调用as_ref方法，得到另外一个类型的共享引用: pub trait AsRef { fn as_ref(&self) -> &T; } 这里的?Sized约束指T可以是Sized也可以不是Sized, 如果没有, 则默认T是Sized. 类似的, 还有AsMut有一个as_mut方法，可以得到另外一个类型的可读写 引用: pub trait AsMut { fn as_mut(&mut self) -> &mut T; } 比如标准库的String类型, 就针对好几个类型参数实现了 AsRef trait: impl AsRef for String impl AsRef for String impl AsRef for String impl AsRef for String AsRef这样的trait很适合用在泛型代码中，为一系列类型做统一抽 象。比如，我们可以写一个泛型函数，它接受各种类型，只要可以被转 换为&[u8]即可: fn iter_bytes>(arg: T) { for i in arg.as_ref() { println!(\"{}\", i); } } fn main() { let s: String = String::from(\"this is a string\"); let v: Vec = vec![1, 2, 3]; let c: &str = \"hello\"; // 相当于函数重载。只不过基于泛型实现的重载,一定需要重载的参数类型满足某种共同的约束 iter_bytes(s); iter_bytes(v); iter_bytes(c); } borrow pub trait Borrow { fn borrow(&self) -> &Borrowed; } From/Into AsRef/Borrow做的类型转换都是从一种引用&T到另一种引用&U的转换。而From/Into做的则是从任意类型T到U的类型转换: pub trait From { fn from(T) -> Self; } pub trait Into { fn into(self) -> T; } Into和From是逆操作, 如果存在U: From，则实现T: Into impl Into for T where U: From { fn into(self) -> U { U::from(self) } } 比如标准库的String就实现了From impl From for String 可以使用&str.into()或者String::from(s)调用 fn main() { let s: &'static str = \"hello\"; let str1: String = s.into(); let str2: String = String::from(s); } ToOwned ToOwned trait提供的是一种更“泛化”的Clone的功能。Clone一般是从&T类型变量创造一个新的T类型变量，而ToOwned一般是从一个&T类型变量创造一个新的U类型变量。 impl ToOwned for T where T: Clone, { type Owned = T; fn to_owned(&self) -> T { self.clone() } fn clone_into(&self, target: &mut T) { target.clone_from(self); } } ToString/FromStr ToString trait提供了其他类型转换为String类型的能力 pub trait ToString { fn to_string(&self) -> String; } 一般情况下，我们不需要自己为自定义类型实现ToString trait。因为标准库中已经提供了一个默认实现: impl ToString for T { #[inline] default fn to_string(&self) -> String { use core::fmt::Write; let mut buf = String::new(); buf.write_fmt(format_args!(\"{}\", self)) .expect(\"a Display implementation return an error unexpectedly\"); buf.shrink_to_fit(); buf } } 这意味着，任何一个实现了Display trait的类型，都自动实现了ToString trait。而Display trait是可以自动derive的，我们只需要为类型添加一个attribute即可。 FromStr则提供了从字符串切片&str向其他类型转换的能力: pub trait FromStr { type Err; fn from_str(s: &str) -> Result; } str类型有个方法是parse pub fn parse(&self) -> Result { … } 可以这样用: fn print_type_of(_: &T) { println!(\"{}\", std::any::type_name::()) } fn main() { print_type_of(&\"4\".parse::()); let four1 = \"4\".parse::(); //和结构体一样, 函数的泛型参数也可以用双冒号指定 let four2: Result = \"4\".parse(); println!(\"{:?}\", four1); println!(\"{:?}\", four2); } 运算符重载 Rust允许一部分运算符重载，用户可以让这些运算符支持自定义类型。运算符重载的方式是: 针对自定义类型，impl一些在标准库中预定 义好的trait，这些trait都存在于std::ops模块中。比如前面已经讲过了的Deref trait就属于运算符重载 比如加法运算符重载需要满足下面的trait: trait Add { type Output; fn add(self, rhs: RHS) -> Self::Output; } 基本库为i32实现的加法trait: impl Add for i32 type Output = i32; impl Add for &'a i32 type Output = >::Output; impl Add for i32 type Output = >::Output; impl Add for &'b i32 type Output = >::Output; 这意味着，不仅i32+i32是允许的，而且i32+&i32、&i32+i32、&i32+&i32这几种形式也都是允许的。它们的返回类型都是i32 complex加法重载 use std::ops::Add; #[derive(Copy, Clone, Debug, PartialEq)] struct Complex { real: i32, imaginary: i32, } impl Add for Complex { type Output = Complex; fn add(self, other: Complex) -> Complex { Complex { real: self.real + other.real, imaginary: self.imaginary + other.imaginary, } } } fn main() { let c1 = Complex { real: 1, imaginary: 2, }; let c2 = Complex { real: 2, imaginary: 4, }; println!(\"{:?}\", c1 + c2); } 可以实现多个类型的add: use std::ops::Add; #[derive(Copy, Clone, Debug, PartialEq)] struct Complex { real: i32, imaginary: i32, } impl Add for Complex { type Output = Complex; fn add(self, other: &'a Complex) -> Complex { Complex { real: self.real + other.real, imaginary: self.imaginary + other.imaginary, } } } impl Add for Complex { type Output = Complex; fn add(self, other: i32) -> Complex { Complex { real: self.real + other, imaginary: self.imaginary, } } } IO OsString和OsStr Rust的String和Str是utf-8编码的, 但操作系统的字符串不一定用什么格式. 所以rust设计了OsString和OsStr来屏蔽差异. 使用场景如下: use std::path::PathBuf; fn main() { let mut buf = PathBuf::from(\"/\"); buf.set_file_name(\"bar\"); if let Some(s) = buf.to_str() { println!(\"{}\", s); } else { println!(\"invalid path\"); } } 这里的set_file_name方法声明如下: fn set_file_name>(&mut self, file_name: S) 因为&str满足这个约束: impl AsRef for str 文件和路径 rust对文件的实现在std::fs::File. 对文件的读写，则需要用到std::io模块了. 这个模块内部定义了几个重要的trait，比如Read/Write。File类型也实现了Read和Write两个 trait，因此它拥有一系列方便读写文件的方法. use std::fs::File; use std::io::prelude::*; use std::io::BufReader; fn test_read_file() -> Result { let mut path = std::env::home_dir().unwrap(); path.push(\".rustup\"); path.push(\"settings\"); path.set_extension(\"toml\"); let file = File::open(&path)?; let reader = BufReader::new(file); for line in reader.lines() { println!(\"Read a line: {}\", line?); } Ok(()) } fn main() { match test_read_file() { Ok(_) => {} Err(e) => { println!(\"Error occured: {}\", e); } } } 标准输入输出 use std::io::prelude::*; use std::io::BufReader; fn test_stdin() -> Result { let stdin = std::io::stdin(); let handle = stdin.lock(); let reader = BufReader::new(handle); for line in reader.lines() { let line = line?; if line.is_empty() { return Ok(()); } println!(\"Read a line: {}\", line); } Ok(()) } fn main() { match test_stdin() { Ok(_) => {} Err(e) => { println!(\"Error occured: {}\", e); } } } 进程启动参数 在Rust中，进程启动参数是调用独立的函数std::env::args()来得到的，或者使用std::env::args_os()来得到，进程返回值也是调用独立函数std::process::exit()来指定的。 比如: fn main() { if std::env::args().any(|arg| arg == \"-kill\") { std::process::exit(1); } for arg in std::env::args() { println!(\"{}\", arg); } } Any和反射 Rust标准库中提供了一个乞丐版的“反射”功能，那就是std::any模块。这个模块内，有个trait名字叫作Any。所有的类型都自动实现了Any这个trait，因此我们可以把任何一个对象的引用转为&Any这个trait object，然后调用它的方法。 它可以判断这个对象是什么类型，以及强制转换&Any为某个具体类型。另外，成员函数get_type_id()暂时要求'static约束，这个限制条件以后会放宽。 #![feature(get_type_id)] use std::any::Any; use std::fmt::Display; fn log(value: &T) { let value_any = value as &Any; if let Some(s) = value_any.downcast_ref::() { println!(\"String: {}\", s); } else if let Some(i) = value_any.downcast_ref::() { println!(\"i32: {}\", i); } else { let type_id = value_any.get_type_id(); println!(\"unknown type {:?}: {}\", type_id, value); } } fn do_work(value: &T) { log(value); } fn main() { let my_string = \"Hello World\".to_string(); do_work(&my_string); let my_i32: i32 = 100; do_work(&my_i32); let my_char: char = '❤'; do_work(&my_char); } 线程安全 创建线程 use std::thread; thread::spawn(move || { // 这里是新建线程的执行逻辑 }); 如果我们需要等待子线程执行结束，那么可以使用join方法: use std::thread; // child 的类型是 JoinHandle,这个T是闭包的返回类型 let child = thread::spawn(move || { // 子线程的逻辑 }); // 父线程等待子线程结束 let res = child.join(); 更多线程参数 use std::thread; thread::Builder::new().name(\"child1\".to_string()).spawn(move || { println!(\"Hello, world!\"); }); thread的常用API: thread::sleep(dur: Duration) 使得当前线程等待一段时间继续执行。在等待的时间内，线程调度器会调度其他的线程来执行。 thread::yield_now() 放弃当前线程的执行，要求线程调度器执行线程切换。 thread::current() 获得当前的线程。 thread::park() 暂停当前线程，进入等待状态。当thread::Thread::unpark(&self)方法被调用的时候，这个线程可以被恢复执行。 thread::Thread::unpark(&self) 综合例子如下: use std::thread; use std::time::Duration; fn main() { let t = thread::Builder::new() .name(\"child1\".to_string()) .spawn(move || { println!(\"enter child thread.\"); thread::park(); println!(\"resume child thread\"); }) .unwrap(); println!(\"spawn a thread\"); thread::sleep(Duration::new(5, 0)); t.thread().unpark(); t.join(); println!(\"child thread finished\"); } rust怎么保证线程安全 下面的代码编译不过: use std::thread; fn main() { let mut health = 12; thread::spawn(|| { health *= 2; }); println!(\"{}\", health); } spawn函数接受的参数是一个闭包。我们在闭包里面引用了函数体内的局部变量，而这个闭包是运行在另外一个线程上，编译器无法肯定局部变量health的生命周期一定大于闭包的生命周期，于是发生了错误. 如果在闭包前面加move, 虽然能够编译过, 但运行结果是health还是12, 因为move的语义是copy, 导致在闭包里面的health已经不是外面的health了. rust编译器可以识别下面的代码, 带不带move都编译不过. use std::thread; fn main() { let mut v: Vec = vec![]; thread::spawn(|| { v.push(1); }); println!(\"{:?}\", v); } 那么rust能不能让一个变量在不同的线程中共享呢? 答: 不能. 我们没有办法在多线程中直接读写普通的共享变量，除非使用Rust提供的线程安全相关的设施 。 The compiler prevents all data races. “共享不可变，可变不共享” Send & Sync std::marker::Sync: 如果类型T实现了Sync类型，那说明在不同的线程中使用&T访问同一个变量是安全的。 std::marker::Send: 如果类型T实现了Send类型，那说明这个类型的变量在不同的线程中传递所有权是安全的。 在spawn签名中: pub fn spawn(f: F) -> JoinHandle where F: FnOnce() -> T, F: Send + 'static, T: Send + 'static 我们需要注意的是，参数类型F有重要的约束条件F: Send+'static， T: Send+'static。它要求参数满足传递所有权的约束, 但凡在线程之间传递所有权会发生安全问题的类型，都无法在这个参数中出现，否则就是编译错误。另外，Rust对全局变量也有很多限制，你不可能简单地通过全局变量在多线程中随意共享状态。这样，编译器就会禁止掉可能有危险的线程间共享数据的行为。 什么是Send类型 Types that can be transferred across thread boundaries. This trait is automatically implemented when the compiler determines it's appropriate. Send trait由编译器自动判断并实现. 如果一个类型可以安全地从一个线程move进入另一个线程，那它就是Send类型。比如: 普通的数字类型是Send，因为我们把数字move进入另一个线程之后，两个线程同时执行也不会造成什么安全问题. 更进一步，内部不包含引用的类型，都是Send。因为这样的类型跟外界没有什么关联，当它被move进入另一个线程之后，它所有的部分都跟原来的线程没什么关系了，不会出现并发访问的情况。比如String类型。 稍微复杂一点的，具有泛型参数的类型，是否满足Send大多是取决于参数类型是否满足Send。比如Vec，只要我们能保证T: Send，那么Vec肯定也是Send，把它move进入其他线程是没什么问题的。再比如Cell、RefCell、Option、Box，也都是这种情况。 还比如加锁的类型也是Send类型, 比如Mutex就是这种。 Mutex这个类型实际上不关心它内部类型是怎样的，反正要访问内部数据，一定要调用lock()方法上锁，它的所有权在哪个线程中并不重要，所以把它move到其他线程也是没有问题的. 那什么不是send类型呢? 答案是编译器报错的就不是. 比如Rc 什么是Sync类型 Types for which it is safe to share references between threads. This trait is automatically implemented when the compiler determines it's appropriate. Sync trait由编译器自动判断并实现. 如果类型T实现了Sync trait，那说明在不 同的线程中使用&T访问同一个变量是安全的。注意这里是&T是只读的. 显然，基本数字类型肯定是Sync。假如不同线程都拥有指向同一个i32类型的只读引用&i32，这是没什么问题的。因为这个类型引用只能读，不能写。多个线程读同一个整数是安全的。 -- 这里我有疑问, 如果所有权的线程去写呢? 不就不安全了么? 大部分具有泛型参数的类型是否满足Sync，很多都是取决于参数类 型是否满足Sync。像Box、Vec Option 这种也是Sync的，只要其中的参数T是满足Sync的。 也有一些类型，不论泛型参数是否满足Sync，它都是满足Sync的。 这种类型把不满足Sync条件的类型用它包起来，就变成了满足Sync条件 的。Mutex就是这种。多个线程同时拥有&Mutex型引用，指向同一个变量是没问题的。 保证线程安全的类型 Arc Arc是Rc的线程安全版本。它的全称是“Atomic reference counter”。 use std::sync::Arc; use std::thread; fn main() { let numbers: Vec = (0..100u32).collect(); // 引用计数指针,指向一个 Vec let shared_numbers = Arc::new(numbers); // 循环创建 10 个线程 for _ in 0..10 { // 复制引用计数指针,所有的 Arc 都指向同一个 Vec let child_numbers = shared_numbers.clone(); // move修饰闭包,上面这个 Arc 指针被 move 进入了新线程中 thread::spawn(move || { // 我们可以在新线程中使用 Arc,读取共享的那个 Vec let local_numbers = &child_numbers[..]; // 继续使用 Vec 中的数据 }); } } Mutex 下面我们用一个示例来演示一下Arc和Mutex配合。使用多线程修改共享变量: use std::sync::Arc; use std::sync::Mutex; use std::thread; const COUNT: u32 = 1000000; fn main() { let global = Arc::new(Mutex::new(0)); let clone1 = global.clone(); let thread1 = thread::spawn(move || { for _ in 0..COUNT { let mut value = clone1.lock().unwrap(); *value += 1; } }); let clone2 = global.clone(); let thread2 = thread::spawn(move || { for _ in 0..COUNT { let mut value = clone2.lock().unwrap(); *value -= 1; } }); thread1.join().ok(); thread2.join().ok(); println!(\"final value: {:?}\", global); } 而MutexGuard类型则是一个“智能指针”类型，它实现了DerefMut和Deref这两个trait，所以它可以被当作指向内部数据的普通指针使用。 MutexGuard实现了一个析构函数，通过RAII手法，在析构函数中调用了unlock()方法解锁。因此，用户是不需要手动调用方法解锁的 RwLock use std::sync::Arc; use std::sync::RwLock; use std::thread; const COUNT: u32 = 1000000; fn main() { let global = Arc::new(RwLock::new(0)); let clone1 = global.clone(); let thread1 = thread::spawn(move || { for _ in 0..COUNT { let mut value = clone1.write().unwrap(); *value += 1; } }); let clone2 = global.clone(); let thread2 = thread::spawn(move || { for _ in 0..COUNT { let mut value = clone2.write().unwrap(); *value -= 1; } }); thread1.join().ok(); thread2.join().ok(); println!(\"final value: {:?}\", global); } Atomic use std::sync::atomic::{AtomicIsize, Ordering}; use std::sync::Arc; use std::thread; const COUNT: u32 = 1000000; fn main() { // Atomic 系列类型同样提供了线程安全版本的内部可变性 let global = Arc::new(AtomicIsize::new(0)); let clone1 = global.clone(); let thread1 = thread::spawn(move || { for _ in 0..COUNT { clone1.fetch_add(1, Ordering::SeqCst); } }); let clone2 = global.clone(); let thread2 = thread::spawn(move || { for _ in 0..COUNT { clone2.fetch_sub(1, Ordering::SeqCst); } }); thread1.join().ok(); thread2.join().ok(); println!(\"final value: {:?}\", global); } Barrier use std::sync::{Arc, Barrier}; use std::thread; fn main() { let barrier = Arc::new(Barrier::new(10)); let mut handlers = vec![]; for _ in 0..10 { let c = barrier.clone(); // The same messages will be printed together. // You will NOT see any interleaving. let t = thread::spawn(move || { println!(\"before wait\"); c.wait(); println!(\"after wait\"); }); handlers.push(t); } for h in handlers { h.join().ok(); } } 这个程序创建了一个多个线程之间共享的Barrier，它的初始值是10。我们创建了10个子线程，每个子线程都有一个Arc指针指向了这个Barrier，并在子线程中调用了Barrier::wait方法。这些子线程执行到 wait方法的时候，就开始进入等待状态，一直到wait方法被调用了10 次，10个子线程都进入等待状态，此时Barrier就通知这些线程可以继续了。然后它们再开始执行下面的逻辑。 Condvar 等待和通知的机制 use std::sync::{Arc, Condvar, Mutex}; use std::thread; use std::time::Duration; fn main() { let pair = Arc::new((Mutex::new(false), Condvar::new())); let pair2 = pair.clone(); thread::spawn(move || { thread::sleep(Duration::from_secs(1)); let &(ref lock, ref cvar) = &*pair2; let mut started = lock.lock().unwrap(); *started = true; cvar.notify_one(); println!(\"child thread {}\", *started); }); // wait for the thread to start up let &(ref lock, ref cvar) = &*pair; let mut started = lock.lock().unwrap(); println!(\"before wait {}\", *started); while !*started { started = cvar.wait(started).unwrap(); } println!(\"after wait {}\", *started); } 全局变量 Rust中允许存在全局变量。在基本语法章节讲过，使用static关键字修饰的变量就是全局变量。全局变量有一个特点: 如果要修改全局变量，必须使用unsafe关键字 线程局部存储 线程局部(Thread Local)的意思是，声明的这个变量看起来是一个变量，但它实际上在每一个线程中分别有自己独立的存储地址，是不同的变量，互不干扰。在不同线程中，只能看到与当前线程相关联的那个副本，因此对它的读写无须考虑线程安全问题。 可以使用#[thread_local]attribute 可以使用thread_local！宏 异步管道 相当于无限扩容的go channel, 但只是多生产单消费异步管道是最常用的一种管道类型。它的特点是: 发送端和接收端之间存在一个缓冲区，发送端发送数据的时候，是先将这个数据扔到缓冲区，再由接收端自己去取。因此，每次发送，立马就返回了，发送端不用管数据什么时候被接收端处理 use std::sync::mpsc::channel; use std::thread; fn main() { let (tx, rx) = channel(); thread::spawn(move || { for i in 0..10 { tx.send(i).unwrap(); //这里依次发送10个数字, 改成tx.send(\"hello\")发字符串也行的. } }); while let Ok(r) = rx.recv() { println!(\"received {}\", r); } } channel是个泛型函数: pub fn channel() -> (Sender, Receiver) Sender和Receiver 都是泛型类型，且一组发送者和接收者必定是同样的类型参数，因此保 证了发送和接收端都是同样的类型。因为Rust中的类型推导功能的存在，使我们可以在调用channel的时候不指定具体类型参数，而通过后续的方法调用，推导出正确的类型参数。 Sender和Receiver的泛型参数必须满足T: Send约束。这个条件是显而易见的: 被发送的消息会从一个线程转移到另外一个线程，这个约束是为了满足线程安全。如果用户指定的泛型参数没有满足条件，在编译的时候会发生错误，提醒我们修复bug。 发送者调用send方法，接收者调用recv方法，返回类型都是Result类型，用于错误处理，因为它们都有可能调用失败。当发送者已经被销毁 的时候，接收者调用recv则会返回错误；同样，当接收者已经销毁的时候，发送者调用send也会返回错误。 在管道的接收端，如果调用recv方法的时候还没有数据，它会进入等待状态阻塞当前线程，直到接收到数据才继续往下执行。 同步管道 相当于go channel 异步管道内部有一个不限长度的缓冲区，可以一直往里面填充数据，直至内存资源耗尽。异步管道的发送端调用send方法不会发生阻塞，只要把消息加入到缓冲区，它就马上返回。 同步管道的特点是: 其内部有一个固定大小的缓冲区，用来缓存消息。如果缓冲区被填满了，继续调用send方法的时候会发生阻塞，等待接收端把缓冲区内的消息拿走才能继续发送。缓冲区的长度可以在建立管道的时候设置，而且0是有效数值。如果缓冲区的长度设置为0，那就 意味着每次的发送操作都会进入等待状态，直到这个消息被接收端取走才能返回。 第三方线程库 threadpool: threadpool是一个基本的线程池实现use std::sync::mpsc::channel; use threadpool::ThreadPool; fn main() { let n_workers = 4; let n_jobs = 8; let pool = ThreadPool::new(n_workers); let (tx, rx) = channel(); for _ in 0..n_jobs { let tx = tx.clone(); pool.execute(move || { tx.send(1) .expect(\"channel will be there waiting for the pool\"); }); } assert_eq!(rx.iter().take(n_jobs).fold(0, |a, b| a + b), 8); } scoped-threadpool在前面的章节中，我们已经知道，如果要在多线程之间共享变量， 必须使用Arc这样的保证线程安全的智能指针。然而，Arc是有运行期开销的(虽然很小)。假如我们有时候需要子线程访问当前调用栈中的局部变量，而且能保证当前函数的生命周期一定大于子线程的生命周期， 子线程一定先于当前函数退出，那我们能不能直接在子线程中使用最简单的借用指针&来访问父线程栈上的局部对象呢？ parking_lotRust标准库帮我们封装了一些基本的操作系统的同步原语，比如 Mutex Condvar等。一般情况下这些够我们使用了。但是还有一些对性能有极致要求的开发者对标准库的实现并不满意，于是社区里又有人开发出来了一套替代品，在性能和易用性方面，都比标准库更好，这就是 parking_lot库。 crossbeam标准库给了一份mpsc(多生产者单消费者)管道的实现， 但是它有许多缺陷。crossbeam-channel这个库给我们提供了另外一套管 道的实现方式。不仅包括mpsc，还包括mpmc(多生产者多消费者)， 而且使用便捷，执行效率也很高。下面是一个双端管道的使用示例。它基本实现了go语言的内置管道 功能，在执行效率上甚至有过之而无不及:extern crate crossbeam; #[macro_use] extern crate crossbeam_channel as channel; use channel::{Receiver, Sender}; fn main() { let people = vec![\"Anna\", \"Bob\", \"Cody\", \"Dave\", \"Eva\"]; let (tx, rx) = channel::bounded(1); // Make room for one unmatched send. let (tx, rx) = (&tx, &rx); crossbeam::scope(|s| { for name in people { s.spawn(move || seek(name, tx, rx)); } }); if let Ok(name) = rx.try_recv() { println!(\"No one received {}’s message.\", name); } } // Either send my name into the channel or receive someone else's, whatever happens first. fn seek(name: &'a str, tx: &Sender, rx: &Receiver) { select_loop! { recv(rx, peer) => println!(\"{} received a message from {}.\", name, peer), send(tx, name) => {}, // Wait for someone to receive my message. } } rayon Rayon是Rust核心组成员Nicholas Matsakis开发的一个并行迭代器项目。它可以把一个按顺序执行的任务轻松变成并行执行。它非常轻量级，效率极高，而且使用非常简单。而且它保证了无数据竞争的线程安全。 "},"notes/rust_工程构建.html":{"url":"notes/rust_工程构建.html","title":"Rust 工程构建","keywords":"","body":" 测试和文档测试 详细解释crate和mod的文档 项目和模块 cargo cargo.toml 错误处理 问号运算符 和C的ABI兼容 从C调用Rust库 从Rust调用C库 文档 测试和文档测试 rust原生支持测试, 还支持对文档中的example代码进行测试. https://www.cs.brandeis.edu/~cs146a/rust/doc-02-21-2015/book/testing.html 详细解释crate和mod的文档 https://www.cs.brandeis.edu/~cs146a/rust/doc-02-21-2015/book/crates-and-modules.html cargo build会根据约定俗成的规则来编译bin或者lib, 大体上是通过分析目录结构, 和特殊的文件名, 比如main.rs, lib.rs, mod.rs等. $ tree . . ├── Cargo.lock ├── Cargo.toml ├── src │ ├── english │ │ ├── farewells.rs │ │ ├── greetings.rs │ │ └── mod.rs │ ├── japanese │ │ ├── farewells.rs │ │ ├── greetings.rs │ │ └── mod.rs │ └── lib.rs └── target ├── deps ├── libphrases-a7448e02a0468eaa.rlib └── native 项目和模块 Rust用了两个概念来管理项目：一个是crate，一个是mod。 crate简单理解就是一个项目。crate是Rust中的独立编译单元。每个 crate对应生成一个库或者可执行文件（如.lib .dll .so .exe等）。官方有一 个crate仓库https://crates.io/ ，可以供用户发布各种各样的库，用户也可 以直接使用这里面的开源库。 mod简单理解就是命名空间。mod可以嵌套，还可以控制内部元素的可见性。 crate和mod有一个重要区别是：crate之间不能出现循环引用；而mod是无所谓的，mod1要使用mod2的内容，同时mod2要使用mod1的内容，是完全没问题的。在Rust里面，crate才是一个完整的编译单元 （compile unit）。也就是说，rustc编译器必须把整个crate的内容全部读进去才能执行编译，rustc不是基于单个的.rs文件或者mod来执行编译的。作为对比，C/C++里面的编译单元是单独的.c/.cpp文件以及它们所有的include文件。每个.c/.cpp文件都是单独编译，生成.o文件，再把这些.o文件链接起来。 cargo Cargo是官方的项目管理工具 新建一个hello world工程cargo new hello_world --bin 新建一个hello world库cargo new hello_world --lib 编译cargo build --release cargo.toml 举例: [package] name = \"seccompiler\" version = \"1.1.0\" authors = [\"Amazon Firecracker team \"] edition = \"2018\" build = \"../../build.rs\" description = \"Program that compiles multi-threaded seccomp-bpf filters expressed as JSON into raw BPF programs, serializing them and outputting them to a file.\" homepage = \"https://firecracker-microvm.github.io/\" license = \"Apache-2.0\" [[bin]] name = \"seccompiler-bin\" path = \"src/seccompiler_bin.rs\" [dependencies] bincode = \"1.2.1\" libc = \">=0.2.39\" serde = { version = \">=1.0.27\", features = [\"derive\"] } serde_json = \">=1.0.9\" utils = { path = \"../utils\" } The Cargo.toml file for each package is called its manifest. It is written in the TOML format. Every manifest file consists of the following sections:参考: https://doc.rust-lang.org/cargo/reference/manifest.html cargo-features — Unstable, nightly-only features. [package] — Defines a package. name — The name of the package. version — The version of the package. authors — The authors of the package. edition — The Rust edition. rust-version — The minimal supported Rust version. description — A description of the package. documentation — URL of the package documentation. readme — Path to the package's README file. homepage — URL of the package homepage. repository — URL of the package source repository. license — The package license. license-file — Path to the text of the license. keywords — Keywords for the package. categories — Categories of the package. workspace — Path to the workspace for the package. build — Path to the package build script. links — Name of the native library the package links with. exclude — Files to exclude when publishing. include — Files to include when publishing. publish — Can be used to prevent publishing the package. metadata — Extra settings for external tools. default-run — The default binary to run by cargo run. autobins — Disables binary auto discovery. autoexamples — Disables example auto discovery. autotests — Disables test auto discovery. autobenches — Disables bench auto discovery. resolver — Sets the dependency resolver to use. Target tables: (see configuration for settings) [lib] — Library target settings. [[bin]] — Binary target settings. [[example]] — Example target settings. [[test]] — Test target settings. [[bench]] — Benchmark target settings. Dependency tables: [dependencies] — Package library dependencies. [dev-dependencies] — Dependencies for examples, tests, and benchmarks. [build-dependencies] — Dependencies for build scripts. [target] — Platform-specific dependencies. [badges] — Badges to display on a registry. [features] — Conditional compilation features. [patch] — Override dependencies. [replace] — Override dependencies (deprecated). [profile] — Compiler settings and optimizations. [workspace] — The workspace definition. 错误处理 比如使用Option表示some和none两种可能, 返回Result既有值又有错误 impl str { pub fn find>(&'a self, pat: P) -> Option {} } impl File { pub fn open>(path: P) -> io::Result {} } 比如 use std::mem::{size_of, size_of_val}; use std::str::FromStr; use std::string::ParseError; fn main() { let r: Result = FromStr::from_str(\"hello\"); println!(\"Size of String: {}\", size_of::()); println!(\"Size of `r`: {}\", size_of_val(&r)); } 问号运算符 问号运算符意思是，如果结果是Err，则提前返回一个Result类型，否则继续执行。 标准库的Option、Result实现了问号需要的trait fn file_double>(file_path: P) -> Result { let mut file = File::open(file_path).map_err(|e| e.to_string())?; let mut contents = String::new(); file.read_to_string(&mut contents) .map_err(|err| err.to_string())?; let n = contents .trim() .parse::() .map_err(|err| err.to_string())?; Ok(2 * n) } 进一步简化: use std::fs::File; use std::io::Read; use std::path::Path; fn file_double>(file_path: P) -> Result> { let mut file = File::open(file_path)?; let mut contents = String::new(); file.read_to_string(&mut contents)?; let n = contents.trim().parse::()?; Ok(2 * n) } fn main() { match file_double(\"foobar\") { Ok(n) => println!(\"{}\", n), Err(err) => println!(\"Error: {:?}\", err), } } 跟其他很多运算符一样，问号运算符也对应着标准库中的一个trait std::ops::Try trait Try { type Ok; type Error; fn into_result(self) -> Result; fn from_error(v: Self::Error) -> Self; fn from_ok(v: Self::Ok) -> Self; } 和C的ABI兼容 Rust有一个非常好的特性，就是它支持与C语言的ABI兼容. 所以，我们可以用Rust写一个库，然后直接把它当成C写的库来使用。或者反过来，用C写的库，可以直接在Rust中被调用。而且这个过程是没有额外性能损失的。C语言的ABI是这个世界上最通用的ABI，大部分编程语言都支持与C的ABI兼容。这也意味着，Rust与其他语言之间的交互是没问题的，比如用Rust为Python/Node.js/Ruby写一个模块等。 rust编译选项有--crate-type [bin|lib|rlib|dylib|cdylib|staticlib|proc-macro]其中，cdylib和staticlib就是与C的ABI兼容的 Rust 中有泛型，C语言里面没有，所以泛型这种东西是不可能暴露出来给C语言使用的，这就不是C语言的ABI的一部分。只有符合C语言的调用方式的函数，才能作为FFI的接口。这样的函数有以下基本要求： 使用extern \"C\"修饰，在Rust中extern fn默认等同于extern \"C\" fn； 使用#[no_mangle]修饰函数，避免名字重整； 函数参数、返回值中使用的类型，必须在Rust和C里面具备同样的内存布局。 从C调用Rust库 假设我们要在Rust中实现一个把字符串从小写变大写的函数，然后 由C语言调用这个函数: 在Rust侧: #[no_mangle] pub extern \"C\" fn rust_capitalize(s: *mut c_char) { unsafe { let mut p = s as *mut u8; while *p != 0 { let ch = char::from(*p); if ch.is_ascii() { let upper = ch.to_ascii_uppercase(); *p = upper as u8; } p = p.offset(1); } } } 我们在Rust中实现这个函数，考虑到C语言调用的时候传递的是char*类型，所以在Rust中我们对应的参数类型是*mut std::os:: raw::c_char。这样两边就对应起来了。 用下面的命令生成一个c兼容的静态库: rustc --crate-type=staticlib capitalize.rs 在C里面调用: #include #include // declare extern void rust_capitalize(char *); int main() { char str[] = \"hello world\"; rust_capitalize(str); printf(\"%s\\n\", str); return 0; } 用下面的命令链接: gcc -o main main.c -L. -l:libcapitalize.a -Wl,--gc-sections -lpthread -ldl 从Rust调用C库 比如c的函数 int add_square(int a, int b) { return a * a + b * b; } 在rust里面调用: use std::os::raw::c_int; #[link(name = \"simple_math\")] extern \"C\" { fn add_square(a: c_int, b: c_int) -> c_int; } fn main() { let r = unsafe { add_square(2, 2) }; println!(\"{}\", r); } //编译: //rustc -L . call_math.rs 文档 特殊的文档注释 是///、//！、/**…*/、/*！…*/，它们会被视为文档 mod foo { //! 这块文档是给 `foo` 模块做的说明 /// 这块文档是给函数 `f` 做的说明 fn f() { // 这块注释不是文档的一部分 } } 文档还支持markdown格式 "},"notes/rust_coding_brief.html":{"url":"notes/rust_coding_brief.html","title":"Rust 代码积累","keywords":"","body":"记录平时积累的rust知识. "},"notes/rust_序列化.html":{"url":"notes/rust_序列化.html","title":"Rust 序列化原理","keywords":"","body":" 主流的serde框架 简单例子 serde_json 理论 untype的例子 Index操作符重载 反序列化自定义struct json!宏构建json 序列化结构体 性能 依赖 理论 29种类型 derive 属性 自己实现序列化 序列化 反序列化 反序列化的zero copy和生命周期标记 主流的serde框架 Serde is a framework for _ser_ializing and _de_serializing Rust data structures efficiently and generically. 简单例子 use serde::{Serialize, Deserialize}; extern crate serde_json; // 1.0.82 #[derive(Serialize, Deserialize, Debug)] struct Point { x: i32, y: i32, } fn main() { let point = Point { x: 1, y: 2 }; // Convert the Point to a JSON string. let serialized = serde_json::to_string(&point).unwrap(); // Prints serialized = {\"x\":1,\"y\":2} println!(\"serialized = {}\", serialized); // Convert the JSON string back to a Point. let deserialized: Point = serde_json::from_str(&serialized).unwrap(); // Prints deserialized = Point { x: 1, y: 2 } println!(\"deserialized = {:?}\", deserialized); } serde_json 理论 比如下面的json { \"name\": \"John Doe\", \"age\": 43, \"address\": { \"street\": \"10 Downing Street\", \"city\": \"London\" }, \"phones\": [ \"+44 1234567\", \"+44 2345678\" ] } json中的每个\"value\", 都是一个serde_json::Value enum Value { Null, Bool(bool), Number(Number), String(String), Array(Vec), Object(Map), } 用serde_json::from_str从string里反序列化json 用from_slice从&[u8]反序列化json 用from_reader从文件或者TCP的socket的io::Read反序列化json untype的例子 所谓的untype就是说没有预定义好一个struct, 而是直接从JSON字符串里反序列化出一个对象, 这个对象的类型永远是serde_json::Value use serde_json::{Result, Value}; fn untyped_example() -> Result { // Some JSON input data as a &str. Maybe this comes from the user. let data = r#\" { \"name\": \"John Doe\", \"age\": 43, \"phones\": [ \"+44 1234567\", \"+44 2345678\" ] }\"#; // Parse the string of data into serde_json::Value. let v: Value = serde_json::from_str(data)?; // Access parts of the data by indexing with square brackets. println!(\"Please call {} at the number {}\", v[\"name\"], v[\"phones\"][0]); Ok(()) } fn main() { untyped_example(); } //结果 Please call \"John Doe\" at the number \"+44 1234567\" rust的enum真是强大, 可以对serde_json::Value在运行时做各种操作, 比如v[\"name\"], 甚至v[\"phones\"][0]都是合法的. -- 是因为serde_json::Value自己实现了Index操作符. 显然我们知道这些是合理的, 那如果故意用\"不可能\"的key去访问呢? 比如: println!(\"Please call {} at the number {}\", v[\"nnnnnnnnnnnn\"], v[\"phones\"][0][0][0]); //能正常运行, 没有崩溃, 没有index越界, 结果为null Please call null at the number null 如果最后整个打印v, 会得到: println!(\"{:?}\", v); //结果 Object({\"age\": Number(43), \"name\": String(\"John Doe\"), \"phones\": Array([String(\"+44 1234567\"), String(\"+44 2345678\")])}) Index操作符重载 看起来v支持用index来访问, 是这个库有特殊的实现: https://github.com/serde-rs/json/blob/master/src/value/index.rs impl ops::Index for Value where I: Index, { ... } 这个Index是下面: 关键先match v的类型, 再操作. 相当于操作符重载, 应该是rust比较高阶的用法. impl Index for str { fn index_into(&self, v: &'v Value) -> Option { match v { Value::Object(map) => map.get(self), _ => None, } } fn index_into_mut(&self, v: &'v mut Value) -> Option { match v { Value::Object(map) => map.get_mut(self), _ => None, } } fn index_or_insert(&self, v: &'v mut Value) -> &'v mut Value { if let Value::Null = v { *v = Value::Object(Map::new()); } match v { Value::Object(map) => map.entry(self.to_owned()).or_insert(Value::Null), _ => panic!(\"cannot access key {:?} in JSON {}\", self, Type(v)), } } } 自己实现的Index逻辑: The result of square bracket indexing like v[\"name\"] is a borrow of the data at that index, so the type is &Value. A JSON map can be indexed with string keys, while a JSON array can be indexed with integer keys. If the type of the data is not right for the type with which it is being indexed, or if a map does not contain the key being indexed, or if the index into a vector is out of bounds, the returned element is Value::Null. 反序列化自定义struct 上面untype的例子中, 反序列化出来的对象只能是serde_json::Value, 虽然也可以按照json对号入座的访问里面的filed, 但实际是走的\"通用\"代码. 下面的例子可以直接返回一个strongly typed结构体: use serde::{Deserialize, Serialize}; use serde_json::Result; #[derive(Serialize, Deserialize)] struct Person { name: String, age: u8, phones: Vec, } fn typed_example() -> Result { // Some JSON input data as a &str. Maybe this comes from the user. let data = r#\" { \"name\": \"John Doe\", \"age\": 43, \"phones\": [ \"+44 1234567\", \"+44 2345678\" ] }\"#; // Parse the string of data into a Person object. This is exactly the // same function as the one that produced serde_json::Value above, but // now we are asking it for a Person as output. let p: Person = serde_json::from_str(data)?; //同样是from_str这个API, 会根据左侧变量的type来适配, 强大! // Do things just like with any other Rust data structure. println!(\"Please call {} at the number {}\", p.name, p.phones[0]); //这里对name的访问也变成了p.name Ok(()) } This is the same serde_json::from_str function as before, but this time we assign the return value to a variable of type Person so Serde will automatically interpret the input data as a Person and produce informative error messages if the layout does not conform to what a Person is expected to look like. Any type that implements Serde's Deserialize trait can be deserialized this way. This includes built-in Rust standard library types like Vec and HashMap, as well as any structs or enums annotated with #[derive(Deserialize)]. from_str()会返回不同的类型, 是因为它是个泛型函数: pub fn from_str(s: &'a str) -> Result where T: de::Deserialize, { //这里实际上是return from_trait(read::StrRead::new(s)) //伴随着return, 返回类型T被传递进from_trait from_trait(read::StrRead::new(s)) } 这个泛型函数实例化类型T的传入是从返回值Result推断出来的. 而这个返回值从变量类型而来:let p: Person = serde_json::from_str(data)?编译器推断出T就是Person from_str继续把这个类型传递给from_trait. fn from_trait(read: R) -> Result where R: Read, T: de::Deserialize, { let mut de = Deserializer::new(read); let value = tri!(de::Deserialize::deserialize(&mut de)); // Make sure the whole stream has been consumed. tri!(de.end()); Ok(value) } json!宏构建json serde_json提供了json!宏来在代码里定义原始json文本数据. use serde_json::json; fn main() { // The type of `john` is `serde_json::Value` let john = json!({ \"name\": \"John Doe\", \"age\": 43, \"phones\": [ \"+44 1234567\", \"+44 2345678\" ] }); println!(\"first phone number: {}\", john[\"phones\"][0]); // Convert to a string of JSON and print it out println!(\"{}\", john.to_string()); } json宏还提供更高级的功能: 引用变量 let full_name = \"John Doe\"; let age_last_year = 42; // The type of `john` is `serde_json::Value` let john = json!({ \"name\": full_name, \"age\": age_last_year + 1, //这里可以引用上面的变量 \"phones\": [ format!(\"+44 {}\", random_phone()) //也可以调用其他宏和函数 ] }); One neat thing about the json! macro is that variables and expressions can be interpolated directly into the JSON value as you are building it. Serde will check at compile time that the value you are interpolating is able to be represented as JSON. 序列化结构体 serde_json::to_string 序列化到string serde_json::to_vec序列化到Vec serde_json::to_writer序列化到文件或者TCP stream Any type that implements Serde's Serialize trait can be serialized this way. This includes built-in Rust standard library types like Vec and HashMap, as well as any structs or enums annotated with #[derive(Serialize)]. 性能 性能很好. 比最快的C实现还快 It is fast. You should expect in the ballpark of 500 to 1000 megabytes per second deserialization and 600 to 900 megabytes per second serialization, depending on the characteristics of your data. This is competitive with the fastest C and C++ JSON libraries or even 30% faster for many use cases. Benchmarks live in the serde-rs/json-benchmark repo. 依赖 只依赖内存allocator. 可以禁止其他default的feature, 只保留alloc Disable the default \"std\" feature and enable the \"alloc\" feature: [dependencies] serde_json = { version = \"1.0\", default-features = false, features = [\"alloc\"] } 理论 不同于其他使用发射来实现序列化的语言, serde用的是rust的trait. 实现了Serde's Serialize and Deserialize的struct都可以被序列化/反序列化. 29种类型 serde归纳了29种基础类型: 注意没有指针! 这点和tinygo不一样, tinygo用了反射, 而反射是支持指针的. 14 primitive types bool i8, i16, i32, i64, i128 u8, u16, u32, u64, u128 f32, f64 char string UTF-8 bytes with a length and no null terminator. May contain 0-bytes. When serializing, all strings are handled equally. When deserializing, there are three flavors of strings: transient, owned, and borrowed. This distinction is explained in Understanding deserializer lifetimes and is a key way that Serde enabled efficient zero-copy deserialization. byte array - [u8] Similar to strings, during deserialization byte arrays can be transient, owned, or borrowed. option Either none or some value. unit The type of () in Rust. It represents an anonymous value containing no data. unit_struct For example struct Unit or PhantomData. It represents a named value containing no data. unit_variant For example the E::A and E::B in enum E { A, B }. newtype_struct For example struct Millimeters(u8). newtype_variant For example the E::N in enum E { N(u8) }. seq A variably sized heterogeneous sequence of values, for example Vec or HashSet. When serializing, the length may or may not be known before iterating through all the data. When deserializing, the length is determined by looking at the serialized data. Note that a homogeneous Rust collection like vec![Value::Bool(true), Value::Char('c')] may serialize as a heterogeneous Serde seq, in this case containing a Serde bool followed by a Serde char. tuple A statically sized heterogeneous sequence of values for which the length will be known at deserialization time without looking at the serialized data, for example (u8,) or (String, u64, Vec) or [u64; 10]. tuple_struct A named tuple, for example struct Rgb(u8, u8, u8). tuple_variant For example the E::T in enum E { T(u8, u8) }. map A variably sized heterogeneous key-value pairing, for example BTreeMap. When serializing, the length may or may not be known before iterating through all the entries. When deserializing, the length is determined by looking at the serialized data. struct A statically sized heterogeneous key-value pairing in which the keys are compile-time constant strings and will be known at deserialization time without looking at the serialized data, for example struct S { r: u8, g: u8, b: u8 }. struct_variant For example the E::S in enum E { S { r: u8, g: u8, b: u8 } }. 大多数的rust类型, 都能映射到serde类型: rust bool --> serde bool rust Rgb(u8,u8,u8) --> serde tuple struct ... derive 用#[derive(Serialize, Deserialize)]给每个结构体生成一对Serialize and Deserialize traits. 属性 有很多实用的属性, 比如: #[serde(rename = \"name\")] #[serde(rename_all = \"...\")] //比如UPPERCASE, camelCase...等等 #[serde(default)] #[serde(alias = \"name\")] #[serde(flatten)] ...很多... 自己实现序列化 一般用#[derive(Serialize, Deserialize)]配合attributes就够了. 但特殊case如果想自定义序列化, 可以自己实现Serialize and Deserialize这两个trait 每个trait都只有一个方法 pub trait Serialize { fn serialize(&self, serializer: S) -> Result where S: Serializer; } pub trait Deserialize: Sized { fn deserialize(deserializer: D) -> Result where D: Deserializer; } 序列化 pub trait Serialize { fn serialize(&self, serializer: S) -> Result where S: Serializer; } This method's job is to take your type (&self) and map it into the Serde data model by invoking exactly one of the methods on the given Serializer. Serializer也是个trait, 这个trait是不同类型的format(比如json, Bincode等). 并不是所有的序列化输出都是text或者bin的, 比如serde_json::value::Serializer(注意, 不是serde_json::serializer)就序列化到内存. serde把结构体的到29种内部数据类型抽象成trait Serialize, 而把29种数据类型到输出format, 抽象成trait Serializer, 让二者解耦: 从结构体到29种数据类型OK, 就和最后的output格式无关. 这是个很巧妙的设计. Serialize ==> 29种内部数据类型 ==> Serializer 比如序列化一个Map impl Serialize for MyMap where K: Serialize, V: Serialize, { fn serialize(&self, serializer: S) -> Result where S: Serializer, { let mut map = serializer.serialize_map(Some(self.len()))?; for (k, v) in self { //因为知道self就是个map, 所以能用for in map.serialize_entry(k, v)?; } map.end() } } 结构体有4类: 普通结构体, tuple结构体, newtype, unit结构体 // An ordinary struct. Use three-step process: // 1. serialize_struct // 2. serialize_field // 3. end struct Color { r: u8, g: u8, b: u8, } // A tuple struct. Use three-step process: // 1. serialize_tuple_struct // 2. serialize_field // 3. end struct Point2D(f64, f64); // A newtype struct. Use serialize_newtype_struct. struct Inches(u64); // A unit struct. Use serialize_unit_struct. struct Instance; 反序列化 pub trait Deserialize: Sized { fn deserialize(deserializer: D) -> Result where D: Deserializer; } This method's job is to map the type into the Serde data model by providing the Deserializer with a Visitor that can be driven by the Deserializer to construct an instance of your type. Deserializer需要deserialize传入visitor trait. 反序列化比序列化更复杂点. 大体上根据序列化的format不同, 可以分为: 自解释的文本. 比如json, 序列化后的文本就能看出来对应的结构体. 这个情况可以用通用的反序列化apideserialize_any, 产生的对象是serde_json::Value 非自解释的bincode. 比如二进制的序列化方式, 只看output是不可能知道原始结构体的layout的. 这个情况需要编程时明确目标结构体的类型, 产生的对象是非serde_json::Value 反序列化的zero copy和生命周期标记 rust能做到zero copy是因为生成的目标结构体, 能引用原始buffer里面的比如string等类型, 不用拷贝. 因为有生命周期标记. 比如: #[derive(Deserialize)] struct User { id: u32, name: &'a str, screen_name: &'a str, location: &'a str, } Zero-copy deserialization means deserializing into a data structure, like the User struct above, that borrows string or byte array data from the string or byte array holding the input. This avoids allocating memory to store a string for each individual field and then copying string data out of the input over to the newly allocated field. Rust guarantees that the input data outlives the period during which the output data structure is in scope, meaning it is impossible to have dangling pointer errors as a result of losing the input data while the output data structure still refers to it. 上面的User结构体有多个对str的引用, 这些引用直接引用到原始buffer, 没有拷贝. rust保证原始buffer会一直有效直到这个结构体的生命周期结束. 注: User结构体本身不用生命周期标记, 它的标记'a是给里面的成员用的, 表示所有成员都有同样的生命周期. 反序列化的复杂点在于, 目标结构体要分配内存: where T: Deserialize 其引用的内存可能来自原始input的buffer中, 比如serde_json::from_str中, 原始str的生命周期也是caller提供的, 带生命周期标记的, 就可以被最终的目标结构体引用. This means \"T can be deserialized from some lifetime.\" The caller gets to decide what lifetime that is. Typically this is used when the caller also provides the data that is being deserialized from, for example in a function like serde_json::from_str. In that case the input data must also have lifetime 'de, for example it could be &'de str. where T: DeserializeOwned 原始的input, 以及其伴随的buffer, 在反序列化之后会被free. 此时不能引用. 比如io的reader This means \"T can be deserialized from any lifetime.\" The callee gets to decide what lifetime. Usually this is because the data that is being deserialized from is going to be thrown away before the function returns, so T must not be allowed to borrow from it. For example a function that accepts base64-encoded data as input, decodes it from base64, deserializes a value of type T, then throws away the result of base64 decoding. Another common use of this bound is functions that deserialize from an IO stream, such as serde_json::from_reader. "},"notes/rust_知识点积累.html":{"url":"notes/rust_知识点积累.html","title":"Rust 知识点更新","keywords":"","body":" 生命周期标记 线程 宏 简单例子 例子1 例子2 迭代器 闭包 rust指针cheatsheet ownership 方法和瀑布式设计 小知识点 std库的catch_unwind catch_unwind和FnOnce 什么是FnOnce catch_unwind 代码解释 std库的同步功能 u32可以调用checked_add做溢出检查 tuple返回值 宏调用使用()或{}都行? 该传值的时候传借用也行? 方法impl块里面的Self 条件编译 static变量 trait object 可以在函数定义里干任何事? 结构体定义和C对比 crate和mod bin文件的例子 lib的例子 firecracker/src/utils/src/arg_parser.rs代码走读 use 使用了BTreeMap 重定义了Result ArgParser对象 ArgParser对象方法 new这个对象: 从命令行parse 增加arg项 格式化help output Argument对象 test std collections Sequences性能 Maps性能 BTreeMap iter() keys()和values() 生命周期标记 只有引用才有生命周期标记的说法, 其他都没有: 没有结构体生命周期标记的说法. 看见一个struct带标记, 实际上是对其内部的field的引用的标记 编译器会自动推导一般的生命标记. 比如: fn announce(value: &impl Display) { println!(\"Behold! {}!\", value); } fn main() { let num = 42; let num_ref = &num; announce(num_ref); } 去掉编译器语法糖的版本fn announce(value: &'a T) where T: Display { println!(\"Behold! {}!\", value); } fn main() { 'x: { let num = 42; 'y: { let num_ref = &'y num; 'z: { announce(num_ref); } } } } 线程 use std::thread; fn main() { let guard = thread::scoped(|| { println!(\"Hello from a thread!\"); }); // guard goes out of scope here. 就是说在这里会等着上面的线程结束 } scoped原型是 fn scoped(self, f: F) -> JoinGuard where T: Send + 'a, F: FnOnce() -> T, F: Send + 'a Specifically, F, the closure that we pass to execute in the new thread. It has two restrictions: It must be a FnOnce from () to T. Using FnOnce allows the closure to take ownership of any data it mentions from the parent thread. The other restriction is that F must be Send. We aren't allowed to transfer this ownership unless the type thinks that's okay. 如果用spawn就不会卡住了, main退出会强制退出线程. fn main() { thread::spawn(|| { println!(\"Hello from a thread!\"); }); timer::sleep(Duration::milliseconds(50)); } 宏 比如Vec!宏: let x: Vec = vec![1, 2, 3]; 展开后是: let x: Vec = { let mut temp_vec = Vec::new(); temp_vec.push(1); temp_vec.push(2); temp_vec.push(3); temp_vec }; 对应的宏实现是: macro_rules! vec { ( $( $x:expr ),* ) => { { let mut temp_vec = Vec::new(); $( temp_vec.push($x); )* temp_vec } }; } ( $( $x:expr ),* ) => {...}里, $x:expr是类似match的语法, $(...),*是类似正则的语法, 表示match expr 0次或多次; $x是个临时变量. =>右边的$()*表示重复每个匹配 简单例子 macro_rules! five_times { ($x:expr) => (5 * $x); } fn main() { assert_eq!(25, five_times!(2 + 3)); } 例子1 macro_rules! foo { (x => $e:expr) => (println!(\"mode X: {}\", $e)); (y => $e:expr) => (println!(\"mode Y: {}\", $e)); } fn main() { foo!(y => 3); //这里的y => 3被宏做匹配 //foo!(z => 3);这样调用不行, 会报错. error: no rules expected the token `z` } //输出 mode Y: 3 例子2 macro_rules! o_O { ( $( $x:expr; [ $( $y:expr ),* ] );* ) => { &[ $($( $x + $y ),*),* ] } } fn main() { let a: &[i32] = o_O!(10; [1, 2, 3]; 20; [4, 5, 6]); assert_eq!(a, [11, 12, 13, 24, 25, 26]); 迭代器 for x in 0..10 { println!(\"{}\", x); } for相当于在loop里不断的调用range这个迭代器的next方法. let mut range = 0..10; loop { match range.next() { Some(x) => { println!(\"{}\", x); }, None => { break } } } vec的iter方法返回一个迭代器: let nums = vec![1, 2, 3]; for num in nums.iter() { //这里的num是个引用, println默认会解引用. println!(\"{}\", num); //下面的写法也行 println!(\"{}\", *num); } Now we're explicitly dereferencing num. Why does iter() give us references? Well, if it gave us the data itself, we would have to be its owner, which would involve making a copy of the data and giving us the copy. With references, we're just borrowing a reference to the data, and so it's just passing a reference, without needing to do the copy. 见: https://www.cs.brandeis.edu/~cs146a/rust/doc-02-21-2015/book/iterators.html 有些概念挺有用的: Iterator adapters, 比如map, filter等API Consumers, 比如collect等 闭包 let add_one = |x| { 1 + x }; println!(\"The sum of 5 plus 1 is {}.\", add_one(5)); fn main() { let x: i32 = 5; let printer = || { println!(\"x is: {}\", x); }; printer(); // prints \"x is: 5\" } 带move关键词的闭包的语义是take ownership: a moving closure always takes ownership of all variables that it uses. Ordinary closures, in contrast, just create a reference into the enclosing stack frame. 每个闭包的type都是独特的, 下面的例子用了F和G两个fn, 虽然签名是一模一样的, 但F和G是两个不同的type, 对应了两个不同的闭包. fn compose(x: i32, f: F, g: G) -> i32 where F: Fn(i32) -> i32, G: Fn(i32) -> i32 { g(f(x)) } fn main() { compose(5, |n: i32| { n + 42 }, |n: i32| { n * 2 }); // evaluates to 94 } rust指针cheatsheet Type Name Summary &T Reference Allows one or more references to read T &mut T Mutable Reference Allows a single reference to read and write T Box Box Heap allocated T with a single owner that may read and write T. Rc \"arr cee\" pointer Heap allocated T with many readers Arc Arc pointer Same as above, but safe sharing across threads *const T Raw pointer Unsafe read access to T *mut T Mutable raw pointer Unsafe read and write access to T ownership 说的很细 https://www.cs.brandeis.edu/~cs146a/rust/doc-02-21-2015/book/ownership.html 方法和瀑布式设计 https://www.cs.brandeis.edu/~cs146a/rust/doc-02-21-2015/book/method-syntax.html 小知识点 std库的catch_unwind panic::catch_unwind可以捕获rust运行时的panic use std::panic; let result = panic::catch_unwind(|| { println!(\"hello!\"); }); assert!(result.is_ok()); let result = panic::catch_unwind(|| { panic!(\"oh no!\"); }); assert!(result.is_err()); catch_unwind和FnOnce std::panic::catch_unwind(AssertUnwindSafe(f)) 上面的代码是ok的, 只要F满足约束: where F: FnOnce(), F: Send + 'static, 什么是FnOnce pub trait FnOnce { type Output; extern \"rust-call\" fn call_once(self, args: Args) -> Self::Output; } Instances of FnOnce can be called, but might not be callable multiple times. Because of this, if the only thing known about a type is that it implements FnOnce, it can only be called once. FnOnce只能被调用一次. FnOnce is implemented automatically by closure that might consume captured variables, as well as all types that implement FnMut, e.g. (safe) function pointers (since FnOnce is a supertrait of FnMut). 闭包自动实现了FnOnce, Fn, FnMut中的一个. Since both Fn and FnMut are subtraits of FnOnce, any instance of Fn or FnMut can be used where a FnOnce is expected. Fn和FnMut是FnOnce的子trait Use FnOnce as a bound when you want to accept a parameter of function-like type and only need to call it once. If you need to call the parameter repeatedly, use FnMut as a bound; if you also need it to not mutate state, use Fn. Also of note is the special syntax for Fn traits (e.g. Fn(usize, bool) -> usize). FnOnce是个函数形式的trait, 和Fn一样拥有特殊语法:Fn(usize, bool) -> usize 详见: Closures: Anonymous Functions catch_unwind std::panic::catch_unwind()的入参类型是FnOnce() -> R + UnwindSafe这个特殊语法的Fn trait pub fn catch_unwind R + UnwindSafe, R>(f: F) -> Result { unsafe { panicking::r#try(f) } } 代码解释 std::panic::catch_unwind(AssertUnwindSafe(f)) AssertUnwindSafe是个元组结构体: pub struct AssertUnwindSafe(pub T); 它实现了FnOnce: impl R> FnOnce for AssertUnwindSafe { type Output = R; extern \"rust-call\" fn call_once(self, _args: ()) -> R { (self.0)() } } 所以: FnOnce不一定非要是个闭包, 也可以是个结构体, 比如struct AssertUnwindSafe 记住Fn这个特殊的trait std库的同步功能 在多线程的情况下, 希望所有线程在某个点\"集合\": use std::sync::{Arc, Barrier}; use std::thread; let mut handles = Vec::with_capacity(10); let barrier = Arc::new(Barrier::new(10)); for _ in 0..10 { let c = Arc::clone(&barrier); // The same messages will be printed together. // You will NOT see any interleaving. handles.push(thread::spawn(move|| { println!(\"before wait\"); c.wait(); println!(\"after wait\"); })); } // Wait for other threads to finish. for handle in handles { handle.join().unwrap(); } u32可以调用checked_add做溢出检查 //self.next_gsi类型是u32 self.next_gsi = self.next_gsi.checked_add(1).ok_or(Error::Overflow)?; tuple返回值 一个函数如果想返回多个返回值, 可以这样: fn prepare_default_values() -> (String, String, String) { let default_vcpus = format! {\"boot={},max_phys_bits={}\", config::DEFAULT_VCPUS,config::DEFAULT_MAX_PHYS_BITS}; let default_memory = format! {\"size={}M\", config::DEFAULT_MEMORY_MB}; let default_rng = format! {\"src={}\", config::DEFAULT_RNG_SOURCE}; (default_vcpus, default_memory, default_rng) } 使用的时候用模式匹配: let (default_vcpus, default_memory, default_rng) = prepare_default_values(); 宏调用使用()或{}都行? 比如下面的代码, format和println宏, 用小括号和大括号调用, 作用一模一样. let default_vcpus = format! {\"boot={},max_phys_bits={}\", 8, 6.78}; let default_vcpus2 = format!(\"boot={},max_phys_bits={}\", 8, 6.78); println!(\"{}\", default_vcpus); println! {\"{}\", default_vcpus2}; 该传值的时候传借用也行? 比如这个函数, 第二个参数guest_mem的类型要求是&GuestMemoryMmap pub fn memory_init( &mut self, guest_mem: &GuestMemoryMmap, kvm_max_memslots: usize, track_dirty_pages: bool, ) -> Result 调用的时候: //已知guest_memory是&GuestMemoryMmap类型 guest_memory: &GuestMemoryMmap let mut vm = Vm::new() //这样可以编译, guest_memory的借用传入 vm.memory_init(&guest_memory, kvm.max_memslots(), track_dirty_pages) //这样也可以, 直接传入guest_memory, 这个应该是更符合函数signature vm.memory_init(guest_memory, kvm.max_memslots(), track_dirty_pages) //这样竟然也行 vm.memory_init(&&&&&&&&&guest_memory, kvm.max_memslots(), track_dirty_pages) 可能GuestMemoryMmap实现了Dref trait? 还是这种形式的传参都是被rust支持的????? 方法impl块里面的Self 一个结构体的方法, 并不都是入参一定是Self, 比如类似new()方法, 返回值才是Self(或者&Self等) 比如: pub struct VmResources { ... } impl VmResources { //new函数 pub fn from_json() -> std::result::Result { ...实例化Self } //其他方法 pub fn set_vsock_device(&mut self, config: VsockDeviceConfig) -> Result //等等 } 调用\"new\"方法的时候, 用的是VmResources::from_json(), 调用其他方法的时候, 用的是对象.xxx(). 而且, 一般的方法第一个入参是&mut self或者&self 它们都在一个impl块里. 条件编译 比如只有再cfg的target_arch是aarch64时才编译: #[cfg(target_arch = \"aarch64\")] enable_ssbd_mitigation(); static变量 比如下面的代码: use lazy_static::lazy_static; lazy_static! { static ref _LOGGER_INNER: Logger = Logger::new(); /// Static instance used for handling human-readable logs. pub static ref LOGGER: &'static Logger = { set_logger(_LOGGER_INNER.deref()).expect(\"Failed to set logger\"); _LOGGER_INNER.deref() }; } static说的是被static标记的变量在整个程序的周期内都有效 ref说的是后面的变量在被match做pattern匹配的时候, 使用借用方式.注: match默认采用move方式, 比如下面的maybe_name变量被match后就没法用了.let maybe_name = Some(String::from(\"Alice\")); // The variable 'maybe_name' is consumed here ... match maybe_name { Some(n) => println!(\"Hello, {}\", n), _ => println!(\"Hello, world\"), } // ... and is now unavailable. println!(\"Hello again, {}\", maybe_name.unwrap_or(\"world\".into())); 用ref就可以: 注意Some(ref n)那句let maybe_name = Some(String::from(\"Alice\")); // Using `ref`, the value is borrowed, not moved ... match maybe_name { Some(ref n) => println!(\"Hello, {}\", n), _ => println!(\"Hello, world\"), } // ... so it's available here! println!(\"Hello again, {}\", maybe_name.unwrap_or(\"world\".into())); _LOGGER_INNER.deref()这种神奇操作来自lazy_static!宏, 这是github上实现的第三方库, 用来在运行时声明static变量, 比如:lazy_static! { static ref NAME: TYPE = EXPR; } 大义是自动实现了Deref trait, 在第一次deref的时候, 执行后面的EXPR, 后面再解引用的时候, 就直接返回第一次的值的引用. trait object 比如下面的代码中, 返回值Arc是个trait object, 和golang的iface差不多的意思. 编译时选择虚拟化平台, 比如选了kvm, kvm的那个new函数返回具体的结构体. pub fn new() -> std::result::Result, HypervisorError> { #[cfg(feature = \"kvm\")] let hv = kvm::KvmHypervisor::new()?; #[cfg(feature = \"mshv\")] let hv = mshv::MshvHypervisor::new()?; Ok(Arc::new(hv)) } 可以在函数定义里干任何事? 比如可以在函数定义里定义结构体, 并实现一个trait fn write_fmt(&mut self, fmt: fmt::Arguments) -> Result { // Create a shim which translates a Write to a fmt::Write and saves // off I/O errors. instead of discarding them struct Adapter { inner: &'a mut T, error: Result, } impl fmt::Write for Adapter { fn write_str(&mut self, s: &str) -> fmt::Result { match self.inner.write_all(s.as_bytes()) { Ok(()) => Ok(()), Err(e) => { self.error = Err(e); Err(fmt::Error) } } } } let mut output = Adapter { inner: self, error: Ok(()) }; match fmt::write(&mut output, fmt) { Ok(()) => Ok(()), Err(..) => { // check if the error came from the underlying `Write` or not if output.error.is_err() { output.error } else { Err(error::const_io_error!(ErrorKind::Uncategorized, \"formatter error\")) } } } } 结构体定义和C对比 同样的结构体, C的定义和rust定义分别如下: struct sock_fprog { unsigned short len; /* Number of BPF instructions */ struct sock_filter *filter; /* Pointer to array of BPF instructions */ }; struct sock_filter { /* Filter block */ __u16 code; /* Actual filter code */ __u8 jt; /* Jump true */ __u8 jf; /* Jump false */ __u32 k; /* Generic multiuse field */ }; rust对应的定义更严谨(啰嗦): /// BPF instruction structure definition. /// See /usr/include/linux/filter.h . #[repr(C)] #[derive(Clone, Debug, PartialEq, Deserialize, Serialize)] #[doc(hidden)] pub struct sock_filter { pub code: ::std::os::raw::c_ushort, pub jt: ::std::os::raw::c_uchar, pub jf: ::std::os::raw::c_uchar, pub k: ::std::os::raw::c_uint, } /// Program made up of a sequence of BPF instructions. pub type BpfProgram = Vec; crate和mod bin文件的例子 firecracker工程下, 有个seccompiler目录: 上面的图中: toml文件里的关键字都是规范定的, 见https://doc.rust-lang.org/cargo/reference/manifest.html Cargo.toml里面说这个seccompiler目录是个crate, 会产生一个seccompiler-bin文件, 产生这个bin的源文件是src/seccompiler_bin.rs; [[bin]]是个表数组, 表示可能会有多个bin. src里面每个文件名都是个mod 在主文件seccompiler_bin.rs里要声明这些mod lib的例子 比如下面这个utils, 是多个工具库的集合. 因为都是库, 就没有一个叫utils.rs的文件 外部crate要引用其中某个库的时候, 用use utils::arg_parser::{ArgParser, Argument, Arguments as ArgumentsBag}; firecracker/src/utils/src/arg_parser.rs代码走读 use 使用了BTreeMap use std::collections::BTreeMap; use std::env; use std::fmt; use std::result; 重定义了Result std::result::Result是个泛型, 是对函数返回值的\"标准\"抽象 pub type Result = result::Result; //这个Error就是下面的Error arg_parser自己的Error定义: 其中每个field基本上都是\"元组结构体\"的形式: /// Errors associated with parsing and validating arguments. #[derive(Debug, PartialEq)] pub enum Error { /// The argument B cannot be used together with argument A. ForbiddenArgument(String, String), /// The required argument was not provided. MissingArgument(String), /// A value for the argument was not provided. MissingValue(String), /// The provided argument was not expected. UnexpectedArgument(String), /// The argument was provided more than once. DuplicateArgument(String), } Error这个enum实现了fmt::Display impl fmt::Display for Error { fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result { use self::Error::*; match *self { ForbiddenArgument(ref arg1, ref arg2) => write!( f, \"Argument '{}' cannot be used together with argument '{}'.\", arg2, arg1 ), MissingArgument(ref arg) => write!(f, \"Argument '{}' required, but not found.\", arg), MissingValue(ref arg) => write!( f, \"The argument '{}' requires a value, but none was supplied.\", arg ), UnexpectedArgument(ref arg) => write!( f, \"Found argument '{}' which wasn't expected, or isn't valid in this context.\", arg ), DuplicateArgument(ref arg) => { write!(f, \"The argument '{}' was provided more than once.\", arg) } } } } 这里面用了write!这个宏, 把一个formated文本写入f. ArgParser对象 ArgParser对象是程序的命令行对象: 这里面一直都带着生命周期标记'a /// Keep information about the argument parser. #[derive(Clone, Default)] pub struct ArgParser { arguments: Arguments, } arguments是个BTree /// Stores the arguments of the parser. #[derive(Clone, Default)] pub struct Arguments { // A BTreeMap in which the key is an argument and the value is its associated `Argument`. args: BTreeMap>, // The arguments specified after `--` (i.e. end of command options). extra_args: Vec, } 再里面的Argument是命令行的option的抽象: /// Stores the characteristics of the `name` command line argument. #[derive(Clone, Debug, PartialEq)] pub struct Argument { name: &'a str, required: bool, requires: Option, forbids: Vec, takes_value: bool, allow_multiple: bool, default_value: Option, help: Option, user_value: Option, } ArgParser对象方法 所有对象方法都包在impl块中: impl ArgParser { } new这个对象: new返回Self本身, 而不能返回借用(&Self), 因为这个函数结束后, 所有local的东西都会被drop, 那显然就没有什么可以借用的. /// Create a new ArgParser instance. pub fn new() -> Self { ArgParser::default() } 从命令行parse 从下面的函数能看到, ArgParser对象虽然只包括Arguments, 但明显没有继承. 所以这里还要显式的转一把: /// Parse the command line arguments. pub fn parse_from_cmdline(&mut self) -> Result { self.arguments.parse_from_cmdline() } 增加arg项 这里用了\"瀑布式\"的函数形式, 入参和出参都是Self类型: 这个过程发生了所有权转移, 这里的mut self入参会导致Self move, 但最后返回的时候又move出去了. /// Add an argument with its associated `Argument` in `arguments`. pub fn arg(mut self, argument: Argument) -> Self { self.arguments.insert_arg(argument); self } 调用形式, 在其他的模块中: 下面的连续.arg()调用, 我理解没有发生Self的拷贝. fn build_arg_parser() -> ArgParser { ArgParser::new() .arg( Argument::new(\"input-file\") .required(true) .takes_value(true) .help(\"File path of the JSON input.\"), ) .arg( Argument::new(\"output-file\") .required(false) .takes_value(true) .default_value(DEFAULT_OUTPUT_FILENAME) .help(\"Optional path of the output file.\"), ) .arg( Argument::new(\"target-arch\") .required(true) .takes_value(true) .help(\"The computer architecture where the BPF program runs. Supported architectures: x86_64, aarch64.\"), ) .arg( Argument::new(\"basic\") .takes_value(false) .help(\"Deprecated! Transforms the filters into basic filters. Drops all argument checks \\ and rule-level actions. Not recommended.\"), ) } 格式化help output // Filter arguments by whether or not it is required. // Align arguments by setting width to length of the longest argument. fn format_arguments(&self, is_required: bool) -> String { let filtered_arguments = self .arguments .args .values() //这是个实体的Values, 实现了Iterator; //但它声明了自己实现了Iterator, 能编译过, 那就\"继承\"了Iterator的其他N多方法 //比如就继承了下面的filter .filter(|arg| is_required == arg.required) .collect::>(); //编译器会自动推导出返回类型是Vec let max_arg_width = filtered_arguments .iter() .map(|arg| arg.format_name().len()) .max() .unwrap_or(0); //因为上面的max函数返回Option类型, 这里unwrap Some层, 取得raw数据. filtered_arguments .into_iter() .map(|arg| arg.format_help(max_arg_width)) .collect::>() .join(\"\\n\") //Vec并没有join方法, 这里自动解引用了, 调用了[String]的join } 上面的Values是stable-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/collections/btree/map.rs定义的结构体 pub struct Values { inner: Iter, } 而且它还实现了Iterator impl Iterator for Values { } 在这里这个泛型结构体被实例化成了struct Values 那么这个struct Values对象就能享受Iterator的一系列方法 那么接下来的filter方法, 返回的Filter依旧是个Iterator fn filter(self, predicate: P) -> Filter where Self: Sized, P: FnMut(&Self::Item) -> bool, { Filter::new(self, predicate) } 这里的Self就是struct Values对象本身, filter函数需要一个predicate(谓语)函数来执行filter的具体动作, 而它的入参是&Self::Item, 那么struct Values的Iterator关联类型是什么呢? 见下面, 是&'a V, 这里实例化后是&Argument impl Iterator for Values { type Item = &'a V; fn next(&mut self) -> Option { self.inner.next().map(|(_, v)| v) } fn size_hint(&self) -> (usize, Option) { self.inner.size_hint() } fn last(mut self) -> Option { self.next_back() } } 那么就能得出: .filter(|arg| is_required == arg.required) 其中: arg是struct Values的关联类型&Argument的借用, 即&&Argument rust有自动解引用机制, 所以arg.required可以直接用 Argument对象 这里全程都带生命周期标记, 怎么看着挺啰嗦的. #[derive(Clone, Debug, PartialEq)] pub struct Argument { name: &'a str, required: bool, requires: Option, forbids: Vec, takes_value: bool, allow_multiple: bool, default_value: Option, help: Option, user_value: Option, } test 这个文件1k多行, 有一般都是test test从#[cfg(test)]开始 包在mod里面: #[cfg(test)] mod tests { use super::*; use crate::arg_parser::Value; //即使在同一个文件, 也要显式引用 } 测试项以#[test]标记, 函数以test_开头 fn test_value() { //Test `as_string()` and `as_flag()` functions behaviour. let mut value = Value::Flag; assert!(Value::as_single_value(&value).is_none()); value = Value::Single(\"arg\".to_string()); assert_eq!(Value::as_single_value(&value).unwrap(), \"arg\"); value = Value::Single(\"arg\".to_string()); assert!(!Value::as_flag(&value)); value = Value::Flag; assert!(Value::as_flag(&value)); } 使用了大量的assert_eq!宏, 比如: assert_eq!( arg_parser.formatted_help(), \"optional arguments:\\n \\ --config-file 'config-file' info.\\n \\ --id 'id' info.\\n \\ --seccomp-filter 'seccomp-filter' info.\" ); 判断Result是否ok assert!(arguments.parse(&args).is_ok()); 没有看到golang类似的benchmark测试 std collections 代码在.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/collections/mod.rs 主要是对其他crate的重新引用: pub use alloc_crate::collections::{binary_heap, btree_map, btree_set}; pub use alloc_crate::collections::{linked_list, vec_deque}; pub use alloc_crate::collections::{BTreeMap, BTreeSet, BinaryHeap}; pub use alloc_crate::collections::{LinkedList, VecDeque}; pub use self::hash_map::HashMap; pub use self::hash_set::HashSet; Rust's collections can be grouped into four major categories: Sequences: Vec, VecDeque, LinkedList Maps: HashMap, BTreeMap Sets: HashSet, BTreeSet Misc: BinaryHeap Sequences性能 get(i) insert(i) remove(i) append split_off(i) Vec O(1) O(n-i) O(n-i) O(m) O(n-i) VecDeque O(1) O(min(i, n-i)) O(min(i, n-i)) O(m) O(min(i, n-i)) LinkedList O(min(i, n-i)) O(min(i, n-i)) O(min(i, n-i)) O(1) O(min(i, n-i)) Maps性能 get insert remove range append HashMap O(1)~ O(1)~ O(1)~ N/A N/A BTreeMap O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n+m) BTreeMap iter() BTreeMap的iter()方法返回Iter结构体 这是个 pub struct Iter { range: LazyLeafRange, K, V>, length: usize, } 它实现了Iterator: #[stable(feature = \"rust1\", since = \"1.0.0\")] impl Iterator for Iter { type Item = (&'a K, &'a V); fn next(&mut self) -> Option { if self.length == 0 { None } else { self.length -= 1; Some(unsafe { self.range.next_unchecked() }) } } fn size_hint(&self) -> (usize, Option) { (self.length, Some(self.length)) } fn last(mut self) -> Option { self.next_back() } fn min(mut self) -> Option { self.next() } fn max(mut self) -> Option { self.next_back() } } BTreeMap的iter使用举例: use std::collections::BTreeMap; let mut map = BTreeMap::new(); map.insert(3, \"c\"); map.insert(2, \"b\"); map.insert(1, \"a\"); for (key, value) in map.iter() { println!(\"{}: {}\", key, value); } let (first_key, first_value) = map.iter().next().unwrap(); assert_eq!((*first_key, *first_value), (1, \"a\")); keys()和values() BTreeMap还有keys()和values()方法, 分别返回keys和values. 比如: let mut a = BTreeMap::new(); a.insert(2, \"b\"); a.insert(1, \"a\"); let keys = a.keys(); let values = a.values(); println!(\"{:?}\", keys); println!(\"{:?}\", values); 结果: [1, 2] [\"a\", \"b\"] key就是key, value就只有value, 很通顺. 但实际上, keys()和values()方法分别返回Keys和Values结构体, 而他们的内部都是inner: Iter pub struct Keys { inner: Iter, } pub struct Values { inner: Iter, } 区别在于它们各自实现的迭代器不同, 比如: impl Iterator for Values { type Item = &'a V; fn next(&mut self) -> Option { self.inner.next().map(|(_, v)| v) //把(k,v)map成v } fn size_hint(&self) -> (usize, Option) { self.inner.size_hint() } fn last(mut self) -> Option { self.next_back() } } "},"notes/rust_adaptiveservice.html":{"url":"notes/rust_adaptiveservice.html","title":"rust版本的adaptiveservice探索","keywords":"","body":" 5种\"动态\"类型 rust的泛型是静态的 静态分发 动态分发 即trait objects 从指针获取trait objects adaptiveservice是我用go写的微服务消息框架, 其核心之一是用了go的反射来给每个数据struct绑定一个handler方法. if st.svc.canHandle(tm.msg) { mm := &metaKnownMsg{ stream: ss, msg: tm.msg.(KnownMessage), } mq.putMetaMsg(mm) } else { ss.privateChan 在rust里面没有反射, 如何实现从字节流数据(stream buffer)到具体结构体的生成? 生成的数据结构体能否\"断言\"成实现了KnownMessage的trait? 先看看rust从类型上提供了什么语义, 这些语义能干什么. 5种\"动态\"类型 use std::any::Any; struct Header { uuid: u64, protocol: String } // 第一种, 静态类型, 泛型是静态分发的, 即在编译的时候就生成好了\"类型化\"的代码. // statically typed, no pointer dereference struct GenericPacket { header: Header, data: T } // 第二种, 用Any，任何类型都实现了Any // uses the \"Any\" type to have dynamic typing struct AnyPacket { header: Header, data: Any, } // 第三种，用enum穷尽所有可能的类型 // uses an enum to capture the differnet possible types enum DataEnum { Integer(i32), Float(f32) } struct EnumPacket { header: Header, data: DataEnum, } // 第四种: 用trait object trait DataTrait { // interface your data conforms to } struct TraitPacket { header: Header, data: &'a dyn DataTrait, // uses a pointer dereference to something that implements DataTrait } // 第五种: 和第一种类型类似, 但是带具体方法的trait // statically typed, but will accept any type that conforms to DataTrait struct StaticTraitPacket { header: Header, data: T, } rust的泛型是静态的 参考: https://www.cs.brandeis.edu/~cs146a/rust/doc-02-21-2015/book/static-and-dynamic-dispatch.html 比如下面的trait: trait Foo { fn method(&self) -> String; } impl Foo for u8 { fn method(&self) -> String { format!(\"u8: {}\", *self) } } impl Foo for String { fn method(&self) -> String { format!(\"string: {}\", *self) } } 静态分发 比如下面的代码: fn do_something(x: T) { x.method(); } fn main() { let x = 5u8; let y = \"Hello\".to_string(); do_something(x); do_something(y); } rust会在编译阶段就知道do_something()的入参类型, 会静态的生成类型下面的静态代码: fn do_something_u8(x: u8) { x.method(); } fn do_something_string(x: String) { x.method(); } fn main() { let x = 5u8; let y = \"Hello\".to_string(); do_something_u8(x); do_something_string(y); } 虽然\"静态分发\"会有一定的代码\"重复\"带来代码段的膨胀, 但一般问题不大, 直接函数调用的性能会好点, 比如可以被inline优化 动态分发 即trait objects Rust provides dynamic dispatch through a feature called trait objects. Trait objects, like &Foo or Box, are normal values that store a value of any type that implements the given trait, where the precise type can only be known at runtime. The methods of the trait can be called on a trait object via a special record of function pointers (created and managed by the compiler). A function that takes a trait object is not specialised to each of the types that implements Foo: only one copy is generated, often (but not always) resulting in less code bloat. However, this comes at the cost of requiring slower virtual function calls, and effectively inhibiting any chance of inlining and related optimisations from occurring. Trait objects are both simple and complicated: their core representation and layout is quite straight-forward, but there are some curly error messages and surprising behaviours to discover. 从指针获取trait objects There's two similar ways to get a trait object value: casts and coercions. If T is a type that implements a trait Foo (e.g. u8 for the Foo above), then the two ways to get a Foo trait object out of a pointer to T look like: let ref_to_t: &T = ...; // `as` keyword for casting let cast = ref_to_t as &Foo; // using a `&T` in a place that has a known type of `&Foo` will implicitly coerce: let coerce: &Foo = ref_to_t; fn also_coerce(_unused: &Foo) {} also_coerce(ref_to_t); These trait object coercions and casts also work for pointers like &mut T to &mut Foo and Box to Box, but that's all at the moment. Coercions and casts are identical. This operation can be seen as \"erasing\" the compiler's knowledge about the specific type of the pointer, and hence trait objects are sometimes referred to \"type erasure\" "},"notes/rust_常用设施.html":{"url":"notes/rust_常用设施.html","title":"Rust 常用设施","keywords":"","body":" structopt log env_logger 常用宏1 常用宏2 由编译器实现的builtin宏 trait Iterator 比较常用的Iterator方法 collect IntoIterator 数组泛型的方法impl [T] Vec Option Result Result的方法: Result的方法 其他Result Result实现了如下的trait env 取消impl trait 文件 使用举例 写文件 读文件到String 更有效率的读文件 不同选项open &File也能modify 文件 File对象 常用函数 File的方法 其他impl的trait 实现Read trait 实现Write trait 实现Seek trait Read Write Seek for &File又来一遍!!!!! open最后调用 IO 常用函数 Read trait Write trait Seek trait BufRead是Read的派生trait structopt structopt用来把命令行参数转成结构体 定义结构体的时候, 用structopt来\"标记\", 比如: #[derive(Clone, Debug, StructOpt)] #[structopt(name = \"virtiofsd backend\", about = \"Launch a virtiofsd backend.\")] struct Opt { /// Shared directory path #[structopt(long)] shared_dir: Option, /// vhost-user socket path [deprecated] #[structopt(long, required_unless_one = &[\"fd\", \"socket-path\", \"print-capabilities\"])] socket: Option, /// vhost-user socket path #[structopt(long = \"socket-path\", required_unless_one = &[\"fd\", \"socket\", \"print-capabilities\"])] socket_path: Option, ... } 注: structopt已经停止开发, 建议使用clap: Command Line Argument Parser for Rust log https://crates.io/crates/log 要在cargo.toml里面声明依赖: [dependencies] log = \"0.4\" 这个库设计的很合理. 对用户提供几个log的宏: error!, warn!, info!, debug! and trace! lib里面, 只使用这几个输出宏 bin里面, 负责初始化后端的logging实现, 如果没有初始化, 那上面的几个输出宏就类似noop 使用set_boxed_logger Sets the global logger to a Box. 或set_logger Sets the global logger to a &'static Log. 可选的logging实现有: Simple minimal loggers: env_logger simple_logger simplelog pretty_env_logger stderrlog flexi_logger Complex configurable frameworks: log4rs fern Adaptors for other facilities: syslog slog-stdlog systemd-journal-logger android_log win_dbg_logger [db_logger] For WebAssembly binaries: console_log For dynamic libraries: You may need to construct an FFI-safe wrapper over log to initialize in your libraries env_logger 比如这样在代码里: use log::{debug, error, log_enabled, info, Level}; env_logger::init(); debug!(\"this is a debug {}\", \"message\"); error!(\"this is printed by default\"); if log_enabled!(Level::Info) { let x = 3 * 4; // expensive computation info!(\"the answer was: {}\", x); } 使用时: $ RUST_LOG=debug ./main [2017-11-09T02:12:24Z DEBUG main] this is a debug message [2017-11-09T02:12:24Z ERROR main] this is printed by default [2017-11-09T02:12:24Z INFO main] the answer was: 12 可以按module来指定level $ RUST_LOG=main=info ./main [2017-11-09T02:12:24Z ERROR main] this is printed by default [2017-11-09T02:12:24Z INFO main] the answer was: 12 又比如在virtiofsd里面是这样用的: fn set_default_logger(log_level: LevelFilter) { if env::var(\"RUST_LOG\").is_err() { env::set_var(\"RUST_LOG\", log_level.to_string()); } env_logger::init(); } 常用宏1 代码在lib/rustlib/src/rust/library/std/src/macros.rs panic print println eprint eprintln dbg 常用宏2 代码在lib/rustlib/src/rust/library/core/src/macros/mod.rs panic!panic!(); panic!(\"this is a {} {message}\", \"fancy\", message = \"message\"); assert_eq! assert_ne!assert_eq!(a, b); // a b是两个表达式 assert_ne!(a, b); assert_matches!assert_matches!(a, Some(_)); assert_matches!(b, None); let c = Ok(\"abc\".to_string()); assert_matches!(c, Ok(x) | Err(x) if x.len() debug_assert! debug_assert_eq! debug_assert_ne! 只有在debug版本里才使能debug_assert!(true); matches!let foo = 'f'; assert!(matches!(foo, 'A'..='Z' | 'a'..='z')); let bar = Some(4); assert!(matches!(bar, Some(x) if x > 2)); ?和r#try!try!是个宏, 但需要用raw方式来调用, r#try. 现在可以用?来代替enum MyError { FileWriteError } impl From for MyError { fn from(e: io::Error) -> MyError { MyError::FileWriteError } } // The preferred method of quick returning Errors fn write_to_file_question() -> Result { let mut file = File::create(\"my_best_friends.txt\")?; file.write_all(b\"This is a list of my best friends.\")?; Ok(()) } // The previous method of quick returning Errors fn write_to_file_using_try() -> Result { let mut file = r#try!(File::create(\"my_best_friends.txt\")); r#try!(file.write_all(b\"This is a list of my best friends.\")); Ok(()) } write! writeln! 写入bufferfn main() -> std::io::Result { let mut w = Vec::new(); write!(&mut w, \"test\")?; write!(&mut w, \"formatted {}\", \"arguments\")?; assert_eq!(w, b\"testformatted arguments\"); Ok(()) } let mut s = String::new(); writeln!(&mut s, \"{} {}\", \"abc\", 123)?; // uses fmt::Write::write_fmt unreachable! unimplemented! todo! 代码实现阶段用到的宏 由编译器实现的builtin宏 compile_error!#[cfg(not(any(feature = \"foo\", feature = \"bar\")))] compile_error!(\"Either feature \\\"foo\\\" or \\\"bar\\\" must be enabled for this crate.\"); format_args! const_format_args! format_args_nl! 格式化宏, 用于format!宏 env! option_env! 在编译时获取env, 注意不是运行时let path: &'static str = env!(\"PATH\"); let key: Option = option_env!(\"SECRET_KEY\"); concat_idents! 多个标识符连起来成为一个fn foobar() -> u32 { 23 } let f = concat_idents!(foo, bar); println!(\"{}\", f()); concat_bytes! 连接字符 concat!let s = concat!(\"test\", 10, 'b', true); assert_eq!(s, \"test10btrue\"); line! column! file! 编译的文件, 行号等; module_path! module路径let current_line = line!(); println!(\"defined on line: {}\", current_line); stringify!let one_plus_one = stringify!(1 + 1); assert_eq!(one_plus_one, \"1 + 1\"); include_str! 编译时从文件读入string; include_bytes! 编译时从文件读入bytes; 文件是相对当前编译文件的路径.//spanish.in里面是adiós let my_str = include_str!(\"spanish.in\"); assert_eq!(my_str, \"adiós\\n\"); cfg!let my_directory = if cfg!(windows) { \"windows-specific-directory\" } else { \"unix-directory\" }; include! 把文件导入进来按表达式来编译 assert! derive! test bench global_allocator cfg_accessible trait Iterator 看起来只要实现了next就是个Iterator了... 其他都有默认实现, 真方便 pub trait Iterator { type Item; //需要用户实现的: fn next(&mut self) -> Option; //有默认实现的: fn size_hint(&self) -> (usize, Option) //默认返回0, 用户要自己实现更适合自己的方法. fn count(self) -> usize fn last(self) -> Option fn advance_by(&mut self, n: usize) -> Result fn nth(&mut self, n: usize) -> Option fn step_by(self, step: usize) -> StepBy fn chain(self, other: U) -> Chain fn zip(self, other: U) -> Zip fn intersperse(self, separator: Self::Item) -> Intersperse fn intersperse_with(self, separator: G) -> IntersperseWith fn map(self, f: F) -> Map fn for_each(self, f: F) fn filter(self, predicate: P) -> Filter fn filter_map(self, f: F) -> FilterMap fn enumerate(self) -> Enumerate //还是返回一个Iterator, 元素是(index, value) fn peekable(self) -> Peekable fn skip_while(self, predicate: P) -> SkipWhile fn take_while(self, predicate: P) -> TakeWhile fn map_while(self, predicate: P) -> MapWhile fn skip(self, n: usize) -> Skip fn take(self, n: usize) -> Take fn scan(self, initial_state: St, f: F) -> Scan fn flat_map(self, f: F) -> FlatMap fn flatten(self) -> Flatten fn fuse(self) -> Fuse fn inspect(self, f: F) -> Inspect fn by_ref(&mut self) -> &mut Self fn collect>(self) -> B fn try_collect(&mut self) -> ChangeOutputType fn partition(self, f: F) -> (B, B) fn partition_in_place(mut self, ref mut predicate: P) -> usize fn is_partitioned(mut self, mut predicate: P) -> bool fn try_fold(&mut self, init: B, mut f: F) -> R fn try_for_each(&mut self, f: F) -> R fn fold(mut self, init: B, mut f: F) -> B fn reduce(mut self, f: F) -> Option fn try_reduce(&mut self, f: F) -> ChangeOutputType> fn all(&mut self, f: F) -> bool fn any(&mut self, f: F) -> bool fn find(&mut self, predicate: P) -> Option fn find_map(&mut self, f: F) -> Option fn try_find(&mut self, f: F) -> ChangeOutputType> fn position(&mut self, predicate: P) -> Option fn rposition(&mut self, predicate: P) -> Option fn max(self) -> Option fn min(self) -> Option fn max_by_key(self, f: F) -> Option fn max_by(self, compare: F) -> Option fn min_by_key(self, f: F) -> Option fn min_by(self, compare: F) -> Option fn rev(self) -> Rev fn unzip(self) -> (FromA, FromB) fn copied(self) -> Copied fn cloned(self) -> Cloned fn cycle(self) -> Cycle fn sum(self) -> S fn product(self) -> P fn cmp(self, other: I) -> Ordering fn cmp_by(mut self, other: I, mut cmp: F) -> Ordering fn partial_cmp(self, other: I) -> Option fn partial_cmp_by(mut self, other: I, mut partial_cmp: F) -> Option fn eq(self, other: I) -> bool fn eq_by(mut self, other: I, mut eq: F) -> bool fn ne(self, other: I) -> bool fn lt(self, other: I) -> bool fn le(self, other: I) -> bool fn gt(self, other: I) -> bool fn ge(self, other: I) -> bool fn is_sorted(self) -> bool fn is_sorted_by(mut self, compare: F) -> bool fn is_sorted_by_key(self, f: F) -> bool } 比较常用的Iterator方法 filter: 对Self的关联类型的借用&Self::Item调用闭包函数, 返回另一个Iterator map: 也是返回另一个Iterator 最后collect: 把Iterator\"重组\"成一个collect对象. collect filter在前面讲过. 这里看一下collect: collect基础用法如下: let a = [1, 2, 3]; let doubled: Vec = a.iter() .map(|&x| x * 2) .collect(); assert_eq!(vec![2, 4, 6], doubled); 注意, 目标变量doubled需要显式指定类型, 要不然collect不知道你要\"重组\"成什么样的collect对象. 常用的就是collect成Vec. 下面是Iterator的默认collect实现: //这里面很晦涩, collect返回一个泛型B, 这个B是要满足`FromIterator`即Iterator的关联类型实例化的`FromIterator` //这个B是编译器自己推断的, 或者根据左值(比如上面的let doubled: Vec), 或者用户指定, 比如更上面的.collect::>(); fn collect>(self) -> B where Self: Sized, { //这里trait名称::trait函数这个调用方式看起来无比奇怪, 有点自己调用自己的意思 //但我理解下面的FromIterator已经是个具体的类型, 由编译器自动推导出来的: //比如上面的左值let doubled: Vec, 到这里就应该是调用Vec的from_iter FromIterator::from_iter(self) } //这里是说要想满足FromIterator这个trait, 就必须实现from_iter这个函数; //而from_iter这个函数入参是个满足泛型T约束的iter, 这个T需要是个IntoIterator(一般的容器类型(collect类型)都实现了IntoIterator) pub trait FromIterator: Sized { fn from_iter>(iter: T) -> Self; } 到这里就清楚了, 对左值let doubled: Vec的用.collect方法生成的情况, 最后调用的是Vec的from_iter()函数 impl FromIterator for Vec { #[inline] fn from_iter>(iter: I) -> Vec { >::from_iter(iter.into_iter()) } } 这里把Self转换成了SpecFromIter, 实例化后是SpecFromIter 而这里的I::IntoIter其实就是变量a的类型vec的IntoIter SpecFromIter说的是要干一件从IntoIter I到Self的事. pub(super) trait SpecFromIter { fn from_iter(iter: I) -> Self; } impl SpecFromIter for Vec where I: Iterator, { default fn from_iter(iterator: I) -> Self { SpecFromIterNested::from_iter(iterator) } } 最后调用到: pub(super) trait SpecFromIterNested { fn from_iter(iter: I) -> Self; } impl SpecFromIterNested for Vec where I: Iterator, { default fn from_iter(mut iterator: I) -> Self { // Unroll the first iteration, as the vector is going to be // expanded on this iteration in every case when the iterable is not // empty, but the loop in extend_desugared() is not going to see the // vector being full in the few subsequent loop iterations. // So we get better branch prediction. let mut vector = match iterator.next() { None => return Vec::new(), Some(element) => { let (lower, _) = iterator.size_hint(); let initial_capacity = cmp::max(RawVec::::MIN_NON_ZERO_CAP, lower.saturating_add(1)); let mut vector = Vec::with_capacity(initial_capacity); unsafe { // SAFETY: We requested capacity at least 1 ptr::write(vector.as_mut_ptr(), element); vector.set_len(1); } vector } }; // must delegate to spec_extend() since extend() itself delegates // to spec_from for empty Vecs as SpecExtend>::spec_extend(&mut vector, iterator); vector } } IntoIterator IntoIterator的关联类型type IntoIter是个trait, 即这个关联类型的具体类型要符合Iterator约束. pub trait IntoIterator { /// The type of the elements being iterated over. type Item; /// Which kind of iterator are we turning this into? type IntoIter: Iterator; fn into_iter(self) -> Self::IntoIter; } 自己实现IntoIterator // A sample collection, that's just a wrapper over Vec #[derive(Debug)] struct MyCollection(Vec); // Let's give it some methods so we can create one and add things // to it. impl MyCollection { fn new() -> MyCollection { MyCollection(Vec::new()) } fn add(&mut self, elem: i32) { self.0.push(elem); } } // and we'll implement IntoIterator impl IntoIterator for MyCollection { type Item = i32; type IntoIter = std::vec::IntoIter; //注意这里实例化了IntoIter类型 fn into_iter(self) -> Self::IntoIter { self.0.into_iter() } } // Now we can make a new collection... let mut c = MyCollection::new(); // ... add some stuff to it ... c.add(0); c.add(1); c.add(2); // ... and then turn it into an Iterator: for (i, n) in c.into_iter().enumerate() { assert_eq!(i as i32, n); } 比如BTreeMap就实现了into_iter的方法 impl IntoIterator for &'a BTreeMap { type Item = (&'a K, &'a V); type IntoIter = Iter; fn into_iter(self) -> Iter { self.iter() } } 可以看到BTreeMap的into_iter其实就是self.iter(), 反回的都是Iter这个结构体(这个结构体实现了Iterator). 数组泛型的方法impl [T] 数组泛型实现了多种方法, 比如join. impl [T] { pub fn sort(&mut self) where T: Ord, pub fn sort_by(&mut self, mut compare: F) where F: FnMut(&T, &T) -> Ordering, pub fn sort_by_key(&mut self, mut f: F) where F: FnMut(&T) -> K, K: Ord, pub fn sort_by_cached_key(&mut self, f: F) where F: FnMut(&T) -> K, K: Ord, pub fn to_vec(&self) -> Vec where T: Clone, pub fn to_vec_in(&self, alloc: A) -> Vec where T: Clone, pub fn into_vec(self: Box) -> Vec pub fn repeat(&self, n: usize) -> Vec where T: Copy, pub fn concat(&self) -> >::Output where Self: Concat, pub fn join(&self, sep: Separator) -> >::Output where Self: Join, //这里首先约束Self是Join { Join::join(self, sep) //这里调用了约束的join函数. } pub fn connect(&self, sep: Separator) -> >::Output where Self: Join, } 但因为T是泛型, 要实现有用的方法, 比如对T进行约束. 比如join就要求[T]满足:Self: Join pub trait Join { /// The resulting type after concatenation type Output; /// Implementation of [`[T]::join`](slice::join) fn join(slice: &Self, sep: Separator) -> Self::Output; } 注意上面的Separator是泛型的类型指示符, 指代具体类型. 上面的例子非常有趣, 泛型[T]实现了join方法, 而这个join方法看起来又调用了\"Self\"的join. 是Self有两套join实现吗? -- 是. [T]有join方法, 没毛病. 而[V]实现了Join trait, 也实现了join函数. 如下: impl> Join for [V] { type Output = Vec; fn join(slice: &Self, sep: &T) -> Vec { let mut iter = slice.iter(); let first = match iter.next() { Some(first) => first, None => return vec![], }; let size = slice.iter().map(|v| v.borrow().len()).sum::() + slice.len() - 1; let mut result = Vec::with_capacity(size); result.extend_from_slice(first.borrow()); for v in iter { result.push(sep.clone()); result.extend_from_slice(v.borrow()) } result } } 这是两套语义, rust并没有因为看见Self有join方法, 就像go一样, 自动推断Self实现了Join trait; 相反的, 用户需要明确声明, 我实现了Join trait(for 关键词). 这种情况下, 会同时存在两套join. 在本例中, 前者还调用了后者. 这不是多此一举吗? 一个同名的join调来调去有意思吗? --不是. 有意思. 因为Separator不同, 实际调用的Join trait也不同. 比如如果Separator是&T, 就需要实现Join的trait. 代码见上面 如果Separator是&[T], 就需要实现Join的trait, 如下: impl> Join for [V] { type Output = Vec; fn join(slice: &Self, sep: &[T]) -> Vec { let mut iter = slice.iter(); let first = match iter.next() { Some(first) => first, None => return vec![], }; let size = slice.iter().map(|v| v.borrow().len()).sum::() + sep.len() * (slice.len() - 1); let mut result = Vec::with_capacity(size); result.extend_from_slice(first.borrow()); for v in iter { result.extend_from_slice(sep); result.extend_from_slice(v.borrow()) } result } } 比如字符串的: impl> Join for [S] { type Output = String; fn join(slice: &Self, sep: &str) -> String { unsafe { String::from_utf8_unchecked(join_generic_copy(slice, sep.as_bytes())) } } } Vec Vec结构体: pub struct Vec { buf: RawVec, len: usize, } 方法: impl Vec { pub const fn new() -> Self pub fn with_capacity(capacity: usize) -> Self pub unsafe fn from_raw_parts(ptr: *mut T, length: usize, capacity: usize) -> Self } 带allocator的Vec有更多的方法: impl Vec { pub const fn new_in(alloc: A) -> Self pub fn with_capacity_in(capacity: usize, alloc: A) -> Self pub fn capacity(&self) -> usize pub fn reserve(&mut self, additional: usize) pub fn try_reserve(&mut self, additional: usize) -> Result pub fn shrink_to_fit(&mut self) pub fn shrink_to(&mut self, min_capacity: usize) pub fn into_boxed_slice(mut self) -> Box pub fn truncate(&mut self, len: usize) pub fn as_slice(&self) -> &[T] pub fn as_mut_slice(&mut self) -> &mut [T] pub fn as_ptr(&self) -> *const T pub fn allocator(&self) -> &A pub unsafe fn set_len(&mut self, new_len: usize) pub fn swap_remove(&mut self, index: usize) -> T pub fn insert(&mut self, index: usize, element: T) pub fn remove(&mut self, index: usize) -> T pub fn retain(&mut self, mut f: F) pub fn dedup_by(&mut self, mut same_bucket: F) pub fn push(&mut self, value: T) pub fn pop(&mut self) -> Option pub fn append(&mut self, other: &mut Self) pub fn drain(&mut self, range: R) -> Drain //返回一个iterator, 包括range内的所有元素 pub fn clear(&mut self) //移出所有元素 pub fn len(&self) -> usize pub fn is_empty(&self) -> bool pub fn split_off(&mut self, at: usize) -> Self pub fn resize_with(&mut self, new_len: usize, f: F) pub fn leak(self) -> &'a mut [T] } 更具化的泛型 impl Vec { pub fn resize(&mut self, new_len: usize, value: T) pub fn extend_from_slice(&mut self, other: &[T]) pub fn extend_from_within(&mut self, src: R) } Vec实现了解引用 impl ops::Deref for Vec { type Target = [T]; fn deref(&self) -> &[T] { unsafe { slice::from_raw_parts(self.as_ptr(), self.len) } } } Option Option的方法如下: impl Option { pub const fn is_some(&self) -> bool pub fn is_some_with(&self, f: impl FnOnce(&T) -> bool) -> bool pub const fn is_none(&self) -> bool pub const fn as_ref(&self) -> Option pub const fn as_mut(&mut self) -> Option pub const fn as_pin_ref(self: Pin) -> Option> pub const fn as_pin_mut(self: Pin) -> Option> pub const fn expect(self, msg: &str) -> T pub const fn unwrap(self) -> T pub const fn unwrap_or(self, default: T) -> T pub const fn map(self, f: F) -> Option pub const fn filter(self, predicate: P) -> Self pub const fn inspect(self, f: F) -> Self pub const fn ok_or(self, err: E) -> Result pub const fn iter(&self) -> Iter pub const fn and_then(self, f: F) -> Option pub const fn and(self, optb: Option) -> Option pub const fn or(self, optb: Option) -> Option pub const fn or_else(self, f: F) -> Option pub const fn xor(self, optb: Option) -> Option pub const fn insert(&mut self, value: T) -> &mut T pub const fn get_or_insert(&mut self, value: T) -> &mut T pub const fn take(&mut self) -> Option pub const fn replace(&mut self, value: T) -> Option pub const fn contains(&self, x: &U) -> bool pub const fn zip(self, other: Option) -> Option } Result Result是经常用的rust抽象, 用于返回值的处理, 很优雅. 代码在lib/rustlib/src/rust/library/core/src/result.rs #[derive(Copy, PartialEq, PartialOrd, Eq, Ord, Debug, Hash)] pub enum Result { /// Contains the success value #[lang = \"Ok\"] Ok(#[stable(feature = \"rust1\", since = \"1.0.0\")] T), /// Contains the error value #[lang = \"Err\"] Err(#[stable(feature = \"rust1\", since = \"1.0.0\")] E), } Result的方法: impl Result { pub const fn is_ok(&self) -> bool /// let x: Result = Ok(2); /// assert_eq!(x.is_ok_with(|&x| x > 1), true); /// /// let x: Result = Ok(0); /// assert_eq!(x.is_ok_with(|&x| x > 1), false); /// /// let x: Result = Err(\"hey\"); /// assert_eq!(x.is_ok_with(|&x| x > 1), false); pub fn is_ok_with(&self, f: impl FnOnce(&T) -> bool) -> bool pub const fn is_err(&self) -> bool /// let x: Result = Err(Error::new(ErrorKind::NotFound, \"!\")); /// assert_eq!(x.is_err_with(|x| x.kind() == ErrorKind::NotFound), true); /// /// let x: Result = Err(Error::new(ErrorKind::PermissionDenied, \"!\")); /// assert_eq!(x.is_err_with(|x| x.kind() == ErrorKind::NotFound), false); /// /// let x: Result = Ok(123); /// assert_eq!(x.is_err_with(|x| x.kind() == ErrorKind::NotFound), false); pub fn is_err_with(&self, f: impl FnOnce(&E) -> bool) -> bool pub fn ok(self) -> Option //把Result转换为Option pub fn err(self) -> Option pub const fn as_ref(&self) -> Result pub const fn as_mut(&mut self) -> Result //调用F把Result转为Result pub fn map U>(self, op: F) -> Result pub fn map_or U>(self, default: U, f: F) -> U pub fn map_or_else U, F: FnOnce(T) -> U>(self, default: D, f: F) -> U pub fn map_err F>(self, op: O) -> Result pub fn inspect(self, f: F) -> Self pub fn inspect_err(self, f: F) -> Self /// let x: Result = Ok(\"hello\".to_string()); /// let y: Result = Ok(\"hello\"); /// assert_eq!(x.as_deref(), y); /// /// let x: Result = Err(42); /// let y: Result = Err(&42); pub fn as_deref(&self) -> Result where T: Deref, pub fn iter(&self) -> Iter pub fn iter_mut(&mut self) -> IterMut //返回Ok里面的T, 如果Error就panic pub fn expect(self, msg: &str) -> T where E: fmt::Debug, // unwrap也返回T, 也可能panic, 但似乎就没有panic message pub fn unwrap(self) -> T where E: fmt::Debug, // 也是unwrap, 但不panic, 不Ok就返回default pub fn unwrap_or_default(self) -> T where T: Default, //unwrap不panic, 不ok则返回指定的default pub fn unwrap_or(self, default: T) -> T pub fn unwrap_or_else T>(self, op: F) -> T // 实际上是unwrap E pub fn expect_err(self, msg: &str) -> E where T: fmt::Debug, pub fn unwrap_err(self) -> E where T: fmt::Debug, pub fn into_ok(self) -> T where E: Into, pub fn into_err(self) -> E where T: Into, // and表示检测x, y中只要有Error, 就返回Error /// let x: Result = Ok(2); /// let y: Result = Err(\"late error\"); /// assert_eq!(x.and(y), Err(\"late error\")); pub fn and(self, res: Result) -> Result /// assert_eq!(Ok(2).and_then(sq_then_to_string), Ok(4.to_string())); /// assert_eq!(Err(\"not a number\").and_then(sq_then_to_string), Err(\"not a number\")); pub fn and_then Result>(self, op: F) -> Result pub fn or(self, res: Result) -> Result pub fn or_else Result>(self, op: O) -> Result /// let x: Result = Ok(2); /// assert_eq!(x.contains(&2), true); /// /// let x: Result = Ok(3); /// assert_eq!(x.contains(&2), false); /// /// let x: Result = Err(\"Some error message\"); /// assert_eq!(x.contains(&2), false); pub fn contains(&self, x: &U) -> bool where U: PartialEq, } 注: const fn表示这个fn可以用在const上下文中 Result的方法 Result也能调用Result的方法, 比如下面copied函数中, 直接用了self.map, 因为Result是个范围很广的泛型, 自然也就包括了Result, 这里要把T看成是&T' impl Result { pub fn copied(self) -> Result where T: Copy, { self.map(|&t| t) //注意这里, map的F是FnOnce(T), 是针对Result来说的; 这里应该传入\"&T\", 那么形式上&t就是T, 然后返回t就是返回T. 看起来挺难理解的... } pub fn cloned(self) -> Result where T: Clone, { self.map(|t| t.clone()) //这里就没有那么绕, t就是&T } } 其他Result impl Result impl Result, E> impl Result, E> impl Result Result实现了如下的trait impl Clone for Result impl IntoIterator for Result impl IntoIterator for &'a Result impl IntoIterator for &'a mut Result env // 这个就是golang的os.args let args: Vec = env::args().collect(); 取消impl trait 就是取消 取消 取消! impl !Send for Args {} impl !Sync for Args {} 文件 lib/rustlib/src/rust/library/std/src/fs.rs 使用举例 写文件 use std::fs::File; use std::io::prelude::*; fn main() -> std::io::Result { let mut file = File::create(\"foo.txt\")?; file.write_all(b\"Hello, world!\")?; Ok(()) } 读文件到String use std::fs::File; use std::io::prelude::*; fn main() -> std::io::Result { let mut file = File::open(\"foo.txt\")?; let mut contents = String::new(); file.read_to_string(&mut contents)?; assert_eq!(contents, \"Hello, world!\"); Ok(()) } 注意这里, file.read_to_string(), file对象impl Read, 有read_to_string()方法, 就在本文件; 而vscode点击进去, 却是lib/rustlib/src/rust/library/std/src/io/mod.rs中io:Read这个trait的默认实现. 更有效率的读文件 use std::fs::File; use std::io::BufReader; use std::io::prelude::*; fn main() -> std::io::Result { let file = File::open(\"foo.txt\")?; let mut buf_reader = BufReader::new(file); let mut contents = String::new(); buf_reader.read_to_string(&mut contents)?; assert_eq!(contents, \"Hello, world!\"); Ok(()) } 不同选项open let file = OpenOptions::new() .read(true) .write(true) .create(true) .open(\"foo.txt\"); &File也能modify 文件 Note that, although read and write methods require a &mut File, because of the interfaces for [Read] and [Write], the holder of a &File can still modify the file, either through methods that take &File or by retrieving the underlying OS object and modifying the file that way. Additionally, many operating systems allow concurrent modification of files by different processes. Avoid assuming that holding a &File means that the file will not change. 这里是说read和write方法都要求&mut File, 但实际上, &File的持有者也可以修改文件. 这里提醒大家不要认为持有&File的人就\"没危险\"了, 他们也可以修改你的文件, 因为文件系统允许多进程打开一个文件. File对象 pub struct File { inner: fs_imp::File, } 还有几个\"辅助\"元组对象定义: pub struct Metadata(fs_imp::FileAttr); pub struct ReadDir(fs_imp::ReadDir); pub struct DirEntry(fs_imp::DirEntry); pub struct OpenOptions(fs_imp::OpenOptions); pub struct Permissions(fs_imp::FilePermissions); pub struct FileType(fs_imp::FileType); pub struct DirBuilder { inner: fs_imp::DirBuilder, recursive: bool, } 常用函数 读所有文件内容到Vec pub fn read>(path: P) -> io::Result>use std::fs; use std::net::SocketAddr; fn main() -> Result> { let foo: SocketAddr = String::from_utf8_lossy(&fs::read(\"address.txt\")?).parse()?; Ok(()) } 读所有文件内容到String pub fn read_to_string>(path: P) -> io::Resultuse std::fs; use std::net::SocketAddr; use std::error::Error; fn main() -> Result> { let foo: SocketAddr = fs::read_to_string(\"address.txt\")?.parse()?; Ok(()) } 简单的写slice到文件 pub fn write, C: AsRef>(path: P, contents: C) -> io::Resultuse std::fs; fn main() -> std::io::Result { fs::write(\"foo.txt\", b\"Lorem ipsum\")?; //&str可以被认为是AsRef fs::write(\"bar.txt\", \"dolor sit\")?; Ok(()) } remove文件 pub fn remove_file>(path: P) -> io::Resultuse std::fs; fn main() -> std::io::Result { fs::remove_file(\"a.txt\")?; Ok(()) } metadatafn main() -> std::io::Result { let attr = fs::metadata(\"/some/file/path.txt\")?; // inspect attr ... Ok(()) } symlink_metadata rename copy hard_link soft_link read_link canonicalize 可能和abs path差不多 create_dir create_dir_all remove_dir remove_dir_all read_dir 例子1use std::io; use std::fs::{self, DirEntry}; use std::path::Path; // one possible implementation of walking a directory only visiting files fn visit_dirs(dir: &Path, cb: &dyn Fn(&DirEntry)) -> io::Result { if dir.is_dir() { for entry in fs::read_dir(dir)? { let entry = entry?; let path = entry.path(); if path.is_dir() { visit_dirs(&path, cb)?; } else { cb(&entry); } } } Ok(()) } 例子2use std::{fs, io}; fn main() -> io::Result { let mut entries = fs::read_dir(\".\")? .map(|res| res.map(|e| e.path())) .collect::, io::Error>>()?; // The order in which `read_dir` returns entries is not guaranteed. If reproducible // ordering is required the entries should be explicitly sorted. entries.sort(); // The entries have now been sorted by their path. Ok(()) } set_permissionsuse std::fs; fn main() -> std::io::Result { let mut perms = fs::metadata(\"foo.txt\")?.permissions(); perms.set_readonly(true); fs::set_permissions(\"foo.txt\", perms)?; Ok(()) } File的方法 impl File { } open pub fn open>(path: P) -> io::Resultuse std::fs::File; fn main() -> std::io::Result { let mut f = File::open(\"foo.txt\")?; Ok(()) } create pub fn create>(path: P) -> io::Resultuse std::fs::File; fn main() -> std::io::Result { let mut f = File::create(\"foo.txt\")?; Ok(()) } options pub fn options() -> OpenOptionsuse std::fs::File; fn main() -> std::io::Result { let mut f = File::options().append(true).open(\"example.log\")?; Ok(()) } sync_all 就是fsync pub fn sync_all(&self) -> io::Resultuse std::fs::File; use std::io::prelude::*; fn main() -> std::io::Result { let mut f = File::create(\"foo.txt\")?; f.write_all(b\"Hello, world!\")?; f.sync_all()?; Ok(()) } sync_data 比sync_all少一些disk操作 pub fn sync_data(&self) -> io::Resultuse std::fs::File; use std::io::prelude::*; fn main() -> std::io::Result { let mut f = File::create(\"foo.txt\")?; f.write_all(b\"Hello, world!\")?; f.sync_data()?; Ok(()) } set_len 设文件大小, 若原本size小, 就shrink文件到新size; 如果原size小, 则剩下的都填0use std::fs::File; fn main() -> std::io::Result { let mut f = File::create(\"foo.txt\")?; f.set_len(10)?; Ok(()) } metadata try_clone set_permissions 其他impl的trait impl AsInner for File impl FromInner for File impl IntoInner for File impl fmt::Debug for File 根据注释, 还实现了比如AsFd等trait. 但不知道藏在哪里实现的??? // In addition to the `impl`s here, `File` also has `impl`s for // `AsFd`/`From`/`Into` and // `AsRawFd`/`IntoRawFd`/`FromRawFd`, on Unix and WASI, and // `AsHandle`/`From`/`Into` and // `AsRawHandle`/`IntoRawHandle`/`FromRawHandle` on Windows. 实现Read trait pub struct File { inner: fs_imp::File, } 这里显得很啰嗦, 基本都是调用inner的对应函数. 因为File包括了inner. 因为rust没有继承就要再写一遍wrapper吗??? impl Read for File { fn read(&mut self, buf: &mut [u8]) -> io::Result { self.inner.read(buf) } fn read_vectored(&mut self, bufs: &mut [IoSliceMut]) -> io::Result { self.inner.read_vectored(bufs) } fn read_buf(&mut self, buf: &mut ReadBuf) -> io::Result { self.inner.read_buf(buf) } #[inline] fn is_read_vectored(&self) -> bool { self.inner.is_read_vectored() } // Reserves space in the buffer based on the file size when available. fn read_to_end(&mut self, buf: &mut Vec) -> io::Result { buf.reserve(buffer_capacity_required(self)); io::default_read_to_end(self, buf) } // Reserves space in the buffer based on the file size when available. fn read_to_string(&mut self, buf: &mut String) -> io::Result { buf.reserve(buffer_capacity_required(self)); io::default_read_to_string(self, buf) } } 实现Write trait impl Write for File { fn write(&mut self, buf: &[u8]) -> io::Result { self.inner.write(buf) } fn write_vectored(&mut self, bufs: &[IoSlice]) -> io::Result { self.inner.write_vectored(bufs) } #[inline] fn is_write_vectored(&self) -> bool { self.inner.is_write_vectored() } fn flush(&mut self) -> io::Result { self.inner.flush() } } 实现Seek trait impl Seek for File { fn seek(&mut self, pos: SeekFrom) -> io::Result { self.inner.seek(pos) } } Read Write Seek for &File又来一遍!!!!! 有意思吗? impl Read for &File { fn read(&mut self, buf: &mut [u8]) -> io::Result { self.inner.read(buf) } fn read_buf(&mut self, buf: &mut ReadBuf) -> io::Result { self.inner.read_buf(buf) } fn read_vectored(&mut self, bufs: &mut [IoSliceMut]) -> io::Result { self.inner.read_vectored(bufs) } #[inline] fn is_read_vectored(&self) -> bool { self.inner.is_read_vectored() } // Reserves space in the buffer based on the file size when available. fn read_to_end(&mut self, buf: &mut Vec) -> io::Result { buf.reserve(buffer_capacity_required(self)); io::default_read_to_end(self, buf) } // Reserves space in the buffer based on the file size when available. fn read_to_string(&mut self, buf: &mut String) -> io::Result { buf.reserve(buffer_capacity_required(self)); io::default_read_to_string(self, buf) } } open最后调用 use crate::sys::fs as fs_imp; fn _open(&self, path: &Path) -> io::Result { fs_imp::File::open(path, &self.0).map(|inner| File { inner }) } fs_imp::File::open代码在lib/rustlib/src/rust/library/std/src/sys/unix/fs.rs 里面调用了很多libc的函数. IO 常用函数 pub fn read_to_string(mut reader: R) -> Result //这个是内部使用的函数 fn read_until(r: &mut R, delim: u8, buf: &mut Vec) -> Result Read trait pub trait Read { /// use std::io; /// use std::io::prelude::*; /// use std::fs::File; /// /// fn main() -> io::Result { /// let mut f = File::open(\"foo.txt\")?; /// let mut buffer = [0; 10]; /// /// // read up to 10 bytes /// let n = f.read(&mut buffer[..])?; /// /// println!(\"The bytes: {:?}\", &buffer[..n]); /// Ok(()) /// } //也是传入一个[u8]的数组, 返回大小 fn read(&mut self, buf: &mut [u8]) -> Result; //vector方式读 fn read_vectored(&mut self, bufs: &mut [IoSliceMut]) -> Result { default_read_vectored(|b| self.read(b), bufs) } //有没有vector读, 默认false fn is_read_vectored(&self) -> bool { false } /// use std::io; /// use std::io::prelude::*; /// use std::fs::File; /// /// fn main() -> io::Result { /// let mut f = File::open(\"foo.txt\")?; /// let mut buffer = Vec::new(); /// /// // read the whole file /// f.read_to_end(&mut buffer)?; /// Ok(()) /// } //读完所有byte fn read_to_end(&mut self, buf: &mut Vec) -> Result { default_read_to_end(self, buf) } /// use std::io; /// use std::io::prelude::*; /// use std::fs::File; /// /// fn main() -> io::Result { /// let mut f = File::open(\"foo.txt\")?; /// let mut buffer = String::new(); /// /// f.read_to_string(&mut buffer)?; /// Ok(()) /// } //读完所有string fn read_to_string(&mut self, buf: &mut String) -> Result { default_read_to_string(self, buf) } /// use std::io; /// use std::io::prelude::*; /// use std::fs::File; /// /// fn main() -> io::Result { /// let mut f = File::open(\"foo.txt\")?; /// let mut buffer = [0; 10]; /// /// // read exactly 10 bytes /// f.read_exact(&mut buffer)?; /// Ok(()) /// } //一直读到size fn read_exact(&mut self, buf: &mut [u8]) -> Result { default_read_exact(self, buf) } //read到ReadBuf fn read_buf(&mut self, buf: &mut ReadBuf) -> Result { default_read_buf(|b| self.read(b), buf) } fn read_buf_exact(&mut self, buf: &mut ReadBuf) -> Result //借用 fn by_ref(&mut self) -> &mut Self //把这个Read转换为byte iterator fn bytes(self) -> Bytes /// use std::io; /// use std::io::prelude::*; /// use std::fs::File; /// /// fn main() -> io::Result { /// let mut f1 = File::open(\"foo.txt\")?; /// let mut f2 = File::open(\"bar.txt\")?; /// /// let mut handle = f1.chain(f2); /// let mut buffer = String::new(); /// /// // read the value into a String. We could use any Read method here, /// // this is just one example. /// handle.read_to_string(&mut buffer)?; /// Ok(()) /// } //和next Read成链, 先读Self, 接着读next fn chain(self, next: R) -> Chain /// use std::io; /// use std::io::prelude::*; /// use std::fs::File; /// /// fn main() -> io::Result { /// let mut f = File::open(\"foo.txt\")?; /// let mut buffer = [0; 5]; /// /// // read at most five bytes /// let mut handle = f.take(5); /// /// handle.read(&mut buffer)?; /// Ok(()) /// } //返回一个新的Read, 但只读limit字节 fn take(self, limit: u64) -> Take } Write trait pub trait Write { //很自然的, 这里的buf是个借用 //和read一样, write也可以部分写 fn write(&mut self, buf: &[u8]) -> Result; fn write_vectored(&mut self, bufs: &[IoSlice]) -> Result fn is_write_vectored(&self) -> bool //这个flush是write独有的 fn flush(&mut self) -> Result; //全写 fn write_all(&mut self, mut buf: &[u8]) -> Result fn write_all_vectored(&mut self, mut bufs: &mut [IoSlice]) -> Result //写格式化string到Write fn write_fmt(&mut self, fmt: fmt::Arguments) -> Result fn by_ref(&mut self) -> &mut Self } Seek trait pub trait Seek { fn seek(&mut self, pos: SeekFrom) -> Result; //从头开始 fn rewind(&mut self) -> Result //返回这个stream的字节数 fn stream_len(&mut self) -> Result //stream的当前位置 fn stream_position(&mut self) -> Result } BufRead是Read的派生trait BufRead自带内部buffer 比如stdin.lock()就实现了BufRead use std::io; use std::io::prelude::*; let stdin = io::stdin(); for line in stdin.lock().lines() { println!(\"{}\", line.unwrap()); } 比如可以用BufReader::new(r)把Reader r转换为BufReader use std::io::{self, BufReader}; use std::io::prelude::*; use std::fs::File; fn main() -> io::Result { let f = File::open(\"foo.txt\")?; let f = BufReader::new(f); for line in f.lines() { println!(\"{}\", line.unwrap()); } Ok(()) } 下面是BufRead的定义: pub trait BufRead: Read { //读出内部buffer, 并从内部reader填入新数据 fn fill_buf(&mut self) -> Result; //amt数量的字节已经被consume了 fn consume(&mut self, amt: usize); fn has_data_left(&mut self) -> Result fn read_until(&mut self, byte: u8, buf: &mut Vec) -> Result fn read_line(&mut self, buf: &mut String) -> Result //返回一个按照分隔符byte分割的iterator fn split(self, byte: u8) -> Split //返回按行分隔的iterator fn lines(self) -> Lines } "},"notes/rust_代码小段.html":{"url":"notes/rust_代码小段.html","title":"Rust 代码小段","keywords":"","body":" micro_http channel in channel epoll 反序列化到结构体 match语句块做为值 命令行参数拿到文件名并读出其中字符串 从Vec到Vec 从&str返回任意类型 map_err(Error::Arch)? json文件 compile 序列化 Err的使用方法 micro_http 用的是 micro_http = { git = \"https://github.com/firecracker-microvm/micro-http\", branch = \"main\" } // 起http线程, 用的是micro_http的库 api::start_http_path_thread() let server = HttpServer::new_from_fd() start_http_thread(server) hread::Builder::new() //新线程 loop { match server.requests() { Ok(request_vec) => { for server_request in request_vec { server.respond(server_request.process( |request| { handle_http_request(request, &api_notifier, &api_sender) } )) } } } } 处理就是从全局url路由表中get route 即HTTP_ROUTES.routes.get(&path), 然后调用route的handle_request函数, 即调用route.handle_request(): fn handle_http_request( request: &Request, api_notifier: &EventFd, api_sender: &Sender, ) -> Response { let path = request.uri().get_abs_path().to_string(); let mut response = match HTTP_ROUTES.routes.get(&path) { Some(route) => match api_notifier.try_clone() { Ok(notifier) => route.handle_request(request, notifier, api_sender.clone()), Err(_) => error_response( HttpError::InternalServerError, StatusCode::InternalServerError, ), }, None => error_response(HttpError::NotFound, StatusCode::NotFound), }; response.set_server(\"Cloud Hypervisor API\"); response.set_content_type(MediaType::ApplicationJson); response } 全局变量route是提前静态注册好的: HTTP_ROUTES是个全局变量 lazy_static! { /// HTTP_ROUTES contain all the cloud-hypervisor HTTP routes. pub static ref HTTP_ROUTES: HttpRoutes = { let mut r = HttpRoutes { routes: HashMap::new(), }; r.routes.insert(endpoint!(\"/vm.add-device\"), Box::new(VmActionHandler::new(VmAction::AddDevice(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-user-device\"), Box::new(VmActionHandler::new(VmAction::AddUserDevice(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-disk\"), Box::new(VmActionHandler::new(VmAction::AddDisk(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-fs\"), Box::new(VmActionHandler::new(VmAction::AddFs(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-net\"), Box::new(VmActionHandler::new(VmAction::AddNet(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-pmem\"), Box::new(VmActionHandler::new(VmAction::AddPmem(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-vdpa\"), Box::new(VmActionHandler::new(VmAction::AddVdpa(Arc::default())))); r.routes.insert(endpoint!(\"/vm.add-vsock\"), Box::new(VmActionHandler::new(VmAction::AddVsock(Arc::default())))); r.routes.insert(endpoint!(\"/vm.boot\"), Box::new(VmActionHandler::new(VmAction::Boot))); r.routes.insert(endpoint!(\"/vm.counters\"), Box::new(VmActionHandler::new(VmAction::Counters))); r.routes.insert(endpoint!(\"/vm.create\"), Box::new(VmCreate {})); r.routes.insert(endpoint!(\"/vm.delete\"), Box::new(VmActionHandler::new(VmAction::Delete))); r.routes.insert(endpoint!(\"/vm.info\"), Box::new(VmInfo {})); r.routes.insert(endpoint!(\"/vm.pause\"), Box::new(VmActionHandler::new(VmAction::Pause))); r.routes.insert(endpoint!(\"/vm.power-button\"), Box::new(VmActionHandler::new(VmAction::PowerButton))); r.routes.insert(endpoint!(\"/vm.reboot\"), Box::new(VmActionHandler::new(VmAction::Reboot))); r.routes.insert(endpoint!(\"/vm.receive-migration\"), Box::new(VmActionHandler::new(VmAction::ReceiveMigration(Arc::default())))); r.routes.insert(endpoint!(\"/vm.remove-device\"), Box::new(VmActionHandler::new(VmAction::RemoveDevice(Arc::default())))); r.routes.insert(endpoint!(\"/vm.resize\"), Box::new(VmActionHandler::new(VmAction::Resize(Arc::default())))); r.routes.insert(endpoint!(\"/vm.resize-zone\"), Box::new(VmActionHandler::new(VmAction::ResizeZone(Arc::default())))); r.routes.insert(endpoint!(\"/vm.restore\"), Box::new(VmActionHandler::new(VmAction::Restore(Arc::default())))); r.routes.insert(endpoint!(\"/vm.resume\"), Box::new(VmActionHandler::new(VmAction::Resume))); r.routes.insert(endpoint!(\"/vm.send-migration\"), Box::new(VmActionHandler::new(VmAction::SendMigration(Arc::default())))); r.routes.insert(endpoint!(\"/vm.shutdown\"), Box::new(VmActionHandler::new(VmAction::Shutdown))); r.routes.insert(endpoint!(\"/vm.snapshot\"), Box::new(VmActionHandler::new(VmAction::Snapshot(Arc::default())))); r.routes.insert(endpoint!(\"/vmm.ping\"), Box::new(VmmPing {})); r.routes.insert(endpoint!(\"/vmm.shutdown\"), Box::new(VmmShutdown {})); r }; } 比如create的处理是这样的: // /api/v1/vm.create handler pub struct VmCreate {} impl EndpointHandler for VmCreate { fn handle_request( &self, req: &Request, api_notifier: EventFd, api_sender: Sender, ) -> Response { match req.method() { Method::Put => { match &req.body { Some(body) => { // Deserialize into a VmConfig let vm_config: VmConfig = match serde_json::from_slice(body.raw()) .map_err(HttpError::SerdeJsonDeserialize) { Ok(config) => config, Err(e) => return error_response(e, StatusCode::BadRequest), }; // Call vm_create() match vm_create(api_notifier, api_sender, Arc::new(Mutex::new(vm_config))) .map_err(HttpError::ApiError) { Ok(_) => Response::new(Version::Http11, StatusCode::NoContent), Err(e) => error_response(e, StatusCode::InternalServerError), } } None => Response::new(Version::Http11, StatusCode::BadRequest), } } _ => error_response(HttpError::BadRequest, StatusCode::BadRequest), } } } channel in channel 使用标准库的channel方法 pub fn channel() -> (Sender, Receiver) 发送方: // 编译器会从下文推断出这个channel传输的是ApiRequest let (api_request_sender, api_request_receiver) = std::sync::mpsc::channel(); // 构造内部channel let (response_sender, response_receiver) = std::sync::mpsc::channel(); api_request_sender .send(ApiRequest::VmCreate(config, response_sender)) .map_err(ApiError::RequestSend)?; //send也会出错, 一般都是对端链接断了 response_receiver.recv().map_err(ApiError::ResponseRecv)??; //这里有两个?, 解开2层Result, 因为外层的Result是recv加的. 上面的ApiRequest是类型下面的enum: pub enum ApiRequest { /// Create the virtual machine. This request payload is a VM configuration /// (VmConfig). /// If the VMM API server could not create the VM, it will send a VmCreate /// error back. VmCreate(Arc>, Sender), /// Boot the previously created virtual machine. /// If the VM was not previously created, the VMM API server will send a /// VmBoot error back. VmBoot(Sender), ... } ApiResponse是: /// This is the response sent by the VMM API server through the mpsc channel. pub type ApiResponse = std::result::Result; epoll 用的是rust的epoll // 新建epoll let mut epoll = EpollContext::new().map_err(Error::Epoll)?; // 底层是epoll create let epoll_fd = epoll::create(true) //增加event. 第一个参数是fd, 第二个是关联的dispatch时候用的token epoll.add_event(&exit_evt, EpollDispatch::Exit) // 底层是epoll ctl, 和c版本一样, ctl add可以给fd绑定一个data // wait let num_events = match epoll::wait(epoll_fd, -1, &mut events[..]) //处理 for event in events.iter().take(num_events) { //这个就是当时add event的时候关联的data let dispatch_event: EpollDispatch = event.data.into(); //根据dispatch_event来分发 // 这样的好处是看到这个token, 就知道是哪个事件了, 就知道用哪个fd match dispatch_event { EpollDispatch::Unknown => { } EpollDispatch::Exit => { } EpollDispatch::Api => { } } } 反序列化到结构体 VmmConfig是个结构体: /// Used for configuring a vmm from one single json passed to the Firecracker process. #[derive(Debug, Default, Deserialize, PartialEq, Serialize)] pub struct VmmConfig { #[serde(rename = \"balloon\")] balloon_device: Option, #[serde(rename = \"drives\")] block_devices: Vec, #[serde(rename = \"boot-source\")] boot_source: BootSourceConfig, #[serde(rename = \"logger\")] logger: Option, #[serde(rename = \"machine-config\")] machine_config: Option, #[serde(rename = \"metrics\")] metrics: Option, #[serde(rename = \"mmds-config\")] mmds_config: Option, #[serde(rename = \"network-interfaces\", default)] net_devices: Vec, #[serde(rename = \"vsock\")] vsock_device: Option, } 用第三方库serde_json来反序列化, 得到结构体 let vmm_config: VmmConfig = serde_json::from_slice::(config_json.as_bytes()) .map_err(Error::InvalidJson)?; from_slice::是实例化泛型函数中T的意思 pub fn from_slice(v: &'a [u8]) -> Result match语句块做为值 // let后面pattern匹配, 匹配的是match语句块的值, 即最后的(res, vmm) let (_, vmm) = match build_microvm_from_json( seccomp_filters, &mut event_manager, // Safe to unwrap since '--no-api' requires this to be set. config_json.unwrap(), instance_info, bool_timer_enabled, mmds_size_limit, metadata_json, ) { Ok((res, vmm)) => (res, vmm), Err(exit_code) => return exit_code, }; 命令行参数拿到文件名并读出其中字符串 两个map搞定: 第一个map传入的函数fs::read_to_string就已经把文件内容读出来了. 第二个map把上面的Option>转换成Option, 出错就panic let vmm_config_json = arguments .single_value(\"config-file\") .map(fs::read_to_string) .map(|x| x.expect(\"Unable to open or read from the configuration file\")); 从Vec到Vec let args = vec![\"binary-name\", \"--exec-file\", \"foo\", \"--help\"] .into_iter() .map(String::from) //这里并没有自己写闭包,而是用了现成的函数 .collect::>(); 从&str返回任意类型 比如要从字符串转换为如下enum /// Supported target architectures. #[allow(non_camel_case_types)] #[derive(Debug, PartialEq, Clone, Copy)] pub(crate) enum TargetArch { /// x86_64 arch x86_64, /// aarch64 arch aarch64, } 代码如下 let target_arch: TargetArch = \"x86_64\".try_into().map_err(Error::Arch)?; try_info是个trait方法, 实现了TryInfo, 而后者是个泛型 pub trait TryInto: Sized { /// The type returned in the event of a conversion error. type Error; /// Performs the conversion. fn try_into(self) -> Result; } 实际上, &str有很多种try_into的实现, 它们都和目标类型有关. 这里的目标类型是自己定义的enum TargetArch 因为TryInfo是个泛型, 所以, 这里自己给&str实现了针对性的try_into: impl TryInto for &str { type Error = TargetArchError; fn try_into(self) -> std::result::Result { match self.to_lowercase().as_str() { \"x86_64\" => Ok(TargetArch::x86_64), \"aarch64\" => Ok(TargetArch::aarch64), _ => Err(TargetArchError::InvalidString(self.to_string())), } } } 这也说明, 在rust里, 可以在自己模块给\"别人\"的类型实现某个trait. 注意这里, 原始trait要求try_into的签名是 fn try_into(self) -> Result 而我们实现的时候, 可以实例化: fn try_into(self) -> std::result::Result map_err(Error::Arch)? 有点奇怪, map_err的入参应该是个函数, 但Error::Arch只是个enum, 这样竟然也行? #[derive(Debug)] enum Error { Bincode(BincodeError), FileOpen(PathBuf, io::Error), FileFormat(FilterFormatError), Json(JSONError), MissingInputFile, MissingTargetArch, Arch(TargetArchError), } json文件 compile 序列化 // &mut dyn Read时trait object吗 fn parse_json(reader: &mut dyn Read) -> Result { //用了serde_json库, 在本crate的cargo.toml里面dependencies有 // 从reader读json, 返回JsonFile // 如果有错误, 转换为本模块的Json错误 serde_json::from_reader(reader).map_err(Error::Json) } fn compile(args: &Arguments) -> Result { //一句话打开文件 let input_file = File::open(&args.input_file) .map_err(|err| Error::FileOpen(PathBuf::from(&args.input_file), err))?; // new一个BufReader let mut input_reader = BufReader::new(input_file); // 从这个input_reader读 let filters = parse_json(&mut input_reader)?; // new一个compiler let compiler = Compiler::new(args.target_arch); // transform the IR into a Map of BPFPrograms let bpf_data: HashMap = compiler .compile_blob(filters.0, args.is_basic) //filters.0是元组tuple的数字下标访问方式 .map_err(Error::FileFormat)?; // serialize the BPF programs & output them to a file let output_file = File::create(&args.output_file) .map_err(|err| Error::FileOpen(PathBuf::from(&args.output_file), err))?; bincode::serialize_into(output_file, &bpf_data).map_err(Error::Bincode)?; Ok(()) } 注: Open以后没有Close, 因为input_file会被move进BufReader::new, 再被move出来, 所有权在这个函数结束的时候会被清理. Err的使用方法 fn main() { let mut arg_parser = build_arg_parser(); //这里和golang的if err := xxx(); err != nil {}意思一样 if let Err(err) = arg_parser.parse_from_cmdline() { eprintln!( \"Arguments parsing error: {} \\n\\n\\ For more information try --help.\", err ); process::exit(EXIT_CODE_ERROR); } if arg_parser.arguments().flag_present(\"help\") { println!(\"Seccompiler-bin v{}\\n\", SECCOMPILER_VERSION); println!(\"{}\", arg_parser.formatted_help()); return; } if arg_parser.arguments().flag_present(\"version\") { println!(\"Seccompiler-bin v{}\\n\", SECCOMPILER_VERSION); return; } let args = get_argument_values(arg_parser.arguments()).unwrap_or_else(|err| { eprintln!( \"{} \\n\\n\\ For more information try --help.\", err ); process::exit(EXIT_CODE_ERROR); }); if let Err(err) = compile(&args) { eprintln!(\"Seccompiler error: {}\", err); process::exit(EXIT_CODE_ERROR); } println!(\"Filter successfully compiled into: {}\", args.output_file); } "},"notes/others.html":{"url":"notes/others.html","title":"其他","keywords":"","body":"其他杂项. "},"notes/rust_mdbook_使用.html":{"url":"notes/rust_mdbook_使用.html","title":"Rust 使用mdbook","keywords":"","body":"mdBook是rust写的一个工具, 用来把md文档转成html book.guide: https://rust-lang.github.io/mdBook mdBook本身也是个git repo: https://github.com/rust-lang/mdBook 再更新2022.10 更新 2022.08 安装 book组织 mdbook使用 book.toml SUMMARY.md build book 再更新2022.10 关键参考: https://www.mapull.com/gitbook/comscore/这个site就是用gitbook写的, gitbook的配置写的非常到位! book.json { \"title\": \"GitBook 简明教程\", \"language\": \"zh-hans\", \"author\": \"码谱\", \"links\": { \"sidebar\": { \"码谱\": \"http://www.mapull.com\" } }, \"plugins\": [ \"-search\", \"-lunr\", \"-sharing\", \"-livereload\", \"github\", \"donate\", \"chart\", \"todo\", \"graph\", \"puml\", \"katex\", \"code\", \"ace\", \"sitemap-general\", \"mermaid-gb3\", \"include-csv\", \"flexible-alerts\", \"chapter-fold\", \"anchor-navigation-ex\", \"theme-comscore\" ], \"pluginsConfig\": { \"anchor-navigation-ex\": { \"showLevel\": false, \"showGoTop\": true }, \"sitemap-general\": { \"prefix\": \"https://www.mapull.com/gitbook/comscore/\" }, \"my-toolbar\": { \"buttons\": [ { \"label\": \"下载PDF\", \"icon\": \"fa fa-file-pdf-o\", \"url\": \"https://www.mapull.com/gitbook/comscore/book.pdf\", \"position\": \"left\", \"text\": \"下载PDF\", \"target\": \"_blank\" } ] }, \"donate\": { \"wechat\": \"https://www.mapull.com/logo/mapull-qr.png\", \"button\": \"反馈\", \"wechatText\": \"微信扫码\" }, \"versions\": { \"options\": [ { \"value\": \"https://www.mapull.com/gitbook/api/\", \"text\": \"Theme API\" }, { \"value\": \"https://www.mapull.com/gitbook/comscore/\", \"text\": \"Theme comscore\", \"selected\": true } ] }, \"github\": { \"url\": \"https://gitee.com/mapull/gitbook-guide\" }, \"edit-link\": { \"base\": \"https://gitee.com/mapull/gitbook-guide\", \"label\": \"Edit This Page\" } }, \"variables\": { \"mapull\": \"码谱，让编程更容易。\", \"ides\": [{\"name\": \"Eclipse\"}, {\"name\": \"IntelliJ IDEA\"}, {\"name\": \"Visual Studio Code\"}] }, \"structure\": { \"readme\": \"home.md\" } } 更新 2022.08 mdbook不支持中文搜索, 故弃用. 使用gitbook代替gitbook参考:https://github.com/zhangjikai/gitbook-usehttps://github.com/snowdreams1006/snowdreams1006.github.io/blob/master/book.jsonhttps://snowdreams1006.github.io/ 安装 可以直接去github页面下载: https://github.com/rust-lang/mdBook/releases也可以自己编译, 但需要先安装rust编译器 cargo install mdbook cargo命令会自动从crates.io下载mdbook, 编译, 然后安装到cargo的bin目录(默认是~/.cargo/bin/). crates.io上的版本会比github代码稍微滞后一点, 可以指定用github代码编译: cargo install --git https://github.com/rust-lang/mdBook.git mdbook book组织 book由chapter组成, 每个chapter是一个独立的page, chapter可以有子chapter. mdbook使用 # 新建一个book mdbook init my-first-book cd my-first-book # 开启一个webserver, 修改的内容可以自动刷新到web page mdbook serve book.toml 一个book需要几个特殊文件来定义排版和布局. 根目录下的book.toml就是其中一个: 最常用的, 最简单的: [book] title = \"My First Book\" mdbook自己的实例:https://github.com/rust-lang/mdBook/blob/master/guide/book.toml SUMMARY.md 这个文件在src目录下, 定义了chapter结构: # Summary [Introduction](README.md) - [My First Chapter](my-first-chapter.md) - [Nested example](nested/README.md) - [Sub-chapter](nested/sub-chapter.md) 实例: https://github.com/rust-lang/mdBook/blob/master/guide/src/SUMMARY.md build book mdbook build 这个命令会根据md文件在本地book目录下生成html. "},"notes/as_title_scripts.html":{"url":"notes/as_title_scripts.html","title":"脚本","keywords":"","body":"如题 "},"notes/shell_变量.html":{"url":"notes/shell_变量.html","title":"shell变量","keywords":"","body":"bash手册 搜索: Bash Manual bash手册之变量扩展 变量替换 Variable Description ${parameter:-defaultValue} Get default shell variables value ${parameter:=defaultValue} Set default shell variables value ${parameter:?\"Error Message\"} Display an error message if parameter is not set ${#var} Find the length of the string ${var%pattern} Remove from shortest rear (end) pattern ${var%%pattern} Remove from longest rear (end) pattern ${var:num1:num2} Substring ${var#pattern} Remove from shortest front pattern ${var##pattern} Remove from longest front pattern ${var/pattern/string} Find and replace (only replace first occurrence) ${var//pattern/string} Find and replace all occurrences ${!prefix*} Expands to the names of variables whose names begin with prefix. ${var,} ${var,pattern} Convert first character to lowercase. ${var,,} ${var,,pattern} Convert all characters to lowercase. ${var^} ${var^pattern} Convert first character to uppercase. ${var^^} ${var^^pattern} Convert all character to uppercase. 有没有冒号的区别 ${parameter:-defaultValue}有冒号, 表示如果parameter不存在, 或者为空, 则返回defaultValue ${parameter-defaultValue}没有冒号:, 表示如果parameter不存在, 则返回defaultValue When not performing substring expansion, using the form described below (e.g., ‘:-’), Bash tests for a parameter that is unset or null. Omitting the colon results in a test only for a parameter that is unset. Put another way, if the colon is included, the operator tests for both parameter’s existence and that its value is not null; if the colon is omitted, the operator tests only for existence. 用冒号初始化变量 比如: : \"${LANG_CXX:=true}\" : \"${LANG_D:=true}\" : \"${LANG_OBJC:=true}\" : \"${LANG_GO:=true}\" : \"${LANG_FORTRAN:=true}\" : \"${LANG_ADA:=true}\" : \"${LANG_JIT:=true}\" 冒号是个空命令, 什么也不干, 永远返回0 但冒号命令会展开参数, 最后的效果是给变量赋默认值 加冒号的目的是避免shell把默认值当作命令执行 "},"notes/shell_基础篇.html":{"url":"notes/shell_基础篇.html","title":"shell命令和脚本记录-基础篇","keywords":"","body":" 带超时的重试命令 Here Document的用法: 一行输入 if判断和&&的区别 bash的双中括号 举例 date举例 网络脚本直接执行 ulimit和prlimit curl下载 暂停和继续一个进程的执行, 不用gdb 补充发送其他信号的行为 查看一个进程的子进程 跳过前几行 重定向前面不能有空格 shell单引号和双引号的重要区别 保存多行输出到shell变量 pstree pgrep and ps rsync 实例 sar 汇总 从变量read out put htop 为什么&后面加分号不行? sort按多列排序 shell注释一块代码 默认变量值 mtr 一个更好用的traceroute ssh config 快捷登录 给log文件加时间戳 cp最好加-a 如何查丢包 sar 查看block io的使用情况 telnet记录log kill某用户所有进程 UUID和GUID 关于grub 使用netcat导出打印 查看CPU运行队列情况 网络流量监控 查看uuid 为什么静态ip会丢失 用tail查看系统log 代码打补丁 如何查看io占用高的进程 linux访问win共享 ubuntu杀进程和服务 去掉最后几个字符 在shell里用exec eval source区别 查看线程, 大写H linux启动脚本顺序 文件权限 SELinux的文件权限扩展 shell for 观察82599的中断 关于中断balance 多个jpg图片转到pdf ssh X11及VNC firefox over ssh x11 forwarding 让远程的firefox更快 tools change tmux prefix tmux复制 network mount fae storage 新建一个用户到特定组 查看虚拟分区 thunder通过服务器上网 linux路由参考 brctl就是个交换机 wiz的搜索功能 批量修改jpg图片大小--xarg位置指代 ubuntu查看一个文件属于哪个package ubuntu查看一个package包含哪些文件 egrep 分组或 删除mysql的cmake相关的文件 cpu hotplug grub reserve 内存 给用户组加写权限 修改文件所属用户 增加共享库路经 查看端口占用 查找大于1M的文件 wget下载 通过gcc查找libc路径 -- -print-file-name=xxx 各种进制转十进制 bc --shell下的C计算器 CPU热插拔 hexdump查ascii码 -v选项 hexdump指定格式 查看温感 trap命令 --指定shell中处理signal 创建ram文件系统 heredoc格式代码 install命令 readlink --读取链接文件 重新修改tmpfs的大小 fg bg --前台运行 后台运行 shell写字符串, 以'\\0'结尾 写十六进制数据到文件, 关键在于引号 批量处理软链接之cscope.files readelf结果用sort排序, 按照第三列数字排序 批量删除ZF ZL ZG 批量格式化代码 带时间的平均负载 rpm解压 大小写转换 解析C文件全部函数到h文件声明, 用于偷懒声明所有函数到头文件 strace 重定向 通过elf生成工程文件列表 --利用gdb sync命令使用, 把file buffer 刷进物理器件 查找src下面的代码目录, find可以限制搜索深度 查看当前文件夹大小 删除文件夹下面的所有链接文件 压缩当前目录下sw_c文件夹 根据一个文本文件列表压缩 --hT tar.xz格式压缩解压 在当前路径下的所有文件中，搜索关键字符串 ls按修改时间排序 ls按大小排序 查看键盘映射 限制深度的find 查看系统支持的文件系统类型 替换字符串 批量软链接 tee 处理软链接, Thomas版 关闭printk打印 usb串口 --lsusb --minicom locate--linux下面的everything rsync拷贝目录，去除hg 带超时的重试命令 用timeout和until的组合:比如下面的dockerfile里面的RUN语句, code-server可能会在装extension的时候卡住, 用timeout来强制2m退出, 并用until循环来重试. RUN until timeout 2m code-server \\ --user-data-dir /usr/local/share/code-server \\ --install-extension golang.go \\ --install-extension ms-python.python \\ --install-extension formulahendry.code-runner \\ --install-extension eamodio.gitlens \\ --install-extension oderwat.indent-rainbow \\ --install-extension vscode-icons-team.vscode-icons \\ --install-extension esbenp.prettier-vscode \\ --install-extension streetsidesoftware.code-spell-checker \\ ;do echo Retry installing extensions...; done Here Document的用法: 一行输入 用可以达到bash -c一样的效果 $ bash -c \"echo hello\" hello $ bash 令一个例子是 $ ./gshell 具体见man bash, 搜索Here Strings if判断和&&的区别 返回值不同比如 if true; then echo 111; fi #结果为 111 echo $? #返回值为 0 if false; then echo 111; fi #没有输出, 说明if条件不成立 echo $? #返回值还是 0, 说明shell认为没有错误 而&&就不一样 false echo $? #直接返回 1 false && echo 111 #没有输出, 说明echo没有执行; 到这里和if的效果是一样的 echo $? #但返回值为 1 true && echo 111 #打印111 echo $? #返回值为 0 结论: if和&&都有根据条件控制执行的功能 if语句块结束后, 永远返回0值, 表示成功; 而&&返回值由最后一个语句决定, 可能是0, 也可能是1. 所以在某些脚本中, 如果最后的返回值重要, 就要考虑用哪种判断方式. bash的双中括号 help里面说的很清楚, [[ expression ]]的语义更明确, help [[ [[ ... ]]: [[ expression ]] 是test的扩展语法, 支持 ( EXPRESSION ) ! EXPRESSION EXPR1 && EXPR2 EXPR1 || EXPR2 特别的, 当使用==或!=时, 右侧的string会被当作pattern来匹配; 这个匹配和case in的语义一样 当使用=~时, 右侧的string是正则表达式. 举例 #!/bin/bash # Only continue for 'develop' or 'release/*' branches BRANCH_REGEX=\"^(develop$|release//*)\" if [[ $BRANCH =~ $BRANCH_REGEX ]]; then echo \"BRANCH '$BRANCH' matches BRANCH_REGEX '$BRANCH_REGEX'\" else echo \"BRANCH '$BRANCH' DOES NOT MATCH BRANCH_REGEX '$BRANCH_REGEX'\" fi [[ $TEST =~ ^[[:alnum:][:blank:][:punct:]]+$ ]] 参考: https://stackoverflow.com/questions/18709962/regex-matching-in-a-bash-if-statement/18710850 date举例 #给文件名用 $ date +\"%Y%m%d%H%M%S\" 20201020115510 #给log时间戳用 date +\"%F %H:%M:%S.%N\" 网络脚本直接执行 curl -s http://10.182.105.179:8088/godevtools/godevtool | bash -s -h curl -s http://10.182.105.179:8088/godevtools/godevtool | bash -s vscode start curl下载文件, 输出到stdout; -s是quiet模式 如果加-O就是保存同名文件到本地 bash -s可以加参数 如何通过pipe给bash传参数 -s If the -s option is present, or if no arguments remain after option processing, then commands are read from the standard input. This option allows the positional parameters to be set when invoking an interactive shell. ulimit和prlimit ulimit可以配置当前shell的资源限制, 一般用: ulimit -n 限制打开文件的个数 prlimit是个命令, 对应同名的系统调用, 可以动态配置一个pid的资源 prlimit --pid 13134 --rss --nofile=1024:4095 Display the limits of the RSS, and set the soft and hard limits for the number of open files to 1024 and 4095, respectively. prlimit --pid $$ --nproc=unlimited Set for the current process both the soft and ceiling values for the number of processes to unlimited. prlimit --cpu=10 sort -u hugefile Set both the soft and hard CPU time limit to ten seconds and run 'sort'. 显示当前进程(即执行prlimit进程的进程)的limit $ prlimit --pid $$ RESOURCE DESCRIPTION SOFT HARD UNITS AS address space limit unlimited unlimited bytes CORE max core file size 0 unlimited bytes CPU CPU time unlimited unlimited seconds DATA max data size unlimited unlimited bytes FSIZE max file size unlimited unlimited bytes LOCKS max number of file locks held unlimited unlimited locks MEMLOCK max locked-in-memory address space 16777216 16777216 bytes MSGQUEUE max bytes in POSIX mqueues 819200 819200 bytes NICE max nice prio allowed to raise 0 0 NOFILE max number of open files 1024 1048576 files NPROC max number of processes 289710 289710 processes RSS max resident set size unlimited unlimited bytes RTPRIO max real-time priority 0 0 RTTIME timeout for real-time tasks unlimited unlimited microsecs SIGPENDING max number of pending signals 289710 289710 signals STACK max stack size 8388608 unlimited bytes curl下载 curl -o go.tar.gz -L https://dl.google.com/go/go$GOLANG_VERSION.linux-amd64.tar.gz # -L的意思是让curl follow redirection 暂停和继续一个进程的执行, 不用gdb 我在前台跑了topid程序 #前台运行 ./topid #其他窗口执行 #暂停进程 kill -SIGSTOP `pidof topid` #效果和ctrl+z一样, 原窗口会打印 #[1]+ Stopped ./topid #继续执行 kill -SIGCONT `pidof topid` #最后的效果是程序转入后台执行 补充发送其他信号的行为 kill -SIGTRAP `pidof topid` : go程序退出, 打印调用栈 kill -SIGQUIT `pidof topid` : 同上 kill -SIGSEGV `pidof topid` : 同上, 就像程序本身segment fault一样 kill -SIGTERM `pidof topid` : 没有打印, 直接退出 查看一个进程的子进程 cat /proc/pid/task/tid/children 跳过前几行 #这里的tail -n +2就是跳过n-1行, 也就是跳过1行 (echo time Goroutine Thread numGC heapSys heapIdle heapInuse heapReleased && tail -n +2 vonuerr.log | awk '{printf(\"%s %s %s %s %s %s %s %s\\n\", $3,$5,$7,$9,$13,$17,$19,$21)}') > go.csv 重定向前面不能有空格 重定向前面有空格不行: find / -type d -name usr 2 > /dev/null 错误提示find: paths must precede expression: 2 下面的可以: find / -type d -name usr 2>/dev/null find / -type d -name usr 2> /dev/null shell单引号和双引号的重要区别 双引号展开, 单引号不展开 #单引号 cmd='echo date: $(date +%s%6N)' #执行有变化 $ eval $cmd date: 1539248827389418 bai@CentOS-43 ~/repo/save $ eval $cmd date: 1539248829149500 #双引号 cmd=\"echo date: $(date +%s%6N)\" #执行无变化 $ eval $cmd date: 1539248841366200 bai@CentOS-43 ~/repo/save $ eval $cmd date: 1539248841366200 bai@CentOS-43 ~/repo/save $ eval $cmd date: 1539248841366200 bai@CentOS-43 ~/repo/save #结论: 双引号有展开属性, 这里在赋值的时候就把date算好了 #赋给cmd时date值已经固定了, 所以双引号无变化 #单引号不会展开 保存多行输出到shell变量 #多行的输出可以保存到变量 v1=$(ethtool -S enP5p1s0 | grep packets | grep -v \": 0\" && echo date: $(date +%s%6N)) #但在使用的时候, 要加\"\"来保存换行 echo \"$v1\" #不加\"\",换行会变成空格 echo $v1 pstree pgrep and ps $ pstree `pgrep qemu` qemu-system-aar───9*[{qemu-system-aar}] $ ps -eLo tid,pid,ppid,psr,stat,%cpu,rss,cmd --sort=-%cpu rsync 实例 #第一个是用ssh传输 rsync -av --progress -e ssh DATA white@192.168.2.3:/var/services/homes/white/shl rsync -av --progress white@192.168.2.3:/var/services/homes/white/shl sar 汇总 watch -dn1 'sar -Bwqr -dp -n DEV -u ALL 1 1 | grep Average' sar -Bwqr -dp -n DEV -u ALL 1 sar -qr -dp -n DEV --human -u ALL -P ALL 1 1 -B: 页表 -w: task -q:CPU task q -r: mem -dp: block IO -n DEV: network -u: CPU -P:cpu_list | ALL 后面是interval和count 从变量read # Split the TARGET variable into three elements separated by hyphens IFS=- read -r NAME ARCH SUFFIX out put htop yingjieb@yingjieb-gv ~/tmp $ echo q | htop | aha --black --line-fix >> htop.html 为什么&后面加分号不行? $ for i in {1..10};do echo $i &; done -bash: syntax error near unexpected token `;' 不加分号是可以的 $ for i in {1..10};do echo $i & done 因为&符号表示前面的命令后台执行, 本身就隐含了分隔符的作用. 而此时再加一个分隔符, 但shell认为前面并没有命令, 所以报错. sort按多列排序 #先按第三列排序, 相同再按第二列排序, 最后按第一列 sort -k3,3 -k2,2 -k1 shell注释一块代码 #!/bin/bash echo before comment : 一个冒号是个空命令, 什么都不干. The ' and ' around the END delimiter are important, otherwise things inside the block like for example $(command) will be parsed and executed. That said -- it's running a command (:) that doesn't read its input and always exits with a successful value, and sending the \"comment\" as input. 默认变量值 关键是:- interface=\"$1\" interrupt_name=${interface:-\"eth3\"}\"-TxRx-\" 如果interface不为空, 就用interface, 否则用默认的eth3 也可以从位置参数获取变量值, 如果用户没有传入位置参数, 则使用默认值.比如: work_dir=`pwd` dev=${1:-/dev/sdc} ubuntu_size=${2:-500G} tar_fedora=${3:-fedora-with-native-kernel-repo-gcc-update-to-20150423.tar.bz2} mtr 一个更好用的traceroute mtr -n baidu.com ssh config 快捷登录 man ssh_config cat ~/.ssh/config Host aliclient Hostname 192.168.10.2 User root IdentityFile ~/.ssh/id_rsa_alibaba_bench ServerAliveInterval 60 Host aliserver Hostname 30.2.47.247 User root IdentityFile ~/.ssh/id_rsa_alibaba_bench ProxyCommand ssh -q -W %h:%p aliclient ServerAliveInterval 60 注: ProxyCommand 这句可神奇了, 它可以让ssh先登录到一台中转机器上, 然后再登录到目标机器 给log文件加时间戳 用ts命令 apt install moreutils $ ping www.baidu.com | ts Dec 24 12:59:04 PING www.a.shifen.com (111.13.100.91) 56(84) bytes of data. Dec 24 12:59:04 64 bytes from 111.13.100.91: icmp_seq=1 ttl=54 time=52.2 ms Dec 24 12:59:05 64 bytes from 111.13.100.91: icmp_seq=2 ttl=54 time=40.2 ms Dec 24 12:59:06 64 bytes from 111.13.100.91: icmp_seq=3 ttl=54 time=53.1 ms Dec 24 12:59:07 64 bytes from 111.13.100.91: icmp_seq=4 ttl=54 time=47.8 ms Dec 24 12:59:09 64 bytes from 111.13.100.91: icmp_seq=6 ttl=54 time=39.3 ms Dec 24 12:59:10 64 bytes from 111.13.100.91: icmp_seq=7 ttl=54 time=37.1 ms Dec 24 12:59:11 64 bytes from 111.13.100.91: icmp_seq=8 ttl=54 time=36.1 ms Dec 24 12:59:12 64 bytes from 111.13.100.91: icmp_seq=9 ttl=54 time=51.8 ms Dec 24 12:59:13 64 bytes from 111.13.100.91: icmp_seq=10 ttl=54 time=54.2 ms cp最好加-a 比如当前目录下, *.lua都是链接文件 普通cp会follow这个链接, 拷真正的文件 cp *.lua /tmp/tmp/ -a选项会保留软链接 cp -a *.lua /tmp/tmp/ 如何查丢包 接口侧 $ sudo ethtool -S wlan0 网络测 $ netstat -i sar 查看block io的使用情况 $ sar -dp 1 Linux 3.16.2-031602-generic (mint) 11/03/2015 _x86_64_ (4 CPU) 09:20:27 PM DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 09:20:28 PM sda 1.00 0.00 8.00 8.00 0.03 76.00 32.00 3.20 09:20:28 PM sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 09:20:28 PM DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 09:20:29 PM sda 1.00 0.00 240.00 240.00 0.00 4.00 4.00 0.40 09:20:29 PM sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -d: 查看block dev, 一般-dp一起用, 可以看到具体的sda/sdb名称 telnet记录log telnet 192.168.1.31 10001 | tee crb2s_reboot_at_os.log kill某用户所有进程 [root@cavium tmp]# killall -9 -u james [root@cavium tmp]# killall -9 -u guest UUID和GUID GUID是微软对UUID的一种实现 如何查看UUID 注意, 用dd命令clone的盘有着一样的UUID, 由此可见, UUID是保存在分区的某个地方--据说是超级块里. ls -l /dev/disk/by-uuid/ $ sudo blkid /dev/sdb1 /dev/sdb1: UUID=\"d039c104-49d4-464a-b3df-a962574fd46f\" TYPE=\"ext4\" 在Grub中的应用 title Ubuntu hardy (development branch), kernel 2.6.24-16-generic root (hd2,0) kernel /boot/vmlinuz-2.6.24-16-generic root=UUID=c73a37c8-ef7f-40e4-b9de-8b2f81038441 ro quiet splash initrd /boot/initrd.img-2.6.24-16-generic quiet 关于grub 设dtb devicetree 设kernel linux 使用netcat导出打印 前提是两台机器的ip能连上 服务端: dmesg | nc -l 1234 客户端: nc 127.0.0.1 1234 缺点是在客户端输入的东西好像在服务端解析的不好. 查看CPU运行队列情况 sar -q 1 07:12:09 PM runq-sz plist-sz ldavg-1 ldavg-5 ldavg-15 blocked 07:12:10 PM 0 557 0.02 0.04 0.05 0 07:12:11 PM 0 557 0.02 0.04 0.05 0 07:12:12 PM 0 557 0.02 0.04 0.05 0 07:12:13 PM 0 557 0.02 0.04 0.05 0 07:12:14 PM 0 557 0.01 0.04 0.05 0 07:12:15 PM 0 557 0.01 0.04 0.05 0 07:12:16 PM 0 557 0.01 0.04 0.05 0 07:12:17 PM 0 557 0.01 0.04 0.05 0 其中runq-sz就是等待队列的长度 mpstat -A 可以报告更多详细的CPU使用率等信息 网络流量监控 sar -n DEV 1 这里的-n选项指的是network, 里面有很多子项 DEV, EDEV, NFS, NFSD, SOCK, IP, EIP, ICMP, EICMP, TCP, ETCP, UDP, SOCK6, IP6, EIP6, ICMP6, EICMP6 and UDP6 查看uuid blkid lsblk -f 为什么静态ip会丢失 eth7配了ip以后一会就丢失了 ifconfig eth7 8.8.8.4 up 因为一个service, 叫NetworkManager, 它会感知到eth-all的状态, 然后用dhclient配ip 用tail -f /var/log/messages就能看到dhclient和NetworkManager对eth7的操作 停止NetworkManager就OK. service NetworkManager stop 补充: 在cetos下, /etc/sysconfig/network-scripts下面的脚本应该是管网络的 用tail查看系统log tail -f /var/log/messages 也有人说 tail -f /var/log/kern.log 或 watch 'dmesg | tail -50' 或 cat /proc/kmsg 我的结论: 看这个文件就行了/var/log/messages, 已经包括了demsg的内容 代码打补丁 注意看当前的目录和patch文件的相对关系 比如patch文件里 diff -Naurp base/megaraid_sas.h fc19/megaraid_sas.h 而当前目录下就有megaraid_sas.h, 说明该P1 $ patch -p1 如何查看io占用高的进程 io总体使用情况 iostat -x 1 apt install iotop iotop linux访问win共享 apt install samba-client 这里有个问题, 就是主机名字没法解析, 此时需要先在window上ping主机名win7: ping socrates 得到Ping cafp01.caveonetworks.com [10.17.5.53] ping mafp01 得到Ping mafp01.caveonetworks.com [10.11.1.36] linux: 查看共享 $ smbclient -L //10.17.5.53 -N mount $ sudo mount -t cifs //10.17.5.53/ftproot /mnt -o user=,password= 拷贝--断点续传 $ rsync -v --progress --append \"/mnt/FAE/Users/VarunS/ThunderX/Ubuntu FS/ThunderX-ubuntu-8G-v3-FAE.tar.bz2\" ThunderX-ubuntu-8G-v3-FAE.tar.bz2 ubuntu杀进程和服务 ps aux | grep -v \"\\[.*\\]\" | grep -E \"cron|dhclient|38400|rsyslogd\" | awk '{print $2}' | xargs kill -9 一般上面的命令杀了进程以后, 那些进程又会自动生成. 下面的方法杀的彻底 services=\"cron resolvconf rsyslog udev dbus upstart-file-bridge upstart-socket-bridge upstart-udev-bridge tty1 tty2 tty3 tty4 tty5 tty6 tty7 tty8 tty9\" for s in $services;do service $s stop;done 去掉最后几个字符 byj@byj-Aspire-1830T ~ $echo abcd | sed s'/.$//' abc byj@byj-Aspire-1830T ~ $echo abcd | sed s'/..$//' ab 在shell里用exec eval source区别 eval 执行一个命令 exec 在新进程中执行一个命令，并且终止当前进程 source 在当前进程中执行脚本 查看线程, 大写H $ ps auxH | wc -l 查看tid号 ps -efL LWP那一列就是线程号, 如果是单独的线程, 应该和进程号一样. [byj]注: ps -eLf更实用一点 linux启动脚本顺序 init rc-->/etc/rc.d/rc3.d/*-->rc.local getty-->login-->/bin/bash /etc/profile-->~/.bash_profile-->~/.bash_login-->~/.profile-->~/.bashrc 文件权限 other不可写 chmod o-w repo -R 所有可执行, 一般目录都是这个属性 chmod a+x repo -R 用户组可写 chmod g+w repo/ -R SELinux的文件权限扩展 [root@cavium cavium]# ll total 313568 drwxr-xr-x 2 root root 4096 Jan 19 13:14 nfs drwxrwxr-x 6 root cavium 4096 Dec 28 21:18 repo -rw-r--r-- 1 yingjie cavium 303327016 Jan 19 11:19 RHELSA-1.6-Server.img.xz drwxrwxrwx. 2 root cavium 4096 Dec 23 17:29 share -rw-r--r-- 1 root root 158761 Jan 12 15:19 tests.tar.gz -rw-r--r-- 1 root root 17590232 Jan 20 20:12 thunder-bootfs.img 一般的权限后面有个., 这个是selinux security context, 很多时候, 这个东西会很麻烦 比如明明有写权限的文件不能访问, git库不能push等等 ls -Z可以查看这个权限 [root@cavium cavium]# ls -Z drwxr-xr-x root root ? nfs drwxrwxr-x root cavium ? repo -rw-r--r-- yingjie cavium ? RHELSA-1.6-Server.img.xz drwxrwxrwx. root cavium unconfined_u:object_r:home_root_t:s0 share -rw-r--r-- root root ? tests.tar.gz -rw-r--r-- root root ? thunder-bootfs.img chcon命令可以改这个selinux security context权限 setfattr -x security.selinux 可以去掉这个扩展权限 find . -exec setfattr -x security.selinux {} ; 或者 [root@cavium /]# find cavium/ -exec setfattr -x security.selinux {} ; shell for $ for i in {1..5};do echo next $i;done next 1 next 2 next 3 next 4 next 5 repos='\".\" \"bootloader/edk2/\" \"bootloader/grub2/grub/\" \"bootloader/trusted-firmware/atf/\" \"bootloader/u-boot/\" \"linux/kernel/linux-aarch64/\"' $ for i in $repos;do echo 1111111 $i;done 1111111 \".\" 1111111 \"bootloader/edk2/\" 1111111 \"bootloader/grub2/grub/\" 1111111 \"bootloader/trusted-firmware/atf/\" 1111111 \"bootloader/u-boot/\" 1111111 \"linux/kernel/linux-aarch64/\" 注:做为一个变量, repos表示一个list, 此时也可不加每个元素的双引号\"\", 也可以打印每个元素; 但在for里面直接写引号, 比如for i in '1 2 3 4 ';do echo dddd$i;done则只能打印一行. 观察82599的中断 watch -d -n 1 \"cat /proc/interrupts | egrep 'eth5|CPU0'\" 关于中断balance 设置哪几个CPU可以处理中断 $ echo f0 > /proc/irq/11/smp_affinity 在网络非常 heavy 的情况下，对于文件服务器、高流量 Web 服务器这样的应用来说，把不同的网卡 IRQ 均衡绑定到不同的 CPU 上将会减轻某个 CPU 的负担，提高多个 CPU 整体处理中断的能力；对于数据库服务器这样的应用来说，把磁盘控制器绑到一个 CPU、把网卡绑定到另一个 CPU 将会提高数据库的响应时间、优化性能。合理的根据自己的生产环境和应用的特点来平衡 IRQ 中断有助于提高系统的整体吞吐能力和性能。 注意：在手动绑定 IRQ 到 CPU 之前需要先停掉 irqbalance 这个服务，irqbalance 是个服务进程、是用来自动绑定和平衡 IRQ 的： $ /etc/init.d/irqbalance stop 使用的脚本 #!/bin/sh -e interrupt_name=\"eth3-TxRx-\" core_num=`cat /proc/cpuinfo | grep \"processor.*:\" -c` irq_num=`cat /proc/interrupts | grep $interrupt_name -c` [ $irq_num -eq $core_num ] || (echo 'Error, Call Cavium FAE!!!' && exit) irq_list=`cat /proc/interrupts | grep $interrupt_name | awk -F \":\" '{print $1}'` first_riq=`cat /proc/interrupts | grep ${interrupt_name}'0' | awk -F \":\" '{print $1}'` for irq in $irq_list do duty_core=$(($irq - $first_riq)) cpu_mask=$((1/proc/irq/$irq/smp_affinity done 多个jpg图片转到pdf convert *.jpg myeduction.pdf ssh X11及VNC firefox over ssh x11 forwarding 本来想开VNC server来远程图形方式访问办公室的机器, 但发现其实ssh的xorg forwarding功能就基本满足需求: 需要打开-X选项, 默认是关闭的. 比如在我的笔记本上: ssh -X baiyingjie@caviumsh.f3322.net 而此时登录到了办公室DELL的机器上, 此时敲firefox, 会在我的笔记本上打开DELL机器的firefox baiyingjie@mserver-dell ~ $ firefox 此时的firefox里面的内容其实是DELL机器上的, 比如我可以访问inventec2机器的BMC, 直接在地址栏里192.168.1.12就OK了. 让远程的firefox更快 上面的方法在功能上已经没什么问题了, 但使用下来感觉很卡, 延时太高. 这里面的原因, 网上说法是ssh的加密算法问题, 而不是firefox本身的问题--有待确认, 但感觉加了压缩选项就好很多, 似乎和加密算法关系不大. 默认的ssh由于使用比较强力的加解密算法, 比如AES, 可能会比较卡; 改成下面的加密算法会好很多 ssh -X -C -c blowfish-cbc,arcfour baiyingjie@caviumsh.f3322.net -X: 使能x11 forwarding -C: 使能压缩, 经验证这个选项提升速度最明显 -c: 指定cipher, 大约就是加解密算法吧 tools change tmux prefix C-b : set prefix C-a tmux复制 .tmuxrc里面加 bind-key -t vi-copy 'v' begin-selection bind-key -t vi-copy 'y' copy-selection 先C-b [进入copy mode v选中, y复制 C-b ]粘贴 network mount fae storage sudo mount -t cifs castr1:/FAE -o username=ybai,password=CQUbyj@2010 castorage 这个地址好像不认, 但没关系, 可以这样找到ip $ ping castr1.caveonetworks.com PING castr1.caveonetworks.com (10.18.5.15) 56(84) bytes of data. 所以这样应该可以了--还是不行 sudo mount -t cifs 10.18.5.15:/FAE -o username=ybai,password=CQUbyj@2010 castorage 最终版本 --验证可以 sudo mount -t cifs //10.18.5.15/FAE -o username=ybai,password=CQUbyj@2011 castorage 如果想断点续传 rsync -rv --progress --append /mnt/MARKETING/Product_Releases/ThunderX/CDK/Linux/GoldenImage . 新建一个用户到特定组 useradd joel -g cavium 查看虚拟分区 kpatx和losetup --原理可能是自动检测image的分区表 先创建一个1G大小的映像文件来做实验 dd bs=4096 if=/dev/zero of=~/hd.img count=262144 将映像文件挂接到loopX中去 losetup /dev/loopX ~/hd.img 对loopX进行分区 fdisk /dev/loopX 我这里分了两个区，每个去512M大小 Device Boot Start End Blocks Id System /dev/loopXpY 2048 1050623 524288 83 Linux /dev/loopXpY 1050624 2097151 523264 83 Linux 正戏来了，使用kpartd装载映像，使用kpartx是需要root用户的，因为是用root登录的，所以不用使用sudo。从前面的命令就可以看出来... kpartx -av ～/hd.img 装载之后，就可以在/dev/mapper/目录下看到两个loopXpY的文件了。 接下来对loopXpY进行格式化了。 mkfs.vfat /dev/mapper/loopXpY 然后挂载文件系统。 mount /dev/mapper/loop1p1 /media/hd1 OK，罗嗦完了。 thunder通过服务器上网 服务器em1 192.168.1.5可上外网 eth5是10G口, 和thunder的10G口eth3相连 首先, iptables是个用户态工具, 负责配规则. kernel里面的netfilter负责执行这些规则. 编译内核时需要打开netfilter功能 在服务器上: 先查link状态 ethtool eth5 设ip ifconfig eth5 9.9.9.99 up 打开转发 sysctl -w net.ipv4.ip_forward=1 设iptable, 所有9.9.9.0网络通过em1走nat iptables -t nat -A POSTROUTING -j MASQUERADE -s 9.9.9.0/24 -o em1 所有配置OK 但这里我记两个问题: 为什么看不到POSTROUTING链? 也看不到刚才设的规则. 在mint上也一样, 但可以工作 [root@cavium yingjie]# iptables -L Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT udp -- anywhere anywhere udp dpt:domain ACCEPT tcp -- anywhere anywhere tcp dpt:domain ACCEPT udp -- anywhere anywhere udp dpt:bootps ACCEPT tcp -- anywhere anywhere tcp dpt:bootps Chain FORWARD (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere 192.168.122.0/24 state RELATED,ESTABLISHED ACCEPT all -- 192.168.122.0/24 anywhere ACCEPT all -- anywhere anywhere REJECT all -- anywhere anywhere reject-with icmp-port-unreachable REJECT all -- anywhere anywhere reject-with icmp-port-unreachable Chain OUTPUT (policy ACCEPT) target prot opt source destination 解答: iptables -L这个命令没有指定table, 默认是filter, 那只显示filter涉及的链, 就只有三个. 用这个可以查刚才的nat规则 iptables -t nat -L 我没有配route. --需要确认是不是真的不用配? 在thunder上: 配ip ifconfig eht3 9.9.9.98 up 配路由 route add default gw 9.9.9.9 配DNS --这里需要确认以下, DNS没有更好的办法配了么? cat /et/resolv.conf ping一下对端 `ping 9.9.9.9`` ping一下真正的网关 ping 192.168.1.1 ping一下baidu ping www.baidu.com linux路由参考 在linux主机中开启iptables,做SNAT(源地址转换)；下面我就以RHEL为例了，， 1.eth0(网卡1----外网)，设置公网IP，开启路由转发（#vim /etc/sysctl.conf 修改net.ipv4.ip_forward = 1）， 2.eth1(网卡2----内网)，设置内网IP， 3.iptalbes添加规则： iptables -t nat -A POSTROUTING -o eth1 -s 192.168.100.0/24 -j SNAT --to-source 外网IP 注：192.168.100.0/24是内网网段，如果外网地址不是静态的，SNAT也可以如下添加：iptables -t nat -A POSTROUTING -o eth1 -s 192.168.100.0 -j MASQUERADE（表示自动匹配IP地址） 4.就是设置路由器了，自己搞定吧。 brctl就是个交换机 brctl addbr testbridge brctl addif testbridge eth5 brctl addif testbridge eth6 这里需要解释一下, br下面的port应该是没有ip的, 从逻辑上, 几个port合并成一个聚合的interface, 就是br, 对外只有一个ip 这也是为什么要给br设一个ip ifconfig eth5 0.0.0.0 ifconfig eth6 0.0.0.0 ifconfig testbridge up ifconfig testbridge 192.168.1.30 netmask 255.255.255.0 up wiz的搜索功能 一般为\"或\"搜索 linux版可以这样搜: \"struct*file\" windows版: s:struct AND file 批量修改jpg图片大小--xarg位置指代 find . -iname \"*.jpg\" | xargs -l -i convert -resize 50% {} /tmp/{} mkdir tmp && find . -iname \"*.jpg\" | xargs -l -i convert -resize 40% -rotate 180 {} tmp/{} 注: xargs有个参数-i 表示用{}替代上个命令的输出; 但现在推荐用-I replace-str 我自己的例子: 把CNNIC-SDK-SRC目录下的源文件考到src目录下, 去掉路径 find CNNIC-SDK-SRC/ -iname \"*.[ch]\" | xargs -i cp {} src ubuntu查看一个文件属于哪个package 第一种情况, 检查已经安装的文件 $ dpkg -S /usr/share/man/man1/qemu-system-mips.1.gz qemu-system-mips: /usr/share/man/man1/qemu-system-mips.1.gz 第二种情况, 即使没装过这个文件 apt install apt-file sudo apt-file update $ apt-file search /usr/share/doc/qemu/q35-chipset.cfg qemu: /usr/share/doc/qemu/q35-chipset.cfg ubuntu查看一个package包含哪些文件 dpkg -L qemu dpkg -L qemu-system-mips egrep 分组或 hg st -q | egrep -vi \"cmake|Makefile\" 删除mysql的cmake相关的文件 find -iname '*cmake*' -not -name CMakeLists.txt -not -path \"*/.hg/*\" -exec rm -rf {} \\+ cpu hotplug echo 0 > /sys/devices/system/cpu/cpuX/online grub reserve 内存 改/boot/grub/grub.conf, 在kernel 一行加 memmap=10M$1024M memmap=7G$1G memmap=nn[KMG]@ss[KMG] [KNL] Force usage of a specific region of memory Region of memory to be used, from ss to ss+nn. memmap=nn[KMG]#ss[KMG] [KNL,ACPI] Mark specific memory as ACPI data. Region of memory to be used, from ss to ss+nn. memmap=nn[KMG]$ss[KMG] [KNL,ACPI] Mark specific memory as reserved. Region of memory to be used, from ss to ss+nn. Example: Exclude memory from 0x18690000-0x1869ffff memmap=64K$0x18690000 or memmap=0x10000$0x18690000 给用户组加写权限 chmod g+w OCTEON-SDK-3.1.0 -R 修改文件所属用户 chown byj.byj Makefile 增加共享库路经 LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH 查看端口占用 netstat -apn | grep 500 -a: all -p: 显示进程名 -n: 不解析地址名, 这会快很多 显示是ipsec charon这个进程, 进程号是1670 udp 0 0 0.0.0.0:4500 0.0.0.0:* 1670/charon udp 0 0 0.0.0.0:500 0.0.0.0:* 1670/charon udp6 0 0 :::4500 :::* 1670/charon udp6 0 0 :::500 :::* 1670/charon 先不着急用kill命令 用ipsec stop 查找大于1M的文件 find -type f -size +1M | xargs ls -lh wget下载 $ wget --no-check-certificate https://www.kernel.org/pub/software/scm/git/git-2.1.0.tar.gz 通过gcc查找libc路径 -- -print-file-name=xxx $ mips64-octeon-linux-gnu-gcc -march=octeon3 -mabi=n32 -EB -pipe -msoft-float -print-file-name=libc.a /repo/yingjieb/fgltb/sw/vobs/esam/build/reborn/buildroot-isam-reborn-cavium-fgltb/output/host/opt/ext-toolchain/bin/../mips64-octeon-linux-gnu/sys-root/usr/lib/../lib32-fp/libc.a 各种进制转十进制 $ echo $((2#111111)) 63 $ echo $((16#4f)) 79 $ echo $((0x4f)) 79 附: 十进制转十六进制, 以下几种都行 printf \"%x\\n\" 34 echo \"obase=16; 34\" | bc ( echo \"obase=16\" ; cat file_of_integers ) | bc bc --shell下的C计算器 其实bc可以进行任意进制转换 ibase是输入的进制 obase是输出的进制 bc其实是个语言, 语法类似C $ echo \"i=10; i++;i++\" | bc 10 11 CPU热插拔 # echo 0 > /sys/devices/system/cpu/cpu2/online Reset core 2. Available Coremask = 3fc # grep \"processor\" /proc/cpuinfo processor : 0 processor : 1 # echo 1 > /sys/devices/system/cpu/cpu2/online SMP: Booting CPU02 (CoreId 2)... CPU revision is: (Cavium Octeon II) Cpu 2 online # grep \"processor\" /proc/cpuinfo processor : 0 processor : 1 processor : 2 hexdump查ascii码 -v选项 //-v表示接受输入, -n8表示有8个字节的数据, 缺点是只能输入键盘字符 /isam/user # hexdump -v -n8 01abAB%^ 0000000 3031 6162 4142 255e 0000008 hexdump指定格式 //跳过6个字节, 显示6个字节, 1/1表示1次, 每次1个字节, 按照%02X格式 /isam/user # hexdump -v -n6 -s6 -e'1/1 \"%02X\"' /isam/prozone/PBDD 060000000010/isam/user # 查看温感 dts i2c compatible = \"ti,tmp432\",\"ti,tmp431\"; compatible = \"ti,lm75\"; cat /sys/class/hwmon/hwmon1/device/temp3_input 45250 表示45.25°C trap命令 --指定shell中处理signal 用kill -l查看所有的signal号 # Ignore SIGPIPE # When a script is run from an ssh session that is closed before the script is # finished, and the script writes to stdout (which no longer is connected to # something) then the script would get a SIGPIPE signal and be aborted. # This is normally not desired, so ignore the SIGPIPE signal entirely. log_sigpipe() { # only if __sigpipe_occurred is not set if [ -z ${__sigpipe_occurred+x} ]; then echo \"$(date): $0 ($$): SIGPIPE occurred, ignoring (parent $(pid_to_procname $PPID) ($PPID))\" >> /isam/logs/info_siglog __sigpipe_occurred=1 fi } trap log_sigpipe SIGPIPE 创建ram文件系统 mount -o size=16G -t tmpfs none /mnt/tmpfs heredoc格式代码 astyle --mode=c --style=linux -spfUcH install命令 在做共享库的Makefile中, 用到了install命令, 这个命令和cp功能差不多. mkdir -p $(DESTDIR)/usr/lib install $(LIB) $(DESTDIR)/usr/lib ln -sf $(LIB) $(DESTDIR)/usr/lib/$(NAME) ln -sf $(LIB) $(DESTDIR)/usr/lib/$(SONAME) readlink --读取链接文件 比如build是个链接 $ readlink -f build /repo/yingjieb/fdt063/sw/vobs/esam/build 重新修改tmpfs的大小 mount -o remount,size=XXX /tmp 或者 $ hg diff -c7639b7191348 diff --git a/board/Alcatel-Lucent/isam-reborn/common/post_build.sh b/board/Alcatel-Lucent/isam-reborn/common/post_build.sh --- a/board/Alcatel-Lucent/isam-reborn/common/post_build.sh +++ b/board/Alcatel-Lucent/isam-reborn/common/post_build.sh @@ -37,3 +37,10 @@ sed -i 's%^root:[^:]*:%root:$6$Pb/CNtO0N # cat $archdir/mdev-extra.conf >> $targetdir/etc/mdev.conf # cat $commondir/mdev-base.conf > $targetdir/etc/mdev.conf + +# Restrict size of /tmp +# By default /tmp has a maximum size of RAMsize/2, which is very big. +# This means that someone can write up to RAMsize/2 to /tmp, causing only half +# of RAM memory to be usable by real software. Since we should not need that +# large a /tmp directory, limit its size. +sed -i 's%^.*[ \\t]/tmp[ \\t].*$%tmpfs /tmp tmpfs defaults,size=64M 0 0%' $targetdir/etc/fstab fg bg --前台运行 后台运行 fg %1 bg后台运行的好处是程序还在跑, 而ctrl+z程序不跑了 hello: 后台运行的程序用printf向默认控制台打印也能输出 shell写字符串, 以'\\0'结尾 printf \"$string\\0\" > $tmpfile 写十六进制数据到文件, 关键在于引号 /isam/user # printf '\\xde\\xad\\xbe\\xef' > file /isam/user # hexdump -C file 00000000 de ad be ef |....| 00000004 /isam/user # echo -ne \"\\xde\\xad\\xbe\\xef\" > file /isam/user # hexdump -C file 00000000 de ad be ef |....| 00000004 没有引号就不行 /isam/user # echo -e \\xde\\xad\\xbe\\xef > file /isam/user # hexdump -C file 00000000 78 64 65 78 61 64 78 62 65 78 65 66 0a |xdexadxbexef.| 0000000d 批量处理软链接之cscope.files cat cscope.files | while read f; do if [ ! -h $f ]; then echo $f; fi; done readelf结果用sort排序, 按照第三列数字排序 cat uboot.readelf | sed -e '1,79d' -e '3128,$d' | sort -rnk 3 批量删除ZF ZL ZG cat cscope.files | egrep \"isam|fpxt|fglt|nvps|vipr|rant\" | xargs sed -i -e \"s%Z[LFG]_%%g\" 批量格式化代码 使用4个空格 cat cscope.files | xargs astyle --mode=c --style=linux -spfUcH 使用tab cat cscope.files | xargs astyle --mode=c --style=linux -tpfUcH 最终版 cat cscope.files | egrep \"isam|fpxt|fglt|nvps|vipr|rant\" | xargs astyle --mode=c --style=linux -tpfUcH 带时间的平均负载 $ echo \"$(date +%H:%M:%S) # $(cat /proc/loadavg)\" 10:07:14 # 0.98 1.00 1.15 2/723 26465 rpm解压 rpm2cpio OCTEON-LINUX-2.3.0-427.i386.rpm | cpio -div 大小写转换 tr '[:upper:]' '[:lower:]' output.txt sed -e 's/\\(.*\\)/\\L\\1/' input.txt > output.txt sed -e 's/\\(.*\\)/\\U\\1/' input.txt > output.txt 解析C文件全部函数到h文件声明, 用于偷懒声明所有函数到头文件 cat src/board_cpld.c | egrep \"(void|unsigned char|u_int8)[[:blank:]]+[0-9a-zA-Z_]+\\(.*\\)\" | sed -e 's/\\(.*\\)/\\1;/g' strace 重定向 strace echo \"groad\" &> myfile.txt strace echo \"groad\" > myfile.txt 2>&1 通过elf生成工程文件列表 --利用gdb mips64-octeon-linux-gnu-gdb -ex=\"info sources\" -ex=\"quit\" isam_app.nostrip | sed -e '1,15d' -e 's/,/\\n/g' | sed -e '/^ *$/d' -e 's/^ *//g' > temp.list find -L `cat cscope.files| egrep \"/flat/\" | sed 's!\\(.*/flat/[^/]*\\).*!\\1!g' | sort -u` -iname \"*.h\" -o -iname \"*.hh\" -o -iname \"*.hpp\" sync命令使用, 把file buffer 刷进物理器件 / # time dd if=/dev/zero of=/bigfile bs=65536 count=2048 2048+0 records in 2048+0 records out real 0m 0.29s user 0m 0.00s sys 0m 0.30s The command line (and similar ones) I used when sync was in order: / # time dd if=/dev/zero of=/bigfile bs=65536 count=2048 && time sync 2048+0 records in 2048+0 records out real 0m 3.11s user 0m 0.01s sys 0m 0.15s 查找src下面的代码目录, find可以限制搜索深度 output=`find -maxdepth 1 -type d | grep -i socket` echo $output echo $output >> src_dirs.byj sort -u src_dirs.byj mkcsfiles `cat src_dirs.byj.sort` 注: sort -u能够去掉重复行 查看当前文件夹大小 du -sh 删除文件夹下面的所有链接文件 find . -type l | xargs rm -rf 压缩当前目录下sw_c文件夹 tar -zcvf sw_c.tar.gz sw_c 根据一个文本文件列表压缩 --hT tar cvf /repo1/yingjieb/isam_lis/sw/build/fant-f/OS/fant-f_osw.tar -hT /repo1/yingjieb/isam_lis/sw/build/fant-f/OS/path tar.xz格式压缩解压 压缩 tar -Jcvf etc.tar.xz /etc 解压 tar -Jxf etc.tar.xz 在当前路径下的所有文件中，搜索关键字符串 grep -rn byj . ls按修改时间排序 cat cscope.files | xargs ls -lt | more ls按大小排序 cat cscope.files | xargs ls -lhS | more 查看键盘映射 stty -a 限制深度的find find -maxdepth 2 -name \".stamp*\" | xargs rm -f 查看系统支持的文件系统类型 cat /proc/filesystems 替换字符串 $ sst=abcdefcd 只替换第一个位置 $ echo ${sst/cd/55} ab55efcd ASBLX28:/home/yingjieb 全替换 $ echo ${sst//cd/55} ab55ef55 ${string/#substring/replacement} 如果$substring匹配$string的开头部分, 那么就用$replacement来替换$substring. ${string/%substring/replacement} 如果$substring匹配$string的结尾部分, 那么就用$replacement来替换$substring. 批量软链接 for file in *; do; if [ ! -L $file -a \"$file\" != \"Makefile\" ]; then; echo ln $file...;ln -sf ../../../../../executive/$file $file;fi;done tee tee指令会从标准输入设备读取数据，将其内容输出到标准输出设备,同时保存成文件。我们可利用tee把管道导入的数据存成文件，甚至一次保存数份文件。 处理软链接, Thomas版 find arch -type l | xargs ls -1l | grep executive | awk '{sub(\"../../\",\"\",$11); printf \"ln -sf %s %s\\n\",$11,$9}' 关闭printk打印 echo 1 1 1 1>/proc/sys/kernel/printk usb串口 --lsusb --minicom 插入usb串口以后，lsusb可以查看usb总线下面的设备。动态的。 $lsusb 此时查看 $ls /dev/tty* 可以看到新增了四个ttyUSB /dev/ttyUSB0 /dev/ttyUSB1 /dev/ttyUSB2 /dev/ttyUSB3 现在可以安装minicom apt install minicom locate--linux下面的everything byj@byj-mint ~/repo/hg/OCTEON-SDK-3.1-tools $locate OCTEON-SDK-3.1-p2-tools.tar.xz /home/byj/repo/alu/buildroot-isam-reborn-cavium-fgltb/dl/OCTEON-SDK-3.1-p2-tools.tar.xz /home/byj/repo/alu/fgltb/sw/vobs/esam/build/reborn/buildroot-isam-reborn-cavium-fgltb/dl/OCTEON-SDK-3.1-p2-tools.tar.xz /home/byj/share/OCTEON-SDK-3.1-p2-tools.tar.xz rsync拷贝目录，去除hg rsync -av --exclude .hg OCTEON-SDK-3.1-pristine OCTEON-SDK-3.1 "},"notes/shell_高级篇.html":{"url":"notes/shell_高级篇.html","title":"shell命令和脚本记录-高级篇","keywords":"","body":" 用socat连接pty 什么是tty pty 读写pts socat概览 常用选项 地址格式 地址类型 选项 FD组 NAMED option group OPEN option group REG and BLK option group PROCESS option group APPLICATION option group SOCKET option group IP4 and IP6 option groups TERMIOS option group FORK option group 例子 脚本命令行解析之declare -f shell变量扩展和引号保留 背景: 简单wrapper 参数重载 变量赋值导致字符串多变一 shell变量扩展方式 printf 最终版, 用eval 补充: shell的变量扩展 curl使用说明和举例 使用举例1 使用举例2 使用举例3 URL格式 curl和RESTful gitlab API举例 简介 举例 创建新project 获取gitignore模板 分支维护 sed解析log CPU利用率条件触发perf record top的记录导入到excel 比较粗糙的版本 CPU mem 同时导出CPU利用率和mem 只导出cpu利用率 改进版 用tr处理多余字符 一次shell重定向过程 shell重定向和exec 动态变量名及其引用: eval的使用 用eval生成动态变量名 间接引用变量 举例: 读出*.conf文件内容到动态文件名变量 uaes进程管理, 关系数组方式实现 判断字串是否包含字串 exec重定向 在shell里直接重定向stdin 重定向整个shell的stdout 输入和输出的例子 crosstool ng的例子 补充: test可以检测FD是否是terminal shell读写socket 更简单的写法 /dev/tcp是shell内建的设备节点 shell进程间通信 有名管道和nc shell检查一个脚本是否已经被include了 sed 向sed传递变量 sed使用记录 sed命令 sed匹配2行之间的内容 提取文件特定行 sed正则表达式 sed定址替换 sed删除 sed替换引用 sed的分组匹配, 用()分组, 用\\1引用 awk shell变量传给awk 再说patten awk在if里匹配 awk支持浮点 awk逻辑与 awk多行解析 awk分组提取 awk变量 awk数组 awk字符串替换 awk的条件判断和数字运算 awk执行其他程序 awk替换命令 shell管道和循环 看一个interface是否存在 用/proc/net/dev 用ifconfig intf返回值 awk计算时间差 ping延迟大于0.5ms触发动作 计算cpu掩码, mask shell的进程创建 粘贴多行文本管道后再用后续命令处理 关系数组 sed 和 awk 改进版 awk多维数组 原始版 shell关系数组 -- 或者说是list shell数组 shell脚本解析文件, 输出可以导入到excel的文本 shell处理命令行选项 shell也可以递归调用函数 改进版本 local system monitor system monitor for pangu 增加fedora分区的脚本 用socat连接pty 什么是tty pty linux终端(Terminals)有三种: 串口: 比如ttyS0, ttyUSB0 pty: /dev/pts/xx 屏幕命令行终端: tty1, tty2... 这里我们使用pty, pty是master slave模式的, master用于创建slave, 向slave读写内容; slave就是模拟的终端 master: /dev/ptmx slave: /dev/pts/* 读写pts 比如cloud-hypervisor使用pty文件/dev/pts/2做为VM的控制台, cat /dev/pts/2能够在host上查看VM的输出, echo xxxx > /dev/pts/2可以向VM输入. 但怎么同时输入输出? 就像进入VM的控制台一样? 先给出结果, 用socat: socat -,rawer,escape=29 /dev/pts/2 这个命令的意思是, 建立从raw模式的stdio到/dev/pts/2的双向stream连接 -: 表示stdin和stdout rawer是raw模式, 不echo; 在raw模式下, 比如ctrl+c会发送给VM, 不再用来退出socat escape=29: 使用ctrl+]来退出socat /dev/pts/2: 是pty文件 socat概览 socat是强大的双向stream对接工具, 基本作用就是建立address1到address2的双向byte stream连接. socat [options] 常用选项 -d -d: 打印verbose debug信息, 可多个-d -T: 空闲timeout时间自动退出 -u/-U: 单向数据传输 -L: lock文件 地址格式 socat支持多种地址类型, 格式为地址类型:地址,选项1,选项2 某些常用地址类型可以用简化写法: -: stdio TCP: TCP4 数字: FD 地址类型 CREATE: 创建文件写入 EXEC: 使用command做为input/output FD: 使用已经存在的fd GOPEN: 打开文件读写. 文件可以是unix socket, 会自动connect, connect失败则假定用sendto. 可以省略GOPEN. INTERFACE: 带L2的raw packet IP-DATAGRAM:: IP4-DATAGRAM:: IP6-DATAGRAM:: IP-RECVFROM: IP4-RECVFROM: IP6-RECVFROM: IP-RECV: IP-SENDTO:: IP4-SENDTO:: IP6-SENDTO:: raw/datagram socket PIPE: PIPE 有名和无名管道 PROXY::: proxy PTY 创建新的pty slave SCTP-CONNECT:: SCTP协议 SOCKET-CONNECT::: SOCKET-DATAGRAM:::: SOCKET-RECV:::: SOCKS4::: socket STDIN STDIO STDOUT SYSTEM: 感觉和exec差不多 TCP:: TCP-LISTEN: TUN[:/] 新建tun设备做为写侧 UDP:: UDP-LISTEN: udp还有listen? UNIX-CONNECT: UNIX-LISTEN: UNIX-SENDTO: UNIX-RECVFROM: UNIX-RECV: UNIX-CLIENT: ABSTRACT-CONNECT: unix socket 选项 socat支持很多选项, 比如上面例子中的raw 不同的group支持不同的option FD组 这个组里的option支持对fd配置某种参数. cloexec setlk flock-ex lock user mode append nonblock null-eof ... 很多 NAMED option group 作用于文件名 umask unlink-close OPEN option group creat noatime rdonly rsync ... REG and BLK option group seek ... PROCESS option group chroot setgid su ... APPLICATION option group 作用于数据 cr Converts the default line termination character NL (’\\n’, 0x0a) to/from CR (’\\r’, 0x0d) when writing/reading on this channel. escape SOCKET option group bind= broadcast rcvbuf sndbuf IP4 and IP6 option groups tos ip-pktinfo TERMIOS option group b0 波特率0, 即断开连接 b19200 echo= rawer ignbrk brkint ... 很多关于特殊字符控制的 FORK option group pipes sighup, sigint, sigquit 例子 #把stdio转到tcp socat - TCP4:www.domain.org:80 #和上面差不多, 但支持line edit socat -d -d READLINE,history=$HOME/.http_history TCP4:www.domain.org:www,crnl #转发listen, 但只accept一个connection socat TCP4-LISTEN:www TCP4:www.domain.org:www #加了fork就能允许多个connection socat TCP4-LISTEN:5555,fork,tcpwrap=script EXEC:/bin/myscript,chroot=/home/sandbox,su-d=sandbox,pty,stderr #stdion转到ttyS0, 用control-O退出socat socat -,escape=0x0f /dev/ttyS0,rawer,crnl 脚本命令行解析之declare -f 下面的代码中, 脚本main里面解析命令行, 只看了通用的--help等, 而其他的命令都通过declare -f \"cmd_$1\" > /dev/null先\"声明\"函数, 再调用:$cmd \"$@\" main() { if [ $# = 0 ]; then die \"No command provided. Please use \\`$0 help\\` for help.\" fi # Parse main command line args. # while [ $# -gt 0 ]; do case \"$1\" in -h|--help) { cmd_help; exit 1; } ;; -y|--unattended) { OPT_UNATTENDED=true; } ;; -*) die \"Unknown arg: $1. Please use \\`$0 help\\` for help.\" ;; *) break ;; esac shift done # $1 is now a command name. Check if it is a valid command and, if so, # run it. # declare -f \"cmd_$1\" > /dev/null ok_or_die \"Unknown command: $1. Please use \\`$0 help\\` for help.\" cmd=cmd_$1 shift # $@ is now a list of command-specific args # $cmd \"$@\" } main \"$@\" 比如想实现$0 build_rootfs命令, 用上面的方法, 只需要增加cmd_build_rootfs()函数: 但需要每个\"子命令\"函数都自己实现命令解析 # `./devtool build_rootfs -s 500MB` # Build a rootfs of custom size. # cmd_build_rootfs() { # Default size for the resulting rootfs image is 300MB. SIZE=\"300MB\" FROM_CTR=ubuntu:18.04 flavour=\"bionic\" # Parse the command line args. while [ $# -gt 0 ]; do case \"$1\" in \"-h\"|\"--help\") { cmd_help; exit 1; } ;; \"-s\"|\"--size\") shift SIZE=\"$1\" ;; \"-p\"|\"--partuuid\") shift PARTUUID=1 ;; *) die \"Unknown argument: $1. Please use --help for help.\" ;; esac shift done ... } shell变量扩展和引号保留 shell变量扩展语法 背景: 这里的测试命令是: go list -f \"{{context.GOARCH}} {{context.Compiler}}\" -- unsafe #正常的结果是 amd64 gc #说明go命令能正确理解传入的参数 但我这里需要用一个bash wrapper封装原始的go命令(重命名为_go), 把原始参数修改后再传入_go 简单wrapper 这样是可以的 #!/bin/bash exec _go \"$@\" 这里 \"$@\"会把所有的参数\"原封不动\"的传入底层命令 补充一下: $#是参数个数, 这里为5, 用for a in \"$@\"来遍历参数, 这5个参数分别是 list -f ` : 这里说明 for in对位置参数的遍历, 不是以空白符为分隔的. 因为这里虽然有空格, 但它们会做为一个整体被赋值到变量a` -- unsafe 参数重载 但我的需求是更改参数, 有如下尝试: #!/bin/bash cmd=$1 shift exec _go $cmd \"$@\" 这个可以. 说明shift对参数移位了, 但不会影响\"$@\"对原始字符串的操作 变量赋值导致字符串多变一 但下面的代码不正常工作: #!/bin/bash cmd=$1 shift args=\"$@\" exec _go $cmd \"$args\" #或者下面的命令 exec _go $cmd $args 的结果是flag provided but not defined: -f {{context.GOARCH}} {{context.Compiler}} -- unsafe 经过检查, 和这样执行原始命令是一样的: _go list \"-f {{context.GOARCH}} {{context.Compiler}} -- unsafe\" 说明: args=\"$@\"的结果是, args把-f -- unsafe会变成一个整体的string shell变量扩展方式 下面的都不行 #!/bin/bash cmd=$1 shift exec _go $cmd ${*@Q} #结果 can't load package: package '-f': malformed module path \"'-f'\": invalid char '\\'' exec _go $cmd \"${*@Q}\" #结果 can't load package: package '-f' '{{context.GOARCH}} {{context.Compiler}}' '--' 'unsafe': malformed module path \"'-f' '{{context.GOARCH}} {{context.Compiler}}' '--' 'unsafe'\": invalid char '\\'' 这里用到了shell的变量扩展: ${parameter@operator} The expansion is either a transformation of the value of parameter or information about parameter itself, depending on the value of operator. Each operator is a single letter: Q The expansion is a string that is the value of parameter quoted in a format that can be reused as input. If parameter is ‘@’ or ‘*’, the operation is applied to each positional parameter in turn, and the expansion is the resultant list. If parameter is an array variable subscripted with ‘@’ or ‘*’, the operation is applied to each member of the array in turn, and the expansion is the resultant list. 意思是把$*当作一个list, 对其中的每个成员做Q操作. 从结果来看Q操作实际上把成员做了转义-引号:比如-f变成了\\'-f\\', {{context.GOARCH}} {{context.Compiler}}变成了\\'{{context.GOARCH}} {{context.Compiler}}\\'传递给_go命令时, 这些转义被当作原始输入, 不能被正确解析.${*@Q}和\"${*@Q}\"的区别是, 后者被当作一个整体的string被传入. printf printf也支持 printf: printf [-v var] format [arguments] %b expand backslash escape sequences in the corresponding argument %q quote the argument in a way that can be reused as shell input The format is re-used as necessary to consume all of the arguments. 比如 #!/bin/bash args=\"$@\" echo \"${args@Q}\" #输出 'list -f {{context.GOARCH}} {{context.Compiler}} -- unsafe' printf \"====%s\\n\" ${args@Q} #输出 ===='list ====-f ===={{context.GOARCH}} ===={{context.Compiler}} ====-- ====unsafe' ${args@Q}中, ${args}被当作一个整体, 加上了引号. 这个整体传入printf的时候, printf按空白符分割, 被当作6个argument, 并分别用\"====%s\\n\"来格式化. 最终版, 用eval #!/bin/bash cmd=$1 shift args=${*@Q} echo $cmd $args eval exec _go $cmd $args #不用exec也是可以的 eval _go $cmd $args 解释: 既然${*@Q}能够把每个成员都加转义-引号, 那么用shell的eval命令, 重新解析一下这个命令, 就能正常运作了. 补充: shell的变量扩展 还是这个链接 变量扩展能够: 变量重定向: ${!var} If the first character of parameter is an exclamation point (!), and parameter is not a nameref, it introduces a level of indirection. Bash uses the value formed by expanding the rest of parameter as the new parameter; this is then expanded and that value is used in the rest of the expansion, rather than the expansion of the original parameter. This is known as indirect expansion. 变量替换: ${parameter/pattern/string} 根据匹配规则, 用string替换变量$parameter中的pattern串. 可以全部替换, 可以只替换第一个 parameter支持list模式, 比如array[@] 大小写转换: ${parameter^pattern} ${parameter,pattern} 匹配和删除: ${parameter#word} ${parameter##word} ${parameter%word} ${parameter%%word} 字符个数: ${#parameter} 如果是array[@]的形式, 返回成员个数 变量默认值: ${parameter:-word} ${parameter:=word} ${parameter:?word} ${parameter:+word} ${!prefix*} ${!prefix@} ${!name[@]} ${!name[*]} 没看懂有啥用 Expands to the names of variables whose names begin with prefix, separated by the first character of the IFS special variable. 子串扩展: ${parameter:offset} ${parameter:offset:length} 支持负数 $ string=01234567890abcdefgh $ echo ${string:7} 7890abcdefgh $ echo ${string:7:0} $ echo ${string:7:2} 78 $ echo ${string:7:-2} 7890abcdef $ echo ${string: -7} bcdefgh $ echo ${string: -7:0} $ echo ${string: -7:2} bc $ echo ${string: -7:-2} bcdef $ set -- 01234567890abcdefgh $ echo ${1:7} 7890abcdefgh $ echo ${1:7:0} $ echo ${1:7:2} 78 $ echo ${1:7:-2} 7890abcdef $ echo ${1: -7} bcdefgh $ echo ${1: -7:0} $ echo ${1: -7:2} bc $ echo ${1: -7:-2} bcdef $ array[0]=01234567890abcdefgh $ echo ${array[0]:7} 7890abcdefgh $ echo ${array[0]:7:0} $ echo ${array[0]:7:2} 78 $ echo ${array[0]:7:-2} 7890abcdef $ echo ${array[0]: -7} bcdefgh $ echo ${array[0]: -7:0} $ echo ${array[0]: -7:2} bc $ echo ${array[0]: -7:-2} bcdef 支持list模式(array模式). $ array=(0 1 2 3 4 5 6 7 8 9 0 a b c d e f g h) $ echo ${array[@]:7} 7 8 9 0 a b c d e f g h $ echo ${array[@]:7:2} 7 8 $ echo ${array[@]: -7:2} b c $ echo ${array[@]: -7:-2} bash: -2: substring expression curl使用说明和举例 curl不仅可以传输http, 还支持几乎所有的传输协议:比如tftp telnet smbs imap 等等等等 curl有非常多的选项 使用举例1 # https://www.jfrog.com/confluence/display/JFROG/Artifactory+REST+API#ArtifactoryRESTAPI-WorkingwithArtifactoryCloud curl -X PUT $ARTIFACTORY_URL/$RELEASE_BASE/$versiondir/$subdir/$arch/$remote_package_name -T $package # https://www.jfrog.com/confluence/display/JFROG/Artifactory+REST+API#ArtifactoryRESTAPI-FileInfo curl -s -X GET $ARTIFACTORY_URL/api/storage/$RELEASE_BASE/$versiondir/$subdir/$arch curl -s -X GET $ARTIFACTORY_URL/api/storage/$RELEASE_BASE/$versiondir/$subdir/$arch/$package_name curl -s -X GET $ARTIFACTORY_URL/$RELEASE_BASE/$versiondir/$subdir/$arch/$package_name -o $local_package 说明: -X, --request (HTTP) Specifies a custom request method to use when communicating with the HTTP server. The specified request method will be used instead of the method otherwise used (which defaults to GET). Read the HTTP 1.1 specification for details and explanations. Common additional HTTP requests include PUT and DELETE, but related technologies like WebDAV offers PROPFIND, COPY, MOVE and more. Normally you don't need this option. All sorts of GET, HEAD, POST and PUT requests are rather invoked by using dedicated command line options. This option only changes the actual word used in the HTTP request, it does not alter the way curl behaves. So for example if you want to make a proper HEAD request, using -X HEAD will not suffice. You need to use the -I, --head option. The method string you set with -X, --request will be used for all requests, which if you for example use -L, --location may cause unintended side-effects when curl doesn't change request method according to the HTTP 30x response codes - and similar. -T, --upload-file This transfers the specified local file to the remote URL. If there is no file part in the specified URL, curl will append the local file name. NOTE that you must use a trailing / on the last directory to really prove to Curl that there is no file name or curl will think that your last directory name is the remote file name to use. That will most likely cause the upload operation to fail. If this is used on an HTTP(S) server, the PUT command will be used. Use the file name \"-\" (a single dash) to use stdin instead of a given file. Alternately, the file name \".\" (a single period) may be specified instead of \"-\" to use stdin in non-blocking mode to allow reading server output while stdin is being uploaded. You can specify one -T, --upload-file for each URL on the command line. Each -T, --upload-file + URL pair specifies what to upload and to where. curl also supports \"globbing\" of the -T, --upload-file argument, meaning that you can upload multiple files to a single URL by using the same URL globbing style supported in the URL, like this: curl --upload-file \"{file1,file2}\" http://www.example.com or even curl -T \"img[1-1000].png\" ftp://ftp.example.com/upload/ 使用举例2 #For example, the following cURL and build-info-permission.json define a new permission target called “java-developers”, for a build called “test-maven”: curl -uadmin:password -XPUT \"http://localhost:8081/artifactory/api/v2/security/permissions/java-developers\" -H \"Content-type: application/json\" -T build-info-permission.json 说明: -u, --user Specify the user name and password to use for server authentication. Overrides -n, --netrc and --netrc-optional. If you just give the user name (without entering a colon) curl will prompt for a password. If you use an SSPI-enabled curl binary and do NTLM authentication, you can force curl to pick up the user name and password from your environment by simply specifying a single colon with this option: \"-u :\". If this option is used several times, the last one will be used. -H, --header (HTTP) Extra header to use when getting a web page. You may specify any number of extra headers. Note that if you should add a custom header that has the same name as one of the internal ones curl would use, your externally set header will be used instead of the internal one. This allows you to make even trickier stuff than curl would normally do. You should not replace internally set headers without knowing perfectly well what you're doing. Remove an internal header by giving a replacement without content on the right side of the colon, as in: -H \"Host:\". If you send the custom header with no-value then its header must be terminated with a semicolon, such as -H \"X-Custom-Header;\" to send \"X-Custom-Header:\". curl will make sure that each header you add/replace is sent with the proper end-of-line marker, you should thus not add that as a part of the header content: do not add newlines or carriage returns, they will only mess things up for you. See also the -A, --user-agent and -e, --referer options. This option can be used multiple times to add/replace/remove multiple headers. 使用举例3 下载vscode server curl -#fL -o ~/.cache/code-server/code-server-3.6.0-linux-amd64.tar.gz.incomplete -C - https://github.com/cdr/code-server/releases/download/v3.6.0/code-server-3.6.0-linux-amd64.tar.gz -# 使用简化的进度条 -f Fail silently (no output at all) on server errors. 主要的目的是给远程执行脚本用的 -L 之前遇到过, follow redirection -C - 这两个连起来是说curl会 continue/Resume a previous file transfer URL格式 curl支持多个url地址, 比如 http://site.{one,two,three}.com ftp://ftp.example.com/file[1-100].txt ftp://ftp.example.com/file[a-z].txt #在一个url里面包含多个可变部分 http://example.com/archive[1996-1999]/vol[1-4]/part{a,b,c}.html #带步进的 http://example.com/file[1-100:10].txt http://example.com/file[a-z:2].txt curl和RESTful gitlab API举例 简介 gitlab API的根目录是https://gitlab.example.com/api/v4 比如 # 获取gitlabe1.ext.net.nokia.com下所有的projects, 返回值是json格式的 curl \"https://gitlabe1.ext.net.nokia.com/api/v4/projects\" POST是新建, PUT是更新: 使用curl的-X, --request选项指定. 默认是GET Methods Description --header \"PRIVATE-TOKEN: \" Use this method as is, whenever authentication needed --request POST Use this method when creating new objects --request PUT Use this method when updating existing objects --request DELETE Use this method when removing existing objects 有些API是需要权限的, 没有权限可能返回public data或者直接返回错误. 有几种方式提供权限, 比如 # 在参数里 curl https://gitlab.example.com/api/v4/projects?private_token= # 在header里 curl --header \"Private-Token: \" https://gitlab.example.com/api/v4/projects 比如获取group信息 curl --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/groups/gitlab-org\" 举例 创建新project 使用POST curl --request POST --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects?name=foo\" 也可以使用curl的--data curl --data \"name=foo\" --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects\" 再举个创建新group的例子: 这个例子使用json格式的数据做为输入 curl --request POST --header \"PRIVATE-TOKEN: \" --header \"Content-Type: application/json\" --data '{\"path\": \"my-group\", \"name\": \"My group\"}' \"https://gitlab.example.com/api/v4/groups\" 获取gitignore模板 对每个语言, gitlab都有gitignore模板, 对应GET /templates/gitignores请求 # 获取所有模板, 返回json格式的列表 curl https://gitlab.example.com/api/v4/templates/gitignores # 实例返回结果 [ { \"key\": \"Autotools\", \"name\": \"Autotools\" }, { \"key\": \"C\", \"name\": \"C\" }, { \"key\": \"C++\", \"name\": \"C++\" }, { \"key\": \"CFWheels\", \"name\": \"CFWheels\" }, { \"key\": \"CMake\", \"name\": \"CMake\" }, { \"key\": \"CUDA\", \"name\": \"CUDA\" } ] 比如看Ruby的gitignore模板, 需要用这样的语法:GET /templates/gitignores/:key curl https://gitlab.example.com/api/v4/templates/gitignores/Ruby #结果 { \"name\": \"Ruby\", \"content\": \"*.gem\\n*.rbc\\n/.config\\n/coverage/\\n/InstalledFiles\\n/pkg/\\n/spec/reports/\\n/spec/examples.txt\\n/test/tmp/\\n/test/version_tmp/\\n/tmp/\\n\\n# Used by dotenv library to load environment variables.\\n# .env\\n\\n## Specific to RubyMotion:\\n.dat*\\n.repl_history\\nbuild/\\n*.bridgesupport\\nbuild-iPhoneOS/\\nbuild-iPhoneSimulator/\\n\\n## Specific to RubyMotion (use of CocoaPods):\\n#\\n# We recommend against adding the Pods directory to your .gitignore. However\\n# you should judge for yourself, the pros and cons are mentioned at:\\n# https://guides.cocoapods.org/using/using-cocoapods.html#should-i-check-the-pods-directory-into-source-control\\n#\\n# vendor/Pods/\\n\\n## Documentation cache and generated files:\\n/.yardoc/\\n/_yardoc/\\n/doc/\\n/rdoc/\\n\\n## Environment normalization:\\n/.bundle/\\n/vendor/bundle\\n/lib/bundler/man/\\n\\n# for a library or gem, you might want to ignore these files since the code is\\n# intended to run in multiple environments; otherwise, check them in:\\n# Gemfile.lock\\n# .ruby-version\\n# .ruby-gemset\\n\\n# unless supporting rvm 分支维护 获取分支格式 GET /projects/:id/repository/branches 或者 GET /projects/:id/repository/branches/:branch 比如: curl https://gitlabe1.ext.net.nokia.com/api/v4/projects/57103/repository/branches curl https://gitlabe1.ext.net.nokia.com/api/v4/projects/57103/repository/branches/master 创建分支格式 POST /projects/:id/repository/branches 需要下面的参数: Attribute Type Required Description id integer yes ID or URL-encoded path of the project owned by the authenticated user. branch string yes Name of the branch. ref string yes Branch name or commit SHA to create branch from. 举例: curl --request POST --header \"PRIVATE-TOKEN: \" https://gitlab.example.com/api/v4/projects/5/repository/branches?branch=newbranch&ref=master # 返回 { \"commit\": { \"author_email\": \"john@example.com\", \"author_name\": \"John Smith\", \"authored_date\": \"2012-06-27T05:51:39-07:00\", \"committed_date\": \"2012-06-28T03:44:20-07:00\", \"committer_email\": \"john@example.com\", \"committer_name\": \"John Smith\", \"id\": \"7b5c3cc8be40ee161ae89a06bba6229da1032a0c\", \"short_id\": \"7b5c3cc\", \"title\": \"add projects API\", \"message\": \"add projects API\", \"parent_ids\": [ \"4ad91d3c1144c406e50c7b33bae684bd6837faf8\" ] }, \"name\": \"newbranch\", \"merged\": false, \"protected\": false, \"default\": false, \"developers_can_push\": false, \"developers_can_merge\": false, \"can_push\": true } 删除分支:DELETE /projects/:id/repository/branches/:branch 举例: curl --request DELETE --header \"PRIVATE-TOKEN: \" https://gitlab.example.com/api/v4/projects/5/repository/branches/newbranch sed解析log 要在一个很大的log里面提取OMCI的消息: 格式1 [trace] 08:36:48.888471 Dir: Tx --> Onu: ont1-fwlt-b Retry: 0 00000000 1c ee 49 0a 00 02 00 00 80 00 00 00 00 00 00 00 |..I.............| 00000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00000020 00 00 00 00 00 00 00 00 00 00 00 28 |...........(| 格式2 [2018/04/28 07:54:20.393226][omci][INFO]Dir: Rx 格式1和格式2就是打印的header不一样 用下面的sed命令提取 sed -rn -e '/.*Dir: .*--.*/,+4{s/.* (..:..:..\\.......).*(Dir.*)/\\1 \\2/g;p}' stdout_824128194_ok.log 其中 -r 使用扩展正则 -n 不自动打印pattern空间. 不加-n会打印所有原始行和修改行 -e 后面跟定址 /.*Dir: .*--.*/,+4 意思是所有匹配到Dir: Tx --> Onu和Dir: Rx CPU利用率条件触发perf record 条件是两个进程pidof onumgnt_hypervisor_app和pidof onu_engine的CPU利用率同时到10%以上. 注意用整个while块使用小括号括起来的, 小括号会产生子进程, 目的是让块内的exit退出这个子进程. 不用小括号的话, 这个代码块是在当前shell进程执行, 那exit会退出当前shell. ./topid -p `pidof onumgnt_hypervisor_app` -p `pidof onu_engine` > pid.log & ( while true do if [[ $(tail -n 2 pid.log | awk '{if ($3>10) print $5}' | wc -l) == 2 ]];then echo doing profiling LD_LIBRARY_PATH=`pwd` ./perf record -F 500 -e cycles -g --call-graph dwarf -p `pidof onumgnt_hypervisor_app` -- sleep 60 exit fi sleep 3 done ) 另外, 这个这个代码块不要用tab来缩进, 否则会被shell解析成auto completion来自动联想命令. top的记录导入到excel 比较粗糙的版本 CPU (echo time switch_hwa_app vonu xpon_hwa_app xponhwadp && cat top.log | awk '/GMT/{printf \"\\n\"$4} /run\\/switch_hwa_app/{printf(\" switch_hwa_app>%s\",$8)} /run\\/xpon_hwa_app/{printf(\" xpon_hwa_app>%s\",$8)} /run\\/xponhwadp/{printf(\" xponhwadp>%s\",$8)} /vONU-PoC-1 -skipav yes/{printf(\" vonu>%s\",$8)}' | while read line; do echo $line | xargs -n1 | sort | xargs; done | tr \">\" \" \" | awk '{printf(\"%s %s %s %s %s\\n\", $1, $3, $5, $7, $9)}' ) > ~/sf_work/tmp/$(basename `pwd`).csv mem #这里实际上是统计的VSS, 是虚拟内存占用 cat top.log | awk '/GMT/{printf \"\\n\"$4\"\\t\"} /vONU-PoC-1 -skipav yes/ {if($5 ~ /.*m/) printf $5; else printf $5/1024}' | tr -d 'm' > ~/sf_work/tmp/$(basename `pwd`).csv 同时导出CPU利用率和mem 这个top.log是用如下命令记录的, 每秒一次. #grep -E记录的是RSS, 物理内存. while true; do date top -bn1 | grep -E \"$patten|CPU|Mem|COMMAND\" top -bn1 -m | grep -E \"$patten|COMMAND\" sleep 1 done >> log/top.log 原始的log如下: Tue Jan 6 13:05:29 GMT 1970 Mem: 884920K used, 1072956K free, 152152K shrd, 42636K buff, 356908K cached CPU: 54% usr 16% sys 0% nic 29% idle 0% io 0% irq 0% sirq PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 24514 1 root S 110m 6% 1 50% ./vONU-PoC-1 -profilingperiod 5 18455 165 root S 204m 11% 1 0% /isam/slot_default/xpon_hwa_app/run/xpon_hwa_app --json=/isam/slot_default/xpon_hwa_app/config/app_config --pty=/isam/slot_default/xpon_hwa_app/run/tty1_ext --tnd-client 18047 131 root S 184m 10% 0 0% /isam/slot_default/switch_hwa_app/run/switch_hwa_app --json=/isam/slot_default/switch_hwa_app/config/app_config --pty=/isam/slot_default/switch_hwa_app/run/tty1_ext --tnd-client --json=/isam/slot_default/switch_hwa_app/config/app_config --pty=/isam/slot_default/switch_hwa_app/run/tty1_ext --tnd-client 22037 1 root S 51844 3% 1 0% //lib/confd/erts/bin/confd -K false -MHe true -- -root //lib/confd -progname confd -- -home / -- -smp disable -boot confd -delayed-detach -noshell -noinput -yaws embedded true -stacktrace_depth 24 -shutdown_time 30000 -conffile //etc/confd/confd.conf -start_phase0 -max_fds 1024 -detached-fd 4 18067 166 root S 36168 2% 1 0% /isam/slot_default/xponhwadp/run/xponhwadp --json=/isam/slot_default/xponhwadp/config/app_config --pty=/isam/slot_default/xponhwadp/run/tty1_ext --tnd-client 18601 124 root S 34260 2% 1 0% /isam/slot_default/dmsp_app/run/dmsp_app --json=/isam/slot_default/dmsp_app/config/app_config --pty=/isam/slot_default/dmsp_app/run/tty1_ext --tnd-client 24522 24515 root S 2952 0% 1 0% grep -E vONU-PoC-1|xponhwadp|switch_hwa_app|xpon_hwa_app|confd|dmsp_app|CPU|Mem|COMMAND 124 1 root S 2788 0% 0 0% s6-supervise dmsp_app 131 1 root S 2788 0% 0 0% s6-supervise switch_hwa_app 165 1 root S 2788 0% 0 0% s6-supervise xpon_hwa_app 166 1 root S 2788 0% 0 0% s6-supervise xponhwadp PID^^^VSZ^VSZRW RSS (SHR) DIRTY (SHR) STACK COMMAND 18047 176m 148m 123m 6948 95676 5896 132 /isam/slot_default/switch_hwa_app/run/switch_hwa_app --json=/isam/slot_default/switch_hwa_app/config/app_config --pty=/isam/slot_default/switch_hwa_app/run/tty1_ext --tnd-client --json=/isam/slot_default/switch_hwa_app/config/app_config --pty=/isam/slot_default/switch_hwa_app/run/tty1_ext --tnd-client 24514 110m 100m 11020 0 11020 0 132 ./vONU-PoC-1 -profilingperiod 5 22037 51844 39868 38424 2608 38420 2604 132 //lib/confd/erts/bin/confd -K false -MHe true -- -root //lib/confd -progname confd -- -home / -- -smp disable -boot confd -delayed-detach -noshell -noinput -yaws embedded true -stacktrace_depth 24 -shutdown_time 30000 -conffile //etc/confd/confd.conf -start_phase0 -max_fds 1024 -detached-fd 4 18067 36168 7624 20760 11224 11272 6816 128 /isam/slot_default/xponhwadp/run/xponhwadp --json=/isam/slot_default/xponhwadp/config/app_config --pty=/isam/slot_default/xponhwadp/run/tty1_ext --tnd-client 18601 34260 14452 22652 11304 17688 6916 132 /isam/slot_default/dmsp_app/run/dmsp_app --json=/isam/slot_default/dmsp_app/config/app_config --pty=/isam/slot_default/dmsp_app/run/tty1_ext --tnd-client 18455 29104 8324 14208 7192 8392 5540 132 /isam/slot_default/xpon_hwa_app/run/xpon_hwa_app --json=/isam/slot_default/xpon_hwa_app/config/app_config --pty=/isam/slot_default/xpon_hwa_app/run/tty1_ext --tnd-client 24527 2952 384 1532 1464 1528 1460 132 grep -E vONU-PoC-1|xponhwadp|switch_hwa_app|xpon_hwa_app|confd|dmsp_app|COMMAND 124 2788 400 1464 1348 1460 1344 132 s6-supervise dmsp_app 166 2788 400 1424 1312 1420 1308 132 s6-supervise xponhwadp 131 2788 400 1396 1284 1392 1280 132 s6-supervise switch_hwa_app 165 2788 400 1396 1284 1392 1280 132 s6-supervise xpon_hwa_app 用下面的脚本解析 cat top.log | grep -v -E \"s6-supervise|grep|COMMAND|confd -B| tar| cat\" | awk '/GMT/{printf \"\\ntime=\"$4} /Mem:/{sub(\"K\",\"\",$4);printf \" free=\" $4/1024} /CPU:/{printf(\" usr=%s sys=%s idle=%s sirq=%s\",$2,$4,$8,$14)} $1 ~ /[0-9]+/{sub(\".*/\",\"\",$9);printf(\" %s\",$9);if($8 ~ /.*%/) printf \":cpu=\"$8;else {printf \":rss=\";if($4 ~ /.*m/) {sub(\"m\",\"\",$4);printf $4} else printf $4/1024}}' | while read line; do echo $line | xargs -n1 | sort | xargs; done awk的行匹配支持表达式, 这里的$1 ~ /[0-9]+/就是, 意思是第一个field匹配一个number sub是原地替换, 不返回字符串 输出如下: confd:cpu=0% confd:rss=37.5234 dmsp_app:cpu=0% dmsp_app:rss=22.1211 free=1047.81 idle=29% sirq=0% switch_hwa_app:cpu=0% switch_hwa_app:rss=123 sys=16% time=13:05:29 usr=54% vONU-PoC-1:cpu=50% vONU-PoC-1:rss=10.7617 xpon_hwa_app:cpu=0% xpon_hwa_app:rss=13.875 xponhwadp:cpu=0% xponhwadp:rss=20.2734 confd:cpu=0% confd:rss=37.5234 dmsp_app:cpu=0% dmsp_app:rss=22.1211 free=1047.55 idle=75% sirq=0% switch_hwa_app:cpu=4% switch_hwa_app:rss=123 sys=16% time=13:05:30 usr=8% vONU-PoC-1:cpu=0% vONU-PoC-1:rss=10.7617 xpon_hwa_app:cpu=0% xpon_hwa_app:rss=13.875 xponhwadp:cpu=0% xponhwadp:rss=20.2734 confd:cpu=0% confd:rss=37.5234 dmsp_app:cpu=0% dmsp_app:rss=22.1211 free=1047.42 idle=83% sirq=4% switch_hwa_app:cpu=4% switch_hwa_app:rss=123 sys=4% time=13:05:32 usr=8% vONU-PoC-1:cpu=0% vONU-PoC-1:rss=10.7617 xpon_hwa_app:cpu=0% xpon_hwa_app:rss=13.875 xponhwadp:cpu=0% xponhwadp:rss=20.2734 confd:cpu=0% confd:rss=37.5234 dmsp_app:cpu=0% dmsp_app:rss=22.1211 free=1047.24 idle=90% sirq=0% switch_hwa_app:cpu=2% switch_hwa_app:rss=123 sys=4% time=13:05:33 usr=4% vONU-PoC-1:cpu=0% vONU-PoC-1:rss=10.7617 xpon_hwa_app:cpu=0% xpon_hwa_app:rss=13.875 xponhwadp:cpu=0% xponhwadp:rss=20.2734 confd:cpu=0% confd:rss=37.5234 dmsp_app:cpu=0% dmsp_app:rss=22.1211 free=1047.22 idle=81% sirq=0% switch_hwa_app:cpu=4% switch_hwa_app:rss=123 sys=13% time=13:05:35 usr=4% vONU-PoC-1:cpu=0% vONU-PoC-1:rss=10.7852 xpon_hwa_app:cpu=0% xpon_hwa_app:rss=13.875 xponhwadp:cpu=0% xponhwadp:rss=20.2734 最后用tr \"=\" \" \"休整一下. 最终版: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 confd:cpu 0% confd:rss 37.5234 dmsp_app:cpu 0% dmsp_app:rss 22.1211 free 1047.81 idle 29% sirq 0% switch_hwa_app:cpu 0% switch_hwa_app:rss 123 sys 16% time 13:05:29 usr 54% 25 26 27 28 29 30 31 32 33 34 35 36 vONU-PoC-1:cpu 50% vONU-PoC-1:rss 10.7617 xpon_hwa_app:cpu 0% xpon_hwa_app:rss 13.875 xponhwadp:cpu 0% xponhwadp:rss 20.2734 (echo time free usr sys idle sirq confd:cpu confd:rss dmsp:cpu dmsp:rss switch_hwa:cpu switch_hwa:rss vONU:cpu vONU:rss xpon_hwa:cpu xpon_hwa:rss xponhwadp:cpu xponhwadp:rss && cat top.log | grep -v -E \"s6-supervise|grep|COMMAND|confd -B| tar| cat\" | awk '/GMT/{printf \"\\ntime=\"$4} /Mem:/{sub(\"K\",\"\",$4);printf \" free=\" $4/1024} /CPU:/{printf(\" usr=%s sys=%s idle=%s sirq=%s\",$2,$4,$8,$14)} $1 ~ /[0-9]+/{sub(\".*/\",\"\",$9);printf(\" %s\",$9);if($8 ~ /.*%/) printf \":cpu=\"$8;else {printf \":rss=\";if($4 ~ /.*m/) {sub(\"m\",\"\",$4);printf $4} else printf $4/1024}}' | while read line; do echo $line | xargs -n1 | sort | xargs; done | tr \"=\" \" \" | awk 'NF == 36 {printf(\"%s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s\\n\", $22,$10,$24,$20,$12,$14,$2,$4,$6,$8,$16,$18,$26,$28,$30,$32,$34,$36)}') > top.csv 只导出cpu利用率 top的输出, 每秒记录一次, 这里的patten是xponhwadp switch_hwa_app xpon_hwa_app vONU-PoC-1这几个app的名字前面打个时间戳date && top -bn1 | grep -E $patten 输出是这样的: 怎么把这个输出, 导入到excel里面, 然后对每个app做cpu占用的图呢? 要用到awk, xargs, sort cat top.log | awk '/GMT/{printf \"\\n\"$4} /run\\/switch_hwa_app/{printf(\" switch_hwa_app:%s\",$8)} /run\\/xpon_hwa_app/{printf(\" xpon_hwa_app:%s\",$8)} /run\\/xponhwadp/{printf(\" xponhwadp:%s\",$8)} /vONU-PoC-1 -skipav yes/{printf(\" vonu:%s\",$8)}' | while read line; do echo $line | xargs -n1 | sort | xargs; done awk负责过滤关键词, 用/patten/{action}的形式 awk过滤后, 每个时间戳下, 这四个app都列出来了, 但顺不一定一样, 因为top输出会按照CPU排序 此时要用read, 从stdin读每一行, 然后对该行排序 xargs -n1实际上是对每个字段加回车, 因为默认的xargs会对每个输入做echo sort是按行工作的, 前面的xargs的输出就是多行 最后的xargs把排好序的多行输出还原成一行 最后的输出: 改进版 (echo time switch_hwa_app vonu xpon_hwa_app xponhwadp && cat top.log | awk '/GMT/{printf \"\\n\"$4} /run\\/switch_hwa_app/{printf(\" switch_hwa_app>%s\",$8)} /run\\/xpon_hwa_app/{printf(\" xpon_hwa_app>%s\",$8)} /run\\/xponhwadp/{printf(\" xponhwadp>%s\",$8)} /vONU-PoC-1 -skipav yes/{printf(\" vonu>%s\",$8)}' | while read line; do echo $line | xargs -n1 | sort | xargs; done | tr \">\" \" \" | awk '{printf(\"%s %s %s %s %s\\n\", $1, $3, $5, $7, $9)}' ) > top.csv 用()括起来echo和后面的处理, 否则echo不会重定向到top.csv 用tr处理多余字符 原始文件是带^M字符的, 它实际上是windows的\\r (to get ^M type CTRL+V followed by CTRL+M i.e. don’t just type the carat symbol and a capital M. It will not work) #重点是tr -d, 把两边的方括号删掉, 把\\r删掉 cat onustatus.log | tr -d '[]\\r' | awk '/onustatus/{printf $6 \" \"} /ONUs done/{print $3}' 一次shell重定向过程 用strace观察一个重定向的过程: strace bash -c \"echo 5555 > testpipe\" 这里的testpipe是个有名管道, 用mkfifo testpipe生成的 注意, 我用了'bash -c'命令, 意思是新起一个bash进程来执行-c后面的命令, 目的是观察bash怎么处理重定向的. 不加'bash -c'是看不到这个过程的. #本bash执行另外一个bash, 从这里到结束都没有再次exec bash execve(\"/bin/bash\", [\"bash\", \"-c\", \"echo 5555 > testpipe\"] #省略动态库加载过程, 省略brk 和mmap过程, 省略挂载sighandler过程 ... #进入主题 # 打开testpipe, fd是3 openat(AT_FDCWD, \"testpipe\", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 fcntl(1, F_GETFD) = 0 #保存现在的fd1, 到fd10 fcntl(1, F_DUPFD, 10) = 10 fcntl(1, F_GETFD) = 0 #如果后面调用了exec, 则exec出的子进程不继承fd10; 换句话说, exec成功后, fd10会被close #这个例子里, 都没有再次exec子进程, 所以这个语句实际没起到作用. fcntl(10, F_SETFD, FD_CLOEXEC) = 0 #int dup2(int oldfd, int newfd); #把fd3复制到fd1, 即现在fd1就是fd3 dup2(3, 1) = 1 #关闭fd3 close(3) write(1, \"5555\\n\", 5) = 5 #从fd10还原fd1 dup2(10, 1) = 1 fcntl(10, F_GETFD) = 0x1 (flags FD_CLOEXEC) close(10) = 0 shell重定向和exec 把上面例子的命令修改一下, 把echo改成外部程序cat, 用strace的-f选项跟踪全部进程. strace -f -o s.log bash -c \"cat testpipe > testpipe2\" 过程简析如下: #8123进程执行bash 8123 execve(\"/bin/bash\", [\"bash\", \"-c\", \"cat testpipe > testpipe2\"] #省略动态库加载过程, 省略brk 和mmap过程, 省略挂载sighandler过程 #找到cat命令 8123 access(\"/bin/cat\", R_OK) = 0 8123 rt_sigprocmask(SIG_BLOCK, [INT CHLD], [], 8) = 0 #clone父进程 8123 clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7fefa27eda10) = 8124 #8123进入wait 8123 wait4(-1, #省略子进程挂载sighandler ... #处理重定向, 打开testpipe2为fd3, fd3复制到fd1, 随即close fd3 8124 openat(AT_FDCWD, \"testpipe2\", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 8124 dup2(3, 1) = 1 8124 close(3) = 0 #重定向发生在exec cat之前, 是bash做的工作 #exec执行cat, 替换原进程空间 8124 execve(\"/bin/cat\", [\"cat\", \"testpipe\"], 0x5600ee4a38f0 /* 26 vars */) = 0 #以此调用brk扩展内存空间, mmap等, 装载so #打开要cat的文件 8124 openat(AT_FDCWD, \"testpipe\", O_RDONLY) = 3 #从fd3(即testpipe)读, 写到fd1 8124 read(3, \"5555\\n\", 131072) = 5 8124 write(1, \"5555\\n\", 5) = 5 #关闭相关fd ... 8124 +++ exited with 0 +++ #父进程bash的wait4返回 8123 [{WIFEXITED(s) && WEXITSTATUS(s) == 0}], 0, NULL) = 8124 #父进程处理SIGCHLD信号 #再次wait4发现没有子进程了, 父进程退出 8123 rt_sigaction(SIGINT, {sa_handler=SIG_DFL, sa_mask=[], sa_flags=SA_RESTORER, sa_restorer=0x7fefa1df5f20}, {sa_handler=0x5600edd40160, sa_mask=[], sa_flags=SA_RESTORER, sa_restorer=0x7fefa1df5f20}, 8) = 0 8123 rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0 8123 --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=8124, si_uid=1003, si_status=0, si_utime=0, si_stime=0} --- 8123 wait4(-1, 0x7ffd543dca50, WNOHANG, NULL) = -1 ECHILD (No child processes) 8123 +++ exited with 0 +++ 动态变量名及其引用: eval的使用 有时候, 我们想根据某些条件, 来生成一个shell变量, 其名字可以是用户输入, 也可以是从文件里读出来的某个字段. 用eval可以做到 用eval生成动态变量名 #option的值可以从任何地方, 比如取自文件名 option=vendor_name #这个值被eval后, 成为了新的变量名. 在当前shell就有效. eval $option=my_vendor echo $vendor_name #输出 my_vendor 注意: 不加eval是不行的: $ $option=my_vendor vendor_name=my_vendor: command not found eval后面有空格不行, 除非加两层引号, 先单后双 $ eval $option=my vendor vendor: command not found $ eval $option='my vendor' vendor: command not found $ eval $option=\"'my vendor'\" byj@byj-envy-notebook ~/tmp/uaes $ echo $vendor_name my vendor 间接引用变量 变量是动态生成的时候, 我们在写脚本的当下, 是不知道具体的变量名的, 就没办法直接引用. 要间接引用, 用${!var}的格式 dvar=hello var=dvar echo ${!var} #输出 hello #用eval也行, $var先被替换为dvar, 转义$再传入eval eval \"echo \\${$var}\" #这样写不行 $ echo ${$var} bash: ${$var}: bad substitution 举例: 读出*.conf文件内容到动态文件名变量 for f in *.conf;do eval ${f%.conf}='`cat $f`';done uaes进程管理, 关系数组方式实现 #! /bin/bash StartProcess() { echo Starting process [$1] ...Done } StopProcess() { echo Stoping process [$1] ...Done } GetTargetState() { cat msgfifo } ReportState() { echo $1 > rplfifo } stopList=\"\" AddToStopList() { stopList=\"$1 $stopList\" } DoStopList() { for p in $stopList; do StopProcess $p done stopList=\"\" } declare -A processes SwitchState(){ local from=$1 local to=$2 if test -z \"$from\"; then from=null processes[$from]=\"\" elif test -z \"${processes[$from]}\"; then processes[$from]=$(cat ${from}.conf) echo Loading ${from}.conf ...Done fi if test -z \"${processes[$to]}\"; then processes[$to]=$(cat ${to}.conf) echo Loading ${to}.conf ...Done fi OLDIFS=$IFS IFS=$'\\n' for p in ${processes[$from]}; do if [[ \"${processes[$to]}\" != *\"$p\"* ]]; then IFS=$OLDIFS #StopProcess $p AddToStopList $p fi done DoStopList IFS=$'\\n' for p in ${processes[$to]}; do if [[ \"${processes[$from]}\" != *\"$p\"* ]]; then StartProcess \"$p\" fi done IFS=$OLDIFS } if ! test -e IDL.conf; then echo Please add IDL.conf first! exit 1 fi SwitchState \"\" IDL current=IDL echo Initial state $current running ... while true; do target=$(GetTargetState) if test \"$target\" = \"$current\"; then echo Keep state $current unchanged continue fi if test -e ${target}.conf; then echo Switching state from $current to $target ... echo \">>>>>>>>\" SwitchState $current $target else echo File ${target}.conf not found! continue fi current=$target ReportState $current if test \"$current\" = \"DOWN\"; then echo ========Shutdown. exit 1 fi echo Running in state $current .. done 判断字串是否包含字串 判断一个字符串是否包含一个字串 注意, 不带通配符的必须在判断的左边; 否则永远为false if [[ \"$str\" == *\"$substr\"* ]]; then exec重定向 对某个命令的重定向很常见, 比如 cat : cat从stdin读取字符, 并显示到stdou上. 这里用README文件重定向到stdin 当然直接cat README也是可以的, 因为cat命令后面带文件名, 它会打开这个文件作为输入.这种情况它不从stdin读取东西, 和重定向没关系. 那这里说的是对整个脚本重定向. 要用到shell的内置命令exec 在shell里直接重定向stdin #新建fd6(或重新打开), 复制fd0给fd6, 箭头表示fd是只读的 #也可以理解成Link file descriptor #6 with stdin #这里的意思是, 把fd0保存到新建的fd6中. 单独用这个没什么意思 exec 6 完整例子 #!/bin/bash # Redirecting stdin using 'exec'. #新建fd6(或重新打开), 复制fd0给fd6, 箭头表示fd是只读的 exec 6 重定向整个shell的stdout #新建fd6(或重新打开), 复制fd1给fd6, 箭头表示fd是只写. 并不是把6赋值给1 #这里的意思是保存fd1到fd6, 即保存原始的stdout exec 6>&1 #从这里开始, 输出变为logfile.txt exec > logfile.txt #输出 echo xxx ... #从fd6还原fd1, 即还原原始的stdout. #并关闭fd6 exec 1>&6 6>&- 完整例子 #!/bin/bash # reassign-stdout.sh LOGFILE=logfile.txt # 新建fd6(或重新打开), 复制fd1给fd6, 箭头表示fd是只写. 并不是把6赋值给1 exec 6>&1 # Link file descriptor #6 with stdout. # Saves stdout. exec > $LOGFILE # stdout replaced with file \"logfile.txt\". # ----------------------------------------------------------- # # All output from commands in this block sent to file $LOGFILE. echo -n \"Logfile: \" date echo \"-------------------------------------\" echo echo \"Output of \\\"ls -al\\\" command\" echo ls -al echo; echo echo \"Output of \\\"df\\\" command\" echo df # ----------------------------------------------------------- # exec 1>&6 6>&- # Restore stdout and close file descriptor #6. echo echo \"== stdout now restored to default == \" echo ls -al echo exit 0 输入和输出的例子 #!/bin/bash # upperconv.sh # Converts a specified input file to uppercase. E_FILE_ACCESS=70 E_WRONG_ARGS=71 if [ ! -r \"$1\" ] # Is specified input file readable? then echo \"Can't read from input file!\" echo \"Usage: $0 input-file output-file\" exit $E_FILE_ACCESS fi # Will exit with same error #+ even if input file ($1) not specified (why?). if [ -z \"$2\" ] then echo \"Need to specify output file.\" echo \"Usage: $0 input-file output-file\" exit $E_WRONG_ARGS fi exec 4&1 exec > $2 # Will write to output file. # Assumes output file writable (add check?). # ----------------------------------------------- cat - | tr a-z A-Z # Uppercase conversion. # ^^^^^ # Reads from stdin. # ^^^^^^^^^^ # Writes to stdout. # However, both stdin and stdout were redirected. # Note that the 'cat' can be omitted. # ----------------------------------------------- exec 1>&7 7>&- # Restore stout. exec 0 crosstool ng的例子 # Log policy: # - first of all, save stdout so we can see the live logs: fd #6 # (also save stdin and stderr for use by CT_DEBUG_INTERACTIVE) # FIXME: it doesn't look like anyone is overriding stdin/stderr. Do we need # to save/restore them? CT_LogEnable() { local clean=no local arg for arg in \"$@\"; do eval \"$arg\"; done #复制1到6, 2到7, 0到8；箭头只表示输入还是输出 exec 6>&1 7>&2 8>\"${CT_BUILD_LOG}\" } # Restore original stdout, stderr and stdin CT_LogDisable() { exec >&6 2>&7 补充: test可以检测FD是否是terminal test -t FD -t FD True if FD is opened on a terminal. shell读写socket 用nc命令可以方便的读写socket 其实shell还有更简单的方法: 在一个窗口里, 用nc监听一个端口 nc -l 1985 在另外一个窗口里 #用socket做文件fd 3 exec 3<>/dev/tcp/localhost/1985 #读 echo 111 >&3 #写 cat 更简单的写法 在另一个窗口里 echo 111 > /dev/tcp/localhost/1985 直接就可以发送. 但前提是1985端口有人监听. 如果对端没有监听, 会出现Connection refused /dev/tcp是shell内建的设备节点 这个节点在文件系统里是没有的. 这是shell提供的访问socket的方法. shell进程间通信 我们经常用的管道就是shell进程间通信的一种: 比如ls | grep txt 这是匿名管道. 还有有名管道, shell的mkfifo命令就能创建个有名管道 mkfifo pipe2 会在当前目录下创建一个管道文件 Linux Mint 19.1 Tessa $ ll pipe2 #管道文件以p开头 prw-rw-r-- 1 yingjieb yingjieb 0 Oct 12 14:31 pipe2 以后两个独立的进程就可以重定向到这个文件通信了. ls > pipe2 在另一个窗口里 cat 注: 单独读写这个管道会阻塞进程. 比如echo abcd > pipe2如果没人读的话, 会阻塞. 读一个空的pipe也会阻塞. 有名管道和nc 上面的图片实现了一个远程登陆shell的服务在服务端:cat pipe2 | /bin/sh -i 2>&1 | nc -l 1985 > pipe2 在client端:nc localhost 1985 解释: 服务端起了三个进程, 这三个进程是独立的, 并不存在先后顺序. cat pipe2的作用是, 如果有名管道pipe2有东西, 则输出到stdout; 如果没有则阻塞在read /bin/sh -i 2>&1从stdin读数据, 也就是前面cat pipe2的输出, 然后做shell交互, 输出到stdout nc -l 1985 > pipe2从stdin读数据, 也就是前面sh的输出, 再重定向到pipe2; 注意这里不是管道到pipe2 重定向到pipe2是写, 写了pipe就有东西了, cat pipe2就能读到数据了. 这是个\"乌洛波洛斯\"蛇, 头尾相连, 无限循环. nc -l 1985建立了双向的tcp连接, client的输入, 会被nc接收重定向到pipe2. /bin/sh的输出, 会被nc通过tcp连接, 在client端显示. nc默认用tcp, 用选项-u可以使用udp. -u在上面的例子里效果是一样的. shell检查一个脚本是否已经被include了 用type命令 $ type echo echo is a shell builtin $ type find find is /usr/bin/find # -t输出类型, 可以是`alias', `keyword',`function', `builtin', `file' or `' $ type -t find file $ type -t echo builtin 那么可以用type命令来看, 比如一个shell函数在另外一个文件里定义, 想看看是否它已经被包含了. type -t source_once >/dev/null || . /isam/scripts/prepare_script_env.sh sed sed的逻辑和awk一样, 也是按行处理的, 被处理的那一行叫做pattern空间; 而且, sed也有行匹配功能. 基本的命令格式是: sed -r -n -e '行定址/{命令1;命令2}' file 其中: -E, -r, --regexp-extended 使用扩展正则 -n, --quiet, --silent 不自动打印pattern空间. 默认每行都打印, 即使不匹配的行也打印; 区别是匹配到的行, 执行了命令才打印. 行定址见下文. 命令两侧的大括号{}可以省略 向sed传递变量 用双括号就行: instacne=yingjieb_devtool_vscode sed -e \"/$instacne /d\" test sed使用记录 # 使用grep先正则匹配, 然后用sed删除匹配到的字符串(即用空串替换) bzcat build.log.bz2 | grep -E '\\[DEBUG\\][[:space:]]*(# )?CT_' | sed s'/\\[DEBUG\\] //' sed命令 sed命令的对象是当前的pattern空间, 大部分情况是正在处理的那行. sed命令如果没有指定address, 就会对所有行操作. 如果有指定address区间, 对区间内的所有行操作. Sed commands can be given with no addresses, in which case the command will be executed for all input lines; with one address, in which case the command will only be executed for input lines which match that address; or with two addresses, in which case the command will be executed for all input lines which match the inclusive range of lines start‐ ing from the first address and continuing to the second address. 0地址命令 : label 用于后续的跳转label命令, 比如b或t命令 0地址或1个地址命令 = 打印行号 a \\text append text i \\text 插入text q [exit-code] 退出 r filename 追加filename的内容 可以使用定址的命令: b label 跳转到label c \\text 使用text替换pattern space, text的换行前面加\\ d 删除pattern space h H Copy/append pattern space to hold space. g G Copy/append hold space to pattern space. l List out the current line in a ``visually unambiguous'' form. n N Read/append the next line of input into the pattern space. p Print the current pattern space. P Print up to the first embedded newline of the current pattern space. 只打印一行 s/regexp/replacement/ 替换 w filename Write the current pattern space to filename. x Exchange the contents of the hold and pattern spaces. y/source/dest/ Transliterate the characters in the pattern space which appear in source to the corresponding character in dest. sed匹配2行之间的内容 some.log里面, 保留abc和efg之间的行, 删掉其他行 sed -e '/abc/,/efg/!d' some.log 这里!表示逻辑反, 否则会删掉之间的行. 提取文件特定行 grep方式 cat ddr_test.log | egrep \"Evaluating Read-Leveling Scoreboard|Initializing cgroup subsys|EDAC MC.:.. [UC]E DIMM\" > ddr_test.log.analysis 但是有个缺陷, 不能提取连续的多行信息. 可能grep是按行处理的. sed方式 $ sed -rn -e '/Evaluating Read-Leveling Scoreboard/,/Rlevel Rank/p' -e '/Initializing cgroup subsys/p' -e '/EDAC MC.:.. [UC]E DIMM/p' ddr_test.log | sed -r -e '/Evaluating Read-Leveling Scoreboard/i ==============' | sed 's/^M//' > ddr_test.log.analysis $ sed -rn -e '/Cavium Inc. OCTEON SDK version/,/DRAM: 2 GiB/p' -e '/Initializing cgroup subsys/p' -e '/EDAC MC.:.. [UC]E DIMM/p' ddr_test.log | cut -d' ' -f 4- | sed -r -e '/Cavium Inc. OCTEON SDK version/i \\\\n==============' | sed 's/^M//' > ddr_test.log.analysis sed的定址可以达到此目的 解释如下: -r 使用扩展正则 -n 不打印pattern空间 -e 多重操作. 注意, sed会把匹配到的行全部进行-e后面的command, 所以多个-e的操作应该是互不影响的 /pattern1/,/pattern2/ 匹配范围 p 打印模式空间 /pattern/i \\text 在pattern之前插入text, a是在之后插入 sed 's/^M//' 是删除^M, 输入^M的方法是先ctrl+v, 在ctrl+m sed正则表达式 注意: sed -r 选项直接可以使用扩展正则 默认使用basic regexp, 但也能解释\\|',+', \\?',`', \\'',\\\\>',\\b', \\B',\\w', and `\\W' 比如 `x\\+' matches one or more occurrences of `x'. `abc\\|def' matches either `abc' or `def'. 在/regexp/后面加I可以大小写不敏感, 比如 /regexp/Ip, 打印匹配regexp的行, 大小写不敏感 比如查找所有源文件, $ find -type f | sed -n '/\\.\\([chs]\\)\\1\\{0,1\\}\\(pp\\)\\?$/Ip' ./test.c ./test.h ./test.cc ./test.hh ./test.cpp ./test.hpp ./test.s ./test.S ./test.C ./test.H sed定址替换 sed -i -e '/# GENERIC_SERIAL$/s%^\\(.*\\)::.*#%\\1::respawn:/usr/bin/chrt -r 30 /bin/sh -l #%' $targetdir/etc/inittab sed 中的操作符比如s或者d, 都可以指定范围, 即定址. 不定址的操作默认是对全部行. 定址有一下几种: addr1 addr1所在行 addr1,addr2 所有addr1和addr2之间的行. 特别的, $代表最后一行 first~step 比如1~2p会打印所有奇数行 addr1,~N 和上面差不多意思 /regexp/ 所有匹配到regexp的行 \\cregexpc 和上面差不多, 但c可以是任意字符, 用于分隔 addr1,+N addr1 所在行以及下面连续N行 sed删除 sed -i -n -e '/^#OUTPUT-MARKER-BEGIN/,/^#OUTPUT-MARKER-END/d; p' $targetdir/etc/init.d/rcS sed替换引用 sed -i -e \"s%#!/bin/sh%&${output_redirection}%\" $targetdir/etc/init.d/rcS sed的分组匹配, 用()分组, 用\\1引用 echo /repo/yingjieb/fdt063/sw/vobs/dsl/sw/flat/fpxt-b_OFLT_MAIN/src/bcm_commands.c | sed 's!\\(.*/flat/[^/]*\\).*!\\1!g' awk shell变量传给awk 向awk传入变量, 用-v, shell变量用双引号括起来:-v td=\"$TimeDiff\" 再说patten 一般的pattern是这样的: awk '/search regex pattern1/ {Actions} /search regex pattern2/ {Actions}' file //中间的是regex pattern 也可以不用//, 其实{}前面的都是pattern 比较表达式也可以做pattern#最后一个字段不是A则执行 awk '$NF != \"A\" { print $0 }' BEGIN核END也是特殊的patternawk 'BEGIN { n=5 } NR==n { print $0 } END { print $0 }' pattern可以是个范围, pattern1, pattern2: pattern1匹配到则开闸放水, pattern2匹配到则关闸. 在开关闸之间做action awk在if里匹配 cat log/top.log | awk '{printf NR\" \"; {if($5 ~ /.*m/) printf $5; else printf $5/1024} printf \" \" $8 \"\\n\"}' | tr -d 'm' 这里的if($5 ~ /.*m/)就是正则匹配, 用~和/patten/的形式 awk支持浮点 bash只支持整数运算, 而awk支持浮点 #比如ping.log最后一行 64 bytes from 5.5.5.12: icmp_seq=11 ttl=64 time=0.187 ms #我想比较time是否大于某个值, 因为time是个浮点, 用bash直接比较会出错 $ tail ping.log -n 1 | awk -F\" *|=\" '{if ($10>0.1) print $10}' 0.187 #改成0.2则无输出 $ tail ping.log -n 1 | awk -F\" *|=\" '{if ($10>0.2) print $10}' awk逻辑与 这样写比下文的ss -ant |awk '{if(NR>1)++s[$1]} END {for(k in s) print k,s[k]}'更简洁一些 #打印五到十行，并在前面加上行号 awk -F: 'NR>=5 && NR awk多行解析 比如nmap的输出如下, 想解析ip和mac的对应关系 Nmap scan report for 10.239.120.208 Host is up (0.00049s latency). MAC Address: EC:B1:D7:2F:90:67 (Unknown) Nmap scan report for 10.239.120.209 Host is up (0.00019s latency). MAC Address: 34:64:A9:CF:8E:62 (Unknown) Nmap scan report for 10.239.120.212 Host is up (0.00020s latency). MAC Address: 8C:FD:F0:06:8B:A1 (Qualcomm Incorporated) sudo nmap -n -sP 10.239.120.1/24 | awk '/Nmap scan report/{printf $5;printf \" \";getline;getline;print $3;}' 重点是getline 另外一种写法我认为更好: sudo nmap -n -sP 10.239.120.1/24 | awk '/Nmap scan report for/{printf $5;}/MAC Address:/{print \" => \"$3;}' | sort awk分组提取 CentOS 7.3 $ cat log/test_report.csv | grep ^[0-9] 1,127.0.0.1,50,500000,10,1,1,set, 81024 1,127.0.0.1,50,500000,10,1,1,get, 89078 1,127.0.0.1,50,25000000,10,1,100,get, 315923 2,127.0.0.1,50,500000,10,1,1,set, 160236 2,127.0.0.1,50,500000,10,1,1,get, 174584 2,127.0.0.1,50,25000000,10,1,100,get, 616733 4,127.0.0.1,50,500000,10,1,1,set, 310916 4,127.0.0.1,50,500000,10,1,1,get, 333704 4,127.0.0.1,50,25000000,10,1,100,get, 1155443 8,127.0.0.1,50,500000,10,1,1,set, 617204 8,127.0.0.1,50,500000,10,1,1,get, 640997 8,127.0.0.1,50,25000000,10,1,100,get, 2244430 16,127.0.0.1,50,500000,10,1,1,set, 433805 16,127.0.0.1,50,500000,10,1,1,get, 435868 16,127.0.0.1,50,25000000,10,1,100,get, 4253953 #现在想对上面的输出, 按照第一列的信息整理 #tmp[$1]=tmp[$1]\",\"$9是说把tmp[$1]拼接上第九个字段, 还赋值回tmp[$1], 这就有点像PATH=xxxxx:$PATH awk -F, '{tmp[$1]=tmp[$1]\",\"$9}; END{ for(i in tmp) {print i tmp[i]}}' | sort -h 1, 81024, 89078, 315923 2, 160236, 174584, 616733 4, 310916, 333704, 1155443 8, 617204, 640997, 2244430 16, 433805, 435868, 4253953 awk变量 CentOS 7.3 $ awk '{s+=$1} END {print s}' log/*.csv 1.18365e+07 CentOS 7.3 $ awk '{s+=$1} END {printf(\"%d\\n\", s)}' log/*.csv 11836539 s是个变量, 直接用, 不加$, 那么和字符串的区别在于字符串需要加双引号 print默认用科学计数法 awk数组 $ ss -ant |awk '{if(NR>1)++s[$1]} END {for(k in s) print k,s[k]}' LISTEN 15 ESTAB 10 注: 数组的下标可以是数字也可以是字符串, ++s[$1]是统计个数 awk字符串替换 for f in log.fio*; do echo -n \"$f \"; awk '/IOPS/ {sub(\",\",\"\",$2);match($4,\"[0-9]*MB/s\",arr);printf \"BW=%s %s \",arr[0],$2} /95.00th/ {print $2 $3, $8 $9}' $f; done | column -t 解释: awk 后面斜杠中间的部分是匹配字符串, 匹配到的行才做后面的事情 match函数可以传入arr, arr[0]表示匹配到的字符串, 如果正则里面有分组标记(), 则arr[1], arr[2]依次是子分组awk '/search pattern1/ {Actions} /search pattern2/ {Actions}' file awk '{pattern + action}' {filenames}, 按行处理, 对匹配patten的行, 顺序执行{}里面的操作, - patten就是//里面的东西 - $0代表整行,$1是第一个字段 sub(match, replace, string)是字符串替换, 上面的sub(/,/,\"\",$2)也是可以的, 效果一样. BEGIN是说在扫描之前执行的, 相对的END是在最后都扫描完了再执行的 OFS是输出的分隔符, FS是输入的分隔符, 默认都是space print输出字符串要加\"\", 比如print \"hello\" echo -n 不换行 printf不换行, 而print换行 awk的条件判断和数字运算 cat iperf3_client_x710.log | awk '{if($8==\"Mbits/sec\") printf $7\" \";if ($8==\"Gbits/sec\") printf $7*1024\" \"}' awk执行其他程序 $ awk '{system(\"echo \"$1 \" and \"$2)}' ../linklist awk替换命令 $ echo $var | awk '{sub(\" \", \"_\", $0); printf(\"%s\\n\", $0);}' $ echo $var | awk '{gsub(\" \", \"_\", $0); printf(\"%s\\n\", $0);}' shell管道和循环 for f in `find -name *.so`; do echo $f; readelf -h $f | grep Flag | grep fp64; done > so.log for里面可以直接用pipe done后面可以直接跟重定向 看一个interface是否存在 用/proc/net/dev ~ # cat /proc/net/dev Inter-| Receive | Transmit face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed dummy0: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 npi1: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 loop1: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 lo: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 npi2: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 loop2: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 bond0: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 eth0: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 agl0: 123857375 91502 0 0 0 0 0 6 4491714 52744 0 0 0 0 0 0 npi3: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 loop3: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 npi0: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 loop0: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 eth1: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 用ifconfig intf返回值 ~ # ifconfig itefwd ifconfig: itefwd: error fetching interface information: Device not found ~ # echo $? 1 ~ # ifconfig eth0 eth0 Link encap:Ethernet HWaddr 00:BA:0B:AB:00:0A BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) ~ # echo $? 0 awk计算时间差 # sudo perf script | grep -n1 netdev_send 207567- pmd8 6965 [013] 111673.977146: probe_ovs:netdev_rxq_recv: (698e80) 207568: pmd8 6965 [013] 111673.977193: probe_ovs:netdev_send: (698fd8 #第一个awk得出时间字段, 比如111673.977146: 第二awk用分隔符集合.或:在NR除4余2的时候, 打印$2-t; 而t每行都保存 sudo perf script | grep -n1 netdev_send | awk '{print $5}' | awk -F\"[.:]\" 'NR%4==2 {print $2-t} {t=$2}' # 结果 47 19 43 19 42 19 42 ping延迟大于0.5ms触发动作 ping 5.5.5.11 | tee ping.log tail ping.log -n 1 | awk -F\" *|=\" '{if ($10>0.5) {printf \"ping latency %s ms\\n\", $10; system(\"sudo perf sched record -a\")}}' 注: awk也支持-F\"[.:]\"的方式来指定分隔符, 这里指用.和:分割. while true do #用[[ $(command) ]]来判断command是否有输出 if [[ $(tail ping.log | awk -F\" *|=\" '{if ($10>0.1) print $10}') ]];then sudo perf sched record -a exit fi sleep 10 done 计算cpu掩码, mask bitmask() { #eg. 0,5,8-11 17,26-30 return 7c020f21 local bm=0 for arg in $(echo $* | tr ',' ' ');do #[ expression ] && statement1 || statement2 is equal to if expression then statement1 else statement2 #for i in $([ $(echo \"$arg\" | cut -d'-' -f1) == $arg ] && echo $arg || echo $arg | tr '-' ' ' | xargs seq);do #for i in $(seq ${arg%-*} ${arg#*-});do for ((i=${arg%-*};i>1,i++));do #这里的for的循环体为空, 用:表示空操作. for不能没有do ... done for ((h=l=i; x&1; x = x>>1,i++,h++));do :;done #双小括号里面是C格式的运算, 变量前可以不要$; 也可以用作逻辑判断 ((h - l == 1)) && echo -n \"$l \" ((h - l > 1)) && echo -n \"$l-$((h-1)) \" done echo } shell的进程创建 ls -l /proc/self是当前的进程号 echo $$显示当前进程号 这两个不是一回事, echo是shell内建命令, 不会起子进程; 而ls是外部命令, shell会folk子进程来跑外部命令; 所以这俩的进程号不一样, 比如下面一个是41427, 一个是4141 [root@rep2-130 debug]# echo $$ 41427 [root@rep2-130 debug]# ls -l /proc/self lrwxrwxrwx 1 root root 0 Jan 1 1970 /proc/self -> 4141 其次, &&逻辑与的操作, 也不影响shell创建新进程的逻辑. 目前我的理解是, 外部命令会创建新进程. [root@rep2-130 debug]# ls -l /proc/self && ls -l /proc/self && ls -l /proc/self lrwxrwxrwx 1 root root 0 Jan 1 1970 /proc/self -> 3926 lrwxrwxrwx 1 root root 0 Jan 1 1970 /proc/self -> 3927 lrwxrwxrwx 1 root root 0 Jan 1 1970 /proc/self -> 3928 如果想在当前进程下执行命令, 用exec, 会用后面传的命令替换当前shell进程. 注意如果exec写在一个脚本里, 那么下面的行没有机会执行, 因为当前shell被替换掉了 eval执行命令的效果和shell直接执行一样, 只是多了输入解析这一步. 粘贴多行文本管道后再用后续命令处理 # 这里\"和\"空格*\"来分割, strtonum把16进制转成10进制计算, awk不认16进制计算 cat | *\" '{printf \"%s %s %dK\\n\",$2,$3,(strtonum($3)-strtonum($2))/1024}' 0xffff9d070000->0xffff9d080000 at 0x03630000: load100 ALLOC LOAD READONLY HAS_CONTENTS 0xffff9d080000->0xffff9d880000 at 0x03640000: load101 ALLOC LOAD HAS_CONTENTS 0xffff9d880000->0xffff9d890000 at 0x03e40000: load102 ALLOC LOAD READONLY HAS_CONTENTS 0xffff9d890000->0xffff9e090000 at 0x03e50000: load103 ALLOC LOAD HAS_CONTENTS 0xffff9e090000->0xffff9e0a0000 at 0x04650000: load104 ALLOC LOAD READONLY HAS_CONTENTS 0xffff9e0a0000->0xffff9e8a0000 at 0x04660000: load105 ALLOC LOAD HAS_CONTENTS 0xffff9e8a0000->0xffff9e8b0000 at 0x04e60000: load106 ALLOC LOAD READONLY HAS_CONTENTS 0xffff9e8b0000->0xffff9f0b0000 at 0x04e70000: load107 ALLOC LOAD HAS_CONTENTS 0xffff9f0b0000->0xffff9f0c0000 at 0x05670000: load108 ALLOC LOAD READONLY HAS_CONTENTS 0xffff9f0c0000->0xffff9f8d0000 at 0x05680000: load109 ALLOC LOAD HAS_CONTENTS 0xffff9f8d0000->0xffff9f8d0000 at 0x05e90000: load110 ALLOC 0xffff9f910000->0xffff9f920000 at 0x05e90000: load111a ALLOC LOAD READONLY CODE HAS_CONTENTS EOF 关系数组 sed 和 awk 改进版 awk多维数组 要解析的文本样式: $ sudo ovs-appctl dpctl/show --statistics netdev@ovs-netdev: lookups: hit:0 missed:0 lost:0 flows: 0 port 0: ovs-netdev (tap) RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 aborted:0 carrier:0 collisions:0 RX bytes:0 TX bytes:0 port 1: dpdkp0 (dpdk: configured_rx_queues=4, configured_rxq_descriptors=2048, configured_tx_queues=9, configured_txq_descriptors=2048, lsc_interrupt_mode=false, mtu=1500, requested_rx_queues=4, requested_rxq_descriptors=2048, requested_tx_queues=9, requested_txq_descriptors=2048, rx_csum_offload=true) RX packets:0 errors:0 dropped:0 overruns:? frame:? TX packets:0 errors:0 dropped:0 aborted:? carrier:? collisions:? RX bytes:0 TX bytes:0 port 2: dpdkvhostuser1 (dpdkvhostuserclient: configured_rx_queues=4, configured_tx_queues=4, mtu=1500, requested_rx_queues=4, requested_tx_queues=4) RX packets:0 errors:0 dropped:0 overruns:? frame:? TX packets:0 errors:? dropped:0 aborted:? carrier:? collisions:? RX bytes:0 TX bytes:0 port 3: dpdkvhostuser0 (dpdkvhostuserclient: configured_rx_queues=4, configured_tx_queues=4, mtu=1500, requested_rx_queues=4, requested_tx_queues=4) RX packets:0 errors:0 dropped:0 overruns:? frame:? TX packets:0 errors:? dropped:0 aborted:? carrier:? collisions:? RX bytes:0 TX bytes:0 port 4: ovsbr0 (tap) RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 aborted:0 carrier:0 collisions:0 RX bytes:0 TX bytes:0 换了命令, 重新改了思路; 这次不用shell关系数组, 但会用到awk多维数组 ovs-appctl dpctl/show > /dev/null if [ $? -ne 0 ]; then printf \"execution failed, you may need sudo?\\n\" exit 1 fi prep () { #string #grep -o是说只输出正则匹配的部分; 而且匹配到的输出都是单独一行, 便于后续处理 # awk的 -F [ :]是说用空格和:做分隔符;后面的NR表示行号, 从1开始; NF是当前行的field个数 #a[port,$1,$i]=$(i+1), 这是个三维数组, 但awk内部会把下标都连起来, 比如foo[5,12] = \"value\", 内部变成foo[\"5@12\"]=\"value\", 这里的@是个不可见的符号(SUBSEP) ###grep后的文本: #port 0: ovs #RX packets:0 errors:0 dropped:0 overruns:0 frame:0 #TX packets:0 errors:0 dropped:0 aborted:0 carrier:0 ### #三维数组是要把报文统计的数值用三元(port名, 方向, 统计项名称)来标记, 比如a[\"ovs\",\"RX\", \"packets\"]=0 ###最后打印出来每行都是如下形式的统计 #ovs TX aborted:0 #dpdkp1 RX dropped:0 #dpdkp1 RX packets:170 #dpdkvhostuser0 RX dropped:0 ### echo \"$1\" | grep -Eo \"port [0-9]*: [[:alnum:]]*|RX packets.*|TX packets.*\" | tr \"?\" \"0\" \\ | awk -F'[ :]' '{if(NR%3==1) port=$4; else{for(i=2;i 原始版 ovs-vsctl list interface > /dev/null if [ $? -ne 0 ]; then printf \"ovs-vsctl execution failed, you may need sudo?\\n\" exit 1 fi #注意cmd要用单引号, 单引号不会展开变量 #sed -r是用扩展正则, 这里用到了替换, 反向引用等, sed会对行依次进行-e后面的处理, 处理的那行叫做patten空间 cmd='(ovs-vsctl list interface && echo date: $(date +%s%6N)) | grep -E \"^name|^statistics|^date\" | sed -r -e \"s/,|\\{|\\}//g\" -e \"s/name *: (.*)/\\1:/g\" -e \"s/statistics *://g\"' #t1 t2 t3不加引号, 视为单独元素, 用for可以遍历; 加了\"\"就变成一个元素了 for t in t1 t2 t3; do test $t = t1 && v1=$(eval $cmd) #本来该用if, 这里用test和&&做简化处理; 主进程会等着sleep执行完才往后执行 test $t = t2 && sleep 10 && v2=$(eval $cmd) if test $t = t3; then i=0 #声明关系数组, 关系数组就是可以用string做下标的数组 declare -A map1 map2 #NF是awk解析的feild个数, 每行 v1=$(echo \"$v1\" | awk -F: '{printf $0; if(NF==1 || $1==\"date\") print \"\"}') #每行类似于\"dpdkp1\": rx_bytes=865352722 rx_dropped=0 rx_errors=0 rx_packets=14419925 tx_bytes=0 tx_dropped=0 tx_errors=0 tx_packets=0 #先把它以第一个空格为界分割为两部分:k和v, 其实v是第一个空格后面所有的部分 while read k v; do #关系数组赋值 map1[$k]=$v #这里用 shell关系数组 -- 或者说是list 参考: https://www.artificialworlds.net/blog/2012/10/17/bash-associative-array-examples/ 需要bash版本高于4 declare -A map $ map[foo]=bar $ echo ${map[foo]} bar $ K=baz bai@CentOS-43 ~/repo/save $ map[$K]=quux bai@CentOS-43 ~/repo/save $ echo ${map[$K]} quux 遍历 $ declare -A MYMAP=( [foo a]=bar [baz b]=quux ) $ echo \"${!MYMAP[@]}\" # Print all keys - quoted, but quotes removed by echo foo a baz b $ # Loop through all keys in an associative array $ for K in \"${!MYMAP[@]}\"; do echo $K; done foo a baz b $ # Loop through all values in an associative array $ for V in \"${MYMAP[@]}\"; do echo $V; done bar quux shell数组 #括号括起来定义数组 my_array=(A B \"C\" D) #或者 array_name[0]=value0 array_name[1]=value1 array_name[2]=value2 #数组元素 echo \"第一个元素为: ${my_array[0]}\" echo \"第二个元素为: ${my_array[1]}\" #所有元素 echo \"数组的元素为: ${my_array[*]}\" echo \"数组的元素为: ${my_array[@]}\" #数组长度 echo \"数组元素个数为: ${#my_array[*]}\" echo \"数组元素个数为: ${#my_array[@]}\" #数组遍历 for data in ${array[@]} do echo ${data} done 又比如: #这里ETHTOOL是个数组, 而T_PKT也是数组, 由入参index来控制 update_stats () { # $name $index TS_LAST[$2]=${TS[$2]} R_PKT_LAST[$2]=${R_PKT[$2]} R_BYTE_LAST[$2]=${R_BYTE[$2]} T_PKT_LAST[$2]=${T_PKT[$2]} T_BYTE_LAST[$2]=${T_BYTE[$2]} ETHTOOL=($(ethtool -S $1 | awk '/tx_packets_phy/{print $2} /rx_packets_phy/{print $2} /tx_bytes_phy/{print $2} /rx_bytes_phy/{print$2}')) if [ -z \"$ETHTOOL\" ]; then ETHTOOL=($(ethtool -S $1 | awk '/tx_packets/{print $2} /rx_packets/{print $2} /tx_bytes/{print $2} /rx_bytes/{print$2}')) fi TS[$2]=$(date +%s%6N) # in usec T_PKT[$2]=${ETHTOOL[0]} R_PKT[$2]=${ETHTOOL[1]} T_BYTE[$2]=${ETHTOOL[2]} R_BYTE[$2]=${ETHTOOL[3]} } # set initial value index=0 for name in $NETIF; do update_stats $name $index ((index++)) done shell脚本解析文件, 输出可以导入到excel的文本 测了大约500次的stream, 想在excel上绘图. 原始数据格式 STREAM2 fill latency: 0.92 nanoseconds STREAM2 fill bandwidth: 121083.29 MB/sec STREAM2 copy latency: 2.12 nanoseconds STREAM2 copy bandwidth: 105697.16 MB/sec STREAM2 daxpy latency: 3.24 nanoseconds STREAM2 daxpy bandwidth: 103654.31 MB/sec STREAM2 sum latency: 1.57 nanoseconds STREAM2 sum bandwidth: 71472.48 MB/sec STREAM2 fill latency: 1.68 nanoseconds STREAM2 fill bandwidth: 66475.08 MB/sec STREAM2 copy latency: 2.11 nanoseconds STREAM2 copy bandwidth: 105921.94 MB/sec STREAM2 daxpy latency: 3.24 nanoseconds STREAM2 daxpy bandwidth: 103562.46 MB/sec STREAM2 sum latency: 1.57 nanoseconds STREAM2 sum bandwidth: 71506.36 MB/sec 用下面的脚本可以输出一个excel认识的文本 for i in \"fill latency\" \"copy latency\" \"daxpy latency\" \"sum latency\" \"fill bandwidth\" \"copy bandwidth\" \"daxpy bandwidth\" \"sum bandwidth\"; do (printf \"$i \\t\" && cat stream*.log | awk \"/$i/\"'{printf $4 \"\\t\"}' && echo) >> test.txt; done for后面的东西用空格分割 awk是按行操作的, 比如第一行会一次执行多个{ }操作 awk里面用printf输出没有换行 shell变量要传入awk, 用双引号, 上面是用双引号括的/pattern/, awk \"/$i/\"'{printf $4 \"\\t\"}'注意后面不加空格. 参考https://www.gnu.org/software/gawk/manual/html_node/Using-Shell-Variables.html shell的小括号可以一起重定向 脚本输出数据示例: 一共八行 fill latency 0.92 1.68 1.70 1.67 1.69 1.69 0.92 1.67 1.65 copy latency 2.12 2.11 2.12 2.27 2.25 2.23 2.22 2.27 2.11 daxpy latency 3.24 3.24 3.24 3.24 3.11 3.24 3.24 3.24 3.25 sum latency 1.57 1.57 1.55 1.57 1.56 1.57 1.57 1.57 1.57 fill bandwidth 121083.29 66475.08 66018.47 67102.43 66170.01 66209.27 copy bandwidth 105697.16 105921.94 105900.48 98852.60 99648.56 100382.3 daxpy bandwidth 103654.31 103562.46 103639.09 103740.30 108061.48 1036 sum bandwidth 71472.48 71506.36 72182.22 71490.27 71900.50 71472.48 shell处理命令行选项 注意shell变量的删除 插入操作 比如 var=${arg/#--no-} # remove all dashes for variable names: eval ${var//-}=0 while (($#)) do arg=$1 shift case \"${arg:-}\" in --verbose|-v) set -x ;; --help|-h|'') help exit 0 ;; --*\\=*) # make variables from options IFS='=' read var value shell也可以递归调用函数 下面这个例子的作用是深度清除git库下面的未被跟踪的文件, 包括其下面的子git库 #!/bin/sh gitpurge_r() { local cwd=`pwd` local subgit=$1 cd $subgit echo \"deep cleaning $subgit\" local subgits=\"`git clean -fdx | grep 'Skipping repository' | cut -d' ' -f3`\" if [ -n \"$subgits\" ]; then for g in $subgits; do gitpurge_r $g done fi cd $cwd } gitpurge_r . 改进版本 #!/bin/bash B=\"\\033[1;37;40m\" N=\"\\033[0m\" top_dir=`pwd` remote=`git remote -v | grep fetch | awk '{print $2}'` repos_branch='./master bootloader/edk2/thunder-stable bootloader/grub2/grub/master bootloader/trusted-firmware/atf/thunder-stable bootloader/u-boot/thunder-stable linux/kernel/linux-aarch64/thunder-stable' function reset_current() { remote_branch=`git branch -vv | grep \"\\*\" | sed -r 's/.*\\[(.*)\\].*/\\1/g' | cut -d: -f1` echo ===reseting with $remote_branch git reset --hard $remote_branch } for repo_branch in $repos_branch; do repo=`dirname $repo_branch` branch=`basename $repo_branch` cd $top_dir if [ ! -d $repo ]; then echo cloning $repo cd `dirname $repo` if [ $repo == bdk ]; then git clone $remote/$repo else git clone -b fae $remote/$repo fi fi cd $top_dir/$repo echo -e $B\">>>`pwd`\"$N case \"$1\" in purge) echo ==deep cleaning... git clean -fdx ;; fetch) echo ==fetching from remote... git fetch --all ;; merge) echo ==merging workspace with $branch... git merge origin/$branch --no-commit ;; #upsync MUST be only used on my mint mechine upsync) echo ==fetching all git fetch --all echo ==reseting with remote master reset_current echo ==chekouting $branch git checkout $branch echo ==reseting with remote stable reset_current echo ==going back to master git checkout - ;; *) echo ==doing [\"$@\"] \"$@\" ;; esac echo done local system monitor ./sysmonitor.sh -f test.f -t \"just want to test it\" #! /bin/bash B=\"\\033[1;37;40m\" N=\"\\033[0m\" file= interval=5 sleepsec=10 title= while getopts \"f:i:s:t:\" arg #: needs para do case $arg in f) file=$OPTARG #$OPTARG is the para ;; i) interval=$OPTARG ;; s) sleepsec=$OPTARG ;; t) title=$OPTARG ;; ?) echo \"unkonw argument\" exit 1 ;; esac done #exec &> $output #output to file function do_syscollect() { echo; echo $title echo -e $B\"start system statistics collecting\"$N dmesg -c while test -f \"$file\"; do echo; echo -e $B\"========================collecting system status===========================\"$N echo; echo -e $B\"===date\"$N date echo; echo -e $B\"===dmesg\"$N dmesg -c echo; echo -e $B\"===top\"$N top -bn1 | head -15 echo; echo -e $B\"===sar\"$N sar -n DEV -u -dp -r $interval 1 | grep Average sleep $sleepsec done } do_syscollect | ts system monitor for pangu $ cat statistics.sh #! /bin/bash B=\"\\033[1;37;40m\" N=\"\\033[0m\" echo -e $B\"start system statistics collecting\"$N ./alldo.sh dmesg -c while true; do echo -e $B\"========================collecting system status===========================\"$N echo; echo -e $B\"===date\"$N ./alldo.sh 'date' echo; echo -e $B\"===dmesg\"$N ./alldo.sh 'dmesg' echo; echo -e $B\"===top\"$N ./alldo.sh 'top -bn1 | head -15' echo; echo -e $B\"===pangu\"$N ./alldo.sh \"ps -ef | egrep 'pangu|tubo|nuwa_agent|deploy_agent|chunkserver'\" echo; echo -e $B\"===sar\"$N ./alldo.sh 'sar -n DEV -u -dp -r 5 1 | grep Average' sleep $((60*1)) done $ cat logstat.sh #! /bin/bash B=\"\\033[1;37;40m\" N=\"\\033[0m\" logfile=${1:-log} ./statistics.sh | ts | tee $logfile $ cat alldo.sh #! /bin/bash B=\"\\033[1;37;40m\" N=\"\\033[0m\" atlocal=F servers=\" yingjie@192.168.85.10 byj@localhost \" cvmservers=\" root@10.97.219.6 root@10.97.219.55 root@10.97.219.21 root@10.97.219.44 root@10.97.219.53 root@10.97.219.50 root@10.97.219.214 root@10.97.219.13 root@10.97.219.47 root@10.97.219.69 \" if [ \"$1\" = \"-l\" ]; then atlocal=T shift fi for i in $cvmservers; do ip=${i#*@} echo echo -e $B\">>>$i\"$N if [ \"$atlocal\" = \"T\" ]; then eval \"$*\" else ssh $i \"$*\" fi done 增加fedora分区的脚本 这里面有shrink原分区(ubuntu), 增加fedora分区并拷贝fedora的fs work_dir=`pwd` dev=${1:-/dev/sdc} ubuntu_size=${2:-500G} tar_fedora=${3:-fedora-with-native-kernel-repo-gcc-update-to-20150423.tar.bz2} if [ ! -b $dev ]; then echo \"$dev not found\" exit 1 fi if [ -b ${dev}3 ]; then echo \"fedora already exists, nothing to do\" exit 0 fi if [ ! -f $tar_fedora ]; then echo \"file $tar_fedora not found\" exit 1 fi echo \"resizing ubuntu fs\" e2fsck -f ${dev}2 -y resize2fs ${dev}2 $ubuntu_size echo \"resizing ubuntu partition\" gdisk $dev etc/fstab ##grub #sed -i \"/set timeout/a\\\\\\nmenuentry 'Thunder Fedora Boot' {\\n\\tlinux /boot/vmlinuz root=/dev/sda3 console=ttyAMA0,115200n8 earlycon=pl011,0x87e024000000 coherent_pool=16M rootwait rw transparent_hugepage=never\\n\\tboot\\n}\\n\" boot/grub/grub.cfg #find lib/modules -name rtc-efi.ko | xargs -i mv {} {}.bak #cd .. #umount mnt sync cd $work_dir "},"notes/shell_脚本片段.html":{"url":"notes/shell_脚本片段.html","title":"shell脚本片段","keywords":"","body":" mkimage shell devmem脚本 shell判断是否包含字串 awk处理表格, 以mkimage为例 记录日志 等待一个文件 发送SIGHUP到pid并等待进程终止 pid到进程名 mac地址转换到字符串 大小写转换 去掉前后空格 加0x前缀, 负数时加-0x 两个16进制数相加 查看进程是否还在 网络接口是否存在 解析命令参数 mkimage #!/bin/sh get_value_by_key() { local str=\"$1\" local key=$2 local value=\"\" if [ \"${str/$key=}\" != \"$str\" ]; then value=${str##*$key=} value=${value%% *} value=${value%%,*} fi echo $value } generate_ubi_cfg() { #ubi config cat > $ubifs_cfg /tmp/tmp_file dd if=/tmp/tmp_file of=$outfile bs=1 conv=notrunc seek=$offset || return 1 fi if [ -n \"$file\" ]; then if [ -n \"$size\" ]; then dd if=$file of=$outfile bs=1 conv=notrunc seek=$offset count=$size || return 1 else dd if=$file of=$outfile bs=128K conv=notrunc seek=$((offset/128/1024)) || return 1 fi fi if [ \"$fstype\" = \"ubifs\" ]; then local img_out=ubi.img rm -f $img_out make_ubifs_image \"$str\" $img_out || return 1 dd if=$img_out of=$outfile bs=128k conv=notrunc seek=$((offset/128/1024)) || return 1 fi return 0 } #load config . *_image.cfg #generate the whole empty image dd if=/dev/zero bs=$flash_size count=1 | tr '\\000' '\\377' > $outfile echo \"$flash_map\" | while read line do if [ -n \"$line\" ]; then add2image \"$line\" $outfile || exit 1 fi done if [ \"$?\" = \"0\" ]; then #print map echo \"====SUCCESS====\" echo \"$flash_map\" else echo \"====FAILURE====\" echo see *_image.cfg fi shell devmem脚本 #!/bin/sh while true; do devmem 0x100003003005C 32 0x1000000 devmem 0x100003013005C 32 0x1000000 devmem 0x100003043005C 32 0x1000000 devmem 0x100003053005C 32 0x1000000 devmem 0x100003083005C 32 0x1000000 devmem 0x100003093005C 32 0x1000000 devmem 0x1000030c3005C 32 0x1000000 devmem 0x1000030d3005C 32 0x1000000 devmem 0x100005003005C 32 0x1000000 devmem 0x100005013005C 32 0x1000000 devmem 0x100005043005C 32 0x1000000 devmem 0x100005053005C 32 0x1000000 devmem 0x100005083005C 32 0x1000000 devmem 0x100005093005C 32 0x1000000 devmem 0x1000050c3005C 32 0x1000000 devmem 0x1000050d3005C 32 0x1000000 done shell判断是否包含字串 注意:[[ \"$line2\" != \"$line1\"* ]]中 双方括号不能少 通配符*不能在引号内 含通配符的字符串不能是左操作符common_path_2line() { #(line1, line2) local line1=$1 local line2=$2 while [[ \"$line2\" != \"$line1\"* ]];do line1=`dirname $line1` done echo $line1 } awk处理表格, 以mkimage为例 fpxt-b_image.conf: #---------------------------------------------------------------------------------- # start : type : source : descption #---------------------------------------------------------------------------------- 0x00000000 : file : u-boot.bin : bps uboot 0x00120000 : empty : : bps flags sector, leave empty 0x00140000 : string : committed : pkg A status, put committed 0x00160000 : string : uncommitted : pkg B status, put uncommitted 0x00180000 : file : u-boot.itb : uboot package A fit image 0x002C0000 : empty : : uboot package B fit image, leave empty 0x00400000 : file : vmlinux.itb : linux fit image(vmlinux and rootfs) 0x01FE0000 : empty : : uboot env variables, leave empty 处理上面文件的脚本: create_blank_image() #(size) { dd if=/dev/zero bs=$1 count=1 | tr '\\000' '\\377' > $image_file } fill_data_from_file() #(start,file) { local start=$1 local file=$2 [ ! -f $file ] && echo \"#### **ERROR** No such file: $file\" && exit 1 dd if=$file of=$image_file bs=128K conv=notrunc seek=$((start/128/1024)) } fill_data_from_string() #(start,string) { local start=$1 local string=$2 local tmpfile=`mktemp` printf \"$string\\0\" > $tmpfile fill_data_from_file $start $tmpfile rm -rf $tmpfile } awk_wrapper() #(start,type,source) { local start=$1 local type=$2 local source=$3 case \"$type\" in file) fill_data_from_file $start $source_dir/$source ;; string) fill_data_from_string $start $source ;; empty) ;; *) ;; esac } # export for awk sub-shell used export -f awk_wrapper fill_data_from_file fill_data_from_string export source_dir image_file echo \"#### fill image data\" cat \"$image_conf\" | sed '/^#/d'| sed '/^$/d' | awk -F: '{ print \"####\" $4; ret=system(\"awk_wrapper \"$1\" \"$2\" \"$3\" \"); if(ret!=0) { exit ret } }' 记录日志 SYSLOG_NAME=`basename $0` log() { if [ -n \"$SYSLOG_NAME\" ] then LOGGER_ARGS=\"-t $SYSLOG_NAME\" fi if [ -n \"$SYSLOG_PRIORITY\" ] then LOGGER_ARGS=\"$LOGGER_ARGS -p $SYSLOG_PRIORITY\" fi logger -s $LOGGER_ARGS \"$@\" } debug() { if config_flag_enabled verbose; then SYSLOG_PRIORITY=user.debug log \"$@\" fi } info() { if config_flag_enabled verbose; then SYSLOG_PRIORITY=user.info log \"$@\" fi } warning() { SYSLOG_PRIORITY=user.warn log \"$@\" } error() { SYSLOG_PRIORITY=user.error log \"$@\" } 等待一个文件 wait_for_file() { local retval=0 local file=$1 local cnt=0 local cntmax if [ -z \"$2\" ]; then cntmax=20 else cntmax=$2 fi echo -n Waiting for $file... while [ ! -e $file -a $cnt -le $cntmax ]; do usleep 100000 #100 ms let 'cnt++' done if [ -e $file ]; then echo \" ok (waited $cnt * 100 ms)\" else echo \" timeout ($cntmax * 100 ms)! System behavior is unspecified...\" retval=1 fi return $retval } 发送SIGHUP到pid并等待进程终止 wait_app_done_pid() { local pid=$1 # if reading the cmdline in the application's proc directory # fails, the application most has most likely already stopped local app if ! app=$(cat /proc/$pid/task/$pid/stat 2>/dev/null); then return 0 fi app=${app#*\\(} app=${app%%\\)*} local cnt=0 local cntmax if [ -z \"$2\" ]; then cntmax=60 else cntmax=$2 fi echo -n \"Waiting up to $cntmax seconds for $app (PID $pid) to go down...\" while kill -s 0 \"$pid\" 2>/dev/null && [ $cnt -le $cntmax ]; do sleep 1 let 'cnt++' done if [ $cnt -le $cntmax ]; then echo \"ok, $app (PID $pid) finished in $cnt seconds.\" return 0 else echo \"darn, $app (PID $pid) did not finish within $cntmax seconds.. oh well..\" return 1 fi } pid到进程名 pid_to_procname() { local pid=$1 local app=$(cat /proc/$pid/task/$pid/stat 2>/dev/null) app=${app#*(} app=${app%%)*} echo $app } mac地址转换到字符串 mac_hex_to_string() { local hex=$1 echo $hex | sed -e 's/../&:/g' -e 's/:$//' } 大小写转换 tolower() { local str=$1 echo $str | tr '[:upper:]' '[:lower:]' } toupper() { local str=$1 echo $str | tr '[:lower:]' '[:upper:]' } 去掉前后空格 trim() { local str=$1 echo $str | sed -e 's/^ *//g' -e 's/ *$//g' } 加0x前缀, 负数时加-0x hex_prefix() { # add 0x as prefix if not yet present (but take into account negative numbers) echo $1 | sed -e 's/0x//I' -e 's/^\\(-\\?\\)/\\10x/' } 两个16进制数相加 addhex() { # sum two hex numbers # input can but doesn't have to contain 0x prefix, output will be without 0x local res let \"res = $(hex_prefix $1) + $(hex_prefix $2)\" printf '%x\\n' $res } 查看进程是否还在 isalive() { # Simply return the return code of pidof (0=alive, 1=dead) pidof \"$@\" > /dev/null } 网络接口是否存在 interface_exists() { local iface=$1 [ -e /sys/class/net/$iface ] } 解析命令参数 支持key=value格式转换key为变量, 注意until和shift的用法 # Alternative getopt that accepts input of the form # \"key1=value1 key2=value2\" and exposes key1 and key2 # as shell variables. # Usage: getopt_simple getopt_simple() { until [ -z \"$1\" ] do parameter=${1%%=*} # Extract name. value=${1##*=} # Extract value. # Verify if parameter starts with a valid alpha-character, or ignore case $parameter in [A-z]*) eval $parameter=\\\"$value\\\" ;; esac shift # Drop $1 and shift $2 to $1 done } # Expose only the selected key/value pair. This is safer than getopt_simple # because it avoids overwriting variables of the calling script unexpectedly. # Usage: getopt_simple_onevar getopt_simple_onevar() { local key=$1 shift until [ -z \"$1\" ] do parameter=${1%%=*} # Extract name. value=${1##*=} # Extract value. if [ \"$parameter\" = \"$key\" ]; then eval $parameter=\\\"$value\\\" fi shift # Drop $1 and shift $2 to $1 done } "},"notes/python_记录.html":{"url":"notes/python_记录.html","title":"python记录","keywords":"","body":" class的数据成员不用声明 python 调用其他可执行文件 一个节拍发生器的算法 补充:自然对数 解析寄存器 secureCRT停止所有linux启动脚本 class的数据成员不用声明 #!/usr/bin/python3 #类定义 class people: #定义基本属性 name = '' age = 0 #定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print(\"%s 说: 我 %d 岁。\" %(self.name,self.age)) # 实例化类 p = people('runoob',10,30) p.speak() 下面的代码有没有第6行的name = ''都能正常运行, 结果都是 $ python3 test.py runoob 说: 我 10 岁。 如果同时把第17行的self去掉, 只保留name = n, 那么相当于声明了函数的局部变量. 执行时会报错: people类没有name成员 python 调用其他可执行文件 #返回值是uint16, 高8位是executable的返回值 ret = os.system(\"/path/to/executable para1 para2\") 一个节拍发生器的算法 log(1- x/1073741823)/-1 import math import random def test(rate, count): s = 0 for i in range(count): r = math.log(1 - random.uniform(0,0x3fffffff)/0x3fffffff)/-rate s += r print r print \"avg rate: \", s/count 这个函数的牛逼之处在于, 虽然有random操作, 但最后的发生rate的平均值都在rate左右如果按平均值来说, 即使把0x3fffffff改为10, 平均值也是1 >>> test(1, 140000) 0.999586366008 >>> test(1, 140000) 0.997003720261 >>> test(1, 140000) 1.00136841452 >>> test(1, 140000) 0.996789930059 >>> test(1, 140000) 1.00041864097 >>> test(1, 140000) 1.0004003745 >>> test(1, 140000) 1.00209053795 >>> test(1, 140000) 1.00115873931 >>> test(1, 140000) 0.997373828955 取自sysbench --tx-rate发生进程的算法, 这个进程每次先用这个函数算一个时间t, 然后sleep t的时间, 然后发令牌给其他线程;这样, 不管有多少个线程, 每个都在等令牌, 先到先得. 补充:自然对数 lim（1+1/x）^x＝e, e=2.71828 举个例子：年利率100%，存1年，1元钱最多能变成多少？ 如果存满1年，也就是n=1，那么1年后， 1(1+1/1)^1=2，1元变成2元； 如果半年一存，让这半年的利息在下半年也生利息，也就是n=2，存满1年后，式子应该写作 1(1+1/2)^2=2.25，1元变成2.25元，比第一种存法划算； 如果一个季度一存，每个季度的利息在后面的时间里也生利息，也就是n=4，式子写作 1*(1+1/4)^4 =2.44140625，比第二种存法还划算； 看上去，存的次数越多，每次存期越短，1年到期时的利息越多。但是数学告诉你，这个利息不是无限增加的，随着存的次数增多，1年后的本息合计趋于一个极限，也就是文中所提到的e。也就是说，在年利率为100%的情况下，不管你存得多么勤快，1年后1元钱最多变成e=2.71828182845904523536...元钱。 解析寄存器 def reg_decode64(val, high=63, low=0): reg = bin(val)[2:] reg64 = '0' * (64 - len(reg)) + reg field = reg64[-1-high:-1-low] + reg64[-1-low] print 'Bin:', field print 'Dec:', int(field, 2) print 'Hex:', hex(int(field, 2)) secureCRT停止所有linux启动脚本 def main(): crt.Screen.Synchronous = True while True: result = crt.Screen.WaitForStrings([\"ISAM:Press 'f' to enter UBOOT prompt\", \"Starting linux\", \"To interrupt normal boot\", \"/isam/user #\"]) if result == 1: crt.Screen.Send(\"f\\n\") if result == 2: at_linux_shell = 0 if result == 3 and at_linux_shell == 0: crt.Screen.Send(\"stop\\n\") if result == 4: at_linux_shell = 1 main() "},"notes/lua_记录.html":{"url":"notes/lua_记录.html","title":"lua记录","keywords":"","body":" pktgen lua脚本 pktgen lua脚本 package.path = package.path ..\";?.lua;test/?.lua;app/?.lua;../?.lua\" -- 加载Pktgen.lua, 该文件位于pktgen根目录下, 提供一些辅助函数 require \"Pktgen\"; function number2ip(number) -- string.format和printf挺像 local str=string.format(\"%08x\", number); -- 分组匹配, 连续赋值很赞 local s4,s3,s2,s1 = str:match(\"(%x%x)(%x%x)(%x%x)(%x%x)\"); -- ..是字符串连接 return string.format(\"%d.%d.%d.%d\", \"0x\"..s4,\"0x\"..s3,\"0x\"..s2,\"0x\"..s1); end function number2mac(number) local str=string.format(\"%012x\", number); local s6,s5,s4,s3,s2,s1 = str:match(\"(%x%x)(%x%x)(%x%x)(%x%x)(%x%x)(%x%x)\"); return string.format(\"%s:%s:%s:%s:%s:%s\", s6,s5,s4,s3,s2,s1); end function setFlow(port, pkt_size, nb_L2, nb_L3, nb_L4) pktgen.range.ip_proto(port, \"udp\"); pktgen.range.pkt_size(port, \"start\", pkt_size); pktgen.range.pkt_size(port, \"inc\", 0); pktgen.range.pkt_size(port, \"min\", pkt_size); pktgen.range.pkt_size(port, \"max\", pkt_size); pktgen.range.dst_mac(port, \"inc\", \"00:00:00:00:00:00\"); pktgen.range.src_mac(port, \"start\", \"00:00:00:00:00:01\"); pktgen.range.src_mac(port, \"inc\", \"00:00:00:00:00:01\"); pktgen.range.src_mac(port, \"min\", \"00:00:00:00:00:01\"); pktgen.range.src_mac(port, \"max\", number2mac(nb_L2)); pktgen.range.dst_ip(port, \"start\", \"0.0.0.1\"); pktgen.range.dst_ip(port, \"inc\", \"0.0.0.1\"); pktgen.range.dst_ip(port, \"min\", \"0.0.0.1\"); pktgen.range.dst_ip(port, \"max\", number2ip(nb_L3)); pktgen.range.src_ip(port, \"inc\", \"0.0.0.0\"); pktgen.range.dst_port(port, \"inc\", 0); pktgen.range.src_port(port, \"start\", 1); pktgen.range.src_port(port, \"inc\", 1); pktgen.range.src_port(port, \"min\", 1); pktgen.range.src_port(port, \"max\", nb_L4); pktgen.set_range(port, \"on\"); end -- run 64B signle flow 10s or 100000000 packets, full rate, log performance data to file \"whatever_64B_1L2-1L3-1L4_flow.data\" -- runFlowTest(\"whatever\", 0, 0, 100, 64, 1, 1, 1, 10, 100000000) -- run from pktgen shell -- Pktgen:/> load /home/bai/repo/save/pktgen/range-flow.lua -- Pktgen:/> lua 'runFlowTest(\"single\", 0, 0, 100, 64, 1, 1, 1, 10, 0)' -- Pktgen:/> lua 'runFlowTest(\"single\", 0, 0, 100, 64, 1, 1, 1, 0, 100000000)' -- Pktgen:/> lua 'runFlowTest(\"multi\", 0, 0, 100, 64, 2000, 128, 128, 10, 0)' function runFlowTest(mark, sendport, recvport, rate, pkt_size, nb_L2, nb_L3, nb_L4, duration, nb_pkts) sendport = tostring(sendport); recvport = tostring(recvport); local datafile = tostring(mark) .. \"_\" .. pkt_size .. \"B_\" .. nb_L2 .. \"L2-\" .. nb_L3 .. \"L3-\" .. nb_L4 .. \"L4_flow.data\" local file = io.open(datafile, \"w\"); pktgen.stop(\"all\"); pktgen.clr(); pktgen.set(sendport, \"rate\", rate); pktgen.set(sendport, \"count\", nb_pkts); pktgen.latency(\"all\", \"enable\"); setFlow(sendport, pkt_size, nb_L2, nb_L3, nb_L4); pktgen.start(sendport); print(\"Running flow test and collecting data to \" .. datafile); -- local如果在下面的repeat语句块里, 就只在该语句块内可见; local rateTx, rateRx; -- lua的table就是shell的关系数组, python的map local ppsTxTable = {}; local ppsRxTable = {}; local i = 0; repeat pktgen.delay(1 * 1000); rateTx = pktgen.portStats(sendport, \"rate\")[tonumber(sendport)]; rateRx = pktgen.portStats(recvport, \"rate\")[tonumber(recvport)]; i = i + 1; -- lua从1开始计数 ppsTxTable[i] = rateTx.pkts_tx; ppsRxTable[i] = rateRx.pkts_rx; until(i == duration or rateTx.pkts_tx == 0) pktgen.stop(sendport); pktgen.delay(1 * 1000); -- i//2是\"地板除法\", lua数字都是浮点, 所以使用\"地板除法\", 只保留整数部分. local ppsTx = ppsTxTable[i//2]; local ppsRx = ppsRxTable[i//2]; local statTx = pktgen.portStats(sendport, \"port\")[tonumber(sendport)]; local statRx = pktgen.portStats(recvport, \"port\")[tonumber(recvport)]; local num_tx = statTx.opackets; local num_rx = statRx.ipackets; local num_dropped = num_tx - num_rx; local statPkt = pktgen.pktStats(\"all\")[tonumber(recvport)]; -- file:write是个语法糖 file:write(\"Tx pps: \" .. ppsTx .. \"\\n\"); file:write(\"Rx pps: \" .. ppsRx .. \"\\n\"); file:write(\"Tx pkts: \" .. num_tx .. \"\\n\"); file:write(\"Rx pkts: \" .. num_rx .. \"\\n\"); file:write(\"Dropped pkts: \" .. num_dropped .. \"\\n\"); file:write(\"Min avg latency(usec): \" .. statPkt.min_latency .. \"\\n\"); file:write(\"Max avg latency(usec): \" .. statPkt.max_latency .. \"\\n\"); file:close(); pktgen.set(\"all\", \"rate\", 100); pktgen.set(\"all\", \"count\", 0); pktgen.set_range(\"all\", \"off\"); pktgen.latency(\"all\", \"disable\"); end "},"notes/shell_rds脚本阅读.html":{"url":"notes/shell_rds脚本阅读.html","title":"RDS脚本阅读","keywords":"","body":" simsim_charts脚本 process_data.sh R语言 rds lua脚本 rds bash脚本 simsim_charts脚本 process_data.sh 使用方法 BENCH_SERIES=thunder_optimized_xfs MYSQL_TEMPLATES_NAME=optimized SYSBENCH_MAX_TIME=180 bash ./start_benchmarks.sh SYSBENCH_MAX_TIME=180 CHART_SERIES=results/{thx_optimized_60s,thx_network_60s,x86_baseline_60s} CHART_NAME=\"_optimized_vs_network_vs_x86\" sh ./process_data.sh SYSBENCH_MAX_TIME=180 CHART_SERIES=results/{thx_optimized_60s,thx_network_60s,x86_baseline_60s} CHART_NAME=\"_optimized_vs_network_vs_x86\" sh ./create_charts.sh 这个脚本是把所有的CHART_SERIES的tps和lat的相关信息, 全部提取到文件里 for series in $(eval echo $CHART_SERIES)里面eval echo $CHART_SERIES的作用是展开变量 $ CHART_SERIES=results/{thx_optimized_60s,thx_network_60s,x86_baseline_60s} $ echo $CHART_SERIES results/{thx_optimized_60s,thx_network_60s,x86_baseline_60s} $ eval echo $CHART_SERIES results/thx_optimized_60s results/thx_network_60s results/x86_baseline_60s 一个标准的shell从usage开始 usage() { cat for conf in A200 B5 C1 C2 C4 C8重点是in后面的集合就只是用空格隔开 要把一段脚本的输出重定向到文件, 可以这样 ( some commands ) > output-filename awk的例子 awk ' BEGIN {OFS=\",\"} /tps:/ { sub(\"\\\\[ *\", \"\", $0); sub(\"s]\",\"\",$1); sub(\",\",\"\",$5); sub(\"ms\",\"\",$12); print '\\\"$s\\\",\\\"$conf\\\",\\\"$threads\\\",$((counter*duration))'+$1,$1,\"tps\",$5; print '\\\"$s\\\",\\\"$conf\\\",\\\"$threads\\\",$((counter*duration))'+$1,$1,\"lat\",$12 }' $filename awk ' BEGIN {OFS=\",\"} /transactions:/ {sub(\"\\\\(\",\"\",$3); print '\\\"$s\\\",\\\"$conf\\\",\\\"$threads\\\",\\\"tps\\\",'$3} /approx./ {sub(\"ms\",\"\",$4); print '\\\"$s\\\",\\\"$conf\\\",\\\"$threads\\\",\\\"lat\\\",'$4}' $f awk '/search pattern1/ {Actions} /search pattern2/ {Actions}' file - awk '{pattern + action}' {filenames}, 按行处理, 对匹配patten的行, 顺序执行{}里面的操作 - patten就是//里面的东西 - $0代表整行,$1是第一个字段 - sub(match, replace, string)是字符串替换 - BEGIN是说在扫描之前执行的, 相对的END是在最后都扫描完了再执行的 - OFS是输出的分隔符, FS是输入的分隔符, 默认都是space - print输出字符串要加\"\", 比如print \"hello\"; 上面转义了\", 因为print后面接了单引号 输出文件的效果如 thunder_xfs,A200,1,tps,3737.49 thunder_xfs,A200,1,lat,75.43 thunder_xfs,A200,4,tps,5261.52 thunder_xfs,A200,4,lat,216.29 thunder_xfs,A200,8,tps,7595.05 thunder_xfs,A200,8,lat,313.97 thunder_xfs,B5,1,tps,245.66 thunder_xfs,B5,1,lat,25.24 thunder_xfs,B5,4,tps,715.88 thunder_xfs,B5,4,lat,39.76 thunder_xfs,B5,8,tps,1160.96 thunder_xfs,B5,8,lat,50.93 R语言 先要安装R, 似乎不用装那么多 apt install r-base $ cat /etc/apt/sources.list deb http://mirror.bjtu.edu.cn/cran/bin/linux/ubuntu trusty/ apt install libxt-dev apt install libcurl4-openssl-dev apt install gfortran apt install libmpfr-dev apt install liblapack-dev apt install r-base apt install r-cran* 还要进R的shell install.packages('ggplot2', dep = TRUE) install.packages('gridExtra', dep = TRUE) install.packages('reshape2', dep = TRUE) install.packages('plyr', dep = TRUE) install.packages('Matrix', dep = TRUE) data.frame是个表格, 一般每个列都是同样类型的元素(似乎不是必须?), 而每行好像是条记录的感觉 > student student ID Name Gender Birthdate 1 11 Devin M 1984-12-29 2 12 Edward M 1983-5-6 3 13 Wenli F 1986-8-8 代码注释之tps和latency #sb是data.frame类型 sb 代码注释之variance sb rds lua脚本 注释 单行注释-- 多行注释 --[[ **************************************************************************** Input parameters: --simsim-databases = N: the number of databases to create/use (1 by default) **************************************************************************** --]] 感觉上lua和shell的语法类似 sysbench的lua脚本可以用db相关操作 db_query(\"DROP DATABASE IF EXISTS \" .. database_name) db_query(\"CREATE DATABASE \" .. database_name) 在db_query(\"BEGIN\")和db_query(\"COMMIT\")之间的东西就是一次transaction的内容 主要的东西在event里面, sysbench的runner线程会调用 void *runner_thread(void *arg) do execute_request(test, &request, thread_id) test->ops.execute_request(r, thread_id) sb_lua_op_execute_request(sb_request_t *sb_req, int thread_id) lua_getglobal(L, \"event\") //见下面 lua_pcall(L, 1, 1, 0) while ((request.type != SB_REQ_TYPE_NULL) && (!sb_globals.error) ); simsim里面, event里做的事情是很多insert到不同的表, 这些表的格式都是在prepare阶段准备好的. rds bash脚本 set -eu shell内建的命令, 可以用help xxx来看帮助, 比如help for 这里 -e: 如果有命令返回非零值则立刻推出 -u: 没定义的变量直接使用时当成错误 小括号和后台(&)的异同 相同的地方都是会新建一个子进程来执行, 那么既然是子进程, 里面的变量啦,路径啦都不会影响父进程. 不同点在与: 后台执行是非阻塞的, 而小括号是阻塞的; 后台进程id mysqld --defaults-file=$mysql_basedir/my.cnf \\ --user=root >/dev/null 2>&1 & pids[$i]=$! 这里面有两个知识点, $!是最新加入后台的进程id; pids是个数组, 展开后如:pids[1]=1022 如何打印大块文字? --用cat ```shell cat Starting a benchmark series with the following parameters: Data root directory: $BENCH_ROOT Benchmark series label: $BENCH_SERIES Single socket: $BENCH_SINGLE_SOCKET Results directory: $RESULTS_DIR sysbench directory: $SYSBENCH_DIR Configurations to run: $MYSQL_CONFIGURATIONS Subconfiguration for case C: $MYSQL_C_INSTANCES Threads combinations: $SYSBENCH_THREAD_LIST sysbench extra arguments: $SYSBENCH_SIMSIM_ARGS Single run duration: ${SYSBENCH_MAX_TIME}s EOF * wait用来等待所有的后台进程结束 不加参数是等待所有后台进程, 也可以这样 比如在for循环里 `job_ids=\"$job_ids $!\"` 在循环外面: `wait $job_ids` * sysbench测试mysql的时候, 支持不同的端口号;下面的SB_PORTS就是这么一个变量\"3001,3002,3003...\" ```shell SB_SOCKETS=\"$BENCH_ROOT/${conf}${instances}/data3001/tmp/mysql.sock\" SB_PORTS=\"3001\" for i in $(seq 2 $instances) do mysql_port=$((3000 + i)); mysql_basedir=$BENCH_ROOT/${conf}${instances}/data$mysql_port mysql_tmpdir=$mysql_basedir/tmp SB_SOCKETS=\"$SB_SOCKETS,$mysql_tmpdir/mysql.sock\" SB_PORTS=\"$SB_PORTS,$mysql_port\" done # SB_CONNECT_ARGS=\"--mysql-socket=$SB_SOCKETS\" SB_CONNECT_ARGS=\"--mysql-host=127.0.0.1 --mysql-port=$SB_PORTS\" 这里面SB_PORTS=\"$SB_PORTS,$mysql_port\"相当于一个列表变量 "},"notes/Project_Euler.html":{"url":"notes/Project_Euler.html","title":"Project Euler","keywords":"","body":" 1. Multiples of 3 and 5 2. Even Fibonacci numbers 3. Largest prime factor 4. Largest palindrome product 5. Smallest multiple 6. Sum square difference 7. 10001st prime 8. Largest product in a series 9. Special Pythagorean triplet 10. Summation of primes 1. Multiples of 3 and 5 If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23. Find the sum of all the multiples of 3 or 5 below 1000 233168 sum(i for i in range(1000) if i%3 == 0 or i%5 == 0) 2. Even Fibonacci numbers Each new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with 1 and 2, the first 10 terms will be: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... By considering the terms in the Fibonacci sequence whose values do not exceed four million, find the sum of the even-valued terms. 4613732 def fib(max): a,b = 1,2 while a 3. Largest prime factor The prime factors of 13195 are 5, 7, 13 and 29. What is the largest prime factor of the number 600851475143 ? 6857 def prime(max): primes = [2] for test in range(primes[-1]+1, max+1, 2): sqrt = int(test ** 0.5) isprime = True for i in primes: if i > sqrt: break if test % i == 0: isprime = False break if isprime: primes.append(test) for i in primes: yield i num = 600851475143 for i in prime(int(num ** 0.5)): if num % i == 0: max = i print(max) 4. Largest palindrome product A palindromic number reads the same both ways. The largest palindrome made from the product of two 2-digit numbers is 9009 = 91 99. Find the largest palindrome made from the product of two 3-digit numbers. 906609 def reverse(num): renum = 0 while num: renum = renum * 10 + num % 10 num //= 10 return renum def ispalindrome(num): return num == reverse(num) max(i*j for i in range(999) for j in range(999) if ispalindrome(i*j)) 5. Smallest multiple 2520 is the smallest number that can be divided by each of the numbers from 1 to 10 without any remainder. What is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20? 232792560 def gcd(a, b): if b == 0: return a return gcd(b, a%b) def GCD(a, b): while b: a, b = b, a%b return a def lcm(m,n): return m*n//gcd(m,n) def smallest_multiple(n): if n == 1: return 1 return lcm(n, smallest_multiple(n-1)) smallest_multiple(20) 6. Sum square difference The sum of the squares of the first ten natural numbers is, 12 + 22 + ... + 102 = 385 The square of the sum of the first ten natural numbers is, (1 + 2 + ... + 10)2 = 552 = 3025 Hence the difference between the sum of the squares of the first ten natural numbers and the square of the sum is 3025 385 = 2640. Find the difference between the sum of the squares of the first one hundred natural numbers and the square of the sum. 25164150 sum(range(101))** 2 - sum(x**2 for x in range(101)) 7. 10001st prime By listing the first six prime numbers: 2, 3, 5, 7, 11, and 13, we can see that the 6th prime is 13. What is the 10 001st prime number? 104743 def prime(): primes = [2] x = 1 while True: x += 2 sqrt = int(x ** 0.5) isprime = True for i in primes: if i > sqrt: break if x % i == 0: isprime = False break if isprime: yield primes[-1] primes.append(x) def primeByIndex(n): p = prime() x = 0 for i in range(n): x = next(p) return x primeByIndex(10001) 8. Largest product in a series Find the greatest product of five consecutive digits in the 1000-digit number. 73167176531330624919225119674426574742355349194934 96983520312774506326239578318016984801869478851843 85861560789112949495459501737958331952853208805511 12540698747158523863050715693290963295227443043557 66896648950445244523161731856403098711121722383113 62229893423380308135336276614282806444486645238749 30358907296290491560440772390713810515859307960866 70172427121883998797908792274921901699720888093776 65727333001053367881220235421809751254540594752243 52584907711670556013604839586446706324415722155397 53697817977846174064955149290862569321978468622482 83972241375657056057490261407972968652414535100474 82166370484403199890008895243450658541227588666881 16427171479924442928230863465674813919123162824586 17866458359124566529476545682848912883142607690042 24219022671055626321111109370544217506941658960408 07198403850962455444362981230987879927244284909188 84580156166097919133875499200524063689912560717606 05886116467109405077541002256983155200055935729725 71636269561882670428252483600823257530420752963450 40824 d = '''73167176531330624919225119674426574742355349194934 96983520312774506326239578318016984801869478851843 85861560789112949495459501737958331952853208805511 12540698747158523863050715693290963295227443043557 66896648950445244523161731856403098711121722383113 62229893423380308135336276614282806444486645238749 30358907296290491560440772390713810515859307960866 70172427121883998797908792274921901699720888093776 65727333001053367881220235421809751254540594752243 52584907711670556013604839586446706324415722155397 53697817977846174064955149290862569321978468622482 83972241375657056057490261407972968652414535100474 82166370484403199890008895243450658541227588666881 16427171479924442928230863465674813919123162824586 17866458359124566529476545682848912883142607690042 24219022671055626321111109370544217506941658960408 07198403850962455444362981230987879927244284909188 84580156166097919133875499200524063689912560717606 05886116467109405077541002256983155200055935729725 71636269561882670428252483600823257530420752963450''' num = 5 dd = '1'*num + d.replace('\\n','').replace('0','1'*num) product = 1 max_product = 0 for i in range(len(dd)-num): product = product * int(dd[i+num]) // int(dd[i]) max_product = max(product, max_product) 9. Special Pythagorean triplet A Pythagorean triplet is a set of three natural numbers, a a2 + b2 = c2 For example, 32 + 42 = 9 + 16 = 25 = 52. There exists exactly one Pythagorean triplet for which a + b + c = 1000. Find the product abc. ----> a 333 200 375 425 31875000 for a in range(1, 333): for c in range(333, 1000): b = 1000-a-c if a*a + b*b == c*c: print(a, b, c) print(a*b*c) break 10. Summation of primes The sum of the primes below 10 is 2 + 3 + 5 + 7 = 17. Find the sum of all the primes below two million. 142913828922 def prime(limit=0): primes = [2, 3] for i in primes: yield i def is_prime(test): sqrt = int(test ** 0.5) isprime = True for i in primes: if i > sqrt: break if test % i == 0: isprime = False break return isprime t = 0 while True: t += 6 if limit and t > limit: break for x in [t-1, t+1]: if is_prime(x): primes.append(x) yield x sum(p for p in prime(2000000)) "},"notes/as_title_app.html":{"url":"notes/as_title_app.html","title":"应用相关","keywords":"","body":"如题 "},"notes/app_sysbench代码分析.html":{"url":"notes/app_sysbench代码分析.html","title":"sysbench代码分析","keywords":"","body":" lua脚本的event如何调用 关于时间 tx-rate如何实现 lua脚本的event如何调用 C中: sysbench: void *runner_thread(void *arg) do execute_request(test, &request, thread_id) test->ops.execute_request(r, thread_id) sb_lua_op_execute_request(sb_request_t *sb_req, int thread_id) lua_getglobal(L, \"event\") //见下面 lua_pcall(L, 1, 1, 0) while ((request.type != SB_REQ_TYPE_NULL) && (!sb_globals.error) ); lua中 function event(thread_id) local table_name table_name = \"sbtest\".. sb_rand_uniform(1, oltp_tables_count) //这里边db_query和前面yuqing说的就连上了 rs = db_query(\"SELECT c FROM \".. table_name ..\" WHERE id=\" .. sb_rand(1, oltp_table_size)) end 关于时间 相对时间 #ifdef HAVE_CLOCK_GETTIME # define SB_GETTIME(tsp) clock_gettime(CLOCK_REALTIME, tsp) #else 用gettimeofday(&tv, NULL); tx-rate如何实现 pthread_mutex_t event_queue_mutex; static sb_list_t event_queue; static pthread_cond_t event_queue_cv; static event_queue_elem_t queue_array[MAX_QUEUE_LEN]; 100000 main run_test(test) //起report线程 if (sb_globals.report_interval > 0) pthread_create(&report_thread, &thread_attr, &report_thread_proc,NULL) //起tx-rate线程, 这实际上是个节拍器 if (sb_globals.tx_rate > 0) pthread_create(&eventgen_thread, &thread_attr, &eventgen_thread_proc,NULL) for (;;) curr_ns = sb_timer_value(&sb_globals.exec_timer); //随机的 intr_ns = (long) (log(1 - (double)sb_rnd() / (double)SB_MAX_RND) / (-(double)sb_globals.tx_rate)*1000000); next_ns = next_ns + intr_ns*1000; if (next_ns > curr_ns) pause_ns = next_ns - curr_ns; else pause_ns = 1000; usleep(pause_ns / 1000); queue_array[i].event_time = sb_timer_value(&sb_globals.exec_timer); SB_LIST_ADD_TAIL(&queue_array[i].listitem, &event_queue); sb_globals.event_queue_length++; pthread_cond_signal(&event_queue_cv); //起checkpoint线程 if (sb_globals.n_checkpoints > 0) pthread_create(&checkpoints_thread, &thread_attr,&checkpoints_thread_proc, NULL) //起所有测试线程 for(i = 0; i 0) //queue满了直接break //获取全局互斥锁 //queue length为0说明都占满了, 此时需要等待条件量event_queue_cv, 这个东西会在tx-rate线程里面释放 //走到这说明queue又有空间了, 取一个entry出来 sb_globals.event_queue_length-- sb_globals.concurrency++ //释放全局互斥锁 request = get_request(test, thread_id); execute_request(test, &request, thread_id) test->ops.execute_request(r, thread_id) sb_lua_op_execute_request(sb_request_t *sb_req, int thread_id) lua_getglobal(L, \"event\") lua_pcall(L, 1, 1, 0) sb_globals.concurrency--; while ((request.type != SB_REQ_TYPE_NULL) && (!sb_globals.error) ); test->ops.thread_done(thread_id); sb_globals.num_running--; "},"notes/as_title_cos.html":{"url":"notes/as_title_cos.html","title":"C和Operating System","keywords":"","body":"如题 "},"notes/system_libc_part1.html":{"url":"notes/system_libc_part1.html","title":"libc概览1","keywords":"","body":"本文为阅读GNU libc手册的一些笔记 libc的手册从这里下载: https://www.gnu.org/software/libc/manual/pdf/libc.pdf 关于不同环境下函数调用是否安全 错误处理 内存管理 内存分配 改变malloc的行为 内存检查 内存分配钩子 内存debug 在栈上分配内存, 不需要free Obstacks 增大进程数据段 锁页面, 使其不会被换出 字符处理 类型判断和转换 字符函数 搜索与排序 比较函数原型: int comparison_fn_t (const void , const void ); 数组 哈希 树 正则 输入输出之流文件 重定向 流文件操作 多线程与文件锁 关于宽字符 stream写 stream读 unread和peeking ahead block读写 格式化打印 格式化输入 检查文件尾, 常用于判断 文件位置 文件流的buffering 底层输入输出, 与之对应的是文件描述符 文件描述符和文件流互转 mmap io 共享内存和mmap select 同步io 并行io之aio, _POSIX_ASYNCHRONOUS_IO fcntl 文件锁操作 io重定向 ioctl 文件系统接口 目录项 遍历目录项 遍历目录树 文件链接 化简文件路径, 去掉. ..和多余的/ 删除与重命名 文件统计 文件类型判断, 入参是stat的st_mode域 访问权限 特殊文件 临时文件 管道 命名管道 一般来说, 管道的读写是原子的. 除非size大于PIPE_BUF 关于不同环境下函数调用是否安全 MT-Safe: 线程安全, 多线程安全. 该函数可以在多线程环境下同时运行 AS-Safe: 异步调用安全, 一般用于指是否可以在signal handler里面用 错误处理 很多libc函数都会修改errno, 这是个全局变量, 在signal handler里面如果要修改errno, 记住先保存原来的值, 用完恢复. 可以用if来判断错误类型 if (errno == EINTR) 常用errno有: EPERM: 权限错误 ENOENT: 没有这个文件或目录 EINTR: 被中断打断 EIO: 物理读写错误 ENXIO ENODEV: 没有这个设备或地址 EBADF: 文件描述符错误 ECHILD: 没有子进程 ENOMEM: 没有内存了 EBUSY: 忙 EINVAL: 参数错误 EAGAIN: 请重试, 当前操作被打断并立即退出, 操作未完成 ENOTSOCK: 没有这个socket EMSGSIZE: socket 发送的msg太大 ENETDOWN: socket发现net down 相关函数#include void perror (const char *message) char * strerror (int errnum) 举例:#include #include #include #include FILE * open_sesame (char *name) { FILE *stream; errno = 0; stream = fopen (name, \"r\"); if (stream == NULL) { fprintf (stderr, \"%s: Couldn’t open file %s; %s\\n\", program_invocation_short_name, name, strerror (errno)); exit (EXIT_FAILURE); } else return stream; } 内存管理 内存分配 #include void * malloc (size t size) //如果分配的内存大于page size, 则malloc会使用mmap系统调用来分配内存 void free (void *ptr) void * realloc (void *ptr, size t newsize) //在malloc得到的ptr基础上, 改变大小到newsize, 如果系统发现无法直接在后面增大, 那么会找到另外一片足够大的内存, 并拷贝原有的内容到新的ptr, 所以realloc也许会返回一个新的指针 void * calloc (size t count, size t eltsize) //分配内存并清零 void * aligned_alloc (size t alignment, size t size) //分配对齐的内存 改变malloc的行为 #include int mallopt (int param, int value) //比较重要的是M_MMAP_THRESHOLD, 用来设定用mmap的阈值 内存检查 #include int mcheck (void (*abortfn) (enum mcheck status status)) enum mcheck_status mprobe (void *pointer) 用法: 在main（）函数开始时或适当位置显式地调用mcheck() 加-lmcheck重新连接可执行程序 设置环境变量MALLOCCHECK的值, MALLOCCHECK=1 gdb(gdb) break main Breakpoint 1, main (argc=2, argv=0xbffff964) at whatever.c:10 (gdb) command 1 Type commands for when breakpoint 1 is hit, one per line. End with a line saying just \"end\". >call mcheck(0) >continue >end (gdb) ... 内存分配钩子 #include __malloc_hook //函数指针, 类型: void *function (size_t size, const void *caller) __realloc_hook //void *function (void *ptr, size_t size, const void *caller) __free_hook //void function (void *ptr, const void *caller) __memalign_hook //void *function (size_t alignment, size_t size, const void *caller) 内存debug #include void mtrace (void) //这个函数会安装hook, 将malloc相关的strace输出到环境变量MALLOC_TRACE指向的文件 void muntrace (void) 在栈上分配内存, 不需要free 因为栈指针会自动回溯. 但要注意栈大小 #include void * alloca (size t size) //举例: int open2 (char *str1, char *str2, int flags, int mode) { //和局部字符数组相比, alloca的大小是自动扩展的. //GUN C标准支持动态数组, 比如这里也可以这样写 char name[strlen (str1) + strlen (str2) + 1]; char *name = (char *) alloca (strlen (str1) + strlen (str2) + 1); stpcpy (stpcpy (name, str1), str2); return open (name, flags, mode); } Obstacks 更详细信息参考附件 obstack是个内存池, 是个由不同size的object组成的栈, 所以它必须按后进先出的顺序free. 举例1: void test (int n) { struct obstack obst ; obstack_init (& obst ); /* Allocate memory for a string of length n-1 */ char * str = obstack_alloc (& obst , n * sizeof( str [0])); /* Allocate an array for n nodes */ node_t ** nodes = obstack_alloc (& obst , n * sizeof( nodes [0])); /* Store the current mark of the obstack */ void * mark = obstack_base (& obst ); /* Allocate the nodes */ for (i = 0; i 举例2: 自动增长 int * read_ints (struct obstack * obst , FILE *f) { while (! feof (f )) { int x , res ; res = fscanf (f , \"%d\", & x ); if ( res == 1) obstack_int_grow ( obst , x ); else break; } return obstack_finish ( obst ); } 增大进程数据段 int brk (void *addr) //用addr指定进程可用的end地址 void *sbrk (ptrdiff t delta) //用delta增长 锁页面, 使其不会被换出 需注意物理内存不换出会导致其他进程可用的物理页面减少 int mlock (const void *addr, size t len) int munlock (const void *addr, size t len) int mlockall (int flags) //MCL_CURRENT|MCL_FUTURE int munlockall (void) 字符处理 类型判断和转换 #include int islower (int c) int isupper (int c) int isalpha (int c) int isdigit (int c) int isalnum (int c) int isxdigit (int c) int ispunct (int c) int isspace (int c) int isblank (int c) int isgraph (int c) int isprint (int c) int iscntrl (int c) int isascii (int c) int tolower (int c) int toupper (int c) int toascii (int c) 字符函数 #include size_t strlen (const char *s) size_t strnlen (const char *s, size t maxlen) void * memcpy (void *restrict to, const void *restrict from, size t size) void * memmove (void *to, const void *from, size t size) void * memset (void *block, int c, size t size) char * strcpy (char *restrict to, const char *restrict from) char * strncpy (char *restrict to, const char *restrict from, size t size) char * strdup (const char *s) //把字符串s拷到malloc出来的内存里 char * strcat (char *restrict to, const char *restrict from) int memcmp (const void *a1, const void *a2, size t size) int strcmp (const char *s1, const char *s2) int strcasecmp (const char *s1, const char *s2) //忽略大小写 int strncmp (const char *s1, const char *s2, size t size) void * memchr (const void *block, int c, size t size) char * strchr (const char *string, int c) char * strrchr (const char *string, int c) char * strstr (const char *haystack, const char *needle) char * strtok (char *restrict newstring, const char *restrict delimiters) char * basename (const char *filename) char * dirname (char *path) 搜索与排序 libc提供最基础的排序的支持, 比如快排. 下次有人找你写个快排还需要自己写么? #include 比较函数原型: int comparison_fn_t (const void , const void ); 例子: int compare_doubles (const void *a, const void *b) { const double *da = (const double *) a; const double *db = (const double *) b; return (*da > *db) - (*da 数组 #include /* 在base数组内按照key搜索 */ void * lfind (const void *key, const void *base, size t *nmemb, size t size, comparison fn t compar) /* 在排好序的数组内搜索 */ void * bsearch (const void *key, const void *array, size t count, size t size, comparison fn t compare) #include /* 快排 */ void qsort (void *array, size t count, size t size, comparison fn t compare) 哈希 int hcreate (size t nel) 树 /*找到则返回节点指针, 否则就将key加入到tree中*/ void * tsearch (const void *key, void **rootp, comparison_fn_t compar) /*只查找不添加*/ void * tfind (const void *key, void *const *rootp, comparison fn t compar) /*删除节点*/ void * tdelete (const void *key, void **rootp, comparison fn t compar) /*销毁*/ void tdestroy (void *vroot, free fn t freefct) /*遍历void __action_fn_t (const void *nodep, VISIT value, int level);*/ void twalk (const void *root, action fn t action) 正则 #include //在进行正则匹配之前, 要求先编译pattern, 到regex_t结构体中 int regcomp (regex t *restrict compiled, const char *restrict pattern, int cflags) //匹配 int regexec (const regex t *restrict compiled, const char *restrict string, size t nmatch, regmatch t matchptr[restrict], int eflags) //单词解析 int wordexp (const char * words, wordexp_t * word-vector-ptr, int flags) void wordfree(wordexp_t *word-vector-ptr) 举例 int expand_and_execute(const char *program, const char **options) { wordexp_t result; pid_t pid int status, i; /* Expand the string for the program to run. */ switch (wordexp(program, &result, 0)) { case 0: /*Successful*/ break; case WRDE_NOSPACE: /* If the error was WRDE_NOSPACE , then perhaps part of the result was allocated. */ wordfree(&result); default: /*Some other error.*/ return -1; } /* Expand the strings specified for the arguments. */ for (i = 0; options[i] != NULL; i++) { if (wordexp(options[i], &result, WRDE_APPEND)) { wordfree(&result); return -1; } } pid = fork(); if (pid == 0) { /* This is the child process. Execute the command. */ execv(result.we_wordv[0], result.we_wordv); exit(EXIT_FAILURE); } else if (pid 输入输出之流文件 流文件, 用FILE 代表, 默认已经打开的三个 FILE stdin, FILE stdout, FILE stderr 重定向 fclose (stdout); stdout = fopen (\"standard-output-file\", \"w\"); 可能有些系统stdout是个宏, 那可以用freopen(), 这个函数基本上是fclose和fopen的合体 流文件操作 #include FILE *fopen(const char *path, const char *mode); FILE *fdopen(int fd, const char *mode); int fclose(FILE *fp); 带64后缀的fopen能够打开超过4G的文件 多线程与文件锁 每个文件都有个内部锁, 保证每次文件操作函数调用的原子性, 这个锁是隐含的. 所以多线程操作一个文件会隐含的顺序执行. 但有时这样还不够, 那么可以显示使用文件锁 注意: 文件锁加锁的对象是stream, 用于多线程之间的互斥. 如果要在多进程之间用文件互斥, 需使用lockf()调用. 详见代码片段==文件锁 #include void flockfile(FILE *filehandle); //会导致进程阻塞 int ftrylockfile(FILE *filehandle); void funlockfile(FILE *filehandle); 举例: 这里的flockfile()保护了下面的两句操作是原子的. FILE *fp; { ... flockfile (fp); fputs (\"This is test number \", fp); fprintf (fp, \"%d\\n\", test); funlockfile (fp) } 关于宽字符 libc支持在一个stream上允许宽字符和普通字符输出, 但两者只能选其一, 而且一旦选定就不能再更换. 宽字符操作函数和普通字符函数是对应的, 函数名有w的就是宽字符操作. 如果使用了宽字符操作函数, 这个stream就是宽字符模式, 否则就是普通字符模式. int fwide (FILE *stream, int mode) //这个函数可以设置模式, 要求是尽量早的调用 void print_f (FILE *fp) { if (fwide (fp, 0) > 0) /* Positive return value means wide orientation. */ fputwc (L’f’, fp); else fputc (’f’, fp); } stream写 int fputc (int c, FILE *stream) wint_t fputwc (wchar t wc, FILE *stream) int fputs (const char *s, FILE *stream) int fputws (const wchar t *ws, FILE *stream) int puts (const char *s) //输出到stdout stream读 int fgetc (FILE *stream) wint_t fgetwc (FILE *stream) /*以下是行模式*/ ssize_t getline (char **lineptr, size t *n, FILE *stream) //如果*lineptr是NULL而且n为0, 这个函数会自己malloc ssize_t getdelim (char **lineptr, size t *n, int delimiter, FILE *stream) //可以指定分隔符 ssize_t getline (char **lineptr, size_t *n, FILE *stream) { return getdelim (lineptr, n, ’\\n’, stream); } char * fgets (char *s, int count, FILE *stream) wchar_t * fgetws (wchar t *ws, int count, FILE *stream) unread和peeking ahead int ungetc (int c, FILE *stream) //把c放回stream, 只能放一次. 举例: #include #include void skip_whitespace (FILE *stream) { int c; do /* No need to check for EOF because it is not isspace, and ungetc ignores EOF. */ c = getc (stream); while (isspace (c)); ungetc (c, stream); } block读写 size_t fread (void *data, size t size, size t count, FILE *stream) size_t fwrite (const void *data, size t size, size t count, FILE *stream) 格式化打印 #include int printf (const char *template, . . . ) int wprintf (const wchar t *template, . . . ) int fprintf (FILE *stream, const char *template, . . . ) int fwprintf (FILE *stream, const wchar t *template, . . . ) int sprintf (char *s, const char *template, . . . ) int swprintf (wchar t *s, size t size, const wchar t *template, . . . ) /*动态分配内存*/ int asprintf (char **ptr, const char *template, . . . ) 举例: /* Construct a message describing the value of a variable whose name is name and whose value is value. */ char * make_message (char *name, char *value) { char *result; if (asprintf (&result, \"value of %s is %s\", name, value) 举例: #include #include void eprintf (const char *template, ...) { va_list ap; extern char *program_invocation_short_name; fprintf (stderr, \"%s: \", program_invocation_short_name); va_start (ap, template); vfprintf (stderr, template, ap); va_end (ap); } 格式化输入 int scanf (const char *template, . . . ) int wscanf (const wchar t *template, . . . ) int fscanf (FILE *stream, const char *template, . . . ) int sscanf (const char *s, const char *template, . . . ) 检查文件尾, 常用于判断 int feof (FILE *stream) int ferror (FILE *stream) 文件位置 long int ftell (FILE *stream) off_t ftello (FILE *stream) off64_t ftello64 (FILE *stream) int fseek (FILE *stream, long int offset, int whence) //SEEK_SET, SEEK_CUR, or SEEK_END int fseeko (FILE *stream, off t offset, int whence) //用以下一组函数处理位置更好 int fgetpos (FILE *stream, fpos t *position) int fsetpos (FILE *stream, const fpos t *position) 文件流的buffering 文件流都有buffer, 而更底层的io读写没有. buffer有三种模式: unbuffer line buffer full buffer 一般的文件都是full buffer, 但和交互相关的设备比如terminal就是line buffer 刷buffer的函数:int fflush (FILE *stream) //如果入参是NULL, 则会flush所有打开的文件 改变buffer模式:#include int setvbuf (FILE *stream, char *buf, int mode, size t size) //_IOFBF _IOLBF _IONBF 底层输入输出, 与之对应的是文件描述符 #include #include int open (const char *filename, int flags[, mode t mode]) 比如: open (filename, O_WRONLY | O_CREAT | O_TRUNC, mode) int open64 (const char *filename, int flags[, mode t mode]) int close (int filedes) ssize_t read (int filedes, void *buffer, size t size) //EAGAIN EINTR EIO ssize_t pread (int filedes, void *buffer, size t size, off t offset) //从文件开始的offset开始读, 不会影响文件位置 ssize_t write (int filedes, const void *buffer, size t size) ssize_t pwrite (int filedes, const void *buffer, size t size, off t offset) //类似pread off_t lseek (int filedes, off t offset, int whence) //SEEK_SET, SEEK_CUR, or SEEK_END off64_t lseek64 (int filedes, off64 t offset, int whence) 举例: //同一个文件, open两次, 每个描述符有独立的位置 { int d1, d2; char buf[4]; d1 = open (\"foo\", O_RDONLY); d2 = open (\"foo\", O_RDONLY); lseek (d1, 1024, SEEK_SET); read (d2, buf, 4); } //但dup函数复制的描述符都共享文件位置 { int d1, d2, d3; char buf1[4], buf2[4]; d1 = open (\"foo\", O_RDONLY); d2 = dup (d1); d3 = dup (d2); lseek (d3, 1024, SEEK_SET); read (d1, buf1, 4); read (d2, buf2, 4); } 文件描述符和文件流互转 FILE * fdopen (int filedes, const char *opentype) int fileno (FILE *stream) mmap io #include size_t page_size = (size_t) sysconf (_SC_PAGESIZE); void * mmap (void *address, size t length, int protect, int flags, int filedes, off t offset) protect: PROT_READ, PROT_WRITE, and PROT_EXEC flags: MAP_PRIVATE: 对内存region的写不会写回到文件 MAP_SHARED: 对内存region的写会写到文件, 但真正写文件的操作发生在any time. 用msync可以强制写 MAP_ANONYMOUS: 匿名mmap, 不需要文件, 映射的内存被初始化为0; 可用来替代malloc, 但GNU的malloc已包含mmap功能. void * mmap64 (void *address, size t length, int protect, int flags, int filedes, off64 t offset) int munmap (void *addr, size t length) int msync (void *address, size t length, int flags) void * mremap (void *address, size t length, size t new_length, int flag) 共享内存和mmap //用这个函数可以获得一个文件描述符, 用于mmap. 不同进程之间可以访问同名的共享内存 int shm_open (const char *name, int oflag, mode t mode) int shm_unlink (const char *name) select #include void FD_ZERO (fd set *set) void FD_SET (int filedes, fd set *set) void FD_CLR (int filedes, fd set *set) int FD_ISSET (int filedes, const fd set *set) //select int select (int nfds, fd set *read-fds, fd set *write-fds, fd set *except-fds, struct timeval *timeout) 举例: #include #include #include #include #include int input_timeout(int filedes, unsigned int seconds) { fd_set set; struct timeval timeout; /* Initialize the file descriptor set. */ FD_ZERO(&set); FD_SET(filedes, &set); /* Initialize the timeout data structure. */ timeout.tv_sec = seconds; timeout.tv_usec = 0; /* select returns 0 if timeout, 1 if input available, -1 if error. */ return TEMP_FAILURE_RETRY(select(FD_SETSIZE, &set, NULL, NULL, &timeout)); } int main(void) { fprintf(stderr, \"select returned %d.\\n\", input_timeout(STDIN_FILENO, 5)); return 0; } 同步io 即使write成功返回, 也并不意味着数据已经真正写进去了. #include //这个函数保证所有的数据已经写入物理介质 void sync (void) //对单文件 int fsync (int fildes) 并行io之aio, _POSIX_ASYNCHRONOUS_IO #include //异步读写, 操作会被放进队列 int aio_read (struct aiocb *aiocbp) int aio_write (struct aiocb *aiocbp) //既然是异步操作, 就必然有个函数来检查状态 int aio_error (const struct aiocb *aiocbp) //异步操作完成了, 检查返回值 ssize_t aio_return (struct aiocb *aiocbp) //希望在某个点上保证同步 int aio_fsync (int op, struct aiocb *aiocbp) //取消aio操作 int aio_cancel (int fildes, struct aiocb *aiocbp) fcntl #include int fcntl (int filedes, int command, . . . ) F_DUPFD F_GETFD F_SETFD F_GETFL F_SETFL F_GETLK F_SETLK F_SETOWN fcntl (filedes, F_SETFD, new-flags) fcntl (filedes, F_SETOWN, pid) F_GETFL F_SETFL包括: O_RDONLY O_WRONLY O_RDWR //访问权限 O_NONBLOCK //不阻塞 O_APPEND //在文件尾添加 O_ASYNC //异步, 用SIGIO信号表示操作完成? O_FSYNC 举例: /* Set the O_NONBLOCK flag of desc if value is nonzero, or clear the flag if value is 0. Return 0 on success, or -1 on error with errno set. */ int set_nonblock_flag(int desc, int value) { int oldflags = fcntl(desc, F_GETFL, 0); /* If reading the flags failed, return error indication now. */ if (oldflags == -1) return -1; /* Set just the flag we want to set. */ if (value != 0) oldflags |= O_NONBLOCK; else oldflags &= ~O_NONBLOCK; /* Store modified flag word in the descriptor. */ return fcntl(desc, F_SETFL, oldflags); } 文件锁操作 fcntl (filedes, F_SETLK, lockp) 与lockf()等同 需要注意的是, 文件锁是个主动锁, 显示的调用获取锁才能保证互斥. io重定向 int dup (int old) //先close new, 再dup old int dup2 (int old, int new) 举例: pid = fork(); if (pid == 0) { char *filename; char *program; int file; ... file = TEMP_FAILURE_RETRY(open(filename, O_RDONLY)); dup2(file, STDIN_FILENO); TEMP_FAILURE_RETRY(close(file)); execv(program, NULL); } ioctl #include int ioctl (int filedes, int command, . . . ) 文件系统接口 #include struct dirent char d_name[] ino_t d_fileno unsigned char d_namlen unsigned char d_type 目录项 char * getcwd (char *buffer, size t size) int chdir (const char *filename) int fchdir (int filedes) DIR * opendir (const char *dirname) DIR * fdopendir (int fd) int dirfd (DIR *dirstream) struct dirent * readdir (DIR *dirstream) int closedir (DIR *dirstream) //重新init这个dirstream void rewinddir (DIR *dirstream) 举例: #include #include #include int main(void) { DIR *dp; struct dirent *ep; dp = opendir(\"./\"); if (dp != NULL) { while (ep = readdir(dp)) puts(ep->d_name); (void) closedir(dp); } else perror(\"Couldn’t open the directory\"); return 0; } 遍历目录项 int scandir (const char *dir, struct dirent ***namelist, int (*selector) (const struct dirent *), int (*cmp) (const truct dirent **, const struct dirent **)) 遍历目录树 int ftw (const char *filename, ftw func t func, int descriptors) 文件链接 int link (const char *oldname, const char *newname) int symlink (const char *oldname, const char *newname) ssize_t readlink (const char *filename, char *buffer, size t size) 化简文件路径, 去掉. ..和多余的/ char * canonicalize_file_name (const char *name) char * realpath (const char *restrict name, char *restrict resolved) 删除与重命名 int unlink (const char *filename) int rmdir (const char *filename) int remove (const char *filename) int rename (const char *oldname, const char *newname) int mkdir (const char *filename, mode t mode) 文件统计 #include int stat (const char *filename, struct stat *buf) //不follow link int lstat (const char *filename, struct stat *buf) int fstat (int filedes, struct stat *buf) 文件类型判断, 入参是stat的st_mode域 int S_ISDIR (mode t m) int S_ISCHR (mode t m) 访问权限 #include S_IRUSR S_IWUSR S_IXUSR mode_t getumask (void) int chmod (const char *filename, mode t mode) int fchmod (int filedes, mode t mode) 特殊文件 int mknod (const char *filename, mode t mode, dev t dev) 临时文件 FILE * tmpfile (void) //临时文件名 char * tmpnam (char *result) char * mktemp (char *template) 管道 //使用管道必须保证读写两端同时open了, 才能开始读写. int pipe (int filedes[2]) //filedes[0]为读, filedes[1]为写 在使用pipe时, 搭配使用fork(创建子进程), dup2(令子进程的标准输入或输出重定向到pipe) 或者, 使用 FILE * popen (const char *command, const char *mode) //创建子进程执行command, 并重定向到pipe, 方向取决于mode, \"r\", \"w\" int pclose (FILE *stream) 举例: #include #include void write_data(FILE * stream) { int i; for (i = 0; i 命名管道 #include int mkfifo (const char *filename, mode t mode) 一般来说, 管道的读写是原子的. 除非size大于PIPE_BUF "},"notes/system_libc_part2.html":{"url":"notes/system_libc_part2.html","title":"libc概览2","keywords":"","body":" socket sock地址 接口名称, 比如eth0 local socket, 用于进程间通信 PF_LOCAL PF_UNIX PF_FILE 网络socket PF_INET PF_INET6 主机名称, 比如alpha.gnu.org 端口和服务 网络字节序 举例 socket接口 socket读写 stream client举例 stream server举例 oob数据 datagram inetd守护进程 socket配置 terminal terminal的模式 对terminal的改动是对dev来说是全局的 输入输出和控制属性, 见man tcsetattr local属性, ICANON ECHO* ISIG(ctrl+c, ctrl+\\, ctrl+z) 虚拟终端, master side slave side, 一对一 BSD方式打开pty syslog log的处理方式: syslogd会处理 syslogd的两个参数, 用一个数字来同时表示facility/priority syslog接口 数学 日期与时间 格式化时间字符串 定时器 sleep 纳秒睡眠 系统资源 资源统计 限制, 两种 ulimit 实时进程优先级 进程调度 nice值 绑定cpus 可用内存页数目 cpu个数 当前系统负载, 1 5 15分钟平均负载 socket SOCK_STREAM SOCK_DGRAM SOCKRAWPF: protocol familyAF_: address family 与PF_*是一一对应的sock地址 #include AF_LOCAL AF_UNIX AF_FILE: 都是一回事, 进程间通信AF_INET:AF_INET6: //设置socket地址 int bind (int socket, struct sockaddr *addr, socklen t length) //读取socket地址 int getsockname (int socket, struct sockaddr *addr, socklen t *length-ptr) 接口名称, 比如eth0 #include //接口名到index unsigned int if_nametoindex (const char *ifname) //index到接口名 char * if_indextoname (unsigned int ifindex, char *ifname) //获取全部接口 struct if_nameindex * if_nameindex (void) //释放if_nameindex分配的内存 void if_freenameindex (struct if nameindex *ptr) local socket, 用于进程间通信 PF_LOCAL PF_UNIX PF_FILE local socket又称为unix或file socket, socket地址就是个文件名, 一般放在/tmp下面 当一个local socket关闭时, 也需要delete这个文件 用struct sockaddr_un表示地址 举例: #include #include #include #include #include #include #include int make_named_socket(const char *filename) { struct sockaddr_un name; int sock; size_t size; /* Create the socket. */ sock = socket(PF_LOCAL, SOCK_DGRAM, 0); if (sock 网络socket PF_INET PF_INET6 这个socket用struct sockaddr_in或struct sockaddr_in6表示地址 ip地址用struct in_addr表示 #include //将192.168.1.1转换成32bit的ip地址 int inet_aton (const char *name, struct in addr *addr) //相反 char * inet_ntoa (struct in addr addr) 主机名称, 比如alpha.gnu.org #include //用struct hostent表示一个host struct hostent * gethostbyname (const char *name) struct hostent * gethostbyaddr (const void *addr, socklen t length, int format) 端口和服务 #include #include //每个服务对应struct servent struct servent * getservbyname (const char *name, const char *proto) struct servent * getservbyport (int port, const char *proto) 网络字节序 sin_port和sin_addr必须是网络字节序 #include uint16_t htons (uint16 t hostshort) uint16_t ntohs (uint16 t netshort) uint32_t htonl (uint32 t hostlong) uint32_t ntohl (uint32 t netlong) 举例 举例1: #include #include #include #include int make_socket(uint16_t port) { int sock; struct sockaddr_in name; /* Create the socket. */ sock = socket(PF_INET, SOCK_STREAM, 0); if (sock 举例2: #include #include #include #include #include void init_sockaddr(struct sockaddr_in *name, const char *hostname, uint16_t port) { struct hostent *hostinfo; name->sin_family = AF_INET; name->sin_port = htons(port); hostinfo = gethostbyname(hostname); if (hostinfo == NULL) { fprintf(stderr, \"Unknown host %s.\\n\", hostname); exit(EXIT_FAILURE); } name->sin_addr = *(struct in_addr *) hostinfo->h_addr; } socket接口 #include //open int socket (int namespace, int style, int protocol) /*close **可以直接close文件描述符, 此时colse会尝试发送未完成的data **也可以int shutdown (int socket, int how), how可以指定 **0 拒绝接受data **1 停止发送 **2 收发都停 */ //创建一对socket int socketpair (int namespace, int style, int protocol, int filedes[2]) //client连接, 阻塞等待server回应. 除非设了nonblocking模式 int connect (int socket, struct sockaddr *addr, socklen t length) //server端 int listen (int socket, int n) int accept (int socket, struct sockaddr *addr, socklen t *length_ptr) //addr是对端的地址 //获取对端地址, 前提是这个socket已经connect int getpeername (int socket, struct sockaddr *addr, socklen t *length-ptr) socket读写 可以用read和write, 但使用send和recv能获得更多的控制. 其实, write就相当于send的flags为0 不管是write还是send, 向已经损坏的socket写会得到SIGPIPE ssize_t send (int socket, const void *buffer, size t size, int flags) ssize_t recv (int socket, void *buffer, size t size, int flags) flags: MSG_OOB: out of band data MSG_PEEK: 用而不取 MSG_DONTROUTE stream client举例 #include #include #include #include #include #include #include #include #define PORT 5555 #define MESSAGE \"Yow!!! Are we having fun yet?!?\" #define SERVERHOST \"www.gnu.org\" write_to_server(int filedes) { int nbytes; nbytes = write(filedes, MESSAGE, strlen(MESSAGE) + 1); if (nbytes connect(sock, (struct sockaddr *) &servername, sizeof(servername))) { perror(\"connect (client)\"); exit(EXIT_FAILURE); } /* Send data to the server. */ write_to_server(sock); close(sock); exit(EXIT_SUCCESS); } stream server举例 #include #include #include #include #include #include #include #include #define PORT 5555 #define MAXMSG 512 int read_from_client(int filedes) { char buffer[MAXMSG]; int nbytes; nbytes = read(filedes, buffer, MAXMSG); if (nbytes oob数据 OOB数据通过send和recv传递, 使用flag MSG_OOB, 拥有高优先级, 不会和普通数据一起排队. OOB会触发信号SIGURG 发送OOB时, 会在普通数据流内加个mark, 表示\"即将完成\"OOB数据 举例: int discard_until_mark(int socket) { while (1) { /* This is not an arbitrary limit; any size will do. */ char buffer[1024]; int atmark, success; /* If we have reached the mark, return. */ success = ioctl(socket, SIOCATMARK, &atmark); if (success 举例: struct buffer { char *buf; int size; struct buffer *next; }; /* Read the out-of-band data from SOCKET and return it as a ‘struct buffer’, which records the address of the data and its size. It may be necessary to read some ordinary data in order to make room for the out-of-band data. If so, the ordinary data are saved as a chain of buffers found in the ‘next’ field of the value. */ struct buffer * read_oob(int socket) { struct buffer *tail = 0; struct buffer *list = 0; while (1) { /* This is an arbitrary limit. Does anyone know how to do this without a limit? */ #define BUF_SZ 1024 char *buf = (char *) xmalloc(BUF_SZ); int success; int atmark; /* Try again to read the out-of-band data. */ success = recv(socket, buf, BUF_SZ, MSG_OOB); if (success >= 0) { /* We got it, so return it. */ struct buffer *link = (struct buffer *) xmalloc(sizeof(struct buffer)); link->buf = buf; link->size = success; link->next = list; return link; } /* If we fail, see if we are at the mark. */ success = ioctl(socket, SIOCATMARK, &atmark); if (success buf = buf; link->size = success; /* Add the new link to the end of the list. */ if (tail) tail->next = link; else list = link; tail = link; } } } datagram //发到数据addr ssize_t sendto (int socket, const void *buffer, size t size, int flags, struct sockaddr *addr, socklen t length) //接收数据, 源地址保存在*addr ssize_t recvfrom (int socket, void *buffer, size t size, int flags, struct sockaddr *addr, socklen t *length-ptr) 举例: local+datagram 运行在同一个主机上, 进程间通信server #include #include #include #include #include #define SERVER \"/tmp/serversocket\" #define MAXMSG 512 int main(void) { int sock; char message[MAXMSG]; struct sockaddr_un name; size_t size; int nbytes; /* Remove the filename first, it’s ok if the call fails */ unlink(SERVER); /* Make the socket, then loop endlessly. */ sock = make_named_socket(SERVER); while (1) { /* Wait for a datagram. */ size = sizeof(name); nbytes = recvfrom(sock, message, MAXMSG, 0, (struct sockaddr *) & name, &size); if (nbytes client #include #include #include #include #include #include #define SERVER \"/tmp/serversocket\" #define CLIENT \"/tmp/mysocket\" #define MAXMSG 512 #define MESSAGE \"Yow!!! Are we having fun yet?!?\" int main(void) { extern int make_named_socket(const char *name); int sock; char message[MAXMSG]; struct sockaddr_un name; size_t size; int nbytes; /* Make the socket. */ sock = make_named_socket(CLIENT); /* Initialize the server socket address. */ name.sun_family = AF_LOCAL; strcpy(name.sun_path, SERVER); size = strlen(name.sun_path) + sizeof(name.sun_family); /* Send the datagram. */ nbytes = sendto(sock, MESSAGE, strlen(MESSAGE) + 1, 0, (struct sockaddr *) & name, size); if (nbytes inetd守护进程 inetd根据配置文件/etc/inetd.conf来监听端口, 发现连接请求后, 就启动一个新的server进程, 运行配置文件制定的程序. 这个程序的标准输入输出已被重定向到socket. 这个配置文件格式: service style protocol wait username program arguments service来源于/etc/services. 比如, ftp stream tcp nowait root /libexec/ftpd ftpd talk dgram udp wait root /libexec/talkd talkd socket配置 int getsockopt (int socket, int level, int optname, void *optval, socklen t *optlen-ptr) int setsockopt (int socket, int level, int optname, const void *optval, socklen t optlen) 其中, level就是SOL_SOCKET 而optname可以是: SO_DEBUG: 打开socket调试信息 SO_REUSEADDR: 使能两个socket同时使用一个端口 SO_KEEPALIVE: 使能周期发消息检测对端是否broken SO_DONTROUTE: 报文不经过通常的routing过程, 直接发送到interface SO_BROADCAST: datagram是否被广播 SO_OOBINLINE: OOB被放到普通数据中 SO_SNDBUF: 设置发送buffer大小 SO_RCVBUF: 设置接收buffer大小 SO_STYLE: 用于获取协议类型 SO_ERROR: 获取socket错误信息 terminal //是否tty int isatty (int filedes) char * ttyname (int filedes) tty的buffer在内核里, 而不是普通IObuffer terminal的模式 #include struct termios tcflag_t c_iflag //输入 tcflag_t c_oflag //输出 tcflag_t c_cflag //控制 tcflag_t c_lflag //local cc_t c_cc[NCCS] 对terminal的改动是对dev来说是全局的 共享同一个terminal的其他进程也会改变. int tcgetattr (int filedes, struct termios *termios-p) int tcsetattr (int filedes, int when, const struct termios *termios-p) 举例: int set_istrip(int desc, int value) { struct termios settings; int result; result = tcgetattr(desc, &settings); if (result 输入输出和控制属性, 见man tcsetattr local属性, ICANON ECHO* ISIG(ctrl+c, ctrl+\\, ctrl+z) //速率 int cfsetspeed (struct termios *termios-p, speed t speed) //raw模式 void cfmakeraw (struct termios *termios-p) //相当于 termios-p->c_iflag &= ~(IGNBRK|BRKINT|PARMRK|ISTRIP |INLCR|IGNCR|ICRNL|IXON); termios-p->c_oflag &= ~OPOST; termios-p->c_lflag &= ~(ECHO|ECHONL|ICANON|ISIG|IEXTEN); termios-p->c_cflag &= ~(CSIZE|PARENB); termios-p->c_cflag |= CS8; 举例: raw模式, 不回显 #include #include #include #include /* Use this variable to remember original terminal attributes. */ struct termios saved_attributes; void reset_input_mode(void) { tcsetattr(STDIN_FILENO, TCSANOW, &saved_attributes); } void set_input_mode(void) { struct termios tattr; char *name; /* Make sure stdin is a terminal. */ if (!isatty(STDIN_FILENO)) { fprintf(stderr, \"Not a terminal.\\n\"); exit(EXIT_FAILURE); } /* Save the terminal attributes so we can restore them later. */ tcgetattr(STDIN_FILENO, &saved_attributes); atexit(reset_input_mode); /* Set the funny terminal modes. */ tcgetattr(STDIN_FILENO, &tattr); tattr.c_lflag &= ~(ICANON | ECHO); /* Clear ICANON and ECHO. */ tattr.c_cc[VMIN] = 1; tattr.c_cc[VTIME] = 0; tcsetattr(STDIN_FILENO, TCSAFLUSH, &tattr); } main(void) { char c; set_input_mode(); while (1) { read(STDIN_FILENO, &c, 1); if (c == ’\\004’) /* C-d */ break; else putchar(c); } return EXIT_SUCCESS; } 虚拟终端, master side slave side, 一对一 #include //获取一个新的pty(master), 也可以直接open(\"/dev/ptmx\",O_RDWR), 每次open都会返回一个独立的PTM, 并且在/dev/pts里生成一个相应的pys int getpt (void) //设置slave pty的权限 int grantpt (int filedes) //在open slave之前必须先unlock int unlockpt (int filedes) //返回与master相关的slave的pts的文件名 char * ptsname (int filedes) 举例: int open_pty_pair(int *amaster, int *aslave) { int master, slave; char *name; master = getpt(); if (master static int ptym_open (int *p_master, int *p_aux, char *p_slave_name) { char *ptsnam; *p_master=open(\"/dev/ptmx\",O_RDWR); if(*p_master BSD方式打开pty #include int openpty (int *amaster, int *aslave, char *name, const struct termios *termp, const struct winsize *winp) /*fork一个子进程, 使用刚open的slave pts作为子进程的控制终端; **和fork类似, 返回0表示在子进程; 在父进程返回fork后的进程号. -1为错误 */ int forkpty (int *amaster, char *name, const struct termios *termp, const struct winsize *winp) syslog syslogd进程会处理syslog, 与之对应的是unix socket文件/dev/log, 配置文件是/etc/syslog.conf log的处理方式: 控制台 给某人发邮件 写到log文件 传递给另外的进程 丢弃syslogd会处理 网络log. 通过监听syslog UDP port 内核消息, Klogd传递给syslogd. 本地log. 通过/dev/logsyslogd的两个参数, 用一个数字来同时表示facility/priority facility: mail subsystem, FTP server, priority: debug, informational, warning, criticalsyslog接口 #include //open, 选项有LOG_PERROR LOG_CONS LOG_PID, facility默认是LOG_USER void openlog (const char *ident, int option, int facility) //close void closelog (void) /*log接口, 可以不openlog 直接调用 **第一个参数是个参数对, 用宏LOG_MAKEPRI(LOG_USER, LOG_WARNING)可以生成 */ void syslog (int facility_priority, const char *format, . . . ) facility有: LOG_USER LOG_MAIL LOG_DAEMON LOG_FTP 等等 priority有: 如果只填priority, 则用默认的facility LOG_EMERG LOG_ALERT LOG_CRIT LOG_ERR LOG_WARNING LOG_NOTICE LOG_INFO LOG_DEBUG 举例:#include syslog (LOG_MAKEPRI(LOG_LOCAL1, LOG_ERROR), \"Unable to make network connection to %s. Error=%m\", host); 再举例:用setlogmask()过滤掉debug和info, 这些log不会真正到syslog#include setlogmask (LOG_UPTO (LOG_NOTICE)); openlog (\"exampleprog\", LOG_CONS | LOG_PID | LOG_NDELAY, LOG_LOCAL1); syslog (LOG_NOTICE, \"Program started by User %d\", getuid ()); syslog (LOG_INFO, \"A tree falls in a forest\"); closelog (); 数学 //伪随机数 #include int rand (void) void srand (unsigned int seed) 日期与时间 #include //间隔 double difftime (time t time1, time t time0) //进程独占的时间, 不包括阻塞时间 #include clock_t clock (void) 举例: clock_t start, end; double cpu_time_used; start = clock(); ... /* Do the work. */ end = clock(); cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC; //进程的处理器时间 #include clock_t times (struct tms *buffer) //用到的结构体 struct tms //单位是clock clock_t tms_utime //进程用户态处理器时间 clock_t tms_stime //进程内核态处理器时间 clock_t tms_cutime //进程和子进程的用户态时间, 不包括还没结束的子进程 clock_t tms_cstime //进程和子进程的内核态时间, 不包括还没结束的子进程 struct timeval long int tv_sec long int tv_usec time_t //其实就是long int, 从1970年开始算起的seconds //返回简单日历时间, 可以传入NULL time_t time (time t *result) //设置时间, 推荐用settimeofday int stime (const time t *newtime) //精确到微妙的日历时间, 从epoch(1970)算起 int gettimeofday (struct timeval *tp, struct timezone *tzp) int settimeofday (const struct timeval *tp, const struct timezone *tzp) //平滑的调整时间 int adjtime (const struct timeval *delta, struct timeval *olddelta) //年月日小时分秒时间(broken-down time) struct tm int tm_sec int tm_min int tm_hour int tm_mday int tm_mon int tm_year int tm_wday //0-6 int tm_yday //0-365 const char *tm_zone struct tm * localtime (const time t *time) //UTC/GMT时间 struct tm * gmtime (const time t *time) //从broken-down时间到简单时间 time_t mktime (struct tm *brokentime) //转换为时间字符串 char * asctime (const struct tm *brokentime) char * ctime (const time t *time) 格式化时间字符串 //从时间到字符串 size_t strftime (char *s, size t size, const char *template, const struct tm *brokentime) //从字符串到时间 char * strptime (const char *s, const char *fmt, struct tm *tp) 举例: #include #include #define SIZE 256 int main(void) { char buffer[SIZE]; time_t curtime; struct tm *loctime; /* Get the current time. */ curtime = time(NULL); /* Convert it to local time representation. */ loctime = localtime(&curtime); /* Print out the date and time in the standard format. */ fputs(asctime(loctime), stdout); /* Print it out in a nice format. */ strftime(buffer, SIZE, \"Today is %A, %B %d.\\n\", loctime); fputs(buffer, stdout); strftime(buffer, SIZE, \"The time is %I:%M %p.\\n\", loctime); fputs(buffer, stdout); return 0; } 定时器 struct itimerval struct timeval it_interval //周期 struct timeval it_value //单次 /*which 对应三种:ITIMER_REAL, ITIMER_VIRTUAL, or ITIMER_PROF **分别对应三种信号: SIGALRM, SIGVTALRM, SIGPROF **在用sigaction注册信号处理函数时, SA_RESTART被用来表示被打断的系统调用自动重试. **如果希望定时器可以打断阻塞中的系统调用, 则不要设置SA_RESTART标记 */ int setitimer (int which, const struct itimerval *new, struct itimerval *old) int getitimer (int which, struct itimerval *old) //alarm是简化版的ITIMER_REAL, 传入0表示取消这个定时器 unsigned int alarm (unsigned int seconds) alarm相当于: unsigned int alarm(unsigned int seconds) { struct itimerval old, new; new.it_interval.tv_usec = 0; new.it_interval.tv_sec = 0; new.it_value.tv_usec = 0; new.it_value.tv_sec = (long int) seconds; if (setitimer(ITIMER_REAL, &new, &old) sleep #include unsigned int sleep (unsigned int seconds) 注意sleep可以被信号打断而提前返回, 如果不想被打断, 可以用不包含任何fd的select(), 传入超时时间. 在GNU系统中, sleep和SIGALRM可以同时存在, 因为sleep不使用SIGALRM. 纳秒睡眠 #include int nanosleep (const struct timespec *requested_time, struct timespec *remaining) 系统资源 资源统计 #include int getrusage (int processes, struct rusage *rusage) struct rusage struct timeval ru_utime //用户态时间 struct timeval ru_stime //内核态时间 long int ru_maxrss //最大run time内存, kb long int ru_ixrss //和其他进程共享的代码段, kb long int ru_idrss //不共享的数据段, kb long int ru_isrss //不共享的栈空间, kb long int ru_minflt //非io页fault long int ru_majflt //io页fault long int ru_nswap //被换出内存的时间 long int ru_inblock //读硬盘次数 long int ru_oublock //写硬盘次数 long int ru_msgsnd //ipc发送次数 long int ru_msgrcv //ipc接收次数 long int ru_nsignals //被signal的次数 long int ru_nvcsw //进程主动让出cpu次数 long int ru_nivcs //进程被动让出cpu次数 限制, 两种 current limit: 进程可自己修改的limit maximum limit: 只有超级用户才能改的limitint getrlimit (int resource, struct rlimit *rlp) int setrlimit (int resource, const struct rlimit *rlp) ulimit #include long int ulimit (int cmd, . . . ) 实时进程优先级 实时进程0-99, 对同一个优先级的几个实时进程的调度分为: FIFO: 先到先执行, 必须主动让出CPU(sched_yield) RR: round robin, 同级时间片轮转. 这里的时间片不是从父进程继承而来, 在linux下面, 比普通进程的时间片小得多(150ms)进程调度 #include //SCHED_OTHER SCHED_FIFO SCHED_RR int sched_setscheduler (pid t pid, int policy, const struct sched param *param) int sched_getscheduler (pid t pid) /*我估计这个文档里的0优先级是普通优先级(100-139), 而大于0的优先级都是实时进程的优先级(0-99) **后面提到的动态优先级是对这里的0号优先级说的 */ //设置绝对优先级 int sched_setparam (pid t pid, const struct sched param *param) //获取绝对优先级 int sched_getparam (pid t pid, struct sched param *param) //最小 最大的允许优先级 int sched_get_priority_min (int policy) int sched_get_priority_max (int policy) //RR进程时间片 int sched_rr_get_interval (pid t pid, struct timespec *interval) //主动让出cpu int sched_yield (void) nice值 +19: 优先级最低, 时间片大约10ms -20: 优先级最高, 时间片大约400ms 一般进程能够提高nice, 但不能降低nice#include //获取nice值, class = PRIO_PROCESS, PRIO_PGRP, PRIO_USER(同一user下的所有进程) int getpriority (int class, int id) //设置nice值 int setpriority (int class, int id, int niceval) //提高nice int nice (int increment) //相当于: int nice (int increment) { int result, old = getpriority (PRIO_PROCESS, 0); result = setpriority (PRIO_PROCESS, 0, old + increment); if (result != -1) return old + increment; else return -1; } 绑定cpus #include 结构体cpu_set_t, 表示cpuset void CPU_ZERO (cpu set t *set) void CPU_SET (int cpu, cpu set t *set) void CPU_CLR (int cpu, cpu set t *set) int CPU_ISSET (int cpu, const cpu set t *set) //获取进程cpu mask int sched_getaffinity (pid t pid, size t cpusetsize, cpu set t *cpuset) //设置 int sched_setaffinity (pid t pid, size t cpusetsize, const cpu set t *cpuset) 可用内存页数目 #include //获取当前进程page size int getpagesize (void) //获取系统物理页数, 但这些页不一定全能用 sysconf (_SC_PHYS_PAGES) //获取当前进程当时可用的物理页, 不会影响其他进程 sysconf (_SC_AVPHYS_PAGES) //也可用GNU的函数 #include long int get_phys_pages (void) long int get_avphys_pages (void) cpu个数 //全部的cpu个数 sysconf (_SC_NPROCESSORS_CONF) //online的cpu个数, 即可用的cpu个数 sysconf (_SC_NPROCESSORS_ONLN) //也可用GNU的函数 #include int get_nprocs_conf (void) int get_nprocs (void) 当前系统负载, 1 5 15分钟平均负载 #include int getloadavg (double loadavg[], int nelem) "},"notes/system_libc_part3.html":{"url":"notes/system_libc_part3.html","title":"libc概览3","keywords":"","body":" 非本地跳转 默认不改变signal 完全的上下文控制 signal 程序错误类signal, 默认的处理是终止进程 结束类signal 定时类signal 异步signal, 默认忽略 任务控制类signal 操作error signal, 默认是终止进程 其他signal 打印signal类型 signal处理 sigaction signal的flag signal的初始状态 signal处理函数 signal handler被打断? signal合并 signal的处理原则 库函数被打断? 触发signal 向其他进程发signal kill用于进程间同步 signal的阻塞 正在pending的signal(已到达, 未处理) 等待signal signal的栈 基础系统接口 参数解析之getopt_long 参数解析之argp_parse 子选项支持 环境变量 syscall 终结程序 进程 fork exec 等待子进程终止 子进程退出状态 job控制 获得控制终端名字(和ttyname有什么区别?) 进程组api 一个简单的shell 用户和组 api 系统管理 调试 调用栈 pthread gnu扩展 libc的系统桩函数(略) libc基础函数 非本地跳转 #include //结构体, 用来保存上下文 jmp_buf /*返回0表示设置jmp_buf成功 **非零值表示有一次非本地跳转, 值是longjmp给出的 **实际上, segjump是个宏 */ int setjmp (jmp buf state) void longjmp (jmp buf state, int value) 举例: #include #include #include jmp_buf main_loop; void abort_to_main_loop(int status) { longjmp(main_loop, status); } int main(void) { while (1) if (setjmp(main_loop)) puts(\"Back at main loop....\"); else do_command(); } void do_command(void) { char buffer[128]; if (fgets(buffer, 128, stdin) == NULL) abort_to_main_loop(-1); else exit(EXIT_SUCCESS); } 默认不改变signal 但也可以保存当时的signal, 非本地跳转时恢复 #include //结构体 sigjmp_buf //savesigs是要保存的当时的signal set int sigsetjmp (sigjmp buf state, int savesigs) //signal在longjump时会恢复 void siglongjmp (sigjmp buf state, int value) 完全的上下文控制 #include //比jmp_buf信息更多, 开销也更大 ucontext_t ucontext_t *uc_link sigset_t uc_sigmask stack_t uc_stack mcontext_t uc_mcontext //初始化上下文ucp int getcontext (ucontext t *ucp) //修改上下文 void makecontext (ucontext t *ucp, void (*func) (void), int argc, . . . ) //恢复上下文 int setcontext (const ucontext t *ucp) //保存现在的上下文到oucp, 并切换到ucp int swapcontext (ucontext t *restrict oucp, const ucontext t *restrict ucp) 举例:场景是运行一个很复杂的搜索函数fn, 这个函数复杂到不能很简单的返回 int random_search(int n, int (*fp)(int, ucontext_t *)) { volatile int cnt = 0; ucontext_t uc; /* Safe current context. */ if (getcontext(&uc) 举例: #include #include #include #include #include /* Set by the signal handler. */ static volatile int expired; /* The contexts. */ static ucontext_t uc[3]; /* We do only a certain number of switches. */ static int switches; /* This is the function doing the work. It is just a skeleton, real code has to be filled in. */ static void f(int n) { int m = 0; while (1) { /* This is where the work would be done. */ if (++m % 100 == 0) { putchar(’.’); fflush(stdout); } /* Regularly the expire variable must be checked. */ if (expired) { /* We do not want the program to run forever. */ if (++switches == 20) return; printf(\"\\nswitching from %d to %d\\n\", n, 3 - n); expired = 0; /* Switch to the other context, saving the current one. */ swapcontext(&uc[n], &uc[3 - n]); } } } /* This is the signal handler which simply set the variable. */ void handler(int signal) { expired = 1; } int main(void) { struct sigaction sa; struct itimerval it; char st1[8192]; char st2[8192]; /* Initialize the data structures for the interval timer. */ sa.sa_flags = SA_RESTART; sigfillset(&sa.sa_mask); sa.sa_handler = handler; it.it_interval.tv_sec = 0; it.it_interval.tv_usec = 1; it.it_value = it.it_interval; /* Install the timer and get the context we can manipulate. */ if (sigaction(SIGPROF, &sa, NULL) signal 由程序自身引起的error触发的signal一般都是同步的 外部触发的signal一般都是异步的. 如果一个signal正在被处理, 那处理期间这个信号是阻塞的. 如果两个不同的signal间隔很短, one handler can run within another程序错误类signal, 默认的处理是终止进程 SIGFPE: 运算错误, 包括除零和浮点异常等所有的运算错误 SIGILL: 非法指令, 可能是特权指令或根本不是条CPU指令 SIGSEGV: 著名的段错误. 访问非法数据 SIGBUS: bus error. 可能是访问了不存在的地址, 或地址不对齐 SIGABRT: 程序自己调用了abort() SIGTRAP: 断点, 用于debugger SIGSYS: 非法系统调用, 可能不存在这个调用号 SIGEMT: 虚拟指令错误?结束类signal SIGTERM: 礼貌的让一个进程终止. 和SIGKILL不同, SIGTERM可以被block, handle和ignore. 注: shell下的kill命令默认是发SIGTERM信号 SIGINT: ctrl+C, 用户中断程序的执行 SIGQUIT: ctrl+\\, 会生成coredump. 推荐收到这个信号的时候不清理现场, 以便定位. SIGKILL: 马上终止进程. 不能被处理或忽略, 也不会阻塞 SIGHUP: 一般表示用户中断了一个会话定时类signal SIGALRM: 定时器时间到. 比如alarm SIGVTALRM: 虚拟定时器 SIGPROF: 性能分析异步signal, 默认忽略 SIGIO: 异步io, 需要先用fcntl()来使能异步模式 SIGURG: socket的urgent事件 SIGPOLL: 类似SIGIO任务控制类signal SIGCHLD SIGCLD: 子进程终止或停止时发送给父进程, 默认忽略 SIGSTOP: 停止一个进程. 注意不是终止. 不能阻塞, 处理和忽略 SIGTSTP: crtl+Z, 与SIGSTOP不同的是, 它可以处理或忽略. SIGTTIN: 处于后台运行的程序试图从terminal获取输入, 默认停止进程.操作error signal, 默认是终止进程 SIGPIPE: 管道error. 比如在读open之前写pipe, 或在connect之前写socket SIGLOST: 服务进程意外终止 SIGXCPU: 进程超出资源限制. 见系统资源 SIGXFSZ: 文件大小超出限制其他signal SIGUSR1 SIGUSR2: 常用于简单进程间通信, 默认终止进程 SIGWINCH: 窗口大小改变. 默认ignore SIGINFO: ??打印signal类型 #include char * strsignal (int signum) #include void psignal (int signum, const char *message) signal处理 #include //sighandler_t void handler (int signum) { ... } /*注册handler, action可以是个handler函数, 也可以是SIG_DFL, SIG_IGN **注意不同的系统有两种模式: BSD模式, SVID模式 **SVID系统下面, handler只运行一次, 然后handler会被deinstall **BSD系统下面, handler一直有效 **GNU的libc是BSD模式 */ sighandler_t signal (int signum, sighandler t action) 举例: 如果默认是ignore, 则不改变它的行为 #include void termination_handler(int signum) { struct temp_file *p; for (p = temp_file_list; p; p = p->next) unlink(p->name); } int main(void) { ... if (signal(SIGINT, termination_handler) == SIG_IGN) signal(SIGINT, SIG_IGN); if (signal(SIGHUP, termination_handler) == SIG_IGN) signal(SIGHUP, SIG_IGN); if (signal(SIGTERM, termination_handler) == SIG_IGN) signal(SIGTERM, SIG_IGN); ... } sigaction struct sigaction sighandler_t sa_handler sigset_t sa_mask //当sa_handler执行时, 需要阻塞的signal set int sa_flags //推荐用sigaction, action为NULL表示查询 int sigaction (int signum, const struct sigaction *restrict action, struct sigaction *restrict old-action) 举例: #include void termination_handler(int signum) { struct temp_file *p; for (p = temp_file_list; p; p = p->next) unlink(p->name); } int main(void) { ... struct sigaction new_action, old_action; /* Set up the structure to specify the new action. */ new_action.sa_handler = termination_handler; sigemptyset(&new_action.sa_mask); new_action.sa_flags = 0; sigaction(SIGINT, NULL, &old_action); if (old_action.sa_handler != SIG_IGN) sigaction(SIGINT, &new_action, NULL); sigaction(SIGHUP, NULL, &old_action); if (old_action.sa_handler != SIG_IGN) sigaction(SIGHUP, &new_action, NULL); sigaction(SIGTERM, NULL, &old_action); if (old_action.sa_handler != SIG_IGN) sigaction(SIGTERM, &new_action, NULL); ... } signal的flag SA_NOCLDSTOP: 只对SIGCHLD有效. 只报告子进程终止 SA_ONSTACK: 不使用signal栈. SA_RESTART: 被signal打断的库函数会resume, 否则直接返回失败, 此时error是EINTR.signal的初始状态 一般都是从父进程继承的, 但如果子进程用exec*来执行其他程序, 则signal会被初始化为默认状态. 举例: 在signal不是被ignore的时候才用sigaction. 是否因为一旦signal被ignore过, 那么即使重新调用sigaction也不会生效?--经过验证, 不是struct sigaction temp; sigaction (SIGHUP, NULL, &temp); if (temp.sa_handler != SIG_IGN) { temp.sa_handler = handle_sighup; sigemptyset (&temp.sa_mask); sigaction (SIGHUP, &temp, NULL); } signal处理函数 handler可以修改全局变量, 这个变量应该是sig_atomic_t 类型 handler可以return handler可以终止程序 handler可以非本地跳转 举例: return的情况 #include #include #include /* This flag controls termination of the main loop. */ volatile sig_atomic_t keep_going = 1; /* The signal handler just clears the flag and re-enables itself. */ void catch_alarm(int sig) { keep_going = 0; signal(sig, catch_alarm); } void do_stuff(void) { puts(\"Doing stuff while waiting for alarm....\"); } int main(void) { /* Establish a handler for SIGALRM signals. */ signal(SIGALRM, catch_alarm); /* Set an alarm to go off in a little while. */ alarm(2); /* Check the flag once in a while to see when to quit. */ while (keep_going) do_stuff(); return EXIT_SUCCESS; } 举例: 终止的情况 volatile sig_atomic_t fatal_error_in_progress = 0; void fatal_error_signal(int sig) { /* Since this handler is established for more than one kind of signal, it might still get invoked recursively by delivery of some other kind of signal. Use a static variable to keep track of that. */ if (fatal_error_in_progress) raise(sig); fatal_error_in_progress = 1; /* Now do the clean up actions: - reset terminal modes - kill child processes - remove lock files */ ... /* Now reraise the signal. We reactivate the signal’s default handling, which is to terminate the process. We could just call exit or abort, but reraising the signal sets the return status from the process correctly. */ signal(sig, SIG_DFL); raise(sig); } 举例: 非本地跳转的情况. 注意由于一些重要的结构体或变量可能还没有修改完毕就被signal打断, 此时如果进行非本地跳转的话, 那些重要结构可能不完整, 这种情况有两种办法可以避免: 主程序修改重要变量时阻塞signal 在signal handler里面重新初始化这些重要变量 #include #include jmp_buf return_to_top_level; volatile sig_atomic_t waiting_for_input; void handle_sigint(int signum) { /* We may have been waiting for input when the signal arrived, but we are no longer waiting once we transfer control. */ waiting_for_input = 0; longjmp(return_to_top_level, 1); } int main(void) { ... signal(SIGINT, sigint_handler); ... while (1) { prepare_for_command(); if (setjmp(return_to_top_level) == 0) read_and_execute_command(); } } /* Imagine this is a subroutine used by various commands. */ char * read_data() { if (input_from_terminal) { waiting_for_input = 1; ... waiting_for_input = 0; } else { ... } } signal handler被打断? 正在运行的handler会自动block它的signum, 但是其他种类的signal依然可以打断当前的handler. 这也是为什么sigaction有个sa_mask的原因, 被mask的signal会被阻塞. sigprocmask函数可以临时改变这个mask, 但只在handler里面生效. 因为这个handler return以后, mask会恢复. signal合并 同类signal在没处理之前会被合并为一个, 所以对signal的计数是不准的. 尤其注意对SIGCHLD是如此. 举例: struct process { struct process *next; /* The process ID of this child. */ int pid; /* The descriptor of the pipe or pseudo terminal on which output comes from this child. */ int input_descriptor; /* Nonzero if this process has stopped or terminated. */ sig_atomic_t have_status; /* The status of this child; 0 if running, otherwise a status value from waitpid. */ int status; }; struct process *process_list; //This example also uses a flag to indicate whether signals have arrived since some time //in the past—whenever the program last cleared it to zero. /* Nonzero means some child’s status has changed so look at process_list for the details. */ int process_status_change; Here is the handler itself: void sigchld_handler(int signo) { int old_errno = errno; while (1) { register int pid; int w; struct process *p; /* Keep asking for a status until we get a definitive result. */ do { errno = 0; pid = waitpid(WAIT_ANY, &w, WNOHANG | WUNTRACED); } while (pid next) if (p->pid == pid) { p->status = w; /* Indicate that the status field has data to look at. We do this only after storing it. */ p->have_status = 1; /* If process has terminated, stop waiting for its output. */ if (WIFSIGNALED(w) || WIFEXITED(w)) if (p->input_descriptor) FD_CLR(p->input_descriptor, &input_wait_mask); /* The program should check this flag from time to time to see if there is any news in process_list. */ ++process_status_change; } /* Loop around to handle all the processes that have something to tell us. */ } } //Here is the proper way to check the flag process_status_change: if (process_status_change) { struct process *p; process_status_change = 0; for (p = process_list; p; p = p->next) if (p->have_status) { ... Examine p->status ... } } 举例: 主程序怎么知道handler调用过呢? 这个例子是有个变量, 只在handler里面改. 主程序查询这个变量 sig_atomic_t process_status_change; sig_atomic_t last_process_status_change; ... { sig_atomic_t prev = last_process_status_change; last_process_status_change = process_status_change; if (last_process_status_change != prev) { struct process *p; for (p = process_list; p; p = p->next) if (p->have_status) { ... Examine p->status ... } } } signal的处理原则 时刻记住handler的处理是异步的, 可能在任何时间点打断主程序, 包括关键数据正在改写过程中, 或不可重入的函数正在执行中等等, 甚至一个整形变量正在被赋值. 最好的情况下, handler只是对一个变量赋值, 真正的事情, 需要等到主程序检测到这个变量变化以后再处理. 需要注意的是, 一个不可重入的函数, 也不是一定不能在handler里面使用, 要使用它需要保证一个前提: 保证主程序用这个函数的时候不会被handler打断, 怎么保证呢? 主程序根本不会调这个函数, 或类似的函数 主程序调这个函数时显式阻塞影响它的signal 下面是handler的一些基本原则: handler需要访问的全局变量声明成volatile, 和sig_atomic_t(其实就是int, 因为int的读写是原子的)? 比如gethostbyname这个函数, 会返回一个静态变量, 这静态变量可能下次调用gethostbyname就变了. 此时假设主程序正在调用gethostbyname还没返回, 被signal打断了, handler里面也调了gethostbyname, 那么主程序的返回值就不对了. 比如printf, 其内部使用同一个数据结构, 在主程序和handler里面同时调用会互相干扰. malloc和free也不可重入, 在handler使用的内存可以预先在主程序申请好. errno也不可重入, 但是可以在handler开始先保存errno, 最后恢复它. 其实这招也很通用, 如果主程序和handler要修改同一个变量, 在handler里面先保存再恢复. 错误举例: 这个例子里面, 主程序不断的给memory赋值, handler里面可能会打出0,1或1,0. #include #include volatile struct two_words { int a, b; } memory; void handler(int signum) { //printf is safe here because main not use it printf(\"%d,%d\\n\", memory.a, memory.b); alarm(1); } int main(void) { static struct two_words zeros = { 0, 0 }, ones = { 1, 1 }; signal(SIGALRM, handler); memory = zeros; alarm(1); while (1) { memory = zeros; memory = ones; } } 库函数被打断? 库函数返回EINTR表示被打断, 可以用sigaction的SA_RESTART来设置自动重试. 也可以TEMP_FAILURE_RETRY (expression)重试 read和write不受SA_RESTART控制, 因为可能有部分的数据已经传送完毕了. 这种情况通过返回值来表示已经传送的字节数, 表示\"部分成功\"触发signal //向自己发signal int raise (int signum) 典型的应用是在handler里面再次触发signal:#include /* When a stop signal arrives, set the action back to the default and then resend the signal after doing cleanup actions. */ void tstp_handler(int sig) { signal(SIGTSTP, SIG_DFL); /* Do cleanup actions here. */ ... raise(SIGTSTP); } /* When the process is continued again, restore the signal handler. */ void cont_handler(int sig) { signal(SIGCONT, cont_handler); signal(SIGTSTP, tstp_handler); } /* Enable both handlers during program initialization. */ int main(void) { signal(SIGCONT, cont_handler); signal(SIGTSTP, tstp_handler); ... } 向其他进程发signal /* pid > 0 正常的pid ** pid == 0 同一个进程组的所有进程 ** pid kill用于进程间同步 #include #include #include #include /* When a SIGUSR1 signal arrives, set this variable. */ volatile sig_atomic_t usr_interrupt = 0; void synch_signal(int sig) { usr_interrupt = 1; } /* The child process executes this function. */ void child_function(void) { /* Perform initialization. */ printf(\"I'm herehistory! My pid is %d.\\n\", (int) getpid()); /* Let parent know you’re done. */ kill(getppid(), SIGUSR1); /* Continue with execution. */ puts(\"Bye, now....\"); exit(0); } int main(void) { struct sigaction usr_action; sigset_t block_mask; pid_t child_id; /* Establish the signal handler. */ sigfillset(&block_mask); usr_action.sa_handler = synch_signal; usr_action.sa_mask = block_mask; usr_action.sa_flags = 0; sigaction(SIGUSR1, &usr_action, NULL); /* Create the child process. */ child_id = fork(); if (child_id == 0) child_function(); /* Does not return. */ /* Busy wait for the child to send a signal. */ while (!usr_interrupt) ; /* Now continue execution. */ puts(\"That’s all, folks!\"); return 0; } signal的阻塞 用sigprocmask可以显式的阻塞signal, 用于保护敏感变量 sigaction的sa_mask可以设置在运行handler时mask掉相应的signal, 但hander完成后会恢复原来的mask 这里需要用到sigset_t //初始化为全0的set int sigemptyset (sigset t *set) //初始化为全1的set int sigfillset (sigset t *set) //添加 int sigaddset (sigset t *set, int signum) //删除 int sigdelset (sigset t *set, int signum) //判断 int sigismember (const sigset t *set, int signum) //在多线程模型里, 每个线程都有自己的signal mask, 而sigprocmask的行为是不确定的. 此时要用pthread_sigmask int sigprocmask (int how, const sigset t *restrict set, sigset t *restrict oldset) 举例: ```c /* This variable is set by the SIGALRM signal handler. */ volatile sig_atomic_t flag = 0; int main(void) { sigset_t block_alarm; ... /* Initialize the signal mask. */ sigemptyset(&block_alarm); sigaddset(&block_alarm, SIGALRM); while (1) { /* Check if a signal has arrived; if so, reset the flag. */ sigprocmask(SIG_BLOCK, &block_alarm, NULL); if (flag) { actions - if - not - arrived flag = 0; } sigprocmask(SIG_UNBLOCK, &block_alarm, NULL); ... } } 举例: 保护handler不被其他signal打断 #include #include void catch_stop(); void install_handler(void) { struct sigaction setup_action; sigset_t block_mask; sigemptyset(&block_mask); /* Block other terminal-generated signals while handler runs. */ sigaddset(&block_mask, SIGINT); sigaddset(&block_mask, SIGQUIT); setup_action.sa_handler = catch_stop; setup_action.sa_mask = block_mask; setup_action.sa_flags = 0; sigaction(SIGTSTP, &setup_action, NULL); } 正在pending的signal(已到达, 未处理) //一般用不到这函数, 除非是在sigprocmask或sa_mask保护的代码里 int sigpending (sigset t *set) 等待signal #include //阻塞进程直到signal执行 int pause (void) 需要注意: ```c /* usr_interrupt is set by the signal handler. */ if (!usr_interrupt) //如果在这之间signal来了, 并让usr_interrupt=1, 那么这个signal的实际工作不会得到处理. pause (); /* Do work once the signal arrives. */ ... 可以改成这样: 不用pause() /* usr_interrupt is set by the signal handler. while (!usr_interrupt) sleep (1); /* Do work once the signal arrives. */ ... 另外一种方法: //阻塞进程, block set里面的signal. 这个函数一直阻塞, 直到非set成员的signal的handler返回以后才返回. int sigsuspend (const sigset t *set) signal的栈 signal的栈在特殊的地方(不是进程栈吗?--默认是用进程栈), 可以用sigaltstack设定. stack_t void *ss_sp size_t ss_size int ss_flags SS_DISABLE: 不使用signal栈 SS_ONSTACK: 为1表示正在用signal栈. 为0表示用的是进程的栈. 由OS设置 /* 用这个函数在板子上测试, 结果为 ** ss_sp:(nil), ss_size:0, ss_flags:0x2(SS_DISABLE) ** 表示不用signal的栈, 使用进程栈 */ int sigaltstack (const stack t *restrict stack, stack t *restrict oldstack) 基础系统接口 //main int main (int argc, char *argv[]) //长参数形式: --name=value ## 参数解析之getopt ```c #include /* 会用到几个变量 ** int opterr; int optopt; int optind; char * optarg; ** : 表示需要参数 :: 表示参数可选 ** 在循环里调用 */ int getopt (int argc, char *const *argv, const char *options) 举例: #include #include #include #include int main(int argc, char **argv) { int aflag = 0; int bflag = 0; char *cvalue = NULL; int index; int c; opterr = 0; while ((c = getopt(argc, argv, \"abc:\")) != -1) switch (c) { case ’a’: aflag = 1; break; case ’b’: bflag = 1; break; case ’c’: cvalue = optarg; break; case ’?’ : if (optopt == ’c’) fprintf(stderr, \"Option -%c requires an argument.\\n\", optopt); else if (isprint(optopt)) fprintf(stderr, \"Unknown option ‘-%c’.\\n\", optopt); else fprintf(stderr, \"Unknown option character ‘\\x%x’.\\n\", optopt); return 1; default: abort(); } printf(\"aflag = %d, bflag = %d, cvalue = %s\\n\", aflag, bflag, cvalue); for (index = optind; index 结果: % testopt aflag = 0, bflag = 0, cvalue = (null) % testopt -a -b aflag = 1, bflag = 1, cvalue = (null) % testopt -ab aflag = 1, bflag = 1, cvalue = (null) % testopt -c foo aflag = 0, bflag = 0, cvalue = foo % testopt -cfoo aflag = 0, bflag = 0, cvalue = foo % testopt arg1 aflag = 0, bflag = 0, cvalue = (null) Non-option argument arg1 % testopt -a arg1 aflag = 1, bflag = 0, cvalue = (null) Non-option argument arg1 % testopt -c foo arg1 aflag = 0, bflag = 0, cvalue = foo Non-option argument arg1 % testopt -a -- -b aflag = 1, bflag = 0, cvalue = (null) Non-option argument -b % testopt -a -aflag = 1, bflag = 0, cvalue = (null) Non-option argument - 参数解析之getopt_long #include int getopt_long (int argc, char *const *argv, const char *shortopts, const struct option *longopts, int *indexptr) struct option const char *name int has_arg int *flag int val 举例: #include #include #include /* Flag set by ‘--verbose’. */ static int verbose_flag; int main(int argc, char **argv) { int c; while (1) { static struct option long_options[] = { /* These options set a flag. */ {\"verbose\", no_argument, &verbose_flag, 1}, {\"brief\", no_argument, &verbose_flag, 0}, /* These options don’t set a flag. We distinguish them by their indices. */ {\"add\", no_argument, 0, ’a’}, {\"append\", no_argument, 0, ’b’}, {\"delete\", required_argument, 0, ’d’}, {\"create\", required_argument, 0, ’c’}, {\"file\", required_argument, 0, ’f’}, {0, 0, 0, 0} }; /* getopt_long stores the option index here. */ int option_index = 0; c = getopt_long(argc, argv, \"abc:d:f:\", long_options, &option_index); /* Detect the end of the options. */ if (c == -1) break; switch (c) { case 0: /* If this option set a flag, do nothing else now. */ if (long_options[option_index].flag != 0) break; printf(\"option %s\", long_options[option_index].name); if (optarg) printf(\" with arg %s\", optarg); printf(\"\\n\"); break; case ’a’: puts(\"option -a\\n\"); break; case ’b’: puts(\"option -b\\n\"); break; case ’c’: printf(\"option -c with value ‘%s’\\n\", optarg); break; case ’d’: printf(\"option -d with value ‘%s’\\n\", optarg); break; case ’f’: printf(\"option -f with value ‘%s’\\n\", optarg); break; case ’?’ : /* getopt_long already printed an error message. */ break; default: abort(); } } /* Instead of reporting ‘--verbose’ and ‘--brief’ as they are encountered, we report the final status resulting from them. */ if (verbose_flag) puts(\"verbose flag is set\"); /* Print any remaining command line arguments (not options). */ if (optind 参数解析之argp_parse //好像很复杂 error_t argp_parse (const struct argp *argp, int argc, char **argv, unsigned flags, int *arg_index, void *input) 举例:最简单 /* This is (probably) the smallest possible program that uses argp. It won’t do much except give an error messages and exit when there are any arguments, and print a (rather pointless) messages for –help. */ #include #include int main(int argc, char **argv) { argp_parse(0, argc, argv, 0, 0, 0); exit(0); } 举例: 使用默认选项的例子 /* This program doesn’t use any options or arguments, but uses argp to be compliant with the GNU standard command line format. In addition to making sure no arguments are given, and implementing a –help option, this example will have a –version option, and will put the given documentation string and bug address in the –help output, as per GNU standards. The variable ARGP contains the argument parser specification; adding fields to this structure is the way most parameters are passed to argp parse (the first three fields are usually used, but not in this small program). There are also two global variables that argp knows about defined here, ARGP PROGRAM VERSION and ARGP PROGRAM BUG ADDRESS (they are global variables because they will almost always be constant for a given program, even if it uses different argument parsers for various tasks). */ #include #include const char *argp_program_version = \"argp-ex2 1.0\"; const char *argp_program_bug_address = \"\"; /* Program documentation. */ static char doc[] = \"Argp example #2 -- a pretty minimal program using argp\"; /* Our argument parser. The options, parser, and args_doc fields are zero because we have neither options or arguments; doc and argp_program_bug_address will be used in the output for ‘--help’, and the ‘--version’ option will print out argp_program_version. */ static struct argp argp = { 0, 0, 0, doc }; int main(int argc, char **argv) { argp_parse(&argp, argc, argv, 0, 0, 0); exit(0); } 举例: 增加一些option /* This program uses the same features as example 2, and uses options and arguments. We now use the first four fields in ARGP, so here’s a description of them: OPTIONS – A pointer to a vector of struct argp option (see below) PARSER – A function to parse a single option, called by argp ARGS DOC – A string describing how the non-option arguments should look DOC – A descriptive string about this program; if it contains a vertical tab character (\\v), the part after it will be printed *following* the options The function PARSER takes the following arguments: KEY – An integer specifying which option this is (taken from the KEY field in each struct argp option), or a special key specifying something else; the only special keys we use here are ARGP KEY ARG, meaning a non-option argument, and ARGP KEY END, meaning that all arguments have been parsed ARG – For an option KEY, the string value of its argument, or NULL if it has none STATE– A pointer to a struct argp state, containing various useful information about the parsing state; used here are the INPUT field, which reflects the INPUT argument to argp parse, and the ARG NUM field, which is the number of the current non-option argument being parsed It should return either 0, meaning success, ARGP ERR UNKNOWN, meaning the given KEY wasn’t recognized, or an errno value indicating some other error. Note that in this example, main uses a structure to communicate with the parse opt function, a pointer to which it passes in the INPUT argument to argp parse. Of course, it’s also possible to use global variables instead, but this is somewhat more flexible. The OPTIONS field contains a pointer to a vector of struct argp option’s; that structure has the following fields (if you assign your option structures using array initialization like this example, unspecified fields will be defaulted to 0, and need not be specified): NAME – The name of this option’s long option (may be zero) KEY – The KEY to pass to the PARSER function when parsing this option, *and* the name of this option’s short option, if it is a printable ascii character ARG – The name of this option’s argument, if any FLAGS – Flags describing this option; some of them are: OPTION ARG OPTIONAL – The argument to this option is optional OPTION ALIAS – This option is an alias for the previous option OPTION HIDDEN – Don’t show this option in –help output DOC – A documentation string for this option, shown in –help output An options vector should be terminated by an option with all fields zero. */ #include #include const char *argp_program_version = \"argp-ex3 1.0\"; const char *argp_program_bug_address = \"\"; /* Program documentation. */ static char doc[] = \"Argp example #3 -- a program with options and arguments using argp\"; /* A description of the arguments we accept. */ static char args_doc[] = \"ARG1 ARG2\"; /* The options we understand. */ static struct argp_option options[] = { {\"verbose\", ’v’, 0, 0, \"Produce verbose output\" }, {\"quiet\", ’q’, 0, 0, \"Don’t produce any output\" }, {\"silent\", ’s’, 0, OPTION_ALIAS }, { \"output\", ’o’, \"FILE\", 0, \"Output to FILE instead of standard output\" }, { 0 } }; /* Used by main to communicate with parse_opt. */ struct arguments { char *args[2]; /* arg1 & arg2 */ int silent, verbose; char *output_file; }; /* Parse a single option. */ static error_t parse_opt(int key, char *arg, struct argp_state *state) { /* Get the input argument from argp_parse, which we know is a pointer to our arguments structure. */ struct arguments *arguments = state->input; switch (key) { case ’q’: case ’s’: arguments->silent = 1; break; case ’v’: arguments->verbose = 1; break; case ’o’: arguments->output_file = arg; break; case ARGP_KEY_ARG: if (state->arg_num >= 2) /* Too many arguments. */ argp_usage(state); arguments->args[state->arg_num] = arg; break; case ARGP_KEY_END: if (state->arg_num 举例: 更复杂的一个例子 /* This program uses the same features as example 3, but has more options, and somewhat more structure in the -help output. It also shows how you can ‘steal’ the remainder of the input arguments past a certain point, for programs that accept a list of items. It also shows the special argp KEY value ARGP KEY NO ARGS, which is only given if no non-option arguments were supplied to the program. For structuring the help output, two features are used, *headers* which are entries in the options vector with the first four fields being zero, and a two part documentation string (in the variable DOC), which allows documentation both before and after the options; the two parts of DOC are separated by a vertical-tab character (’\\v’, or ’\\013’). By convention, the documentation before the options is just a short string saying what the program does, and that afterwards is longer, describing the behavior in more detail. All documentation strings are automatically filled for output, although newlines may be included to force a line break at a particular point. All documentation strings are also passed to the ‘gettext’ function, for possible translation into the current locale. */ #include #include #include const char *argp_program_version = \"argp-ex4 1.0\"; const char *argp_program_bug_address = \"\"; /* Program documentation. */ static char doc[] = \"Argp example #4 -- a program with somewhat more complicatedoptions\\vThis part of the documentation comes *after* the options;note that the text is automatically filled, but it’s possibleto force a line-break, e.g.\\ninput; switch (key) { case ’q’: case ’s’: arguments->silent = 1; break; case ’v’: arguments->verbose = 1; break; case ’o’: arguments->output_file = arg; break; case ’r’: arguments->repeat_count = arg ? atoi(arg) : 10; break; case OPT_ABORT: arguments->abort = 1; break; case ARGP_KEY_NO_ARGS: argp_usage(state); case ARGP_KEY_ARG: /* Here we know that state->arg_num == 0, since we force argument parsing to end before any more arguments can get here. */ arguments->arg1 = arg; /* Now we consume all the rest of the arguments. state->next is the index in state->argv of the next argument to be parsed, which is the first string we’re interested in, so we can just use &state->argv[state->next] as the value for arguments->strings. In addition, by setting state->next to the end of the arguments, we can force argp to stop parsing here and return. */ arguments->strings = &state->argv[state->next]; state->next = state->argc; break; default: return ARGP_ERR_UNKNOWN; } return 0; } /* Our argp parser. */ static struct argp argp = { options, parse_opt, args_doc, doc }; int main(int argc, char **argv) { int i, j; struct arguments arguments; /* Default values. */ arguments.silent = 0; arguments.verbose = 0; arguments.output_file = \"-\"; arguments.repeat_count = 1; arguments.abort = 0; /* Parse our arguments; every option seen by parse_opt will be reflected in arguments. */ argp_parse(&argp, argc, argv, 0, 0, &arguments); if (arguments.abort) error(10, 0, \"ABORTED\"); for (i = 0; i 子选项支持 int getsubopt (char **optionp, char *const *tokens, char **valuep) 举例: #include #include #include int do_all; const char *type; int read_size; int write_size; int read_only; enum { RO_OPTION = 0, RW_OPTION, READ_SIZE_OPTION, WRITE_SIZE_OPTION, THE_END }; const char *mount_opts[] = { [RO_OPTION] = \"ro\", [RW_OPTION] = \"rw\", [READ_SIZE_OPTION] = \"rsize\", [WRITE_SIZE_OPTION] = \"wsize\", [THE_END] = NULL }; int main(int argc, char **argv) { char *subopts, *value; int opt; while ((opt = getopt(argc, argv, \"at:o:\")) != -1) switch (opt) { case ’a’: do_all = 1; break; case ’t’: type = optarg; break; case ’o’: subopts = optarg; while (*subopts != ’\\0’) switch (getsubopt(&subopts, mount_opts, &value)) { case RO_OPTION: read_only = 1; break; case RW_OPTION: read_only = 0; break; case READ_SIZE_OPTION: if (value == NULL) abort(); read_size = atoi(value); break; case WRITE_SIZE_OPTION: if (value == NULL) abort(); write_size = atoi(value); break; default: /* Unknown suboption. */ printf(\"Unknown suboption ‘%s’\\n\", value); break; } break; default: abort(); } /* Do the real work. */ return 0; } 环境变量 #include char * getenv (const char *name) char * secure_getenv (const char *name) //name=value, 如果没有=, 相当于定义xxxx为空 int putenv (char *string) int setenv (const char *name, const char *value, int replace) int unsetenv (const char *name) int clearenv (void) //env的全局数组, 每一项都是name=value #include char ** environ syscall #include #include 例如: #include #include #include ... int rc; rc = syscall(SYS_chmod, \"/etc/passwd\", 0444); if (rc == -1) fprintf(stderr, \"chmod failed, errno = %d\\n\", errno); 又如: #include #include #include ... int rc; rc = chmod(\"/etc/passwd\", 0444); if (rc == -1) fprintf(stderr, \"chmod failed, errno = %d\\n\", errno); 终结程序 main的return abort会触发SIGABRT exit/*终止进程, 不返回; main的return值随后被传入status **status范围是0-255, 通常0(EXIT_SUCCESS)表示成功, 1(EXIT_FAILURE)表示失败 **用atexit或on_exit注册的函数会被调用 **关闭所有stream, flush buffers, remove tmpfile **最后调用_exit终止进程 */ #include void exit (int status) ```c //exit钩子函数 int atexit (void (*function) (void)) int on_exit (void (*function)(int status, void *arg), void *arg) //abort函数, 会触发SIGABRT. 不会回调exit钩子. 不推荐使用 void abort (void) /*真正的终止函数, 不管是哪种终止, 最终都会调用它: ** 1. 关闭所有文件描述符, 注意它不会把流文件flush buffer(exit会). ** 2. 报告进程的exit status到父进程的wait ** 3. 下面的子进程被init进程接管 ** 4. 向父进程发送SIGCHLD ** 5. 如果这个进程是个终端会话主进程(比如ssh), 那么它会发送SIGHUP到所有前台子进程, 并释放这个终端. ** 6. 如果进程终止导致进程组变为孤儿, 则这个孤儿进程组会变为stop状态, 然后组里的每个进程都会收到SIGHUP和SIGCONT */ void _exit (int status) 进程 //以阻塞方式执行一个子程序 #include int system (const char *command) //进程id和父进程id #include pid_t getpid (void) pid_t getppid (void) fork /* 新进程 ** pid不同 ** 父子共享文件描述符的posion? ** 子进程不继承文件锁 ** 不继承alarm ** 继承signal, 以及block mask, 但清除pending的signal */ #include pid_t fork (void) //共享父进程地址空间 pid_t vfork (void) exec /*exec ** pid ppid group session user pwd不变 ** alarm不变 ** fd不变, 但stream都没了(因为stream属于进程空间, 而fd是内核管理?) ** signal变成default, 但ignore的signal继承 ** 时间片不变 */ int execv (const char *filename, char *const argv[]) int execl (const char *filename, const char *arg0, . . . ) int execve (const char *filename, char *const argv[], char *const env[]) int execle (const char *filename, const char *arg0, . . ., char *const env[]) int execvp (const char *filename, char *const argv[]) int execlp (const char *filename, const char *arg0, . . . ) 等待子进程终止 pid_t waitpid (pid t pid, int *status-ptr, int options) //相当于waitpid (-1, &status, 0) pid_t wait (int *status-ptr) //多了一个参数usage, 子进程的资源 pid_t wait4 (pid t pid, int *status-ptr, int options, struct rusage *usage) 举例: 使用signal来处理子进程终止 void sigchld_handler(int signum) { int pid, status, serrno; serrno = errno; while (1) { pid = waitpid(WAIT_ANY, &status, WNOHANG); if (pid 子进程退出状态 //子进程用exit或_exit退出 int WIFEXITED (int status) //上面为真, 则这个返回main的返回值, 或调用exit()传入的返回值 int WEXITSTATUS (int status) //signal导致子进程终止 int WIFSIGNALED (int status) //上面为真时返回signal号 int WTERMSIG (int status) //是否产生了core dump int WCOREDUMP (int status) //子进程stop int WIFSTOPPED (int status) //signal导致子进程stop int WSTOPSIG (int status) 举例: ```c #include #include #include #include #include /* Execute the command using this shell program. */ #define SHELL \"/bin/sh\" int my_system(const char *command) { int status; pid_t pid; pid = fork(); if (pid == 0) { /* This is the child process. Execute the shell command. */ execl(SHELL, SHELL, \"-c\", command, NULL); /*exec失败, 直接终止子进程; 不要调用exit(), 因为它会flush从父进程继承过来的stream **而这些stream会被父进程flush, 导致output两次 */ _exit(EXIT_FAILURE); } else if (pid job控制 signal的分发是由进程组决定的. 前台进程组的每个进程都会收到由用户触发的SIGTSTP, SIGQUIT, SIGINT 信号 后台进程组不允许读控制台, 否则会触发 SIGTTIN and SIGTTOU信号到组内每个进程(默认stop), 但后台进程组一般可以写控制台 shell会把命令管道分成若干进程组, 并选定一个进程组为前台进程组, 该进程组享有控制台. 剩下的进程组为后台进程组. 一般一次登录产生一个会话, 里面包含若个进程组. 会话leader进程也称为终端的控制进程, 这个进程挂了, 则这个会话的进程组就成了孤儿进程组, 每个进程(还是前天进程?)都会收到SIGHUP信号 tcsetpgrp可以获得终端控制权, shell起前台进程时需要调用, 前台进程返回后, shell也要调用tcsetpgrp一次以重新获得终端控制权. 终端控制权切换时, 需要用tcgetattr()和tcsetattr()来恢复终端属性. 默认终止该进程 获得控制终端名字(和ttyname有什么区别?) char * ctermid (char *string) 进程组api //调用进程会新建会话, 并成为leader, 其pid==进程组id pid_t setsid (void) //获得session leader的进程组id pid_t getsid (pid t pid) //获得调用进程的进程组id pid_t getpgrp (void) //获得pid的进程组id int getpgid (pid t pid) //将pid加入pgid组 int setpgid (pid t pid, pid t pgid) //获得控制台进程组 pid_t tcgetpgrp (int filedes) //设置控制台进程组 int tcsetpgrp (int filedes, pid t pgid) 一个简单的shell ```c / A process is a single process. / typedef struct process { struct process next; / next process in pipeline / char **argv; / for exec / pid_t pid; / process ID / char completed; / true if process has completed / char stopped; / true if process has stopped / int status; / reported status value / } process; / A job is a pipeline of processes. / typedef struct job { struct job next; / next active job / char command; / command line, used for messages / process first_process; / list of processes in this job / pid_t pgid; / process group ID / char notified; / true if user told about stopped job / struct termios tmodes; / saved terminal modes / int stdin, stdout, stderr; / standard i/o channels / } job; / The active jobs are linked into a list. This is its head. / job first_job = NULL; / Find the active job with the indicated pgid. / job find_job(pid_t pgid) { job *j; for (j = first_job; j; j = j->next) if (j->pgid == pgid) return j; return NULL; } / Return true if all processes in the job have stopped or completed. / int job_is_stopped(job j) { process p; for (p = j->first_process; p; p = p->next) if (ps > completed && !p->stopped) return 0; return 1; } / Return true if all processes in the job have completed. / int job_is_completed(job j) { process p; for (p = j->first_process; p; p = p->next) if (!p->completed) return 0; return 1; } / Keep track of attributes of the shell. / include include include pid_t shell_pgid; struct termios shell_tmodes; int shell_terminal; int shell_is_interactive; / Make sure the shell is running interactively as the foreground job before proceeding. / void init_shell() { / See if we are running interactively. / shell_terminal = STDIN_FILENO; shell_is_interactive = isatty(shell_terminal); if (shell_is_interactive) { /* Loop until we are in the foreground. */ while (tcgetpgrp(shell_terminal) != (shell_pgid = getpgrp())) kill(- shell_pgid, SIGTTIN); /* Ignore interactive and job-control signals. */ signal(SIGINT, SIG_IGN); signal(SIGQUIT, SIG_IGN); signal(SIGTSTP, SIG_IGN); signal(SIGTTIN, SIG_IGN); signal(SIGTTOU, SIG_IGN); signal(SIGCHLD, SIG_IGN); /* Put ourselves in our own process group. */ shell_pgid = getpid(); if (setpgid(shell_pgid, shell_pgid) } void launch_process(process *p, pid_t pgid, int infile, int outfile, int errfile, int foreground) { pid_t pid; if (shell_is_interactive) { /* Put the process into the process group and give the process group the terminal, if appropriate. This has to be done both by the shell and in the individual child processes because of potential race conditions. */ pid = getpid(); if (pgid == 0) pgid = pid; setpgid(pid, pgid); if (foreground) tcsetpgrp(shell_terminal, pgid); /* Set the handling for job control signals back to the default. */ signal(SIGINT, SIG_DFL); signal(SIGQUIT, SIG_DFL); signal(SIGTSTP, SIG_DFL); signal(SIGTTIN, SIG_DFL); signal(SIGTTOU, SIG_DFL); signal(SIGCHLD, SIG_DFL); } /* Set the standard input/output channels of the new process. */ if (infile != STDIN_FILENO) { dup2(infile, STDIN_FILENO); close(infile); } if (outfile != STDOUT_FILENO) { dup2(outfile, STDOUT_FILENO); close(outfile); } if (errfile != STDERR_FILENO) { dup2(errfile, STDERR_FILENO); close(errfile); } /* Exec the new process. Make sure we exit. */ execvp(p->argv[0], p->argv); perror(\"execvp\"); exit(1); } void launch_job(job j, int foreground) { process p; pid_t pid; int mypipe[2], infile, outfile; infile = j->stdin; for (p = j->first_process; p; p = p->next) { /* Set up pipes, if necessary. */ if (p->next) { if (pipe(mypipe) stdout; /* Fork the child processes. */ pid = fork(); if (pid == 0) /* This is the child process. */ launch_process(p, j->pgid, infile, outfile, j->stderr, foreground); else if (pid pid = pid; if (shell_is_interactive) { if (!j->pgid) j->pgid = pid; setpgid(pid, j->pgid); } } /* Clean up after pipes. */ if (infile != j->stdin) close(infile); if (outfile != j->stdout) close(outfile); infile = mypipe[0]; } format_job_info(j, \"launched\"); if (!shell_is_interactive) wait_for_job(j); else if (foreground) put_job_in_foreground(j, 0); else put_job_in_background(j, 0); } / Put job j in the foreground. If cont is nonzero, restore the saved terminal modes and send the process group a SIGCONT signal to wake it up before we block. / void put_job_in_foreground(job j, int cont) { / Put the job into the foreground. */ tcsetpgrp(shell_terminal, j->pgid); /* Send the job a continue signal, if necessary. */ if (cont) { tcsetattr(shell_terminal, TCSADRAIN, &j->tmodes); if (kill(- j->pgid, SIGCONT) tmodes); tcsetattr(shell_terminal, TCSADRAIN, &shell_tmodes); } / Put a job in the background. If the cont argument is true, send the process group a SIGCONT signal to wake it up. / void put_job_in_background(job j, int cont) { / Send the job a continue signal, if necessary. */ if (cont) if (kill(-j->pgid, SIGCONT) / Store the status of the process pid that was returned by waitpid. Return 0 if all went well, nonzero otherwise. / int mark_process_status(pid_t pid, int status) { job j; process p; if (pid > 0) { /* Update the record for the process. */ for (j = first_job; j; j = j->next) for (p = j->first_process; p; p = p->next) if (p->pid == pid) { p->status = status; if (WIFSTOPPED(status)) p->stopped = 1; else { p->completed = 1; if (WIFSIGNALED(status)) fprintf(stderr, \"%d: Terminated by signal %d.\\n\", (int) pid, WTERMSIG(p->status)); } return 0; } fprintf(stderr, \"No child process %d.\\n\", pid); return -1; } else if (pid == 0 || errno == ECHILD) /* No processes ready to report. */ return -1; else { /* Other weird errors. */ perror(\"waitpid\"); return -1; } } / Check for processes that have status information available, without blocking. / void update_status(void) { int status; pid_t pid; do pid = waitpid(WAIT_ANY, &status, WUNTRACED | WNOHANG); while (!mark_process_status(pid, status)); } / Check for processes that have status information available, blocking until all processes in the given job have reported. / void wait_for_job(job *j) { int status; pid_t pid; do pid = waitpid(WAIT_ANY, &status, WUNTRACED); while (!mark_process_status(pid, status) && !job_is_stopped(j) && !job_is_completed(j)); } / Format information about job status for the user to look at. / void format_job_info(job j, const char status) { fprintf(stderr, \"%ld (%s): %s\\n\", (long)j->pgid, status, j->command); } / Notify the user about stopped or terminated jobs. Delete terminated jobs from the active job list. / void do_job_notification(void) { job j, jlast, jnext; process p; / Update status information for child processes. / update_status(); jlast = NULL; for (j = first_job; j; j = jnext) { jnext = j->next; /* If all processes have completed, tell the user the job has completed and delete it from the list of active jobs. */ if (job_is_completed(j)) { format_job_info(j, \"completed\"); if (jlast) jlast->next = jnext; else first_job = jnext; free_job(j); } /* Notify the user about stopped jobs, marking them so that we won’t do this more than once. */ else if (job_is_stopped(j) && j > notified) { format_job_info(j, \"stopped\"); j->notified = 1; jlast = j; } /* Don’t say anything about jobs that are still running. */ else jlast = j; } } / Mark a stopped job J as being running again. / void mark_job_as_running(job j) { Process p; for (p = j->first_process; p; p = p->next) p->stopped = 0; j->notified = 0; } / Continue the job J. / void continue_job(job *j, int foreground) { mark_job_as_running(j); if (foreground) put_job_in_foreground(j, 1); else put_job_in_background(j, 1); } # 用户和组 * 一个进程有effective用户ID(权限控制)和real用户ID(who创建了这个进程) ## api ```c uid_t getuid (void) gid_t getgid (void) uid_t geteuid (void) gid_t getegid (void) int getgroups (int count, gid t *groups) int seteuid (uid t neweuid) int setuid (uid t newuid) int setreuid (uid t ruid, uid t euid) int setegid (gid t newgid) int setgid (gid t newgid) int setregid (gid t rgid, gid t egid) int setgroups (size t count, const gid t *groups) int initgroups (const char *user, gid t group) int getgrouplist (const char *user, gid t group, gid t *groups, int *ngroups) char * getlogin (void) char * cuserid (char *string) int login_tty (int filedes) void login (const struct utmp *entry) int logout (const char *ut_line) void logwtmp (const char *ut_line, const char *ut_name, const char *ut_host) 系统管理 int gethostname (char *name, size t size) int sethostname (const char *name, size t length) int getdomainnname (char *name, size t length) int setdomainname (const char *name, size t length) long int gethostid (void) int sethostid (long int id) int uname (struct utsname *info) int mount (const char *special_file, const char *dir, const char *fstype, unsigned long int options, const void *data) int umount (const char *file) 例如: #include mount(\"/dev/hdb\", \"/cdrom\", MS_MGC_VAL | MS_RDONLY | MS_NOSUID, \"\"); mount(\"/dev/hda2\", \"/mnt\", MS_MGC_VAL | MS_REMOUNT, \"\"); //系统参数 #include int sysctl (int *names, int nlen, void *oldval, size t *oldlenp, void *newval, size t newlen) ```c //获取系统运行时配置参数 long int sysconf (int parameter) int have_job_control (void) { #ifdef _POSIX_JOB_CONTROL return 1; #else int value = sysconf (_SC_JOB_CONTROL); if (value 调试 调用栈 /*调用栈 **buffer里一个frame指针对应一项 **返回实际的栈帧数 **优化选项会影响栈回溯, 比如inline和frame pointer elimination */ #include int backtrace (void **buffer, int size) //解析调用栈符号表, 前提是运行的elf不是strip过的? char ** backtrace_symbols (void *const *buffer, int size) 举例: ```c #include #include #include /* Obtain a backtrace and print it to stdout. */ void print_trace(void) { void *array[10]; size_t size; char **strings; size_t i; size = backtrace(array, 10); strings = backtrace_symbols(array, size); printf(\"Obtained %zd stack frames.\\n\", size); for (i = 0; i pthread 详见POSIX Threads Programming(非常强大) gnu扩展 //线程独立的data int pthread_key_create (pthread key t *key, void (*destructor)(void*)) int pthread_key_delete (pthread key t key) void *pthread_getspecific (pthread key t key) int pthread_setspecific (pthread key t key, const void *value) libc的系统桩函数(略) libc基础函数 //断言, false则打印__FILE__ __LINE__并abort(). 断言可以用宏NDEBUG禁止断言检查 #include void assert (int expression) void assert_perror (int errnum) //可变参数 #include 举例: #include #include int add_em_up(int count, ...) { va_list ap; int i, sum; va_start(ap, count); /* Initialize the argument list. */ sum = 0; for (i = 0; i "},"notes/system_原理杂记.html":{"url":"notes/system_原理杂记.html","title":"系统原理杂记","keywords":"","body":" seccomp系统调用 系统权限 smaps解析性能 打开smaps文件本身 加上字符串操做 结论 使用其他用户启动进程 改变文件的owner为nobody:nogroup, 设置setuid属性 ruid euid 什么是uid euid? 补充 实验 然后用普通user启动 用root启动 进程的权限是看ruid, 而不是euid golang权限降级 什么是defunct进程? cgroup v2 进程 线程 控制器使能 不推荐动态迁移pid 资源限制类型 控制器类型 CPU 内存 IO PID Cpuset 配置文件交互 和V1的对比 系统内存占用分析 其他统计 vm参数 文件缓存 CPU占用率分析 per CPU统计 user + sys + softirq + idle + iowait = 100 softirq现象 结论 补充 系统调用都会触发调度吗? Preemption and Context Switching User Preemption Kernel Preemption 调度代码 一些系统调用, 可能会阻塞, 此时会触发调度 调度器的参考文章 signal的默认行为和打断系统调用 默认行为 signal和系统调用 内核收报文的时间片算在哪里? 背景: 驱动中断在哪里执行? 中断线程化后的CPU load 到底什么是中断上下文? 软中断上下文 softirq激活 ksoftirqd线程 上下文分类 收报的时间算在哪里? socket什么情况下会发生短读short read/partial read? socket通信的时候, 要在应用侧做字节序转换吗? socket的stream模式和datagram模式有什么不同? socket基础 SOCK_STREAM SOCK_SEQPACKET SOCK_DGRAM man 7 ip man 7 tcp man 7 udp man 7 unix 什么是message boundires? TCP的stream模式怎么定界? cgroup配置 写入pid rt调度域的配额 一些命令 linux调度方式有哪些? 非实时调度 实时调度 preemptive kernel是什么意思? 为什么一直说内核抢占? 嵌入式设备需要抢占吗? 用户态上下文切换和ucontex.h 用户态上下文 例子 使用ucontext.h的api实现用户态协程 多线程的情况下, signal被deliver到哪个线程? 信号处理函数执行的上下文是什么? 为什么能打印当前进程的调用栈? sighandler执行的上下文 sigaltstack函数用于指定sighandler栈 回答 信号处理原理 sigaction pending和blocked向量 signal的产生和投递 signal和系统调用 siglongjmp futex系统调用 libevent主循环处理timer 再议rm 普通用户可以rm root用户的文件 关于热升级, 正在使用的文件可以被rm 用户态通过系统调用陷入到内核态, 内存映射会变吗? uboot传mtdpart的时候，名字从哪来的？ 为什么直接考过来的ls不能用？ fork与malloc seccomp系统调用 很多sandbox机制都使用了这个系统调用, 它是个系统调用的filter机制. #include #include #include #include #include int seccomp(unsigned int operation, unsigned int flags, void *args); seccomp设置calling进程的Secure Computing属性,有几种operation: SECCOMP_SET_MODE_STRICT: 只有基本的read, write, exit和sigreturn可以用. 其他的系统调用会触发SIGKILL SECCOMP_SET_MODE_FILTER: args指向sock_fprog, 这是个bpf的指令, 可以设置任意组合的系统调用filter规则struct sock_fprog { unsigned short len; /* Number of BPF instructions */ struct sock_filter *filter; /* Pointer to array of BPF instructions */ }; struct sock_filter { /* Filter block */ __u16 code; /* Actual filter code */ __u8 jt; /* Jump true */ __u8 jf; /* Jump false */ __u32 k; /* Generic multiuse field */ }; SECCOMP_RET_KILL_PROCESS: 导致进程终止 SECCOMP_RET_KILL_THREAD: 导致调用者线程终止, 其他线程不受影响. SECCOMP_RET_TRAP: 系统调用会触发SIGSYS信号 SECCOMP_RET_ERRNO: SECCOMP_RET_TRACE: SECCOMP_RET_LOG: SECCOMP_RET_ALLOW: 系统权限 man capabilities 权限检查是基于thread的, 特权用户(root, euid=0)的thread不用检查, 其他用户会走权限检查流程. linux权限有很多类, 可以单独打开和关闭 比如 CAP_DAC_OVERRIDE: 有这个属性就可以跳过文件或目录的rwx检查 CAP_DAC_READ_SEARCH: 跳过read属性检查 CAP_KILL: 是否有kill权限 CAP_MKNOD: 是否能创建ssh设备文件 CAP_NET_ADMIN: 网络配置 CAP_NET_RAW: 使用raw socket CAP_SYS_ADMIN: 比如mount, setns, clone等 还有其他很多属性, 见man capabilities 子进程继承父进程的属性 smaps解析性能 我要解析/proc/1/smaps, 它的格式如下: ~ # cat /proc/1/smaps 00010000-00013000 r-xp 00000000 00:02 8237 /usr/bin/s6-svscan Size: 12 kB Rss: 12 kB Pss: 12 kB Shared_Clean: 0 kB Shared_Dirty: 0 kB Private_Clean: 0 kB Private_Dirty: 12 kB Referenced: 12 kB Anonymous: 0 kB AnonHugePages: 0 kB ShmemPmdMapped: 0 kB Shared_Hugetlb: 0 kB Private_Hugetlb: 0 kB Swap: 0 kB SwapPss: 0 kB KernelPageSize: 4 kB MMUPageSize: 4 kB Locked: 0 kB VmFlags: rd ex mr mw me dw 00022000-00023000 r--p 00002000 00:02 8237 /usr/bin/s6-svscan Size: 4 kB Rss: 4 kB Pss: 4 kB Shared_Clean: 0 kB Shared_Dirty: 0 kB Private_Clean: 0 kB Private_Dirty: 4 kB Referenced: 4 kB Anonymous: 4 kB AnonHugePages: 0 kB ShmemPmdMapped: 0 kB Shared_Hugetlb: 0 kB Private_Hugetlb: 0 kB Swap: 0 kB SwapPss: 0 kB KernelPageSize: 4 kB MMUPageSize: 4 kB Locked: 0 kB VmFlags: rd mr mw me dw ac ... 我写了代码把所有Pss: xx kB加起来. CPU消耗很高. 这里的消耗包括打开这个文件本身, 和字符串搜索和转换的消耗. 打开smaps文件本身 只是打开这个文件就已经很高了: htop显示CPU在40到70之间, 均值大概在50.perf发现大部分时间在内核的smaps_account()函数, 这个函数在for里计算每个page, 确实比较耗时. 加上字符串操做 buf, err := os.ReadFile(\"/proc/\" + pid + \"/smaps_rollup\") if err == nil { i := bytes.Index(buf, []byte(\"\\nPss:\")) if i != -1 { buf = buf[i+1:] size, _ := strconv.ParseUint(string(bytes.TrimSpace(buf[4:24])), 10, 64) //fmt.Fprintln(os.Stderr, string(bytes.TrimSpace(buf[4:24])), size) pssSize = size return } } 感觉CPU没有升高多少, 平均也在50+%. 结论 打开\"/proc/\" + pid + \"/smaps_rollup\"或\"/proc/\" + pid + \"/smaps\"本身就很消耗CPU. 因为kernel在open的时候才去调用smaps_account()函数. 使用其他用户启动进程 在shell里手动启动一个可执行程序, 其user是当前的登陆用户. 但有的时候, 比如开一个daemon进程, 不想用自己的用户来启动, 下面是方法: 改变文件的owner为nobody:nogroup, 设置setuid属性 这步需要sudo权限 # nobody和nogroup一般的linux系统都有 sudo chown nobody:nogroup gshell # user和group都要设置setuid属性 sudo chmod ugo+ws gshell # ls -l看到gshell程序已经是nobody:nogroup了, 而且u和g都有s属性 # 我专门把o的w属性也加上了 -rwsrwsrwx 1 nobody nogroup 22610134 Sep 26 01:14 gshell ruid euid 什么是uid euid? 最主要是看man setuid和man seteuid The distinction between a real and an effective user id is made because you may have the need to temporarily take another user's identity (most of the time, that would be root, but it could be any user). If you only had one user id, then there would be no way of changing back to your original user id afterwards (other than taking your word for granted, and in case you are root, using root's privileges to change to any user). So, the real user id is who you really are (the one who owns the process), and the effective user id is what the operating system looks at to make a decision whether or not you are allowed to do something (most of the time, there are some exceptions). When you log in, the login shell sets both the real and effective user id to the same value (your real user id) as supplied by the password file. Now, it also happens that you execute a setuid program, and besides running as another user (e.g. root) the setuid program is also supposed to do something on your behalf. How does this work? After executing the setuid program, it will have your real id (since you're the process owner) and the effective user id of the file owner (for example root) since it is setuid. The program does whatever magic it needs to do with superuser privileges and then wants to do something on your behalf. That means, attempting to do something that you shouldn't be able to do should fail. How does it do that? Well, obviously by changing its effective user id to the real user id! Now that setuid program has no way of switching back since all the kernel knows is your id and... your id. Bang, you're dead. This is what the saved set-user id is for. 参考: https://stackoverflow.com/questions/32455684/difference-between-real-user-id-effective-user-id-and-saved-user-id https://mudongliang.github.io/2020/09/17/ruid-euid-suid-usage-in-linux.html https://stackoverflow.com/questions/33982789/difference-between-euid-suid-and-ruid-in-linux-systems 补充 使用ps -eo user,pid,euid,ruid,suid,cmd | grep gshell可以查看各种id euid EUID effective user ID (alias uid). ruid RUID real user ID. suid SUID saved user ID. (alias svuid). 实验 当我sudo chmod ugo+ws bin/gshell后, 看到 -rwsrwsrwx 1 nobody nogroup 23M Oct 22 05:34 gshell 然后用普通user启动 bin/gshell -loglevel debug daemon -registry 10.182.105.179:11985 -bcast 9923 看到: $ ps -eo user,pid,euid,ruid,suid,cmd | grep gshell nobody 27870 65534 1003 65534 bin/gshell -loglevel debug daemon -registry 10.182.105.179:11985 -bcast 9923 yingjieb 27893 1003 1003 1003 grep gshell 很明显: 普通进程grep, 其euid ruid suid都是一致的, 即都是1003(yingjieb) 但bin/gshell带s属性(即setuid属性), 用普通用户运行, euid和suid是65534(nobody), ruid是启动用户 用root启动 sudo bin/gshell -loglevel debug daemon -registry 10.182.105.179:11985 -bcast 9923 看到: $ ps -eo user,pid,euid,ruid,suid,cmd | grep gshell root 27897 0 0 0 sudo bin/gshell -loglevel debug daemon -registry 10.182.105.179:11985 -bcast 9923 nobody 27898 65534 0 65534 bin/gshell -loglevel debug daemon -registry 10.182.105.179:11985 -bcast 9923 yingjieb 27910 1003 1003 1003 grep gshell 可以看到, 先用root启动了该程序, 但会以nobody fork这个进程运行, fork的进程的euid和suid是nobody, 但ruid还是root. 进程的权限是看ruid, 而不是euid 比如上面的例子, root启动的进程27898, 虽然euid变成了nobody, 但实际该进程还是可以有root权限, 创建删除权限都是root的. 那euid有啥用? golang权限降级 思路是先让bin文件的owner是nobody, 带setuid属性. 然后任何用户启动这个文件, euid都是nobody的. 但ruid还是启动用户的. 要在代码里改ruid: euid := os.Geteuid() if err := syscall.Setreuid(euid, euid); err != nil { return err } 改了ruid后, 再用ps -eo user,pid,euid,ruid,suid,cmd | grep gshell看, 所有的UID都是nobody了. 这个进程就只有nobody权限了. 补充, gid也要设置: ps -eo user,pid,euid,ruid,suid,egid,rgid,sgid,cmd | grep gshell 什么是defunct进程? gshell起了一个自己的new version的进程, 但显示: $ ps -ef | grep gshell yingjieb 6762 9291 2 12:18 pts/9 00:00:02 bin/gshell -wd .working -loglevel debug daemon -registry 10.182.105.138:11985 -bcast 9923 -root -repo gitlabe1.ext.net.nokia.com/godevsig/grepo/master -update http://10.182.105.179:8088/gshell/release/latest/%s yingjieb 6777 6762 0 12:18 pts/9 00:00:00 bin/gshell -wd .working -loglevel debug __start -e master.v1.1.3 yingjieb 6799 6762 0 12:20 pts/9 00:00:00 [gshell] 注意这里的[gshell] 是僵尸进程: Processes marked are dead processes (so-called \"zombies\") that remain because their parent has not destroyed them properly. These processes will be destroyed by init(8) if the parent process exits. 僵尸进程不能被kill, 因为它已经死了. 它还在这里显示是因为其父进程还在, 但没有清理这个死掉的子进程. 对应的go代码: 因为Start()并不会等待并清理子进程. if err := exec.Command(cmdArgs[0], cmdArgs[1:]...).Start(); err != nil { lg.Errorf(\"start new gshell failed: %v\", err) } else { lg.Infof(\"new version gshell started\") } 注: 这里子进程死掉的原因可以用下面的代码捕捉 out, err := exec.Command(cmdArgs[0], cmdArgs[1:]...).Output() lg.Debugf(\"out: %s, err: %v\", out, err) 是因为传入的-update选项新的binary不认识. cgroup v2 https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html mount -t cgroup2 none $MOUNT_POINT 和v1不同, cgroup v2只有一个树结构 一个进程只能属于一个cgroup mkdir $CGROUP_NAME创建一个子cgroup 进程 把进程pid写进cgroup.procs会migrate这个进程到该cgroup, 包括其所有线程. 每次只能写一个PID到cgroup.procs文件, 即一次write系统调用移动一个pid 没有子cgroup并且里面没有pid的cgrouup可以删除: rmdir $CGROUP_NAME 如果里面只有zombie进程, 也是可以删除的 /proc/$PID/cgroup可以显式pid所属的cgroup # cat /proc/842/cgroup ... 0::/test-cgroup/test-cgroup-nested 如果这个cgroup被删除了, 会是这样: 这个情况下, 这个进程一定是个僵尸进程 # cat /proc/842/cgroup ... 0::/test-cgroup/test-cgroup-nested (deleted) 线程 官方解释 cgroup v2 supports thread granularity for a subset of controllers to support use cases requiring hierarchical resource distribution across the threads of a group of processes. By default, all threads of a process belong to the same cgroup, which also serves as the resource domain to host resource consumptions which are not specific to a process or thread. The thread mode allows threads to be spread across a subtree while still maintaining the common resource domain for them. 默认所有线程属于同一个cgroup, 但也支持分属于多个subtree. 就是说在同一个tree下面 支持thread模式的controller叫threaded controllers; 不支持的叫domain controllers 默认创建的cgroup是domain模式, 用echo threaded > cgroup.type可以将其改为threaded模式, 但要满足如下条件: As the cgroup will join the parent’s resource domain. The parent must either be a valid (threaded) domain or a threaded cgroup. When the parent is an unthreaded domain, it must not have any domain controllers enabled or populated domain children. The root is exempt from this requirement. threaded cgroup下面新建的cgroup默认是无效的 A (threaded domain) - B (threaded) - C (domain, just created) 这样的cgroup树, 在C刚刚创建的时候, 默认是domain控制器, 但它的父节点上都不是domain控制器. 这样C的cgroup.type文件会报告domain (invalid), 直到配置其为threaded模式. 一个cgroup变为threaded模式会导致其父domain cgroup变为threaded domain 一个进程的线程只能在一个threaded domain下存在. The threaded domain cgroup serves as the resource domain for the whole subtree, and, while the threads can be scattered across the subtree, all the processes are considered to be in the threaded domain cgroup. “cgroup.procs” in a threaded domain cgroup contains the PIDs of all processes in the subtree and is not readable in the subtree proper. However, “cgroup.procs” can be written to from anywhere in the subtree to migrate all threads of the matching process to the cgroup. 这段说的是线程domain组的cgroup.procs包含了子树的所有进程, 因为此时子树里面都是线程ID A (threaded domain) - B (threaded)在这个模式下, 一个线程在B中, 那B只算这个线程的资源. 但B所属的进程所有资源都算在A的头上, 因为A是domain控制器 控制器使能 每个cgroup都支持控制器类型# cat cgroup.controllers cpu io memory 默认全部都是不使能的. 需要显式使能: # echo \"+cpu +memory -io\" > cgroup.subtree_control 只有空的domain cgroup才能使能domain控制器. 但root不受此限制 不推荐动态迁移pid Migrating a process across cgroups is a relatively expensive operation and stateful resources such as memory are not moved together with the process. This is an explicit design decision as there often exist inherent trade-offs between migration and various hot paths in terms of synchronization cost. As such, migrating processes across cgroups frequently as a means to apply different resource restrictions is discouraged. A workload should be assigned to a cgroup according to the system’s logical and resource structure once on start-up. Dynamic adjustments to resource distribution can be made by changing controller configuration through the interface files. 以上说的是动态迁移pid的cost有点大. 动态的配置可以作用在控制器的相关接口文件上. 就是说要静态pid到group, 但group的配置可以改. 资源限制类型 Weights 比例方式. 范围从[1, 10000], 默认100 可以超配 Limits 限额方式, 从[0, max], 默认max. 可以超配(这点和v1不一样?) -- 为什么可以超配? 因为普通模式收CFS调度, 完全公平, CPU 100%忙也受调度限制. Protections 保护方式. 看起来是保护最低限额. Allocations 分配方式. 不能超配. 似乎就是现在的用法? 控制器类型 CPU The “cpu” controllers regulates distribution of CPU cycles. This controller implements weight and absolute bandwidth limit models for normal scheduling policy and absolute bandwidth allocation model for realtime scheduling policy. 说的很清楚, CPU类型的控制器实现了普通调度模式下的限额方式(可以超配)以及实时调度模式下的分配方式(不能超配) WARNING: cgroup2 doesn’t yet support control of realtime processes and the cpu controller can only be enabled when all RT processes are in the root cgroup. Be aware that system management software may already have placed RT processes into nonroot cgroups during the system boot process, and these processes may need to be moved to the root cgroup before the cpu controller can be enabled. 这个warn的意思是cgroup2对RT的支持还不好? 接口文件 cpu.stat 统计信息. 竟然就有利用率和用户态 内核态时间 cpu.weight normal调度用的比例方式 cpu.max 应该是给RT用的 allocation方式 内存 内存控制器是有状态的, 实现了limit方式和protection方式. 目前有三种类型的内存使用能够被统计到: 用户态页表: Userland memory - page cache and anonymous memory. 内核态数据: Kernel data structures such as dentries and inodes. TCP的内存: TCP socket buffers. 如下接口文件: memory.current 目前mem占用, 应是实际值 memory.min 受保护的最小值, 默认是0 memory.low 在low下都不会被kernel回收 memory.high 超过high会被kernel严格回收 memory.max 硬上限. 超过OOM memory.stat 详细统计: 匿名页 有名页 文件 共享内存 slab IO 传统上应该是指disk IO PID 用于限制PID可以fork和clone的次数 Cpuset 指定核. 主要用于NUMA场景. 可以和CPU以及mem联用. 比如在一个cgroup tree下面, 同时限制CPU使用, MEM使用以及指定CPU核 cpuset.cpus 配置文件交互 cgroup.type 控制cgroup是否为threaded模式 cgroup.procs 进程加入cgroup. 所有线程也一起加入 在threaded模式下, 读这个文件返回EOPNOTSUPP, 因为这个cgroup只管线程, 线程所属的进程归这个threaded domain管(在cgroup tree的上游). 但写还是一样的语义. cgroup.threads 线程加入cgroup. 只有在同一个domain的线程才能加入. 即一个进程的线程, 只能在同一个domain的子树上. cgroup.subtree_control 管使能控制器的 cgroup.events 能显示这个cgroup及其子树当前是否有有效的pid. 有效就是指有至少一个非zombine的pid cgroup.freeze 写1就freeze这个cgroup 和V1的对比 v1允许多个树, 而v2只有一个树. 看似v1更灵活, 每个tree里面还可以有任意的控制器. 但过设计了. v1允许进程的线程分属于多个cgroup. 而v2只能是在一个domain; ok, 还是v1过设计了. v1会有父子竞争现象, 因为线程可以任意所属. 系统内存占用分析 参考文章: https://www.cnblogs.com/arnoldlu/p/8568330.html http://linuxperf.com/?p=142cat /proc/meminfo MemTotal: 8054880 kB---------------------物理内存总容量，对应totalram_pages大小。 MemFree: 4004312 kB---------------------空闲内存容量，对应vm_stat[NR_FREE_PAGES]大小。 MemAvailable: 5678888 kB---------------------MemFree减去保留内存，加上部分pagecache和部分SReclaimable。 Buffers: 303016 kB---------------------块设备缓冲区大小. Cached: 2029616 kB---------------------主要是vm_stat[NR_FILE_PAGES],再减去swap出的大小和块设备缓冲区大小。Buffers+Cached=Active(file)+Inactive(file)+Shmem。 SwapCached: 0 kB---------------------交换缓存上的内容容量。 Active: 2123084 kB---------------------Active=Active(anon)+Active(file)。 Inactive: 1476268 kB---------------------Inactive=Inactive(anon)+Inactive(file)。 Active(anon): 1273544 kB---------------------活动匿名内存，匿名指进程中堆上分配的内存，活动指最近被使用的内存。 Inactive(anon): 547988 kB---------------------不活动匿名内存，在内存不足时优先释放。 Active(file): 849540 kB---------------------活动文件缓存，表示内存内容与磁盘上文件相关联。 Inactive(file): 928280 kB---------------------不活动文件缓存。 Unevictable: 17152 kB---------------------不可移动的内存，当然也不可释放，所以不会放在LRU中。 Mlocked: 17152 kB---------------------使用mlocked()处理的页面。 SwapTotal: 7812092 kB---------------------交换空间总容量。 SwapFree: 7812092 kB---------------------交换空间剩余容量。 Dirty: 6796 kB---------------------脏数据，在磁盘缓冲区中尚未写入磁盘的内存大小。 Writeback: 0 kB---------------------待回写的页面大小。 AnonPages: 1283984 kB---------------------内核中存在一个rmap(Reverse Mapping)机制，负责管理匿名内存中每一个物理页面映射到哪个进程的那个逻辑地址等信息。rmap中记录的内存页综合就是AnonPages值。 Mapped: 455248 kB---------------------映射的文件占用内存大小。 Shmem: 550260 kB---------------------vm_stat[NR_SHMEM]，tmpfs所使用的内存，tmpfs即利用物理内存来提供RAM磁盘功能。在tmpfa上保存文件时，文件系统暂时将他们保存到RAM中。 Slab: 268208 kB---------------------slab分配器总量，通过slabinfo工具或者/proc/slabinfo来查看更详细的信息。 SReclaimable: 206964 kB---------------------不存在活跃对象，可回收的slab缓存vm_stat[NR_SLAB_RECLAIMABLE]。 SUnreclaim: 61244 kB---------------------对象处于活跃状态，不能被回收的slab容量。 KernelStack: 12736 kB---------------------内核代码使用的堆栈区。 PageTables: 50376 kB---------------------PageTables就是页表，用于存储各个用户进程的逻辑地址和物理地址的变化关系，本身也是一个内存区域。 NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 11839532 kB Committed_AS: 7934688 kB VmallocTotal: 34359738367 kB------------------理论上内核可以用来映射的逻辑地址范围。 VmallocUsed: 0 kB---------------------内核将空闲内存页。 VmallocChunk: 0 kB HardwareCorrupted: 0 kB AnonHugePages: 0 kB ShmemHugePages: 0 kB ShmemPmdMapped: 0 kB CmaTotal: 0 kB CmaFree: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB DirectMap4k: 226256 kB DirectMap2M: 5953536 kB DirectMap1G: 3145728 kB /proc/meminfo和free的对应关系如下： free /proc/meminfo total =MemTotal used =MemTotal - MemFree - (Cached + SReclaimable) - Buffers free =MemFree shared =Shmem buffers =Buffers cache =Cached + SReclaimable available =MemAvailable 其他统计 /proc/buddyinfo /proc/pagetypeinfo /proc/vmstat /proc/vmallocinfo /proc/self/statm /proc/self/maps /proc/zoneinfo /proc/slabinfo /sys/kernel/mm/ksm /proc/sys/vm/compact_memory /proc/sys/vm/panic_on_oom /proc/sys/vm/oom_kill_allocating_task /proc/sys/vm/oom_dump_tasks vm参数 /proc/sys/vm/highmem_is_dirtyable /proc/sys/vm/legacy_va_layout /proc/sys/vm/lowmem_reserve_ratio /proc/sys/vm/max_map_count /proc/sys/vm/mmap_min_addr /proc/sys/vm/min_free_kbytes /proc/sys/vm/stat_interval /proc/sys/vm/vfs_cache_pressure /proc/sys/vm/page-cluster 文件缓存 /proc/sys/vm/dirty_background_bytes /proc/sys/vm/dirty_background_ratio /proc/sys/vm/dirty_bytes /proc/sys/vm/dirty_ratio /proc/sys/vm/dirty_expire_centisecs /proc/sys/vm/drop_caches CPU占用率分析 下面的数据全部都是从proc文件系统里读出来的. per CPU统计 user + sys + softirq + idle + iowait = 100 看到图中core1的这几个值加起来是绝对的100 core0也一样, 绝对的100 softirq现象 系统在打流的时候, 大约每2分钟就有10秒的冲高, 2个核加起来刚好100. 从上面的图看, 在softirq高的时候, 有100的CPU占用. 按理说2核的CPU共200, 那么应该只剩下100的CPU了. 也就是说, 如果softirq也算是\"独立\"的统计的话, 按进程的叠加不应该超过剩下的100. 是吗? 不是. 见下图: 在蓝色尖峰的时候, 按进程的统计已经超过了150. 结论 按CPU视角来统计, softirq是独立的 按进程视角来统计, softirq被统计进了sys. 因为proc文件系统只提供user和sys的占用, 目前我的结论是softirq在这个进程的占比(或者说\"抢占了\"这个进程的占比)会被加到这个进程的sys占比中. 当softirq高发生时, 通常都是burst的网络报文的处理导致的. 如果只看进程的CPU占用, 要注意里面已经包括了softirq的占用. 补充 上图显示了cpu1的softirq + idle + system + irq + user = 100注意softirq和irq不是一个.有个进程ksoftirqd也占了十几个点的CPU. 如果把它加到sys类里, 就超了100.又因为CPU1的system + user都只有不到5个点, 所以ksoftirqd内核线程的时间是算在softirq里面的. 系统调用都会触发调度吗? 不是. 虽然内核会在返回到用户态之前, 检查是否调度, 但是有条件的: 它检查任务描述符里need_resched字段以判断是否需要调度. 这个字段在进程时间片用尽的时候, 由函数scheduler_tick()来置位, 或者是高优先级任务来的时候, 也要置位这个flag. Preemption and Context Switching Context switching, the switching from one runnable task to another, is handled by the context_switch() function defined in kernel/sched.c. It is called by schedule() when a new process has been selected to run. It does two basic jobs: Calls switch_mm(), which is defined in include/asm/mmu_context.h, to switch the virtual memory mapping from the previous process's to that of the new process. Calls switch_to(), defined in include/asm/system.h, to switch the processor state from the previous process's to the current's. This involves saving and restoring stack information and the processor registers. The kernel, however, must know when to call schedule(). If it only called schedule() when code explicitly did so, user-space programs could run indefinitely. Instead, the kernel provides the need_resched flag to signify whether a reschedule should be performed (See Table 3.2). This flag is set by scheduler_tick() when a process runs out of timeslice and by try_to_wake_up() when a process that has a higher priority than the currently running process is awakened. The kernel will check the flag, see that it is set, and call schedule() to switch to a new process. The flag is a message to the kernel that the scheduler should be invoked as soon as possible because another process deserves to run. Functions for Accessing and Manipulating need_resched Function Purpose set_tsk_need_resched(task) Set the need_resched flag in the given process clear_tsk_need_resched(task) Clear the need_resched flag in the given process need_resched() Test the value of the need_resched flag; return true if set and false otherwise Upon returning to user-space or returning from an interrupt, the need_resched flag is checked. If it is set, the kernel invokes the scheduler before continuing. The flag is per-process, and not simply global, because it is faster to access a value in the process descriptor (because of the speed of current and because it might be in a cache line) than a global variable. Historically, the flag was global before the 2.2 kernel. In 2.2 and 2.4, the flag was an int inside the task_struct. In 2.6, it was moved into a single bit of a special flag variable inside the thread_info structure. As you can see, the kernel developers are never satisfied. User Preemption User preemption occurs when the kernel is about to return to user-space, need_resched is set, and therefore, the scheduler is invoked. If the kernel is returning to user-space, it knows it is in a safe quiescent state. In other words, if it is safe to continue executing the current task, it is also safe to pick a new task to execute. Consequently, whenever the kernel is preparing to return to user-space, either on return from an interrupt or after a system call, the value of need_resched is checked. If it is set, the scheduler is invoked to select a new (more fit) process to execute. Both the return paths for return from interrupt and return from system call are architecture-dependent and typically implemented in assembly in entry.S (which, aside from kernel entry code, also contains kernel exit code). In short, user preemption can occur When returning to user-space from a system call When returning to user-space from an interrupt handler Kernel Preemption The Linux kernel, unlike most other Unix variants and many other operating systems, is a fully preemptive kernel. In non-preemptive kernels, kernel code runs until completion. That is, the scheduler is not capable of rescheduling a task while it is in the kernel—kernel code is scheduled cooperatively, not preemptively. Kernel code runs until it finishes (returns to user-space) or explicitly blocks. In the 2.6 kernel, however, the Linux kernel became preemptive; it is now possible to preempt a task at any point, so long as the kernel is in a state in which it is safe to reschedule. So when is it safe to reschedule? The kernel is capable of preempting a task running in the kernel so long as it does not hold a lock. That is, locks are used as markers of regions of non-preemptibility. Because the kernel is SMP-safe, if a lock is not held, the current code is reentrant and capable of being preempted. The first change in supporting kernel preemption was the addition of a preemption counter, preempt_count, to each process's task_struct. This counter begins at zero and increments for each lock that is acquired and decrements for each lock that is released. When the counter is zero, the kernel is preemptible. Upon return from interrupt, if returning to kernel-space, the kernel checks the values of need_resched and preempt_count. If need_resched is set and preempt_count is zero, then a more important task is runnable and it is safe to preempt. Thus, the scheduler is invoked. If preempt_count is nonzero, a lock is held and it is unsafe to reschedule. In that case, the interrupt returns as usual to the currently executing task. When all the locks that the current task is holding are released, preempt_count returns to zero. At that time, the unlock code checks if need_resched is set. If so, the scheduler will be invoked. Enabling and disabling kernel preemption is sometimes required in kernel code and will be discussed in Chapter 8. Kernel preemption can also occur explicitly, when a task in the kernel blocks or explicitly calls schedule(). This form of kernel preemption has always been supported because no additional logic is required to ensure the kernel is in a state that is safe to preempt. It is assumed that the code that explicitly calls schedule() knows it is safe to reschedule. Kernel preemption can occur When returning to kernel-space from an interrupt handler When kernel code becomes preemptible again If a task in the kernel explicitly calls schedule() If a task in the kernel blocks (which results in a call to schedule()) 调度代码 调度发生时, schedule()调用context_switch()完成调度 一些系统调用, 可能会阻塞, 此时会触发调度 基本上大部分IO相关的系统调用都可能阻塞, 比如 open() read() write()等. 这里列出了常见的可能阻塞的系统调用 调度器的参考文章 https://www.cs.montana.edu/~chandrima.sarkar/AdvancedOS/SchedulingLinux/index.html https://www.cs.columbia.edu/~smb/classes/s06-4118/l13.pdf http://lass.cs.umass.edu/~shenoy/courses/spring20/lectures/Lec09.pdf https://medium.com/@bundetcom/understanding-linux-scheduler-5c683ff482d0 signal的默认行为和打断系统调用 man 7 signal 默认行为 Signal Value Action Comment ────────────────────────────────────────────────────────────────────── SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard SIGQUIT 3 Core Quit from keyboard SIGILL 4 Core Illegal Instruction SIGABRT 6 Core Abort signal from abort(3) SIGFPE 8 Core Floating-point exception SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers; see pipe(7) SIGALRM 14 Term Timer signal from alarm(2) SIGTERM 15 Term Termination signal SIGUSR1 30,10,16 Term User-defined signal 1 SIGUSR2 31,12,17 Term User-defined signal 2 SIGCHLD 20,17,18 Ign Child stopped or terminated SIGCONT 19,18,25 Cont Continue if stopped SIGSTOP 17,19,23 Stop Stop process SIGTSTP 18,20,24 Stop Stop typed at terminal SIGTTIN 21,21,26 Stop Terminal input for background process SIGTTOU 22,22,27 Stop Terminal output for background process 注: 有的signal是发给整个group的, 比如SIGINT, 会发给整个PGRP, 也就是说, 如果一个前台父进程A起了子进程B, 那么在前台Ctrl+C掉进程A, 那么除了进程A会收到SIGINT, 进程B也会收到SIGINT 但如果是用kill命令, 比如kill -SIGINT 进程A, 那么只有进程A会收到SIGINT, 其子进程B不会收到SIGINT. 进程A的退出也不会导致其子进程B退出 用setpgid 或 setsid来改变子进程的进程组, 可以避免子进程收到前台的SIGINT 详见https://stackoverflow.com/questions/6803395/child-process-receives-parents-sigint signal和系统调用 signal打断系统调用的行为有几种, 和系统调用的类型以及注册sigaction的时候有没有SA_RESTART标记有关 下面的系统调用, 如果有SA_RESTART标记一般会被内核自动重新启动这个调用. 否则返回error EINTR * read(2), readv(2), write(2), writev(2), and ioctl(2) calls on \"slow\" devices. A \"slow\" device is one where the I/O call may block for an indefinite time, for example, a terminal, pipe, or socket. If an I/O call on a slow device has already transferred some data by the time it is interrupted by a signal handler, then the call will return a success status (normally, the number of bytes transferred). Note that a (local) disk is not a slow device according to this definition; I/O operations on disk devices are not interrupted by signals. * open(2), if it can block (e.g., when opening a FIFO; see fifo(7)). * wait(2), wait3(2), wait4(2), waitid(2), and waitpid(2). * Socket interfaces: accept(2), connect(2), recv(2), recvfrom(2), recvmmsg(2), recvmsg(2), send(2), sendto(2), and sendmsg(2), unless a timeout has been set on the socket (see below). * File locking interfaces: flock(2) and the F_SETLKW and F_OFD_SETLKW operations of fcntl(2) * POSIX message queue interfaces: mq_receive(3), mq_timedreceive(3), mq_send(3), and mq_timedsend(3). * futex(2) FUTEX_WAIT (since Linux 2.6.22; beforehand, always failed with EINTR). * getrandom(2). * pthread_mutex_lock(3), pthread_cond_wait(3), and related APIs. * futex(2) FUTEX_WAIT_BITSET. * POSIX semaphore interfaces: sem_wait(3) and sem_timedwait(3) (since Linux 2.6.22; beforehand, always failed with EINTR). * read(2) from an inotify(7) file descriptor (since Linux 3.8; beforehand, always failed with EINTR). 下面的系统调用直接返回EINTR, 不管是否有SA_RESTART * \"Input\" socket interfaces, when a timeout (SO_RCVTIMEO) has been set on the socket using setsockopt(2): accept(2), recv(2), recvfrom(2), recvmmsg(2) (also with a non-NULL timeout argument), and recvmsg(2). * \"Output\" socket interfaces, when a timeout (SO_RCVTIMEO) has been set on the socket using setsockopt(2): connect(2), send(2), sendto(2), and sendmsg(2). * Interfaces used to wait for signals: pause(2), sigsuspend(2), sigtimedwait(2), and sigwaitinfo(2). * File descriptor multiplexing interfaces: epoll_wait(2), epoll_pwait(2), poll(2), ppoll(2), select(2), and pselect(2). * System V IPC interfaces: msgrcv(2), msgsnd(2), semop(2), and semtimedop(2). * Sleep interfaces: clock_nanosleep(2), nanosleep(2), and usleep(3). * io_getevents(2). 内核收报文的时间片算在哪里? 背景: 驱动中断在哪里执行? 驱动收报要注册irq handler, 一般的, 使用: int request_threaded_irq(unsigned int irq, irq_handler_t handler, irq_handler_t thread_fn, unsigned long irqflags, const char *devname, void *dev_id) 其中: handler: 在中断上下文执行 thread_fn: 不为NULL就会新建一个内核线程\"irq/%irq-%irq_name\"来处理中断下半部. 当kernel启动参数包括threadirqs时, 即使thread_fn为null, 也会强制新建一个内核线程, 在该内核线程内处理中断. threadirqs只有在CONFIG_IRQ_FORCED_THREADING=y的时候才生效. 在板子上实验, 不加threadirqs, fglt-b上没有irq/xxx等进程. 加了threadirqs, 多了十几个irq/xxx进程, 每个中断一个. 一般的系统不会配置threadirqs强制把中断线程化. 所以, 根据驱动中断注册中断处理函数request_threaded_irq()是否有thread_fn, 决定中断是在内核线程上下文还是中断上下文 中断线程化后的CPU load 下面是打开threadirqs后, irq/24-eth0就是板子eth0网口收报的中断被强制线程化后的线程名. 可以看到, 在打流的时候, 这个线程load挺高的. 默认的eth0驱动并没有使用线程化中断, 这里面大概15%的CPU load可能被均摊到其他进程中(看起来是的). 打1000/2000/4000/6000pps 上行dhcpv4 discovery 打开threadirqs功能:topid: http://10.182.105.138:9888/switchPerf/152917918 关闭threadirqs功能:topid：http://10.182.105.138:9888/fgltb_dhcpv4/3501403790 对比两组数据, 基本上可以得到, 每个app的CPU load里, 都包含了中断处理时间, 比较均匀, 大概都是在2%左右. 中断线程化了之后, 每个app的cpu占用统计都稍微降了一点, 这些CPU都被算到irq/24-eth0上了. 另外, 这里面没有看到ksoftirqd等软中断进程, 说明softirq大部分都在\"中断\"中处理了. 到底什么是中断上下文? interrupt context : it specifies that the kernel is currently executing either an interrupt handler or a deferrable function 根据上面的定义, softirq是中断上下文. 从属性上说, 是的. 但严格从CPU角度来讲, softirq并不总是在硬件中断上下文中执行的. 下文会讲到, softirq并不是硬件上的某种\"软件触发中断\", 而是kernel的一种延迟执行的机制, 这些执行可能在硬件中断上下文中, 也可能是在普通的内核态上下文, 但其环境还是类似\"中断\"环境的, 比如抢占级别很高(只能被硬件中断抢占), 单独的softirq栈等等. 软中断上下文 一般硬中断会关中断(比如关闭eth的中断), 而软中断虽然也是在中断上下文执行(这点并不是always true), 但软中断是在使能中断的状态下执行的; 并且, 软中断可以多核同时执行. >中, 把软中断和tasklet叫做延迟执行. 原文 对硬件中断来说, 是关中断情况下串行执行的, 速度越快越好. softirq和tasklet和workqueue就是用来跑下半部的. softirq和tasklet又叫deferrable functions tasklet是基于softirq的 中断上下问的意思是kernel在执行interrupt handler, 或者在执行deferrable functions mpstat可以看软中断统计, 实际上也是从/proc/softirqs得到的数据 下面是个4核A53的ARM板子上的统计 ~ # mpstat -I SCPU Linux 4.9.199-Arm-Cortex_a53 (fglt-b) 03/08/70 _aarch64_ (4 CPU) 01:47:49 CPU HI/s TIMER/s NET_TX/s NET_RX/s BLOCK/s IRQ_POLL/s TASKLET/s SCHED/s HRTIMER/s RCU/s 01:47:49 0 0.00 100.00 0.00 3.58 0.00 0.00 0.14 98.45 0.00 48.13 01:47:49 1 0.00 87.55 0.00 8.61 0.00 0.00 0.00 98.95 0.00 38.53 01:47:49 2 0.00 79.42 0.00 6.46 0.00 0.00 3.09 98.73 0.00 47.22 01:47:49 3 0.00 65.90 0.00 2.41 0.00 0.00 0.00 97.61 0.00 39.11 softirq是静态分配的 tasklet可以动态分配, 比如加载一个内核模块时 同类型的softirq也可以同时运行在多核上, 必须可重入, 用锁保护关键区. 同类tasklet同时只能一个核运行, 不用担心竞争问题. deferrable functions在使用上, 类似中断, 都有如下操作: 初始化(Initialization): 定义一个延迟函数, 一般在kernel初始化时候确定或者在module load的时候做 激活(Activation): 标记为pending, pending的延迟函数会在下次调度到的时候执行. 在中断里也可以标记. 屏蔽(Masking): 临时禁止执行 执行(Execution): 执行pending的deferrable functions, 如果pending的很多, 只执行预定义的一部分. 激活和执行操作是同一个CPU, 这么设计主要是考虑到cache的利用率会高一点. softirq激活 在用open_softirq()注册softirq后, raise_softirq()函数用来激活softirq, 执行流程: 关本地中断 给这个softirq标记为pending 调用wakeup_softirqd()唤醒ksoftirqd进程 开本地中断 kernel会在关键点上检查softirq的pending状态, 这些关键点(checkpoints)包括: kernel调用local_bh_enable 当中断处理函数do_IRQ()完成硬中断处理, 最后调用irq_exit()时 apic timersmp_apic_timer_interrupt()结束时 核间中断处理完成时 ksoftirqd 内核进程运行时 可以看到, 这里面既有中断上下文, 又有进程上下文. 即软中断函数可能在不同的上下文中执行. 当以上checkpoint检查得知有softirq要处理时, 就会调用do_softirq(), 其执行流程如下: in_interrupt()如果是1, 表示已经在中断里调用过了do_softirq(), 或者这个softirq被禁止了. 直接return local_irq_save()关中断 切换到softirq自己的栈 执行_ _do_softirq( ) 这里应该把所有pending的事情都做完, 但这个函数可能是在中断上下文中, 执行太久会有问题. 所以只能执行固定数量的work, 剩下的交给ksoftirqd线程处理. 默认处理10个work local_bh_disable()禁止并发的_ _do_softirq( )执行? why? 不是说好了softirq支持并发吗? local_irq_enable()开中断 执行对应的softirq_vec[n]->action wakeup_softirqd( )唤醒ksoftirqd处理这里剩下的work softirq counter减一, 再次使能 切换回之前的栈 local_irq_restore()开中断 ksoftirqd线程 这个线程是用来做剩下来的工作的. for(;;) { set_current_state(TASK_INTERRUPTIBLE ); schedule( ); /* now in TASK_RUNNING state */ while (local_softirq_pending( )) { preempt_disable(); do_softirq( ); preempt_enable(); cond_resched( ); } } 这个线程是为了解决softirq过快产生的时候, 占用中断时间太长的问题的. 因为softirq可以被自己, 或者被外部事件激活. Softirq functions may reactivate themselves; in fact, both the networking softirqs and the tasklet softirqs do this. Moreover, external events, such as packet flooding on a network card, may activate softirqs at very high frequency. 上下文分类 前面分析了, 软中断上下文可以在硬中断中, 也可能是kernel checkpoints, 但都不是用户进程上下文(可能也不是绝对的, 比如用户态的系统调用里面, 调用的driver函数里有local_bh_enable()调用, 那么softirq就是在这个进程上下文处理的) 用户上下文永远可能被抢占 纯内核线程没有MM softirq是在预定义的kernel \"checkpoint\"里执行的 中断里不能sleep 收报的时间算在哪里? 从用户态调用recv开始, 用户态等待packet 网卡收报, 中断上下文驱动处理, 激活softirq 这个图有点片面. 大概率是在中断上下文处理10个, 剩下的叫给ksoftirqd 唤醒用户进程 补充: kernel的一些api用于判断当前上下文 实际上, 网卡驱动收报, IP层处理, 都不会算在进程上下文上. 但到TCP阶段, 已经绑定到socket了, tcp_v4_do_rcv()有可能在进程上下执行, 也有可能在softirq上下文执行. 结合>的分析, 如果正好用户进程在run, 但还没有调用recv(), 就会在softirq上下文执行tcp_v4_do_rcv() 那么收报的时间算在进程头上吗?答: 应该说大部分时间不会. 比如驱动收报和IP层处理. socket什么情况下会发生短读short read/partial read? 答: 应该主要是和syscall的被打断有关; 或者当时receive queue里面确实没有那么多的字节. A characteristic of earlier UNIX systems was that if a process caught a signal while the process was blocked in a ‘‘slow’’ system call, the system call was interrupted. The system call returned an error and errno was set to EINTR. This was done under the assumption that since a signal occurred and the process caught it, there is a good chance that something has happened that should wake up the blocked system call. To prevent applications from having to handle interrupted system calls, 4.2BSD introduced the automatic restarting of certain interrupted system calls. The system calls that were automatically restarted are ioctl, read, readv, write, writev, wait, and waitpid. As we’ve mentioned, the first five of these functions are interrupted by a signal only if they are operating on a slow device; wait and waitpid are always interrupted when a signal is caught. Since this caused a problem for some applications that didn’t want the operation restarted if it was interrupted, 4.3BSD allowed the process to disable this feature on a per-signal basis. stackoverflow的回答: Interruption of a system call by a signal handler occurs only in the case of various blocking system calls, and happens when the system call is interrupted by a signal handler that was explicitly established by the programmer. Furthermore, in the case where a blocking system call is interrupted by a signal handler, automatic system call restarting is an optional feature. You elect to automatically restart system calls by specifying the SA_RESTART flag when establishing the signal handler. As stated in (for example) the Linux signal(7) manual page: If a signal handler is invoked while a system call or library function call is blocked, then either: * the call is automatically restarted after the signal handler returns; or * the call fails with the error EINTR. Which of these two behaviors occurs depends on the interface and whether or not the signal handler was established using the SA_RESTART flag (see sigaction(2)). As hinted by the last sentence quoted above, even when you elect to use this feature, it does not work for all system calls, and the set of system calls for which it does work varies across UNIX implementations. The Linux signal(7) manual page notes a number of system calls that are automatically restarted when using the SA_RESTART flag, but also goes on to note various system calls that are never restarted, even if you specify that flag when establishing a handler, including: * \"Input\" socket interfaces, when a timeout (SO_RCVTIMEO) has been set on the socket using setsockopt(2): accept(2), recv(2), recvfrom(2), recvmmsg(2) (also with a non-NULL timeout argu‐ ment), and recvmsg(2). * \"Output\" socket interfaces, when a timeout (SO_RCVTIMEO) has been set on the socket using setsockopt(2): connect(2), send(2), sendto(2), and sendmsg(2). * File descriptor multiplexing interfaces: epoll_wait(2), epoll_pwait(2), poll(2), ppoll(2), select(2), and pselect(2). * System V IPC interfaces: msgrcv(2), msgsnd(2), semop(2), and semtimedop(2). For these system calls, manual restarting using a loop of the form described in APUE is essential, something like: while ((ret = some_syscall(...)) == -1 && errno == EINTR) continue; if (ret == -1) /* Handle error */ ; socket通信的时候, 要在应用侧做字节序转换吗? 答: 要. 尤其是binary编码情况下. 一般类似GPB的codec已经做了. socket的stream模式和datagram模式有什么不同? socket基础 man socket #include /* See NOTES */ #include int socket(int domain, int type, int protocol); domain有如下方式 Name Purpose Man page AF_UNIX, AF_LOCAL Local communication unix(7) AF_INET IPv4 Internet protocols ip(7) AF_INET6 IPv6 Internet protocols ipv6(7) AF_IPX IPX - Novell protocols AF_NETLINK Kernel user interface device netlink(7) AF_X25 ITU-T X.25 / ISO-8208 protocol x25(7) AF_AX25 Amateur radio AX.25 protocol AF_ATMPVC Access to raw ATM PVCs AF_APPLETALK AppleTalk ddp(7) AF_PACKET Low level packet interface packet(7) AF_ALG Interface to kernel crypto API type有 SOCK_STREAM Provides sequenced, reliable, two-way, connection-based byte streams. An out-of-band data transmission mechanism may be supported. SOCK_DGRAM Supports datagrams (connectionless, unreliable messages of a fixed maximum length). SOCK_SEQPACKET Provides a sequenced, reliable, two-way connection-based data transmission path for datagrams of fixed maximum length; a consumer is required to read an entire packet with each input system call. SOCK_RAW Provides raw network protocol access. SOCK_RDM Provides a reliable datagram layer that does not guarantee ordering. SOCK_PACKET Obsolete and should not be used in new programs; see packet(7). type还支持OR标记 SOCK_NONBLOCK Set the O_NONBLOCK file status flag on the new open file description. Using this flag saves extra calls to fcntl(2) to achieve the same result. SOCK_CLOEXEC Set the close-on-exec (FD_CLOEXEC) flag on the new file descriptor. See the description of the O_CLOEXEC flag in open(2) for reasons why this may be useful. socket的选项是SO_xxxx形式的, 用setsockopt(2)来设置. 用getsockopt(2) 来获取. SO_SNDBUF Sets or gets the maximum socket send buffer in bytes. The kernel doubles this value (to allow space for bookkeeping overhead) when it is set using setsockopt(2), and this doubled value is returned by getsockopt(2). The default value is set by the /proc/sys/net/core/wmem_default file and the maximum allowed value is set by the /proc/sys/net/core/wmem_max file. The minimum (doubled) value for this option is 2048. 我这里显示, 默认的发送buf是229K ~ # cat /proc/sys/net/core/wmem_default 229376 ~ # cat /proc/sys/net/core/wmem_max 229376 SOCK_STREAM AF_INET domain里面对应TCP, 有链接, 可靠, 保序. 没有记录边界. 这就是字节流的核心要义. 通俗来讲, 发送方发2次5字节, 接收方可以一次读到10个字节. 并且, 接收方并不知道这10个字节是两次发送的还是一次发送的. send()和recv()API, 支持带外数据发送 如果超时后还是有段数据没有收到, 则这个连接就是broken了. 对broken的连接读写会产生SIGPIPE信号 SOCK_SEQPACKET 底层和SOCK_STREAM一致, 有链接, 可靠, 保序 是packet模式, 有界. 一次读会把这个packet的所有数据读出, 超出的数据会被丢弃. all message boundaries in incoming datagrams are preserved SOCK_DGRAM 无连接, 有size限制的数据报模式 使用sendto()和recvfrom() API. recvfrom()返回下一个数据报. Datagrams are generally received with recvfrom(2), which returns the next datagram along with the address of its sender. 天然有界: 所有的收报都是一个packet. 小报直接收, 大包被截断, 剩余部分丢弃. All receive operations return only one packet. When the packet is smaller than the passed buffer, only that much data is returned; when it is bigger, the packet is truncated and the MSG_TRUNC flag is set. 发送端的sendto()和接收端的recvfrom()永远是1:1的, 一个对一个. 比如sendto()两次, 也必须recvfrom()两次才能收完报文. 这点和TCP不一样. 这也是data gram的含义. 有个api, 支持一次系统调用, 收多个datagram: recvmmsg() 估计是在内核态多次recv收报. man 7 ip #include #include #include /* superset of previous */ tcp_socket = socket(AF_INET, SOCK_STREAM, 0); udp_socket = socket(AF_INET, SOCK_DGRAM, 0); raw_socket = socket(AF_INET, SOCK_RAW, protocol); proc下面有些全局的配置: /proc/sys/net/ipv4/ man 7 tcp #include #include #include tcp_socket = socket(AF_INET, SOCK_STREAM, 0); 一些全局的配置~ # cat /proc/sys/net/ipv4/tcp_wmem 4096 16384 4194304 ~ # cat /proc/sys/net/ipv4/tcp_rmem 4096 87380 6291456 /proc/sys/net/core/rmem_max /proc/sys/net/core/wmem_max 支持urgent data, 用send的MSG_OOB选项发送 支持ioctl man 7 udp #include #include #include udp_socket = socket(AF_INET, SOCK_DGRAM, 0); 默认最大MTU, 写报文超过MTU会有EMSG‐SIZE错误. man 7 unix #include #include unix_socket = socket(AF_UNIX, type, 0); error = socketpair(AF_UNIX, type, 0, int *sv); 同时支持SOCK_STREAM和SOCK_DGRAM, 并且SOCK_DGRAM是可靠和保序的 也支持SOCK_SEQPACKET 不支持out-of-band数据 支持fd传递到其他进程, 见SCM_RIGHTS Send or receive a set of open file descriptors from another process. The data portion contains an integer array of the file descriptors. The passed file descriptors behave as though they have been created with dup(2). datagram模式时, SO_SNDBUF起作用, 这个是send()数据报的上限. 上限是: 2*SO_SNDBUF-32 SO_RCVBUF没有作用 什么是message boundires? UDP preserves message boundaries. If you send \"FOO\" and then \"BAR\" over UDP, the other end will receive two datagrams, one containing \"FOO\" and the other containing \"BAR\". If you send \"FOO\" and then \"BAR\" over TCP, no message boundary is preserved. The other end might get \"FOO\" and then \"BAR\". Or it might get \"FOOBAR\". Or it might get \"F\" and then \"OOB\" and then \"AR\". TCP does not make any attempt to preserve application message boundaries -- it's just a stream of bytes in each direction. 对于datagram类型的报文接收, 用 ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen); The recvfrom function reads one packet from the socket socket into the buffer buffer. The size argument specifies the maximum number of bytes to be read. UDP operates on messages, not streams like TCP does. There is a 1-to-1 relationship between sendto() and recvfrom() when using UDP If the packet is longer than size bytes, then you get the first size bytes of the packet and the rest of the packet is lost. There’s no way to read the rest of the packet. Thus, when you use a packet protocol, you must always know how long a packet to expect. The arguments to this call are basically the same as the standard socket call. The Recvfrom() call reads one packet at a time. It returns the length of the message written to the buffer pointed to by the buf argument (the second argument). Even if one packet worth of message does not fill up the buffer, Recvfrom() will return immediately and will not read the second packet. However, if a message in a packet is too long to fit in the supplied buffer, the excess bytes are discarded. By default, Recvfrom() is blocking: when a process issues a Recvfrom() that cannot be completed immediately (because there is no packet), the process is put to sleep waiting for a packet to arrive at the socket. Therefore, a call to Recvfrom() will return immediately only if a packet is available on the socket. When the argument flags of Recvfrom() is set to MSG_NOBLOCK, Recvfrom() does not block if there is no data to be read, but returns immediately with a return value of 0 bytes. MSG_NOBLOCK is defined in $PDIR/include/systm.h. In an actual UNIX system, socket descriptors are set to be non-blocking using fcntl() with type O_NONBLOCK, and Recvfrom() returns errno EWOULDBLOCK when there is no data to be read on the non-blocking socket. TCP的stream模式怎么定界? stream流, 接收方并不知道发送放分多少次发送的. 接收方只看到一个字节流. 这就需要在应用层定界, 即双方约定如何分割和理解这个字节流. 通常的方法有: 加固定size的头. 这个头里有size信息 So you first receive the header (fixed size), extract the message size information and then receive in a second loop the real user data. 加delimiter, 即特殊符号标记 Alternatively some protocols are using delimiters to mark message boundaries. cgroup配置 写入pid /sys/fs/cgroup/cpu的cgroups树下面, 有两个文件 cgroup.procs : 将pid写入这个文件, 这个pid下面的所有线程都受cgroups控制 tasks : 只有这个pid的线程受cgroups控制 rt调度域的配额 /mnt/cgroups/cpu # cat cpu.rt_runtime_us 950000 一些命令 # 查看cgBase组里的进程 cat /mnt/cgroups/cpu/cgBase/cgroup.procs | xargs -i cat /proc/{}/comm # 查看cgBase组里的线程 cat /mnt/cgroups/cpu/cgBase/tasks | xargs -i cat /proc/{}/comm # 查看cgNonDelayCrit组里的进程 cat /mnt/cgroups/cpu/cgBase/cgNonDelayCrit/cgroup.procs | xargs -i cat /proc/{}/comm # 查看cgNonDelayCrit组里的线程 cat /mnt/cgroups/cpu/cgBase/cgNonDelayCrit/tasks | xargs -i cat /proc/{}/comm # 查看onu_engine group cat /mnt/cgroups/cpu/cgBase/cgNonDelayCrit/onu_engine/cgroup.procs | xargs -i cat /proc/{}/comm # 查看所有不在cgroup组的进程 cat /mnt/cgroups/cpu/cgroup.procs | xargs -i cat /proc/{}/comm linux调度方式有哪些? man sched 所有的调度策略是对同一个优先级下面的runnable队列而言的; 高优先级抢占低优先级是宇宙法则, 所有调度策略必须都要遵守. 非实时调度 SCHED_OTHER, SCHED_IDLE, SCHED_BATCH: 静态优先级是0 实时调度 SCHED_FIFO, SCHED_RR: 静态优先级是1 - 99 SCHED_FIFO: 没有时间片, 低优先级随时被高优先级抢占. 同一个优先级按先进先出排队 SCHED_RR: 有时间片, 按时间片轮转. 对于实时调度, 所有的实时优先级组都共享三个配置: sched_rr_timeslice_ms : 管轮转的时间片的 sched_rt_period_us : 实时优先级和普通优先级的总时间. 默认1秒. 对应100% CPU. sched_rt_runtime_us : 可以认为是所有实时优先级占比. 默认0.95秒. 即95% CPU ~ # cat /proc/sys/kernel/sched_rr_timeslice_ms 10 ~ # cat /proc/sys/kernel/sched_rt_period_us 1000000 ~ # cat /proc/sys/kernel/sched_rt_runtime_us 950000 ~ # ls ~ # zcat /proc/config.gz | grep -i empt # CONFIG_PREEMPT_NONE is not set # CONFIG_PREEMPT_VOLUNTARY is not set CONFIG_PREEMPT=y CONFIG_PREEMPT_COUNT=y CONFIG_PREEMPT_RCU=y CONFIG_DEBUG_PREEMPT=y ~ # zcat /proc/config.gz | grep -i hz # CONFIG_HZ_24 is not set # CONFIG_HZ_48 is not set CONFIG_HZ_100=y # CONFIG_HZ_128 is not set # CONFIG_HZ_250 is not set # CONFIG_HZ_256 is not set # CONFIG_HZ_1000 is not set # CONFIG_HZ_1024 is not set CONFIG_SYS_SUPPORTS_ARBIT_HZ=y CONFIG_HZ=100 CONFIG_HZ_PERIODIC=y # CONFIG_NO_HZ_IDLE is not set # CONFIG_NO_HZ_FULL is not set # CONFIG_NO_HZ is not set 解释一下: 几乎所有的moswa app 都是SCHED_RR, 静态分配优先级; 同一个优先级内按时间片轮转. 默认时间片10ms 所有实时优先级进程占CPU比例上限 95% 即有5%的CPU留给了非实时优先级, 目前只有内核线程loop* bio* ubi* spi1等几个线程享用 这个不归cgroup管, 也是为什么cgroup的cg_base最大只能配950000 目前是抢占式调度, 也就是说虽然5%留给了非实时进程, 但这些进程大概率经常被抢占. 应该说系统越忙, 雪崩效应越明显: 有更多的抢占发生 推荐优化思路: 禁止内核抢占CONFIG_PREEMPT=n或者CONFIG_PREEMPT_VOLUNTARY=y 重新整理系统实时进程和非实时进程策略, 一个比较粗糙的想法是业务处理搞SCHED_RR, 并且不那么在内部细分优先级, 比如按业务组定几个就好了, 让调度器去轮转调度; 其他进程, 比如ping, ssh, 各种脚本的衍生进程, 非核心path下的eqpt等进程, 都放到非实时 可以尝试增大SCHED_RR到100ms 调整实时进程组和非实时进程组的比例, 现在是95% : 5% preemptive kernel是什么意思? 内核现在有三个抢占模式: CONFIG_PREEMPT=y的时候, 打开内核抢占; 为n的时候关闭内核抢占 后来又加了CONFIG_PREEMPT_VOLUNTARY, 意思是主动在内核特定点可以抢占. 抢占的意思是低优先级被高优先级抢占. 为什么一直说内核抢占? 答: 因为用户态代码总是可以被抢占的, 无论CONFIG_PREEMPT怎么配置. 例如用户态代码的死循环变量加一, 也是有时间片的, 时间片耗尽也是要被kernel切换出去的. 内核抢占说的是, 当一个进程陷入到内核态, 代表这个进行运行的内核代码能否被抢占. 在古老的kernel版本里面, 内核态代码是不能被抢占的. 后来为了能够即使响应桌面等UI互动等场景, 加入了抢占. CONFIG_PREEMPT的解释如下: This option reduces the latency of the kernel by making all kernel code (that is not executing in a critical section) preemptible. This allows reaction to interactive events by permitting a low priority process to be preempted involuntarily even if it is in kernel mode executing a system call and would otherwise not be about to reach a natural preemption point. This allows applications to run more 'smoothly' even when the system is under load, at the cost of slightly lower throughput and a slight runtime overhead to kernel code. Select this if you are building a kernel for a desktop or embedded system with latency requirements in the milliseconds range. 嵌入式设备需要抢占吗? 抢占主要是给用户体验用的, 比如用户的鼠标键盘希望能响应快一点. 对时延要求高的系统, 比如工业控制系统, 需要打开内核抢占. 而一般的嵌入式系统, 应该更追求处理业务的吞吐量, 此时不抢占更合适. 一般的x86服务器, 都开的是CONFIG_PREEMPT_VOLUNTARY=y, 这是一种介于中间的状态. 这篇文章对比过 CONFIG_PREEMPT_VOLUNTARY有很好的平衡, 所以一般的主流OS(CentOS, Ubuntu, SUSE)都默认此模式(估计是server版本). 用户态上下文切换和ucontex.h 参考: 我所理解的ucontext族函数(主要是概念和使用) 协程：posix::ucontext用户级线程实现原理分析(包括汇编实现原理) posix提供了用户态上下文切换的API #include int getcontext(ucontext_t *ucp); int setcontext(const ucontext_t *ucp); void makecontext(ucontext_t *ucp, void (*func)(), int argc, ...); int swapcontext(ucontext_t *oucp, const ucontext_t *ucp); man getcontext 用户态上下文 ucontext_t描述了用户态上下文: 其中mcontext_t是个硬件相关的结构 typedef struct ucontext_t { struct ucontext_t *uc_link; sigset_t uc_sigmask; stack_t uc_stack; mcontext_t uc_mcontext; ... } ucontext_t; getcontext()函数把当前的上下文保存在ucp指针指向的ucontext_t中 setcontext()函数恢复到ucp指向的上下文, 然后从那个上下文执行. 这个函数不retrun. makecontext()函数新生成一个上下文, 并指定在这个上下文中执行的func sighandler也可以返回一个上下文 例子 下面的程序不断打印\"hello world\" 因为第10行转而在第7行保存的上下文中执行, 效果就像直接goto到第8行一样. #include #include #include int main(int argc, char *argv[]) { ucontext_t context; getcontext(&context); puts(\"Hello world\"); sleep(1); setcontext(&context); return 0; } 使用ucontext.h的api实现用户态协程 基于ucontext.h的轻量级协程库 协程可以理解为一种用户态的轻量级线程, 切换由用户定义 协程上下文切换很快, 因为不会陷入内核态 协程拥有自己的寄存器上下文和栈, 协程调度切换时，将寄存器上下文和栈保存到其他地方，在切换回来的时候，恢复先前保存的寄存器上下文和栈 协程具有极高的执行效率 因为子程序切换不是线程切换，是由程序自身控制，因此协程没有线程切换的开销, 多线程的线程数量越多，协程的性能优势就越明显 访问共享资源不需要多线程的锁机制, 因为只有一个线程, 也不存在同时写变量冲突, 所以在协程中控制共享资源无需加锁, 只需要判断状态就好了，执行效率比多线程高很多, 而且代码编写难度也可以相应降低 以同步代码的方式写异步逻辑 无法利用多核资源, 除非和多进程配合 多线程的情况下, signal被deliver到哪个线程? 问答: https://stackoverflow.com/questions/11679568/signal-handling-with-multiple-threads-in-linux 先准备几个知识: 所有线程都在一个进程空间 signal都是先入queue, 待线程被调度到运行时再执行的. signal是共享的, 但每个thread可以有自己的maskpthread_sigmask(3) 内核里deliver signal的代码: /* * Now find a thread we can wake up to take the signal off the queue. * * If the main thread wants the signal, it gets first crack. * Probably the least surprising to the average bear. */ if (wants_signal(sig, p)) t = p; else if (!group || thread_group_empty(p)) /* * There is just one thread and it does not need to be woken. * It will dequeue unblocked signals before it runs again. */ return; else { /* * Otherwise try to find a suitable thread. */ t = signal->curr_target; while (!wants_signal(sig, t)) { t = next_thread(t); if (t == signal->curr_target) /* * No thread needs to be woken. * Any eligible threads will see * the signal in the queue soon. */ return; } signal->curr_target = t; } /* * Found a killable thread. If the signal will be fatal, * then start taking the whole group down immediately. */ if (sig_fatal(p, sig) && !(signal->flags & SIGNAL_GROUP_EXIT) && !sigismember(&t->real_blocked, sig) && (sig == SIGKILL || !p->ptrace)) { /* * This signal will be fatal to the whole group. */ 结论: 默认是deliver给main thread, 如果main thread不want这个signal, 就尝试下一个thread 用户可以用pthread_sigmask(3)设置thread的mask, 从而让signal被deliver到特定的thread. 可以向指定的thread发signal. 见pthread_kill(3) tgkill(2) 一些同步异常, 比如SIGSEGV和SIGFPE, 是由当前thread的某个指令引起的, 那么signal就直接被deliver到这个线程. 有些signal是以进程为单位产生的, 理论上会被deliver到任意一个线程. 但参考第一条, 通常是deliver到main thread. 信号处理函数执行的上下文是什么? 为什么能打印当前进程的调用栈? 目前已知的知识, 以golang的signal处理为例: SIGKILL and SIGSTOP不能被捕获 同步的signal, 一般是SIGBUS, SIGFPE, and SIGSEGV, 是由正在执行的go程序引起的 在go里, 这些signal被转换为运行时的panic 剩下的signal, 是其他进程异步通知的signal, 用os/signal包来处理 经过实验得到的现象, 还是以golang为例: case 1: 对于一个纯用户态循环, ctrl+c(SIGINT)能够立即终止该进程 for { i++ } 从shell执行kill -SIGQUIT命令发送SIGQUIT信号给目标进程, 目标进程的call stack能精确定位到for循环里的i++那一行 $ ./signal hello ^\\SIGQUIT: quit PC=0x48cfd5 m=0 sigcode=128 goroutine 1 [running]: main.main() /repo/yingjieb/godev/practice/src/signal/main.go:15 +0x75 fp=0xc0000aef60 sp=0xc0000aef00 pc=0x48cfd5 runtime.main() /usr/local/go/src/runtime/proc.go:203 +0x206 fp=0xc0000aefe0 sp=0xc0000aef60 pc=0x42b136 runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:1357 +0x1 fp=0xc0000aefe8 sp=0xc0000aefe0 pc=0x453651 case 2: 对上面的for稍加一句sleep for { i++ time.Sleep(time.Second * 10) } 发送SIGQUIT也一样能够立即打印调用栈; 不意外的, ctrl+c也能够立即终止这个进程. 都不受sleep的干扰. 调用栈显示两个goroutine(实际上, 如果有环境变量GOTRACEBACK=system, 能显示更多goroutine), sleep的调用栈就是main程序当前的代码. 结合代码和现象来看, 这个进程在收到SIGQUIT时, 大概率是在sleep, 没有在运行. 这时操作系统发现有人发送SIGQUIT给该进程, 就执行该进程的sighandler. 在本例中, 这个sighandler就是golang默认的处理. $ ./signal hello ^\\SIGQUIT: quit PC=0x455813 m=0 sigcode=128 goroutine 6 [syscall]: runtime.notetsleepg(0x5613a0, 0x2540bc392, 0x0) /usr/local/go/src/runtime/lock_futex.go:227 +0x34 fp=0xc000064760 sp=0xc000064730 pc=0x409d04 runtime.timerproc(0x561380) /usr/local/go/src/runtime/time.go:311 +0x2f1 fp=0xc0000647d8 sp=0xc000064760 pc=0x4450b1 runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:1357 +0x1 fp=0xc0000647e0 sp=0xc0000647d8 pc=0x453911 created by runtime.(*timersBucket).addtimerLocked /usr/local/go/src/runtime/time.go:169 +0x10e goroutine 1 [sleep]: runtime.goparkunlock(...) /usr/local/go/src/runtime/proc.go:310 time.Sleep(0x2540be400) /usr/local/go/src/runtime/time.go:105 +0x157 main.main() /repo/yingjieb/godev/practice/src/signal/main.go:24 +0x88 golang对于各种signal, 都有默认的sighandler, 那么如标题的问题 sighandler一般被认为是异步的方式执行, 和正常的进程代码是两回事. 那么sighandler究竟是在什么上下文执行的? 以SIGQUIT的handler为例, 为什么一个异步执行的handler, 能够知道正常代码的调用栈? 结合case 1和2, sighandler被调用的时机是怎样的? case 2中, 大概率是在进程没有被执行的时候发生了SIGQUIT. 但case 1中, 进程死循环做i++, 会用尽调度器分给它的时间片. 那么sighandler又是什么时候执行呢? sighandler执行的上下文 程序运行在用户态时->进程由于系统调用或中断进入内核->转向用户态执行信号处理函数->信号处理函数完毕后进入内核->返回用户态继续执行程序首先程序执行在用户态，在进程陷入内核并从内核返回的前夕，会去检查有没有信号没有被处理，如果有且没有被阻塞就会调用相应的信号处理程序去处理。首先，内核在用户栈上创建一个层，该层中将返回地址设置成信号处理函数的地址，这样，从内核返回用户态时，就会执行这个信号处理函数。当信号处理函数执行完，会再次进入内核，主要是检测有没有信号没有处理，以及恢复原先程序中断执行点，恢复内核栈等工作，这样，当从内核返回后便返回到原先程序执行的地方了。 关键点在kernel在收到signal的时候, 会在用户栈上新建一个栈帧, 作用是给sighandler提供运行上下文. 我理解, 如果这个进程正在运行(在另外一个核上), 内核应该会把它调度出去, 再建立sighandler的栈帧. sigaltstack函数用于指定sighandler栈 man sigaltstack解释到: sigaltstack用于显式建立一个栈帧. 默认情况下, kernel会在用户栈上建立这个栈帧, 但对于用户栈溢出造成的SIGSEGV的情况, 在用户栈上的sighandler也就不能执行了. sigaltstack()函数用于这种情况, 在别处指定这个栈帧. #include int sigaltstack(const stack_t *ss, stack_t *old_ss); The most common usage of an alternate signal stack is to handle the SIGSEGV signal that is generated if the space available for the normal process stack is exhausted: in this case, a signal handler for SIGSEGV cannot be invoked on the process stack; if we wish to handle it, we must use an alternate signal stack. Establishing an alternate signal stack is useful if a process expects that it may exhaust its standard stack. This may occur, for example, because the stack grows so large that it encounters the upwardly growing heap, or it reaches a limit established by a call to setrlimit(RLIMIT_STACK, &rlim). If the standard stack is exhausted, the kernel sends the process a SIGSEGV signal. In these circumstances the only way to catch this signal is on an alternate signal stack. 回答 sighandler一般被认为是异步的方式执行, 和正常的进程代码是两回事. 那么sighandler究竟是在什么上下文执行的?答: 默认在用户栈上新建新的栈帧来执行. 可以用sigaltstack改变这个栈帧的位置 以SIGQUIT的handler为例, 为什么一个异步执行的handler, 能够知道正常代码的调用栈?答: handler执行的时候, 是异步的. 此时\"正常\"的进程代码位置应该可以通过上下文的PC指针查到, 那么就可以栈回溯. 结合case 1和2, sighandler被调用的时机是怎样的? case 2中, 大概率是在进程没有被执行的时候发生了SIGQUIT. 但case 1中, 进程死循环做i++, 会用尽调度器分给它的时间片. 那么sighandler又是什么时候执行呢?答: 内核在返回用户态进程的时候, 会执行sighandler. 从实验结果来看, 向进程发送信号会唤醒这个进程. 信号处理原理 signal原理讲义 sigaction sigaction结构体定义了handler的形式: 第三个参数就是ucontext pending和blocked向量 kernel给每个进程维护这两个向量 顾名思义, pending向量是要发给目标进程的向量表; 而blocked向量是不允许发送给进程的向量表. 当一个signal已经被deliver到进程, 该signal会自动被kernel放到blocked向量, 阻止进程在处理singal的时候, 又被同类型signal中断. 类似于关中断. 但不同类型的signal可以打断当前的siganl handler函数. signal的产生和投递 signal产生时, kernel要填的结构体 产生和投递是两个过程 产生signal是填一些结构体, 然后把进程状态转为ready(如果之前是睡眠) 投递signal到进程, 进程必须拥有CPU执行权才能运行其handler 默认的handler由内核执行, 自定义的handler必须等到用户态执行 handler执行完还要回到内核态 signal可以打断系统调用 handler可以调用系统调用, 系统调用返回后还是回到handler上下文 handler可以调用siglongjmp()来跳转到用户态的其他部分代码, 但执行上下文还在handler? SIGCHLD默认是ignore的, handler是SIGIGN也是要被跳过的.![](img/system原理杂记_20220829152133.png) 默认的handler SIG_DFL在内核执行: 不到用户态 不是内核处理的signal, 内核要唤醒这个进程到其用户态处理. 不能简单的把sighandler设为这次返回用户态的入口, 而是要为sighandler建立自己的上下文; 有一部分的上下文是从kernel栈拷到用户栈的. 新建的sighandler栈帧在用户态栈上, 称为uctxt handler返回的时候, 要返回到内核态. 这是通过一个间接的sigreturn系统调用实现的.man sigreturn说的很清楚: 现代linux系统上, 是vdso或libc提供的sigreturn wrapper, 它的作用是利用之前保存在栈上的相关信息, undo所有之前为sig handler运行做的准备工作, 回复进程被signal中断之前的上下文.do_signal是给用户的sighandler设置运行环境, 实际的handler不是在它里面执行的, 而是后面内核态切换到用户态时, 因为eip的改变, 导致用户的sighandler被执行. sighandler可以执行系统调用, 没有任何问题. signal和系统调用 系统调用会被signal打断, 内核需要返回EINTR来指示系统调用被打断了. 被打断的read和write可能会被内核重新执行. 可以配置是返回EINTR还是rerun 进程在系统调用期间收到signal, 那它的之前都在内核态. siglongjmp 调用siglongjmp会导致handler退出, 并把执行上下文交给用户态进程的那部分代码段. siglongjmp和longjmp差不多, 只是多了一些sig mask的操作. siglongjmp并没有破环内核的sig投递 执行 返回的流程. futex系统调用 man futex linux的futex可以当作比较-阻塞的原子操作使用. #include #include int futex(int *uaddr, int futex_op, int val, const struct timespec *timeout, /* or: uint32_t val2 */ int *uaddr2, int val3); Note: There is no glibc wrapper for this system call; see NOTES. int *uaddr是个用户提供的地址, 其值时32位的, 即使在64位机器上, 也是32位. 这个地址可以在共享内存中, 比如用mmap(2) or shmat(2)创建的共享内存, 这样不同的进程也可以使用同一个futex, futex在内核中看的是这个指针的物理地址. futex支持像epoll等系统调用的超时机制. 当futex_op可以是FUTEX_WAIT也可以是FUTEX_WAKE, 即futex有wait和wakeup两种功能. FUTEX_WAIT时, futex比较这个地址指向的32位值, 如果和val相等则休眠; 不等的话, 马上返回, errorno为EAGAIN. FUTEX_WAKE时, 唤醒val个等待在uaddr上的线程. 通常val为1个随机的线程, 或者所有(INT_MAX)的线程. libevent主循环处理timer 在libevent/event.c int event_base_loop(struct event_base *base, int flags) while (!done) { //没有event则退出循环 //从定时器堆里算下次超时时间 timeout_next(base, &tv_p); //调用底层poll, 对linux来说, 是epoll res = evsel->dispatch(base, tv_p); //handle 定时器堆, 把有效的(active)的定时器回调函数加入base的active队列, 带优先级的入队列 timeout_process(base); //真正执行active队列里面的回调; int n = event_process_active(base); } /* Activate every event whose timeout has elapsed. */ static void timeout_process(struct event_base *base) { gettime(base, &now); //获取每个timer while ((ev = min_heap_top_(&base->timeheap))) { if (evutil_timercmp(&ev->ev_timeout, &now, >)) break; /* delete this event from the I/O queues */ event_del_nolock_(ev, EVENT_DEL_NOBLOCK); event_debug((\"timeout_process: event: %p, call %p\", ev, ev->ev_callback)); //正如上面打印的提示一样, 执行每个timer的回调. event_active_nolock_(ev, EV_TIMEOUT, 1); //实际上这里是加入到一个链表: base->activequeues, 由event_process_active()执行这个链表 } } 再议rm rm实际上是unlink调用, 实际上是减小文件的link计数. 如果link减小到0, 而且没有进程open它, 文件会被删除. 如果link减小到0, 但有进程在使用它, 那么文件在进程close它之前都存在. 见man 2 unlink 实验中, 我用vim打开一个文件, 在另外一个窗口rm这个文件, rm没有返回任何错误, 文件已经从文件系统不可见. 但实际上, 这个文件还存在, vim依旧可以访问它.普通用户可以rm root用户的文件 在一次实验中, 我在test文件夹中, 用root账户创建了文件aaa, 但退出root后, 用普通账户就能删除aaa文件.yingjieb@yingjieb-VirtualBox ~/test Linux Mint 19.1 Tessa $ ll total 0 yingjieb@yingjieb-VirtualBox ~/test Linux Mint 19.1 Tessa $ sudo touch aaa yingjieb@yingjieb-VirtualBox ~/test Linux Mint 19.1 Tessa $ ll total 0 -rw-r--r-- 1 root root 0 Mar 4 21:12 aaa yingjieb@yingjieb-VirtualBox ~/test Linux Mint 19.1 Tessa $ rm -f aaa yingjieb@yingjieb-VirtualBox ~/test Linux Mint 19.1 Tessa $ ll total 0 怎么, aaa是-rw-r--r--权限, 理论上不应该能被普通用户删除呀? 有人问了这个问题, 见: https://superuser.com/questions/1336951/user-can-delete-root-owned-files-in-their-home-directory-or-what-are-the-rules其实原理是:目录和文件的关系是, 目录是对其下文件的link, 所以shell命令rm其实是unlink调用, unlink减小对文件的link引用数.如果引用数减到0, 这个文件就没人引用了, 就被删除了.删除一个文件, 并不是作用于这个文件, 而是作用于它的目录, 减小目录对文件的引用.所以, 在本例中, rm作用于目录~/test, 这是个用户有权限访问的目录. rm操作和文件aaa的权限没有关系. 关于热升级, 正在使用的文件可以被rm 比如ubuntu系统在update的时候, 一般是不需要重启的. 但既然要升级, 就必然要替换掉原来的bin或者so之类的文件, 又要app不重启, 怎么做到的呢? 以vim为例, 当前正在打开vim窗口编辑, 同时在另外一个窗口升级vim, 升级成功了, 原来打开的vim还能继续使用. 那打开的vim是新版本还是老版本呢? --是老版本 这主要是文件系统的工作, 每个打开的文件, 都有个文件句柄, 这个句柄是这个文件的一个访问实例; 句柄里保存了文件的实体(inode)在文件系统中的引用. 升级的过程, 是先删除旧文件, 再写入新文件, 虽然文件名相同, 但inode不同. 已经打开的旧文件, 在其句柄里保存的inode, 指向的文件\"看起来\"不存在了, 但实际还在文件系统里. 所有对这个老的inode的引用结束后, 文件系统把这部分在磁盘上的实体空间标记为空闲. #打开vim, 进程是28919 #pmap发现其mmap的文件 Linux Mint 19.1 Tessa $ pmap 28919 28919: vim drivers/spi/spidev.c 0000556832e5d000 2940K r-x-- vim.gtk 000055683333b000 64K r---- vim.gtk 000055683334b000 104K rw--- vim.gtk #此时, 对vim.gtk的open是不能写的, 因为写是对同一个inode操作. Linux Mint 19.1 Tessa $ sudo dd if=/dev/zero of=/usr/bin/vim.gtk bs=4M count=1 dd: failed to open '/usr/bin/vim.gtk': Text file busy yingjieb@yingjieb-VirtualBox ~ Linux Mint 19.1 Tessa $ sudo cp /usr/bin/x86_64-linux-gnu-gcc-7 /usr/bin/vim.gtk cp: cannot create regular file '/usr/bin/vim.gtk': Text file busy #但是可以删除 Linux Mint 19.1 Tessa $ sudo rm -f /usr/bin/vim.gtk #此时pmap能知道这个文件被删除了 #但28919进程的vim还能继续使用, 因为1. 其文件并没有真正消亡. 2. 文件被装载到内存里了(page). #我估计这两项都为真. 文件被mmap到进程内存空间, 即使只有部分文件被page了, 访问另外的文件部分, 会有page fault, 进而通过文件系统装载文件内容到物理页. #我认为即使page fault, 也会访问到老的inode的文件, 也会成功 -- 未验证 Linux Mint 19.1 Tessa $ pmap 28919 28919: vim drivers/spi/spidev.c 0000556832e5d000 2940K r-x-- vim.gtk (deleted) 000055683333b000 64K r---- vim.gtk (deleted) 000055683334b000 104K rw--- vim.gtk (deleted) #也可以mv Linux Mint 19.1 Tessa $ sudo mv /usr/bin/vim.gtk /usr/bin/vim.gtk.back #此时pmap知道inode没变, 文件变了. Linux Mint 19.1 Tessa $ pmap 28919 28919: vim drivers/spi/spidev.c 0000556832e5d000 2940K r-x-- vim.gtk.back 000055683333b000 64K r---- vim.gtk.back 000055683334b000 104K rw--- vim.gtk.back #删除vim.gtk后, 可以新建个同名文件; 但新文件有新的inode Linux Mint 19.1 Tessa $ sudo cp vim.gtk.save /usr/bin/vim.gtk Linux Mint 19.1 Tessa $ llh /usr/bin/vim.gtk -rwxr-xr-x 1 root root 3.1M Jun 13 17:08 /usr/bin/vim.gtk #pmap依然显示vim.gtk是已经删除状态, 因为这里用的老的inode Linux Mint 19.1 Tessa $ pmap 28919 28919: vim drivers/spi/spidev.c 0000556832e5d000 2940K r-x-- vim.gtk (deleted) 000055683333b000 64K r---- vim.gtk (deleted) 000055683334b000 104K rw--- vim.gtk (deleted) 结论: mv文件不改变文件的inode, 只改变文件名; cp新文件到正在打开的文件是非法的, 因为cp要open这个目的文件, 是对同一个inode操作. 如果合法, 那正在执行的文件, 其内容更改会引发未知错误. rm打开的文件是可以的, 或者说\"看起来\"是马上生效的, 但实际上, 如果老的inode还在被引用, 还是能被文件系统访问到. 升级正在运行的可执行文件, 比如vim.gtk, 要制造新的inode, 具体是: 先rm, 再cp成同名文件 升级后的文件, 只有app重新加载的时候才生效. 用户态通过系统调用陷入到内核态, 内存映射会变吗? 不会 You've got the general idea mostly right, but make this adjustment: there's only one \"kernelspace\" for the whole machine, and all processes share it. When a process is active, it can either be running in \"user mode\" or \"kernel mode\". In user mode, the instructions being executed by the CPU are in the userspace side of the memory map. The program is running its own code, or code from a userspace library. In user mode, a process has limited abilities. There is a flag in the CPU which tells it not to allow the use of privileged instructions, and kernel memory, although it exists in the process's memory map, is inaccessible. (You wouldn't want let any program just read and write the kernel's memory - all security would be gone.) When a process wants to do something other than move data around in its own (userspace) virtual memory, like open a file for example, it must make a syscall. Each CPU architecture has its own unique quirky method of making syscalls, but they all boil down to this: a magic instruction is executed, the CPU turns on the \"privileged mode\" flag, and jumps to a special address in kernelspace, the \"syscall entry point\". Now the process is running in kernel mode. Instructions being executed are located in kernel memory, and they can read and write any memory they want to. The kernel examines the request that the process just made and decides what to do with it. In the open example, the kernel receives 2 or 3 parameters corresponding to the arguments of int open(const char *filename, int flags[, int mode]). The first argument provides an example of when kernelspace needs access to userspace. You said open(\"foo\", O_RDONLY) so the string \"foo\" is part of your program in userspace. The syscall mechanism only passed a pointer, not a string, so the kernel must read the string from user memory. To find the requested file, the kernel may consult with filesystem drivers (to figure out where the file is) and block device drivers (to load the necessary blocks from disk) or network device drivers and protocols (to load the file from a remote source). All of those things are part of the kernel, i.e. in kernelspace, regardless of whether they are built-in or were loaded as modules. If the request can't be satisfied immediately, the kernel may put the process to sleep. That means the process will be taken off the CPU until a response is received from the disk or network. Another process may get a chance to run now. Later, when the response comes in, your process starts running again (still in kernel mode). Now that it's found the file, the open syscall can finish up (check the permissions, create a file descriptor) and return to userspace. Returning to userspace is a simple matter of putting the CPU back in non-privileged mode and restoring the registers to what they were before the user->kernel transition, with the instruction pointer pointing at the instruction after the magic syscall instruction. Besides syscalls, there are other things that can cause a transition from user mode to kernel mode, including: page faults - if your process accesses a virtual memory address that doesn't have a physical address assigned to it, the CPU enters kernel mode and jumps to the page fault handler. The kernel then decides whether the virtual address is valid or not, and it either creates a physical page and resumes the process in userspace where it left off, or sends a SIGSEGV. interrupts - some hardware (network, disk, serial port, etc.) notifies the CPU that it requires attention. The CPU enters kernel mode and jumps to a handler, the kernel responds to it and then resumes the userspace process that was running before the interrupt. Loading a module is done with a syscall that asks the kernel to copy the module's code and data into kernelspace and run its initialization code in kernel mode. This is pretty long, so I'm stopping. I hope the walk-through focusing on user-kernel transitions has provided enough examples to solidify the idea. uboot传mtdpart的时候，名字从哪来的？ 比如uboot会传cmdline给内核 mtdparts=octeon_nand0:0x20000000@0x0(nand);bootflash:0x20000@0x140000(statusA),0x20000@0x160000(statusB),0x140000@0x180000(bootA),0x140000@0x2c0000(bootB),0x1900000@0x400000(linuxA),0x1900000@0x1d00000(linuxB) 那么octeon_nand0和bootflash怎么来的？ 下面是内核的启动记录： [02.11] [ 30.741082] Bootbus flash: Setting flash for 64MB flash at 0x1bc00000 [02.11] [ 30.760360] bootflash: Found 1 x16 devices at 0x0 in 8-bit bank. Manufacturer ID 0x0000c2 Chip ID 0x00007e [02.11] [ 30.782739] Amd/Fujitsu Extended Query Table at 0x0040 [02.11] [ 30.800628] Amd/Fujitsu Extended Query version 1.3. [02.11] [ 30.818400] number of CFI chips: 1 [02.11] [ 30.834545] 6 cmdlinepart partitions found on MTD device bootflash [02.11] [ 30.853523] Creating 6 MTD partitions on \"bootflash\": [02.11] [ 30.871305] 0x000000140000-0x000000160000 : \"statusA\" [02.11] [ 30.889434] 0x000000160000-0x000000180000 : \"statusB\" [02.11] [ 30.907519] 0x000000180000-0x0000002c0000 : \"bootA\" [02.11] [ 30.925437] 0x0000002c0000-0x000000400000 : \"bootB\" [02.11] [ 30.943334] 0x000000400000-0x000001d00000 : \"linuxA\" [02.11] [ 30.961321] 0x000001d00000-0x000003600000 : \"linuxB\" [02.11] [ 30.979710] cvmx_nand_initialize: Setting timing parameter mode to 0 [02.11] [ 30.998826] octeon-nand 1070001000000.nand-flash-interface: NAND using BCH ecc [02.11] [ 31.018967] NAND 1 OOB size: 64, write size: 2048, erase size: 131072 [02.11] [ 31.038133] NAND device: Manufacturer ID: 0x2c, Chip ID: 0xf1 (Micron NAND 128MiB 3,3V 8-bit), 128MiB, page size: 2048, OOB size: 64 [02.11] [ 31.063631] Scanning device for bad blocks [02.11] [ 31.159228] mtd: octeon_nand0: partitioning exceeds flash size, truncating [02.11] [ 31.178825] 1 cmdlinepart partitions found on MTD device octeon_nand0 [02.11] [ 31.197990] Creating 1 MTD partitions on \"octeon_nand0\": [02.11] [ 31.216023] 0x000000000000-0x000008000000 : \"nand\" [02.11] [ 31.238898] Freeing unused kernel memory: 8952K (ffffffff80792000 - ffffffff81050000) 先说bootflash：在arch/mips/cavium-octeon/flash_setup.c中， 是cfi_flash的驱动 在它的probe函数里面，写死了名字：flash_map.name = \"bootflash\";并调用mtd_device_parse_register来解析mtd分区， 传入type cmdlinepart，意思是优先使用cmdlinepart来解析mtd分区 实际上，mtd支持两种，按顺序是\"cmdlinepart\"和\"ofpart\"这个函数里面调用：parse_mtd_partitions会根据以上两种规则创建mtd分区。 另外：uboot里面也有关于mtd的定义，估计是给uboot自己看的。 #define MTDPARTS_DEFAULT \\ \"octeon_nor0:2560k(bootloader)ro,\" \\ \"2m(kernel),\" \\ \"3520k(cramfs),\" \\ \"64k(environment)ro\\0\" #define MTDIDS_DEFAULT \"nor0=octeon_nor0\\0\" 为什么直接考过来的ls不能用？ mount debian的rootfs，从下面考了一个perl但不能直接运行，提示not found。 其实不是这个可执行文件找不到，而是它依赖的动态库不满足。本质上还是个依赖问题。 可以在编译的时候用-static来编成静态链接，这样随便考到哪里都能用了。 fork与malloc 问题: 在fork之前, 父进程malloc了内存, 比如char *p; 那么: 子进程能正常访问p吗? 内容与父进程一样吗? 子进程修改了p内存的内容, 父进程能看到吗? 子进程需要free(p)吗? 回答: #include #include #include #include #include int main(void) { char *something = malloc(256); sprintf(something, \"hello\"); pid_t child_pid = fork(); if (child_pid == 0) //child { /*子进程能够访问这个指针, 并且内容一样*/ printf(\"from child:%s\\n\", something); /*子进程修改这块内存, 但不影响父进程 C-O-W*/ sprintf(something, \"change to 11111\"); printf(\"from child:%s\\n\", something); /*虽然父子进程是独立的进程空间, 但这里指针的地址却是一样的*/ printf(\"from child:addr %p\\n\", something); /*如果不调用exec*函数, 和exit*函数, 那么确实需要在子进程也free, 所以这个例子里free应该写在最后return之前, 这样父子都都要调用*/ free(something); } else //parent { int status; while(wait(&status) != child_pid); printf(\"from parent:%s\\n\", something); printf(\"from parent:addr %p\\n\", something); free(something); } printf(\"dddddddddddddddddd\\n\"); return 0; } 运行结果 $ gcc test_fork_malloc.c ASBLX28:/home/yingjieb/tmp $ ./a.out from child:hello from child:change to 11111 from child:addr 0x84ec010 dddddddddddddddddd from parent:hello from parent:addr 0x84ec010 dddddddddddddddddd "},"notes/system_alpine.html":{"url":"notes/system_alpine.html","title":"Alpine Linux","keywords":"","body":" 现代化的工程系统 使用subgroup来组织repo 组织清爽, 源代码干净 aports 交叉编译 openrc musl libc 现代化的工程系统 alpine linux的全部开发都在https://gitlab.alpinelinux.org/alpinemirror: https://git.alpinelinux.org/ 自己搭建的gitlab服务器, 允许外部用户注册, fork库, 并提交MR 使用gitlab-ci的CI/CD做build test 用gitlab issue来跟踪bug 文档也是repo管理, 使用Antora Playbook发布, 网页入口是 https://alpinelinux.org/ 使用subgroup来组织repo 比如CI/CD工具库在alpine/infra/docker/alpine-gitlab-ci下面, 先是根alpine, 再是infra, 再是docker, 最后是repo 组织清爽, 源代码干净 比如alpine-gitlab-ci/-/blob/master/overlay/usr/local/bin/build.sh里面的shell 输出代码: : \"${CI_ALPINE_BUILD_OFFSET:=0}\" : \"${CI_ALPINE_BUILD_LIMIT:=9999}\" msg() { local color=${2:-green} case \"$color\" in red) color=\"31\";; green) color=\"32\";; yellow) color=\"33\";; blue) color=\"34\";; *) color=\"32\";; esac printf \"\\033[1;%sm>>>\\033[1;0m %s\\n\" \"$color\" \"$1\" | xargs >&2 } verbose() { echo \"> \" \"$@\" # shellcheck disable=SC2068 $@ } debugging() { [ -n \"$CI_DEBUG_BUILD\" ] } debug() { if debugging; then verbose \"$@\" fi } die() { msg \"$1\" red exit 1 } capture_stderr() { \"$@\" 2>&1 } report() { report=$1 reportsdir=$APORTSDIR/logs/ mkdir -p \"$reportsdir\" tee -a \"$reportsdir/$report.log\" } aports alpine支持的package都放在aports这个库下面. main: alpine core team直接支持的package community: 由社区支持的package 参考: https://wiki.alpinelinux.org/wiki/Repositories /etc/apk/repositories是package的配置 / # cat /etc/apk/repositories https://dl-cdn.alpinelinux.org/alpine/v3.15/main https://dl-cdn.alpinelinux.org/alpine/v3.15/community 比如https://dl-cdn.alpinelinux.org/alpine/v3.15/main目录下包括了所有arch的预编译好的apk点进去看这些apk的修改时间是不一样的, 说明apk是按需编译的. 交叉编译 似乎可以用bootstrap.sh来生成交叉编译的工具链参考: musl-cross-make openrc alpine使用openrchttps://wiki.alpinelinux.org/wiki/OpenRC musl libc alpine使用musl libc, 我看好musl的轻量简洁.几种libc的比较 "},"notes/system_进程间通信.html":{"url":"notes/system_进程间通信.html","title":"进程间通信","keywords":"","body":" ipcs查看进程间通信的情况, 包括消息队列, 共享内存, semaphore 进程间通信类型 signal 匿名管道 有名管道和FIFO 消息队列 共享内存 信号量 futex Unix domain socket Netlink socket Network socket Inotify机制 FUSE文件系统 D-BUS 参考: http://www.chandrashekar.info/articles/linux-system-programming/introduction-to-linux-ipc-mechanims.html https://tldp.org/LDP/tlk/ipc/ipc.html#:~:text=1%20System%20V%20IPC%20Mechanisms,all%20share%20common%20authentication%20methods. ipcs查看进程间通信的情况, 包括消息队列, 共享内存, semaphore Linux Mint 19 Tara $ ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x00000000 65536 bai 600 524288 2 dest 0x00000000 163841 bai 600 33554432 2 dest 0x00000000 196610 bai 600 33554432 2 dest 0x00000000 294915 bai 600 524288 2 dest 0x00000000 393220 bai 600 524288 2 dest 0x00000000 491525 bai 600 4194304 2 dest 0x00000000 524294 bai 777 2006712 2 0x00000000 622599 bai 777 3035136 2 0x00000000 655368 bai 600 4194304 2 dest 0x00000000 753673 bai 600 524288 2 dest 0x00000000 786442 bai 600 1048576 2 dest 0x00000000 884747 bai 600 524288 2 dest 0x00000000 917516 bai 777 2304 2 0x00000000 950285 bai 600 33554432 2 dest ------ Semaphore Arrays -------- key semid owner perms nsems 进程间通信类型 signal overhead最小, 用来通知进程关于内核和其他进程状态的改变 内核打断进程正常的流程, 调用其注册的handler或者默认的handler Signals are the cheapest forms of IPC provided by Linux. Their primary use is to notify processes of change in states or events that occur within the kernel or other processes. We use signals in real world to convey messages with least overhead - think of hand and body gestures. For example, in a crowded gathering, we raise a hand to gain attention, wave hand at a friend to greet and so on. On Linux, the kernel notifies a process when an event or state change occurs by interrupting the process's normal flow of execution and invoking one of the signal handler functinos registered by the process or by the invoking one of the default signal dispositions supplied by the kernel, for the said event. 匿名管道 生成两个描述符分别用于读和写 用于父子进程, 父进程创建管道, 在folk的时候, 这个管道被dup进子进程的空间 Anonymous pipes (or simply pipes, for short) provide a mechanism for one process to stream data to another. A pipe has two ends associated with a pair of file descriptors - making it a one-to-one messaging or communication mechanism. One end of the pipe is the read-end which is associated with a file-descriptor that can only be read, and the other end is the write-end which is associated with a file descriptor that can only be written. This design means that pipes are essentially half-duplex. Anonymous pipes can be setup and used only between processes that share parent-child relationship. Generally the parent process creates a pipe and then forks child processes. Each child process gets access to the pipe created by the parent process via the file descriptors that get duplicated into their address space. This allows the parent to communicate with its children, or the children to communicate with each other using the shared pipe. Pipes are generally used to implement Producer-Consumer design amongst processes - where one or more processes would produce data and stream them on one end of the pipe, while other processes would consume the data stream from the other end of the pipe. 有名管道和FIFO 在两个独立的进程间, 打开一个特定的FIFO文件来通信 Named pipes (or FIFO) are variants of pipe that allow communication between processes that are not related to each other. The processes communicate using named pipes by opening a special file known as a FIFO file. One process opens the FIFO file from writing while the other process opens the same file for reading. Thus any data written by the former process gets streamed through a pipe to the latter process. The FIFO file on disk acts as the contract between the two processes that wish to communicate. 消息队列 类似于邮箱, 一个进程写消息然后退出, 另一个进程可以从同一个消息队列里读消息 通信双方不需要建立连接, 而作为对比, pipe是需要先建立连接的. 支持多对多 Message queues allow one or more processes to write messages, which will be read by one or more reading processes linux支持两种消息队列 system V: 带message号 posix: 带message优先级 man mq_overview mq_open mq_send mq_receive等函数都是系统调用 Message Queues are synonymous to mailboxes. One process writes a message packet on the message queue and exits. Another process can access the message packet from the same message queue at a latter point in time. The advantage of message queues over pipes/FIFOs are that the sender (or writer) processes do not have to wait for the receiver (or reader) processes to connect. Think of communication using pipes as similar to two people communicating over phone, while message queues are similar to two people communicating using mail or other messaging services. There are two standard specifications for message queues. SysV message queues. The AT&T SysV message queues support message channeling. Each message packet sent by senders carry a message number. The receivers can either choose to receive message that match a particular message number, or receive all other messages excluding a particular message number or all messages. POSIX message queues. The POSIX message queues support message priorities. Each message packet sent by the senders carry a priority number along with the message payload. The messages get ordered based on the priority number in the message queue. When the receiver tries to read a message at a later point in time, the messages with higher priority numbers get delivered first. POSIX message queues also support asynchronous message delivery using threads or signal based notification. 共享内存 一个进程把自己进程空间的一部分共享给另一个. 有两种类型: sysv: 比较古老 posix: 现代的, 使用ram文件系统的文件 As the name implies, this IPC mechanism allows one process to share a region of memory in its address space with another. This allows two or more processes to communicate data more efficiently amongst themselves with minimal kernel intervention. There are two standard specifications for Shared memory. SysV Shared memory. Many applications even today use this mechanism for historical reasons. It follows some of the artifacts of SysV IPC semantics. POSIX Shared memory. The POSIX specifications provide a more elegant approach towards implementing shared memory interface. On Linux, POSIX Shared memory is actually implemented by using files backed by RAM-based filesystem. I recommend using this mechanism over the SysV semantics due to a more elegant file based semantics. 信号量 Semaphores are locking and synchronization mechanism used most widely when processes share resources. Linux supports both SysV semaphores and POSIX semaphores. POSIX semaphores provide a more simpler and elegant implementation and thus is most widely used when compared to SysV semaphores on Linux. futex linux系统调用 Futexes are high-performance low-overhead locking mechanisms provided by the kernel. Direct use of futexes is highly discouraged in system programs. Futexes are used internally by POSIX threading API for condition variables and its mutex implementations. Unix domain socket C-S架构, 全双工, 支持stream和datagram方式 大型软件用的很多 UNIX Domain Sockets provide a mechanism for implementing applications that communicate using the Client-Server architecture. They support both stream and datagram oriented communication, are full-duplex and support a variety of options. They are very widely used for developing many large-scale frameworks. Netlink socket 和socket接口一样, 但主要用于: 内核线程和用户进程通信 用户控件的进程间的广播通信 Netlink sockets are similar to UNIX Domain Sockets in its API semantics - but used mainly for two purposes: For communication between a process in user-space to a thread in kernel-space For communication amongst processes in user-space using broadcast mode. Network socket 网络socket Based on the same API semantics like UNIX Domain Sockets, Network Sockets API provide mechanisms for communication between processes that run on different hosts on a network. Linux has rich support for features and various protocol stacks for using network sockets API. For all kinds of network programming and distributed programming - network socket APIs form the core interface. Inotify机制 监控文件系统改变的, 可以和poll select等联用. inotify是Linux内核2.6.13 (June 18, 2005)版本新增的一个子系统（API），它提供了一种监控文件系统（基于inode的）事件的机制，可以监控文件系统的变化如文件修改、新增、删除等，并可以将相应的事件通知给应用程序。该机制由著名的桌面搜索引擎项目beagle引入用于替代此前具有类似功能但存在诸多缺陷的dnotify。 The Inotify API on Linux provides a method for processes to know of any changes on a monitored file or a directory asynchronously. By adding a file to inotify watch-list, a process will be notified by the kernel on any changes to the file like open, read, write, changes to file stat, deleting a file and so on. FUSE文件系统 FUSE provides a method to implement a fully functional filesystem in user-space. Various operations on the mounted FUSE filesystem would trigger functions registered by the user-space filesystem handler process. This technique can also be used as an IPC mechanism to implement Client-Server architecture without using socket API semantics. D-BUS 桌面系统用的多, 是建立在socket API基础上的多进程通信的系统 D-Bus is a high-level IPC mechanism built generally on top of socket API that provides a mechanism for multiple processes to communicate with each other using various messaging patterns. D-Bus is a standards specification for processes communicating with each other and very widely used today by GUI implementations on Linux following Freedesktop.org specifications. "},"notes/system_特殊功能fd.html":{"url":"notes/system_特殊功能fd.html","title":"eventfd timerfd signalfd和fd共享","keywords":"","body":"除了普通文件fd, socket fd, Linux还提供了比较特殊的几种fd, 比如eventfd timerfd signalfd等等. signalfd timerfd timerfd API 注epoll使用简介 边沿触发和电平触发 epoll_wait eventfd概念 用法 进程间共享文件描述符 各种fd是系统调用 用perf看所有带fd的系统调用 perf list | grep syscalls | grep fd | grep enter root@godev-server:/home/yingjieb# perf list | grep syscalls | grep fd | grep enter syscalls:sys_enter_eventfd [Tracepoint event] syscalls:sys_enter_eventfd2 [Tracepoint event] syscalls:sys_enter_fdatasync [Tracepoint event] syscalls:sys_enter_gettimeofday [Tracepoint event] syscalls:sys_enter_memfd_create [Tracepoint event] syscalls:sys_enter_settimeofday [Tracepoint event] syscalls:sys_enter_signalfd [Tracepoint event] syscalls:sys_enter_signalfd4 [Tracepoint event] syscalls:sys_enter_timerfd_create [Tracepoint event] syscalls:sys_enter_timerfd_gettime [Tracepoint event] syscalls:sys_enter_timerfd_settime [Tracepoint event] syscalls:sys_enter_userfaultfd [Tracepoint event] 下面就介绍一下这些fd的使用. signalfd #include #include #include #include #include #define handle_error(msg) \\ do { perror(msg); exit(EXIT_FAILURE); } while (0) int main(int argc, char *argv[]) { sigset_t mask; int sfd; struct signalfd_siginfo fdsi; ssize_t s; sigemptyset(&mask); sigaddset(&mask, SIGINT); sigaddset(&mask, SIGQUIT); /* 阻塞信号以使得它们不被默认的处理试方式处理 */ if (sigprocmask(SIG_BLOCK, &mask, NULL) == -1) handle_error(\"sigprocmask\"); sfd = signalfd(-1, &mask, 0); if (sfd == -1) handle_error(\"signalfd\"); for (;;) { s = read(sfd, &fdsi, sizeof(struct signalfd_siginfo)); if (s != sizeof(struct signalfd_siginfo)) handle_error(\"read\"); if (fdsi.ssi_signo == SIGINT) { printf(\"Got SIGINT\\n\"); } else if (fdsi.ssi_signo == SIGQUIT) { printf(\"Got SIGQUIT\\n\"); exit(EXIT_SUCCESS); } else { printf(\"Read unexpected signal\\n\"); } } } 这个例子只是很简单的说明了使用signalfd的方法，并没有真正发挥它的作用，有了这个API，就可以将信号处理作为IO看待.每一个信号集合（或者某一个对应的信号）就会有对应的文件描述符，这样将信号处理的流程大大简化，将应用程序中的业务作为文件来操作，也体现了linux下的一切皆文件的说法，非常好，假如有很多种信号等待着处理，每一个信号描述符对待一种信号的处理，那么就可以将信号文件描述符设置为非阻塞，同时结合epoll使用，对信号的处理转化为IO复用，和这个有相似之处的API还有timerfd timerfd 使用timerfd的一个例子是libevent. 在linux下面, 默认的libevent使用epoll, epoll有个超时时间.libevent利用这个超时时间来做定时器的触发源, 即在event loop里面, 把定时器堆的时间做为超时时间.更具体一些, 在libevent初始化时:默认情况下, libevent不使用timerfd, epollop->timerfd = -1.但有EVENT_BASE_FLAG_PRECISE_TIMER情况下, if ((base->flags & EVENT_BASE_FLAG_PRECISE_TIMER) && base->monotonic_timer.monotonic_clock == CLOCK_MONOTONIC) { int fd; fd = epollop->timerfd = timerfd_create(CLOCK_MONOTONIC, TFD_NONBLOCK|TFD_CLOEXEC); if (epollop->timerfd >= 0) { struct epoll_event epev; memset(&epev, 0, sizeof(epev)); epev.data.fd = epollop->timerfd; epev.events = EPOLLIN; //把这个fd加入epoll epoll_ctl(epollop->epfd, EPOLL_CTL_ADD, fd, &epev) 参考: https://blog.csdn.net/KangRoger/article/details/47844443 所以这里的timerfd_create()就是timerfd的使用方法. timerfd API #include int timerfd_create(int clockid, int flags); //这个时间精度应该比ms高. int timerfd_settime(int fd, int flags, const struct itimerspec *new_value, struct itimerspec *old_value); int timerfd_gettime(int fd, struct itimerspec *curr_value); read系统调用会返回超时次数, 如果一次超时都没到, read阻塞. timerfd_create用于创建一个定时器文件，函数返回值是一个文件句柄fd。 timerfd_settime用于设置新的超时时间，并开始计时。flag为0表示相对时间，为1表示绝对时间。new_value为这次设置的新时间，old_value为上次设置的时间。返回0表示设置成功。 timerfd_gettime用于获得定时器距离下次超时还剩下的时间。如果调用时定时器已经到期，并且该定时器处于循环模式（设置超时时间时struct itimerspec::it_interval不为0），那么调用此函数之后定时器重新开始计时。 注epoll使用简介 man epoll可以看到, epoll的API有 epoll_create() : 用于创建epoll对象 epoll_ctl() : 用于管理fd set epoll_wait() : 用于等待IO 边沿触发和电平触发 epoll有类似硬件的触发概念 edge-triggered (ET): 边沿触发, 只有fd的状态有变化才触发. 例如epoll_wait返回一个fd可读, 它实际有2k的数据可以读, 但回调函数里只读了1k. 即使还有1k数据可读, 在ET触发模式下, epoll不会再触发fd可读. 用ET触发的推荐场景是: 这个fd是非阻塞的 并且read或者write返回EAGAIN level-triggered (LT): 电平触发, 这是默认触发方式 epoll_wait #include int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); 这里的timeout单位是ms, 是用CLOCK_MONOTONIC度量的. epoll_wait返回有几种可能: 底层driver有数据, event到达 超时, -1表示永久等待. 0表示立即返回 被signal eventfd概念 eventfd()是个系统调用, 生成一个event fd对象, 内核为其维护一个计数器, 用来做用户进程间的wait/nofify机制, 也可以被kernel用来通知用户态进程. 可以用来代替pipe(), 相比于pipe来说，少用了一个文件描述符，而且不必管理缓冲区，单纯的事件通知的话，方便很多在fork()的时候, 子进程继承eventfd, 对应同一个eventfd对象, 并且, 这个fd在execve()后仍然保持, 但如果有close-on-exec选项则不保持. 创建eventfd以后, 可以子进程写, 父进程读; 读的时候, 只要计数器非0, 就返回计数器值, 并reset到0; 计数器为0则block 计数器不溢出就可以写, 写的值加到计数器上. 溢出的话会阻塞. 也可以配合poll(), select()使用 用法 #include int eventfd(unsigned int initval, int flags); read() write() close() //glibc还提供: typedef uint64_t eventfd_t; int eventfd_read(int fd, eventfd_t *value); int eventfd_write(int fd, eventfd_t value); 进程间共享文件描述符 在 OVS架构和代码中, qemu通过unix socket传递eventfd的文件描述符给OVS, 实际上是用了socket的SCM_RIGHTS方法. 实际上, 也可以用pipe之类的进程间通信为载体, 其底层是通过ioctl的I_SENDFD和I_RECVFD完成的. 详见 http://poincare.matf.bg.ac.rs/~ivana/courses/ps/sistemi_knjige/pomocno/apue/APUE/0201433079/ch17lev1sec4.html 出自Advanced Programming in the UNIX® Environment: Second Edition 2005看来这技术有十几年的时间了发送进程: #include \"apue.h\" #include /* * Pass a file descriptor to another process. * If fd= 0) if (ioctl(fd, I_SENDFD, fd_to_send) 接收进程: //接收时, 第三个参数是strrecvfd 结构体 struct strrecvfd { int fd; /* new descriptor */ uid_t uid; /* effective user ID of sender */ gid_t gid; /* effective group ID of sender */ char fill[8]; }; #include \"apue.h\" #include /* * Receive a file descriptor from another process (a server). * In addition, any data received from the server is passed * to (*userfunc)(STDERR_FILENO, buf, nbytes). We have a * 2-byte protocol for receiving the fd from send_fd(). */ int recv_fd(int fd, ssize_t (*userfunc)(int, const void *, size_t)) { int newfd, nread, flag, status; char *ptr; char buf[MAXLINE]; struct strbuf dat; struct strrecvfd recvfd; status = -1; for ( ; ; ) { dat.buf = buf; dat.maxlen = MAXLINE; flag = 0; if (getmsg(fd, NULL, &dat, &flag) 0) if ((*userfunc)(STDERR_FILENO, buf, nread) != nread) return(-1); if (status >= 0) /* final data has arrived */ return(newfd); /* descriptor, or -status */ } } 下面的例子说明, 两个进程想要传递fd, 要先有个通道, 比如pipe, 或者socket, 用来传递fd. #include #include #include #include #include #define TESTFILE \"/dev/null\" main(int argc, char *argv[]) { int fd; int pipefd[2]; struct stat statbuf; stat(TESTFILE, &statbuf); statout(TESTFILE, &statbuf); pipe(pipefd); if (fork() == 0) { close(pipefd[0]); sendfd(pipefd[1]); } else { close(pipefd[1]) recvfd(pipefd[0]); } } sendfd(int p) { int tfd; tfd = open(TESTFILE, O_RDWR); ioctl(p, I_SENDFD, tfd); } recvfd(int p) { struct strrecvfd rfdbuf; struct stat statbuf; char fdbuf[32]; ioctl(p, I_RECVFD, &rfdbuf); fstat(rfdbuf.fd, &statbuf); sprintf(fdbuf, \"recvfd=%d\", rfdbuf.fd); statout(fdbuf, &statbuf); } statout(char *f, struct stat *s) { printf(\"stat: from=%s mode=0%o, ino=%ld, dev=%lx, rdev=%lx\\n\", f, s->st_mode, s->st_ino, s->st_dev, s->st_rdev); fflush(stdout); } "},"notes/c_automake_autoconf.html":{"url":"notes/c_automake_autoconf.html","title":"automake autoconf","keywords":"","body":" configure autogen 两个文件需要编辑 Autotools运行流程 configure # 参考README和INSTALL # 对于大多数用automake系统的开源组件, 一般的操作如下: ./configure #configure允许很多参数, 比如 # ./configure CC=c99 CFLAGS=-g LIBS=-lposix # ./configure --prefix=$HOME CC=c99 # 用下面这个可以编过 ./configure --prefix=$HOME CFLAGS=-std=gnu99 make make install autogen htop出2.0版本了, 这个工具似乎已经逐渐取代top, 成为linux日常使用额必备利器. 这里记录编译htop的过程. 从git clone下来的代码, 根据readme所说, 需要这样来编译安装 ./autogen.sh && ./configure && make 那么autogen里面是什么呢? 自动生成configure和Makefile又是什么原理呢? autogen.sh只有短短几行 $ cat autogen.sh #!/bin/sh mkdir -p m4 autoreconf --install --force autoreconf的说明, 见 https://www.gnu.org/savannah-checkouts/gnu/autoconf/manual/autoconf-2.69/html_node/autoreconf-Invocation.html autoreconf运行autoconf, autoheader, aclocal, automake, libtoolize, 和autopoint 也可参考: http://www.baidu.com/link?url=y_4Or_vnHU2ZgcBOZFsRMYGSrQvMsdRAt6yGbE4R1Lx3Xvtpe-wMFtNGBhF5jLcRA80X9-hcBnStMBzFH7Z4N2fkYeq8ssusBRP-E8Bzply 两个文件需要编辑 configure.ac（主要被 autoconf 使用）configure.ac 置于项目根目录 Makefile.am（主要被 automake 使用）Makefile.am 必须有一个放在项目根目录，可以在所有有必要的子目录下单独编写放置Makefile.am --(automake)--> Makefile.inMakefile.in ---(./configure)--> Makefile Autotools运行流程 执行autoscan命令。这个命令主要用于扫描工作目录，并且生成configure.scan文件。 修改configure.scan为configure.ac文件，并且修改配置内容。 执行aclocal命令。扫描 configure.ac 文件生成 aclocal.m4文件。 执行autoconf命令。这个命令将 configure.ac 文件中的宏展开，生成 configure 脚本。 执行autoheader命令。该命令生成 config.h.in 文件。 新增Makefile.am文件，修改配置内容 执行automake --add-missing命令。该命令生成 Makefile.in 文件。 执行 ./congigure命令。将Makefile.in命令生成Makefile文件。 执行make命令。生成可执行文件。 "},"notes/makefile_原理和实践.html":{"url":"notes/makefile_原理和实践.html","title":"makefile","keywords":"","body":" target_lib的makefile Makefile进阶 根据target改变变量 传入变量到Makefile Makefile配方传递变量 Makefile很简单 第一版 第二版 第三版 封存版 target_lib的makefile 这个Makefile用来生成动态库 VERSION = 1 RELEASE = 0.0 NAME = libapp-config.so SONAME = $(NAME).$(VERSION) LIB = $(SONAME).$(RELEASE) LIBOBJS = app_config.o INCLUDES = app_config.h override CFLAGS += -Wall -Werror .PHONY: all install clean all: $(LIB) $(LIBOBJS): override CFLAGS += -fPIC $(LIB): $(LIBOBJS) $(CC) $(LDFLAGS) -shared -Wl,-soname,$(SONAME) -o $@ $^ install: mkdir -p $(DESTDIR)/usr/include/isam install $(INCLUDES) $(DESTDIR)/usr/include/isam mkdir -p $(DESTDIR)/usr/lib install $(LIB) $(DESTDIR)/usr/lib ln -sf $(LIB) $(DESTDIR)/usr/lib/$(NAME) ln -sf $(LIB) $(DESTDIR)/usr/lib/$(SONAME) clean: rm -f *~ *.o $(LIB) Makefile进阶 根据target改变变量 BLDTAGS := stdlib,adaptiveservice LDFLAGS = -X 'github.com/godevsig/gshellos.version=$(GIT_TAG)' -X 'github.com/godevsig/gshellos.buildTags=$(BLDTAGS)' build: dep @go build -tags $(BLDTAGS) -ldflags=\"$(LDFLAGS)\" -o bin ./cmd/gshell debug: BLDTAGS := $(BLDTAGS),debug debug: build ## Build debug binary to bin dir lite: BLDTAGS := $(BLDTAGS) lite: build ## Build lite release binary to bin dir full: BLDTAGS := $(BLDTAGS),echart,database full: build ## Build full release binary to bin dir 关键是 target … : variable-assignment 参考: Target-specific Variable Values 传入变量到Makefile 参考 You have several options to set up variables from outside your makefile: From environment - each environment variable is transformed into a makefile variable with the same name and value. You may also want to set -e option (aka --environments-override) on, and your environment variables will override assignments made into makefile (unless these assignments themselves use the override directive . However, it's not recommended, and it's much better and flexible to use ?= assignment (the conditional variable assignment operator, it only has an effect if the variable is not yet defined): FOO?=default_value_if_not_set_in_environment Note that certain variables are not inherited from environment: MAKE is gotten from name of the script SHELL is either set within a makefile, or defaults to /bin/sh (rationale: commands are specified within the makefile, and they're shell-specific). From command line - make can take variable assignments as part of his command line, mingled with targets: make target FOO=bar But then all assignments to FOO variable within the makefile will be ignored unless you use the override directive in assignment. (The effect is the same as with -e option for environment variables). Exporting from the parent Make - if you call Make from a Makefile, you usually shouldn't explicitly write variable assignments like this: # Don't do this! target: $(MAKE) -C target CC=$(CC) CFLAGS=$(CFLAGS) Instead, better solution might be to export these variables. Exporting a variable makes it into the environment of every shell invocation, and Make calls from these commands pick these environment variable as specified above. # Do like this CFLAGS=-g export CFLAGS target: $(MAKE) -C target You can also export all variables by using export without arguments. Makefile配方传递变量 比如下面的例子的作用是用gofmt检查go文件的code格式, 有差异就出错 format: ## Check coding style @DIFF=`gofmt -d .`; echo \"$$DIFF\"; test -z \"$$DIFF\" 比如格式不对: $ make format diff -u log/log.go.orig log/log.go --- log/log.go.orig 2020-09-22 12:53:25.561731789 +0000 +++ log/log.go 2020-09-22 12:53:25.561731789 +0000 @@ -30,7 +30,7 @@ // All predefined loglevels const ( - Ltrace Loglevel = 1 + Ltrace Loglevel = 1 Ldebug Loglevel = 2 Linfo Loglevel = 3 Lwarn Loglevel = 4 make: *** [Makefile:8: format] Error 1 要点: 配方要写在一行, 用;或者&&分隔. 多行要用\\来连接 没有\\的话, 每行都会被make单独执行, 也就是说变量不能多行传递的. $需要转义. 用$$ echo后面跟引号是保留原始换行 结论: makefile里面的配方(recipe)是特殊的语法, 先由make解析一遍再调用shell命令. 具体来讲, 是make先展开变量, 所以一个$就会被make直接展开. 要把$传递给shell, 就要转义$ 比如 COVER_GOAL := 90 coverage: ## Generate global code coverage report @OUT=$$(./tools/coverage.sh \"${PKG_LIST}\"); echo \"$$OUT\"; \\ echo \"$$OUT\" | tail -n1 | awk -F\"\\t*| *|%\" '{if ($$3 COVER_GOAL就是一个$直接展开的 要给shell看的$, 包括awk里面的$, 都需要转义(即两个$) Makefile很简单 我至今还没真正搞懂make怎么工作的, 但读了下面的说明, 尝试写了比较\"复杂\"的makefile后, 感觉还挺简单的 详细说明: https://www.gnu.org/software/make/manual/make.html 需要注意的: make有两个阶段 第一阶段读所有被包含的makefile文件, 初始化变量, 构建依赖树 第二阶段执行配方(recipe) 变量的展开规则是: 立即展开发生在第一阶段; 延迟展开发生在需要时 immediate = deferred immediate ?= deferred immediate := immediate immediate ::= immediate immediate += deferred or immediate immediate != immediate define immediate deferred endef define immediate = deferred endef define immediate ?= deferred endef define immediate := immediate endef define immediate ::= immediate endef define immediate += deferred or immediate endef define immediate != immediate endef 规则也有立即展开和延迟展开的区别 immediate : immediate ; deferred deferred 第一版 build: godev/godev-tools godev/godev-tools: godev/vscode godev/gobin godev/go3rdparty godev/%: Dockerfile.% @docker build -f $ godev/godev-tools实际上匹配了两个规则, 一个是显式的规则, 一个是%规则; 效果是同时生效 多个%规则匹配目标的话, 只有一个生效; 在依赖都存在的前提下, make选择一个最\"具体\"的%规则 第二版 goVersions := 1.10 1.11 1.12 1.13 1.14 build: 1.13 $(goVersions): %: godev/godev-tools\\:% godev/godev-tools\\:%: Dockerfile.godev-tools godev/vscode\\:v2 godev/gobin\\:1.13 godev/go3rdparty\\:1.13 docker build --build-arg TAG=$* -f $ 用make 1.14 --dry-run来模拟执行配方(recipe) $*表示%匹配到的部分 删掉了touch文件部分, 因为docker build会使用cache功能, 即使重新构建没有改动也很快 第三版 goVersions := 1.10 1.11 1.12 1.13 1.14 dockerFiles := $(wildcard Dockerfile.*) images := $(subst Dockerfile.,, $(dockerFiles)) build: 1.13 $(goVersions): %: godev/godev-tools\\:% define GEN_TOOLS_RULE godev/godev-tools\\:$(version): godev/vscode\\:v2 godev/godev-tools\\:$(version): godev/gobin\\:1.13 godev/godev-tools\\:$(version): godev/go3rdparty\\:1.13 endef $(foreach version, $(goVersions), $(eval $(GEN_TOOLS_RULE))) define GEN_IMG_RULE godev/$(image)\\:%: Dockerfile.$(image) docker build --build-arg TAG=$$* -f $$ 这个版本用foreach和eval自动生成规则 eval会展开2次, 所以这里的变量定义里加两个$; 没错, define是变量定义 %规则和显式规则能同时生效, 在GEN_TOOLS_RULE里面, 每行的依赖是增量式的. 如果依赖都写在一行, 解析的时候似乎空格没有起到作用 这个版本稍显繁琐, 但结构更清晰, 避免了重复; 只是为了演示makefile的能力; 实际还是第二个版本简洁 封存版 goVersions := 1.10.8 1.11.13 1.12.17 1.13.12 1.14.4 officialTags := $(foreach v, $(goVersions), official-$(v)) repoHead := $(shell git rev-parse --short HEAD) artifactory := artifactory-blr1.int.net.nokia.com godevsigArtifactory := godevsig-docker-local.$(artifactory) img := godevsig/devtool define docker_push #from,to @echo Are you sure to push $(1) to $(2) ? @read -p \"if not press Ctrl+C - enter to continue\" tmp docker tag $(1) $(2) docker push $(2) endef default: godevsig $(call docker_push,$(img):latest,$(godevsigArtifactory)/$(img):latest) #customized by godevsig(https://gitlabe1.ext.net.nokia.com/godevsig) with gccgo supporting ppc/ppc64 godevsig: godevsig-$(repoHead) docker tag $(img):$ "},"notes/c_编程杂记高级篇.html":{"url":"notes/c_编程杂记高级篇.html","title":"C编程杂记高级篇","keywords":"","body":" raw socket收报全丢掉, 测试性能 scanf忽略某个filed IO模型 普通阻塞读写 非阻塞系统调用 多路IO复用 O_ASYNC和O_NONBLOCK有什么区别? 父子进程与FD -- fcntl的使用 使用gcc内建函数返回调用者 printk支持多种内部结构打印 trace_printk()很有意思 结论 Uboot的命令注册机制 C实现基类派生 同类型检查 实例 基类和派生类 把rootfs.cpio嵌入到vmlinux--incbin的使用 如何用C设计面向对象的中间层 父类子类公用一个结构体 一个公共C文件, 和若干个\"子类\"C文件 总结 syslog记录log到syslogd 使用宏拼接 raw socket收报全丢掉, 测试性能 #include #include #include #include #include #include #include #include #include #include #include #include #include extern unsigned if_nametoindex(const char *ifname); char *interface = \"eth-mgnt\"; int create_raw_eth_socket(void) { int eth_sock; struct sockaddr_ll eth_dest; unsigned long itf_ifindex = if_nametoindex(interface); /*ETH socket, RAW communication*/ eth_dest.sll_family = PF_PACKET; eth_dest.sll_protocol = htons(ETH_P_ALL); eth_dest.sll_ifindex = itf_ifindex; eth_dest.sll_hatype = ARPHRD_ETHER; eth_dest.sll_pkttype = PACKET_OTHERHOST; eth_dest.sll_halen = ETH_ALEN; eth_sock = socket(AF_PACKET, SOCK_RAW, htons(ETH_P_ALL)); bind(eth_sock,(struct sockaddr*)&eth_dest,sizeof(eth_dest)); return eth_sock; } int main(int argc,char *argv[], char**envp) { int eth_sock; struct sockaddr_ll eth_src; unsigned char data[1600]; unsigned long packet_count = 0; eth_sock = create_raw_eth_socket(); while (1) { socklen_t addr_len = sizeof(eth_src); int size = recvfrom(eth_sock,data,sizeof(data),0,(struct sockaddr*)&eth_src,&addr_len); if (size scanf忽略某个filed %*关键词告诉scanf忽略这部分input fscanf(fpstat, \"%*d %*s %*c %*d %*d %*d %*d %*d %*u %*u %*u %*u %*u %lu\" \"%lu %ld %ld %*d %*d %*d %*d %*u %lu %ld\", &result->utime_ticks, &result->stime_ticks, &result->cutime_ticks, &result->cstime_ticks, &result->vsize, &rss) IO模型 普通阻塞读写 正常的读写是阻塞的. ssize_t read(int fd, void *buf, size_t count); ssize_t write(int fd, const void *buf, size_t nbytes); 非阻塞系统调用 非阻塞的读写需要先给fd设置标记 int flags = fcntl(fd, F_GETFL, 0); fcntl(fd, F_SETFL, flags | O_NONBLOCK); 后面的读写会马上返回, 如果没准备好, 返回EAGAIN. 多路IO复用 int select(int nfds, fd_set *restrict readfds, fd_set *restrict writefds, fd_set *restrict errorfds, struct timeval *restrict timeout); 除了标准的 select 函数之外，操作系统中还提供了一个比较相似的 poll 函数，它使用链表存储文件描述符，摆脱了 1024 的数量上限。 O_ASYNC和O_NONBLOCK有什么区别? 异步编程模型下, 一般设socket的fd为O_NONBLOCK, 结合epoll, 等待数据的时候阻塞在epoll, 写socket不阻塞, 有错误直接返回. 那O_ASYNC有什么用? 和O_NONBLOCK有什么区别?答:O_ASYNC请求kernel在有数据的时候发送SIGIO or SIGPOLL给所属进程. 一般不常用, 因为在sighandler里面处理io不好, signal的deliver过程比epoll复杂. 参考StackOverflow 父子进程与FD -- fcntl的使用 子进程集成父进程的FD, 但子进程exec后呢? 有个函数, 如果调用, 表示子进程exec后, 关闭该fd. 意思就是子进程即使exec, 默认也是继承父进程的fd的. fcntl(fd, F_SETFD, FD_CLOEXEC) fcntl还有其他作用: #include #include //定义函数 int fcntl(int fd , int cmd); int fcntl(int fd,int cmd,long arg); int fcntl(int fd,int cmd,struct flock * lock); //fcntl()用来操作文件描述词的一些特性。参数fd代表欲设置的文件描述词，参数cmd代表欲操作的指令。 //有以下几种情况: F_DUPFD用来查找大于或等于参数arg的最小且仍未使用的文件描述词，并且复制参数fd的文件描述词。执行成功则返回新复制的文件描述词。请参考dup2()。F_GETFD取得close-on-exec旗标。若此旗标的FD_CLOEXEC位为0，代表在调用exec()相关函数时文件将不会关闭。 F_SETFD 设置close-on-exec 旗标。该旗标以参数arg 的FD_CLOEXEC位决定。 F_GETFL 取得文件描述词状态旗标，此旗标为open（）的参数flags。 F_SETFL设置文件描述词状态旗标，参数arg为新旗标，但只允许O_APPEND、O_NONBLOCK和O_ASYNC位的改变，其他位的改变将不受影响。 F_GETLK 取得文件锁定的状态。 F_SETLK 设置文件锁定的状态。此时flcok 结构的l_type值必须是F_RDLCK、F_WRLCK或F_UNLCK。如果无法建立锁定，则返回-1，错误代码为EACCES或EAGAIN。 F_SETLKW F_SETLK作用相同，但是无法建立锁定时，此调用会一直等到锁定动作成功为止。若在等待锁定的过程中被信号中断时，会立即返回-1，错误代码为EINTR。参数lock指针为flock结构指针，定义如下 struct flcok { short int l_type; short int l_whence; off_t l_start; off_t l_len; pid_t l_pid; }; l_type 有三种状态: F_RDLCK 建立一个供读取用的锁定 F_WRLCK 建立一个供写入用的锁定 F_UNLCK 删除之前建立的锁定 l_whence 也有三种方式: SEEK_SET 以文件开头为锁定的起始位置。 SEEK_CUR 以目前文件读写位置为锁定的起始位置 SEEK_END 以文件结尾为锁定的起始位置。 //返回值 成功则返回0，若有错误则返回-1，错误原因存于errno. 使用gcc内建函数返回调用者 在kernel启动过程中, 有下面的打印: memblock_reserve: [0x00000100000000-0x00000100829fff] flags 0x0 early_init_devtree+0x104/0x350 memblock_reserve: [0x00000000c10000-0x0000000166fdc7] flags 0x0 early_init_fdt_scan_reserved_mem+0x5c/0x7c memblock_reserve: [0x000001fe000000-0x000001feffffff] flags 0x0 memblock_alloc_range_nid+0x70/0x7c 看起来memblock_reserve是个函数, 表示哪段区间的内存被reserve了. 它的最后一个打印字段有意思, 是调用者的函数名吗? 打印出自: int __init_memblock memblock_reserve(phys_addr_t base, phys_addr_t size) { memblock_dbg(\"memblock_reserve: [%#016llx-%#016llx] flags %#02lx %pF\\n\", (unsigned long long)base, (unsigned long long)base + size - 1, 0UL, (void *)_RET_IP_); return memblock_add_range(&memblock.reserved, base, size, MAX_NUMNODES, 0); } 而_RET_IP_是#define _RET_IP_ (unsigned long)__builtin_return_address(0) __builtin_return_address是gcc提供的内建函数, 返回当前函数的返回地址. level0表示当前函数, level1表示返回其父函数的返回地址, 以此类推.Built-in Function: void * __builtin_return_address (unsigned int level) 详见: https://gcc.gnu.org/onlinedocs/gcc/Return-Address.html 有了程序地址, printk传入%pS就能打印符号了. 见linux/Documentation/core-api/printk-formats.rst printk支持多种内部结构打印 实际上, printk支持很多很多常见格式的打印, 比如IP地址(%pI4), MAC地址格式的打印, 物理地址phys_addr_t类型的(%pa), DMA地址... 还有一些linux内部常用结构, 比如dentry, block_device, device tree, 时间, clock等等等等. trace_printk()很有意思 在代码里加了trace_printk(), 和ftrace连用, 可以很方便的调试代码. 而且, 有个地方很有意思, 你调用了trace_printk(), 在kernel启动的很早期, 会打印一个warning message 这个地方非常早, trace_printk()还没来得及被调用. 那kernel怎么知道要打印这个呢? 如果代码里没有调用trace_printk(), 是没有这个打印的 这个warning是在trace_printk_init_buffers()打印的, 条件是if (__stop___trace_bprintk_fmt != __start___trace_bprintk_fmt)这两个\"变量\"是指针: extern const char *__start___trace_bprintk_fmt[]; extern const char *__stop___trace_bprintk_fmt[]; 在链接脚本里include/asm-generic/vmlinux.lds.h #define TRACE_PRINTKS() VMLINUX_SYMBOL(__start___trace_bprintk_fmt) = .; \\ *(__trace_printk_fmt) /* Trace_printk fmt' pointer */ \\ VMLINUX_SYMBOL(__stop___trace_bprintk_fmt) = .; OK, 再来看看trace_printk()的定义: 原来, trace_printk()的入参fmt, 是保存在__trace_printk_fmt这个section的, 用trace_printk_fmt指针指向它. 如果有代码调用了trace_printk(), 那就传了fmt参数, 从而__trace_printk_fmt里面有东西, 这是在编译时就知道了的事情; 那在运行时判断, 这个section里面有没有东西if (__stop___trace_bprintk_fmt != __start___trace_bprintk_fmt) 结论 这是个很好的技巧, 根据编译时代码的状态, 来决定运行时的行为. Uboot的命令注册机制 要点: 这样可以免去在运行时\"注册\"命令. 使用连接器的自定义section里面的数组, 命令静态的被连接到可执行程序里. 这样做, 可以在一个C文件里, 同时完成命令的实现, 和命令的注册. 这个特殊的section, 可以在运行时访问. 比如bootm命令, 在common/cmd_bootm.c中, 是这样注册的: U_BOOT_CMD( bootm, CONFIG_SYS_MAXARGS, 1, do_bootm, \"boot application image from memory\", bootm_help_text ); 这个宏, 会把bootm相关的要素: 命令, 函数, 说明, 写入特殊的段内, 需要链接器配合. linux kernel也经常用这个机制. #define U_BOOT_CMD(_name, _maxargs, _rep, _cmd, _usage, _help) \\ U_BOOT_CMD_COMPLETE(_name, _maxargs, _rep, _cmd, _usage, _help, NULL) #define U_BOOT_CMD_COMPLETE(_name, _maxargs, _rep, _cmd, _usage, _help, _comp) \\ ll_entry_declare(cmd_tbl_t, _name, cmd) = \\ U_BOOT_CMD_MKENT_COMPLETE(_name, _maxargs, _rep, _cmd, \\ _usage, _help, _comp); 关键在于include/linker_lists.h中: #define ll_entry_declare(_type, _name, _list) \\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4) \\ __attribute__((unused, \\ section(\".u_boot_list_2_\"#_list\"_2_\"#_name))) 可以在运行时访问, 因为可以找到数组的\"基地址\" struct my_sub_cmd *msc = ll_entry_start(struct my_sub_cmd, cmd_sub); #define ll_entry_start(_type, _list) \\ ({ \\ static char start[0] __aligned(4) __attribute__((unused, \\ section(\".u_boot_list_2_\"#_list\"_1\"))); \\ (_type *)&start; \\ }) C实现基类派生 关键点: C里面, 派生类的结构体, 显式的在最开始包含其基类的结构体.这样, 在知道一个指针是基类, 但实际是派生类的情况下, 可以指针强转成派生类. 使用gcc提供的类型检查, 可以保证这个强转的合法性. 同类型检查 GNU提供了一个runtime宏来检查两个类型是否一样: int __builtin_types_compatible_p(type_a, type_b); type_a和type_b一致, 返回1; 否则返回0使用 #define __COMPARE_TYPES(v, t) \\ __builtin_types_compatible_p(__typeof__(v), t) ... if (__COMPARE_TYPES(p, double) != 0) err(EX_DATAERR, \"invalid type\"); 实例 以上代码简化为 struct stack_entry *field = (struct stack_entry *) iter->ent 基类和派生类 field是stack_entry, 而iter->ent是trace_entry, 是不一样的. 为什么能强转呢? 这中间的奥秘在于, straceentry相当于基类, stack_entry是它的派生类. strace_entry被stack_entry包含在其结构体头部.具体见kernel/trace/trace.h![](img/c编程杂记高级篇_20221011161311.png) 这样, 指向基类的指针, 被强制转换为指向其派生类的指针. 这要求trace_stack_print的入参struct trace_iterator *iter, 本身就是stack_entry派生类. 把rootfs.cpio嵌入到vmlinux--incbin的使用 在linux-octeon-3.0/usr/initramfs_data.S中使用了.incbin其中, INITRAMFS_IMAGE为initramfs_data.cpio.gz #include #include .section .init.ramfs,\"a\" __irf_start: .incbin __stringify(INITRAMFS_IMAGE) __irf_end: .section .init.ramfs.info,\"a\" .globl VMLINUX_SYMBOL(__initramfs_size) VMLINUX_SYMBOL(__initramfs_size): #ifdef CONFIG_64BIT .quad __irf_end - __irf_start #else .long __irf_end - __irf_start #endif 如何用C设计面向对象的中间层 父类子类公用一个结构体 设计一个结构体, 并声明这个结构体的全局变量: 这个结构体有点像C++的类, 但可以认为是父类和子类都共用这个结构体, 所以, 这个结构体应该设计成既包括公有方法的指针, 又包括私有方法的指针. typedef struct { 一些控制变量; //父类方法指针, 由父类C文件提供默认值; void (*lock)(void); void (*unlock)(void); int (*reset)(int stop_core); uint64_t (*read_register)(int core, int reg); void (*write_register)(int core, int reg, uint64_t value); uint64_t (*read_csr)(uint64_t physical_address); void (*write_csr)(uint64_t physical_address, uint64_t value); //子类方法, 分别由子类C文件提供 int (*open)(const char *remote_spec); void (*close)(void); void (*read_mem)(void *buffer, uint64_t physical_address, int length); void (*write_mem)(uint64_t physical_address, const void *buffer, int length); uint64_t (*get_running_cores)(void); uint32_t (*get_num_cores)(void); } octeon_remote_funcs_t; /*全局变量*/ static octeon_remote_funcs_t remote_funcs; 一个公共C文件, 和若干个\"子类\"C文件 父类: octeon-remote.c 子类: octeon-remote-cvmx.c octeon-remote-gdbremote.c octeon-remote-map.c octeon-remote-console.c octeon-remote-debug-handler.c octeon-remote-linux.c octeon-remote-pci.c 父类C文件提供了对外统一的api(虚函数), 使调用者不用关心底层实现, 特别的, 在子类C文件中, 虽然是有具体的\"虚函数\"的实现, 但最好还是调用父类的虚函数接口. 父类C文件负责总的初始化, 包括公用结构体变量的父类方法指针的赋值, 并根据情况(如用户输入), 来调用某个子类的初始化接口. 子类的初始化接口里, 负责子类特殊的初始化, 以及子类方法指针的赋值. 总结 这种方式实现的封装, 要求全部子类的代码都要参与编译. 只是运行阶段, 根据情况, 选择使用某个子类. syslog记录log到syslogd #include setlogmask (LOG_UPTO (LOG_NOTICE)); openlog (\"exampleprog\", LOG_CONS | LOG_PID | LOG_NDELAY, LOG_LOCAL1); syslog (LOG_NOTICE, \"Program started by User %d\", getuid ()); syslog (LOG_INFO, \"A tree falls in a forest\"); closelog (); 使用宏拼接 定义寄存器表board_cpld_registers.h, 注意不要加头文件卫士 cpld_declare_reg(CIPHER1, 0x00, 0, 8) cpld_declare_reg(CIPHER2, 0x01, 0, 8) cpld_declare_reg(CIPHER3, 0x02, 0, 8) cpld_declare_reg(CIPHER4, 0x03, 0, 8) cpld_declare_reg(VERSION, 0x07, 0, 8) cpld_declare_reg(REVISION, 0x08, 0, 8) cpld_declare_reg(REG_GICI_SHELF_BPSW, 0x0b, 0, 8) cpld_declare_reg(SHELF_ID, 0x0b, 0, 2) cpld_declare_reg(BPSW, 0x0b, 4, 3) cpld_declare_reg(GICI_P, 0x0b, 7, 1) cpld_declare_reg(REG_NTR_STAT, 0x0e, 0, 8) 在公共头文件里定义宏接口, 使用宏拼接##来简化操作 extern spinlock_t cpld_lock; #undef cpld_declare_reg #define cpld_declare_reg(a,b,c,d) a, enum cpld_field_id { #include \"board_cpld_registers.h\" }; #undef cpld_declare_reg #define cpld_declare_reg(a,b,c,d) a##_OFFSET_ = b, enum cpld_field_offset { #include \"board_cpld_registers.h\" }; #undef cpld_declare_reg #define cpld_declare_reg(a,b,c,d) a##_START_BIT_ = c, enum cpld_field_start_bit { #include \"board_cpld_registers.h\" }; #undef cpld_declare_reg #define cpld_declare_reg(a,b,c,d) a##_MASK_ = ((1 > field_nr##_START_BIT_; \\ }while(0) #define wr_cpld_field(base, field_nr, val) \\ do { \\ unsigned long __flags; \\ u8 __tmp; \\ spin_lock_irqsave(&cpld_lock, __flags); \\ __tmp = ioread8((base) + field_nr##_OFFSET_); \\ __tmp &= (u8)~field_nr##_MASK_; \\ __tmp |= ((val) "},"notes/c_编程杂记基础篇.html":{"url":"notes/c_编程杂记基础篇.html","title":"C编程杂记基础篇","keywords":"","body":" kernel链表 定义全局hash表 对classhash_table进行初始化 数据成员 hash桶的拉链 遍历链表 打印table vsystem C遍历目录 pipe产生2个fd, 0为读, 1为写 从内存地址拷贝成文件 C处理文件 变长参数 文件锁 带缩进的debug打印, 主要用于跟踪函数调用 c下的函数重载, 用weak属性 寄存器变量 字符串转int getopt 参数处理函数 getenv() 从环境变量获取值 feof() 判断文件结尾 fwrite和fread /dev/mem和mmap 转为字符串的宏 使用if else if else结构 uboot中fdt库函数使用 如何判断大小端 标准的int类型 kernel链表 linux通过/linux/list.h文件，提供了标准的链表操作接口。内部是用双向链表实现的，并且体现了一个重要的思想，就是把链表操作和数据成员分离，这样一来，对这个双向链表本身的操作就可以抽象出来，不会因为数据成员的不同而变化。在软件设计中，分离变与不变，\"求同存异\"，是非常重要的思想。 下面以一个hash表为例，简介其用法。 在linux启动代码中，会对锁的依赖管理模块进行初始化，在kernel/lockdep.c中 定义全局hash表 static struct list_head classhash_table[4k]; classhash_table是个4k大小的数组，每个元素都是一个链表，是个典型的拉链hash结构。这个链表以list_head为节点，并不包含其他的数据成员。 struct list_head { struct list_head *next, *prev; }; 对classhash_table进行初始化 for (i = 0; i INIT_LIST_HEAD()是list.h提供的内联函数，用于在运行时初始化一个链表，使链表头尾指向自己。 另外，list.h还提供了一个宏，用于定义并初始化一个链表：LIST_HEAD(name) 数据成员 后面会看到，实际上每个链表节点并不仅是一个list_head结构体，而是一个包含list_head的带其他数据成员的结构： struct lock_class { struct list_head hash_entry; struct list_head lock_entry; struct lockdep_subclass_key *key; unsigned int subclass; unsigned int dep_gen_id; unsigned long usage_mask; struct stack_trace usage_traces[LOCK_USAGE_STATES]; struct list_head locks_after, locks_before; unsigned int version; unsigned long ops; const char *name; int name_version; }; 可以看到，hash_entry就是被包含的list_head结构，它的作用是代表整个lock_class结构体，被链接到链表中。list_head可以在结构体的任意位置，用container_of(ptr, type, member)宏就可以找到母体的位置。 #define container_of(ptr, type, member) ({ \\ const typeof( ((type *)0)->member ) *__mptr = (ptr); \\ (type *)( (char *)__mptr - offsetof(type,member) );}) hash桶的拉链 在用hash函数算出key的hash值之后，就得到了一个在hash桶里的一个链表hash_head hash_head = classhash_table + hash_long(key) 遍历链表 最后用list_for_each_entry(pos, head, member)宏来遍历这个链表，其中，pos是个迭代变量，指向节点实体类型，在这里是lock_class；head是要遍历的链表头，在这里是hash_head；member是list_head在lock_class里的名称，在这里是hash_entry struct lock_class *class; list_for_each_entry(class, hash_head, hash_entry) { if (class->key == key) { WARN_ON_ONCE(class->name != lock->name); return class; } } 打印table printf(\"%-25s%-20s%-10s%-10s%-10s\\n\", \"Name\", \"Title\", \"Gross\", \"Tax\", \"Net\"); printf(\"%-25s%-20s%-10.2f%-10.2f%-10.2f\\n\", name.c_str(), title.c_str(), gross, tax, net); vsystem int vsystem(char** command) { int sysStatus = -1; pid_t pid = -1, waitResult = -1; pid = vfork(); if (pid == 0) { /* This is the child process. Execute the shell command. */ if (execvp(command[0], command) C遍历目录 /* print files in current directory in reverse order */ #include main(){ struct dirent **namelist; int n; n = scandir(\".\", &namelist, 0, alphasort); if (n d_name); free(namelist[n]); } free(namelist); } } pipe产生2个fd, 0为读, 1为写 fcntl可以对fd进行若干操作, 比如F_GETFL F_SETFL static int updater_pipe_fd[2] = {-1,-1}; /* File descriptors for pipe to update the readset */ static void init_readset_updater(void) { int flags; if (pipe(updater_pipe_fd) 在上面的函数中, 创建了一个pipe, 共生成两个文件描述符, updater_pipe_fd[0],updater_pipe_fd[1]. 并在最后, 把updater_pipe_fd[0]放到readset中, 另外一个进程将会不断的select这个readset. 所以, 只要有人用updater_pipe_fd[1]写入(可能是另外一个不同的进程), 则select将会返回. 下面这个函数, 就演示了在select任务运行时, 通过在其他任务写updater_pipe_fd[1]任意一个字符(我们并不关心这个字符是什么意思), 来触发select任务中select返回; 那么为什么一定要触发select返回呢? 因为下面的函数想添加/删除一个fd到readSet, 但此时readSet虽然更新了, 但select任务可能还在阻塞, 这样就不能及时take这个新的readSet的变更. int uiodrv_set_intr_enable_state(DeviceNbr dev, IntrEnableState state) { DeviceInfo *inst = findInstance(dev); if ((inst == NULL) || (inst->fd == -1)) return -1; switch (state) { case E_uiodrv_disable_intr: FD_CLR(inst->fd, &readset); break; case E_uiodrv_enable_intr: FD_SET(inst->fd, &readset); break; default: return -1; } /* update readset by writing to selfpipe. This will trigger the processing of all the devices */ if (write(updater_pipe_fd[WRITER_INDEX], \"x\", 1) 从内存地址拷贝成文件 可用dd命令代替,dd if=/dev/mem of=some_file bs=1M skip=1984 count=5 /* * File: copyFromMem.c * Author: lcarlier * * Created on March 18, 2014, 11:33 AM */ #include #include #include #include #include #include #include #include #include #include #include long readNumFromStr(char *str,int base); int main(int argc, char** argv) { if (argc != 4) { fprintf(stderr, \"Wrong number of arguments. Usage:\\n\"); fprintf(stderr, \"\\tcopyFromMem addrInMemory size outfile\\n\"); return EXIT_FAILURE; } long int fromPhyMem; long int size; char *outFile; fromPhyMem = readNumFromStr(argv[1],16); size = readNumFromStr(argv[2],10); outFile = argv[3]; if(fromPhyMem C处理文件 static const char *PCI_DEVICE_FILENAME = \"/proc/bus/pci/devices\"; in = fopen(PCI_DEVICE_FILENAME, \"r\"); while (!feof(in)) int count = fscanf(in, \"%2x%2x %8x %x %Lx %Lx %Lx %Lx %Lx %Lx %Lx %Lx %Lx %Lx\", &bus, &devfn, &pci_id, &irq, &bar0, &unused, &bar1, &unused, &unused, &unused, &unused, &siz0, &unused, &siz1); fgets(rest_of_line, sizeof(rest_of_line), in) fclose(in); 变长参数 查看帮助: man stdarg #include void va_start(va_list ap, last); type va_arg(va_list ap, type); void va_end(va_list ap); void va_copy(va_list dest, va_list src); 文件锁 #include int lockf(int fd, int cmd, off_t len); lock_file_name = \"/tmp/octeon-remote-lock-xxxxxxxxx\" 加锁: lock_fd = open(lock_file_name, O_CREAT|O_WRONLY, 0666); lockf(lock_fd, F_LOCK, 0) 解锁: close(lock_fd); 原理: 互斥文件锁, 如果一个文件被锁住, 则调用者会阻塞(F_LOCK), 或者返回error(F_TLOCK) 带缩进的debug打印, 主要用于跟踪函数调用 比如使用时: static void default_unlock(void) OCTEON_REMOTE_DEBUG_CALLED(); //函数体... OCTEON_REMOTE_DEBUG_RETURNED(); 具体实现: 用全局变量debug_indent来实现缩进 #define OCTEON_REMOTE_DEBUG_CALLED(format, ...) \\ octeon_remote_debug_call(0, \"Called %s(\" format \")\\n\", __FUNCTION__, ##__VA_ARGS__); #define OCTEON_REMOTE_DEBUG_RETURNED(format, ...) \\ octeon_remote_debug_call(1, \"Return %s(\" format \")\\n\", __FUNCTION__, ##__VA_ARGS__); void octeon_remote_debug_call(int is_return, const char *format, ...) { const int level = 3; if (level c下的函数重载, 用weak属性 int env_init(void) __attribute__((__weak__)); 寄存器变量 #define DECLARE_GLOBAL_DATA_PTR register volatile gd_t *gd asm (\"k0\") 字符串转int #include int atoi(const char *nptr); getopt 参数处理函数 #include int getopt(int argc, char * const argv[], const char *optstring); #include int getopt_long(int argc, char * const argv[], const char *optstring, const struct option *longopts, int *longindex); 例子: static struct option long_options[] = { /* These options set a flag. */ {\"memonly\", no_argument, &memonly_flag, 1}, {\"scantwsi\", no_argument, &twsi_scan_flag, 1}, {\"dumpspd\", no_argument, &dump_spd_flag, 1}, {\"readlevel\", no_argument, &read_level_flag, 1}, /* These options don't set a flag. We distinguish them by their indices. */ {\"board\", required_argument, 0, 0}, {\"board_type\",required_argument, 0, 0}, {\"envfile\", required_argument, 0, 0}, {\"ddr0spd\", required_argument, 0, 0}, {\"ddr1spd\", required_argument, 0, 0}, {\"ddr2spd\", required_argument, 0, 0}, {\"ddr3spd\", required_argument, 0, 0}, {\"help\", no_argument, 0, 'h'}, {\"version\", no_argument, 0, 'v'}, {\"board_delay\", required_argument, 0, 0}, {\"ddr_ref_hz\", required_argument, 0, 0}, {\"ddr_clock_hz\", required_argument, 0, 0}, {\"cpu_ref_hz\", required_argument, 0, 0}, {0, 0, 0, 0} }; /* getopt_long stores the option index here. */ int option_index = 0; c = getopt_long (argc, (char * const *)argv, \"hv\", long_options, &option_index); getenv() 从环境变量获取值 #include char *getenv(const char *name); 例子: char *remote_spec = getenv(\"OCTEON_REMOTE_PROTOCOL\"); feof() 判断文件结尾 #include void clearerr(FILE *stream); int feof(FILE *stream); int ferror(FILE *stream); int fileno(FILE *stream); 例子: int copy_file_to_octeon_dram(uint64_t address, const char *filename) { char buffer[65536] = {0}; uint64_t bytes_copied = 0; FILE *infile = fopen(filename, \"r\"); if (infile == NULL) { perror(\"Unable to open file\"); return -1; } while (!feof(infile)) { int count = fread(buffer, 1, sizeof(buffer), infile); if (count > 0) { octeon_remote_write_mem(address, buffer, count); address += count; bytes_copied += count; } } fclose(infile); return bytes_copied; } fwrite和fread 这两个函数时c库函数, 是对基础IO read和write的封装. 函数返回已经成功操作的单元个数. 如果返回值不足入参指定的个数, 只有两种可能: 到了文件eof 出现严重IO错误, 比如文件不存在等. 这两种可能都不能用retry-loop来循环, 但可以用feof()和ferror()来判断状态, 如果出现严重IO错误, 程序退出即可, 不必重试, 因为一般重试也没用. /dev/mem和mmap int file_handle = open(filename, O_RDWR); result = mmap64(NULL, alength, PROT_READ|PROT_WRITE, MAP_SHARED, file_handle, physical_address & (int64_t)pagemask); int pagemask = ~(sysconf(_SC_PAGESIZE)-1); uint64_t offset = physical_address - (physical_address & (int64_t)pagemask); int alength = (length + offset + ~pagemask) & pagemask; close(file_handle); return result + offset; 转为字符串的宏 /* Indirect stringification. Doing two levels allows the parameter to be a * macro itself. For example, compile with -DFOO=bar, __stringify(FOO) * converts to \"bar\". */ #define __stringify_1(x...) #x #define __stringify(x...) __stringify_1(x) 使用if else if else结构 处理顺序执行的几个步骤, 每步都可能有异常分支的情形. 把实际干的活放在if判断语句里面.比如下面这个例子, 正常情况下, 每个if后面的判断都执行, 最后进入最后的else分支. if (!dev->netdev_ops) { free_netdev(dev); } else if (register_netdev(dev) dev, \"Failed to register ethernet device for interface %d, port %d\\n\", interface, priv->ipd_port); free_netdev(dev); } else { list_add_tail(&priv->list, &cvm_oct_list); if (cvm_oct_by_pkind[priv->ipd_pkind] == NULL) cvm_oct_by_pkind[priv->ipd_pkind] = priv; else cvm_oct_by_pkind[priv->ipd_pkind] = (void *)-1L; cvm_oct_add_ipd_port(priv); /* Each transmit queue will need its * own MAX_OUT_QUEUE_DEPTH worth of * WQE to track the transmit skbs. */ cvm_oct_mem_fill_fpa(wqe_pool, PER_DEVICE_EXTRA_WQE); num_devices_extra_wqe++; queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, 5*HZ); } uboot中fdt库函数使用 bootbus_node_offset = fdt_path_offset(gd->fdt_blob, \"/soc/bootbus\") node_offset = fdt_next_node(gd->fdt_blob, node_offset, &depth); name = fdt_get_name(gd->fdt_blob, node_offset, NULL); fdt_node_check_compatible(gd->fdt_blob, node_offset, \"cfi-flash\") nodep = fdt_getprop(fdt_addr, node_offset, \"reg\", (int *)&len); p = fdt_get_alias(working_fdt, name); fdt_add_subnode(working_fdt, 0, \"memory\"); 如何判断大小端 关键是头文件endian.h 这里面大小端的宏定义__*_ENDIAN #include #include int main() { #ifdef __LITTLE_ENDIAN printf(\"little\\n\"); #else printf(\"big\\n\"); #endif } 标准的int类型 在C库里面提供 #include int8_t uint64_t 类似的 "},"notes/c_networking_socket高阶用法.html":{"url":"notes/c_networking_socket高阶用法.html","title":"网络编程: Advanced Socket Topics(网摘)","keywords":"","body":" Advanced Socket Topics Out-of-Band Data- [Example 8–10 Flushing Terminal I/O on Receipt of Out-of-Band Data](#example-810-flushing-terminal-io-on-receipt-of-out-of-band-data) Nonblocking Sockets- [Example 8–11 Set Nonblocking Socket](#example-811-set-nonblocking-socket) Asynchronous Socket I/O- [Example 8–12 Making a Socket Asynchronous](#example-812-making-a-socket-asynchronous) Interrupt-Driven Socket I/O- [Example 8–13 Asynchronous Notification of I/O Requests](#example-813-asynchronous-notification-of-io-requests) Signals and Process Group ID Selecting Specific Protocols Address Binding Socket Options inetd Daemon Broadcasting and Determining Network Configuration- [Example 8–14 net/if.h Header File](#example-814-ttnetifhtt-header-file) - [Example 8–15 Obtaining Interface Flags](#example-815-obtaining-interface-flags) - [Example 8–16 Broadcast Address of an Interface](#example-816-broadcast-address-of-an-interface) 参考 Advanced Socket Topics For most programmers, the mechanisms already described are enough to build distributed applications. This section describes additional features. Out-of-Band Data The stream socket abstraction includes out-of-band data. Out-of-band data is a logically independent transmission channel between a pair of connected stream sockets. Out-of-band data is delivered independent of normal data. The out-of-band data facilities must support the reliable delivery of at least one out-of-band message at a time. This message can contain at least one byte of data. At least one message can be pending delivery at any time. With in-band signaling, urgent data is delivered in sequence with normal data, and the message is extracted from the normal data stream. The extracted message is stored separately. Users can choose between receiving the urgent data in order and receiving the data out of sequence, without having to buffer the intervening data. Using MSG_PEEK, you can peek at out-of-band data. If the socket has a process group, a SIGURG signal is generated when the protocol is notified of its existence. A process can set the process group or process ID to deliver SIGURG to with the appropriate fcntl(2) call, as described in Interrupt-Driven Socket I/O for SIGIO. If multiple sockets have out-of-band data waiting for delivery, a select(3C) call for exceptional conditions can determine which sockets have such data pending. A logical mark is placed in the data stream at the point at which the out-of-band data was sent. The remote login and remote shell applications use this facility to propagate signals between client and server processes. When a signal is received, all data up to the mark in the data stream is discarded. To send an out-of-band message, apply the MSG_OOB flag to send(3SOCKET) or sendto(3SOCKET). To receive out-of-band data, specify MSG_OOB to recvfrom(3SOCKET) or recv(3SOCKET). If out-of-band data is taken in line the MSG_OOB flag is not needed. The SIOCATMARK ioctl(2) indicates whether the read pointer currently points at the mark in the data stream: int yes; ioctl(s, SIOCATMARK, &yes); If yes is 1 on return, the next read returns data after the mark. Otherwise, assuming out-of-band data has arrived, the next read provides data sent by the client before sending the out-of-band signal. The routine in the remote login process that flushes output on receipt of an interrupt or quit signal is shown in the following example. This code reads the normal data up to the mark to discard the normal data, then reads the out-of-band byte. A process can also read or peek at the out-of-band data without first reading up to the mark. Accessing this data when the underlying protocol delivers the urgent data in-band with the normal data, and sends notification of its presence only ahead of time, is more difficult. An example of this type of protocol is TCP, the protocol used to provide socket streams in the Internet family. With such protocols, the out-of-band byte might not yet have arrived when recv(3SOCKET) is called with the MSG_OOB flag. In that case, the call returns the error of EWOULDBLOCK. Also, the amount of in-band data in the input buffer might cause normal flow control to prevent the peer from sending the urgent data until the buffer is cleared. The process must then read enough of the queued data to clear the input buffer before the peer can send the urgent data. Example 8–10 Flushing Terminal I/O on Receipt of Out-of-Band Data #include #include ... oob() { int out = FWRITE; char waste[BUFSIZ]; int mark = 0; /* flush local terminal output */ ioctl(1, TIOCFLUSH, (char *) &out); while(1) { if (ioctl(rem, SIOCATMARK, &mark) == -1) { perror(\"ioctl\"); break; } if (mark) break; (void) read(rem, waste, sizeof waste); } if (recv(rem, &mark, 1, MSG_OOB) == -1) { perror(\"recv\"); ... } ... } A facility to retain the position of urgent in-line data in the socket stream is available as a socket-level option, SO_OOBINLINE. See getsockopt(3SOCKET) for usage. With this socket-level option, the position of urgent data remains. However, the urgent data immediately following the mark in the normal data stream is returned without the MSG_OOB flag. Reception of multiple urgent indications moves the mark, but does not lose any out-of-band data. Nonblocking Sockets Some applications require sockets that do not block. For example, a server would return an error code, not executing a request that cannot complete immediately. This error could cause the process to be suspended, awaiting completion. After creating and connecting a socket, issuing a fcntl(2) call, as shown in the following example, makes the socket nonblocking. Example 8–11 Set Nonblocking Socket #include #include ... int fileflags; int s; ... s = socket(AF_INET6, SOCK_STREAM, 0); ... if (fileflags = fcntl(s, F_GETFL, 0) == -1) perror(\"fcntl F_GETFL\"); exit(1); } if (fcntl(s, F_SETFL, fileflags | FNDELAY) == -1) perror(\"fcntl F_SETFL, FNDELAY\"); exit(1); } When performing I/O on a nonblocking socket, check for the error EWOULDBLOCK in errno.h, which occurs when an operation would normally block. accept(3SOCKET), connect(3SOCKET), send(3SOCKET), recv(3SOCKET), read(2), and write(2) can all return EWOULDBLOCK. If an operation such as a send(3SOCKET) cannot be done in its entirety but partial writes work, as when using a stream socket, all available data is processed. The return value is the amount of data actually sent. Asynchronous Socket I/O Asynchronous communication between processes is required in applications that simultaneously handle multiple requests. Asynchronous sockets must be of the SOCK_STREAM type. To make a socket asynchronous, you issue a fcntl(2) call, as shown in the following example. Example 8–12 Making a Socket Asynchronous #include #include ... int fileflags; int s; ... s = socket(AF_INET6, SOCK_STREAM, 0); ... if (fileflags = fcntl(s, F_GETFL ) == -1) perror(\"fcntl F_GETFL\"); exit(1); } if (fcntl(s, F_SETFL, fileflags | FNDELAY | FASYNC) == -1) perror(\"fcntl F_SETFL, FNDELAY | FASYNC\"); exit(1); } After sockets are initialized, connected, and made nonblocking and asynchronous, communication is similar to reading and writing a file asynchronously. Initiate a data transfer by using send(3SOCKET), write(2), recv(3SOCKET), or read(2). A signal-driven I/O routine completes a data transfer, as described in the next section. Interrupt-Driven Socket I/O Set up a SIGIO signal handler with the signal(3C) or sigvec(3UCB) calls. Use fcntl(2) to set the process ID or process group ID to route the signal to its own process ID or process group ID. The default process group of a socket is group 0. Convert the socket to asynchronous, as shown in Asynchronous Socket I/O. The following sample code enables receipt of information on pending requests as the requests occur for a socket by a given process. With the addition of a handler for SIGURG, this code can also be used to prepare for receipt of SIGURG signals. Example 8–13 Asynchronous Notification of I/O Requests #include #include ... signal(SIGIO, io_handler); /* Set the process receiving SIGIO/SIGURG signals to us. */ if (fcntl(s, F_SETOWN, getpid()) Signals and Process Group ID For SIGURG and SIGIO, each socket has a process number and a process group ID. These values are initialized to zero, but can be redefined at a later time with the F_SETOWN fcntl(2) command, as in the previous example. A positive third argument to fcntl(2) sets the socket's process ID. A negative third argument to fcntl(2) sets the socket's process group ID. The only allowed recipient of SIGURG and SIGIO signals is the calling process. A similar fcntl(2), F_GETOWN, returns the process number of a socket. You can also enable reception of SIGURG and SIGIO by using ioctl(2) to assign the socket to the user's process group. /* oobdata is the out-of-band data handling routine */ sigset(SIGURG, oobdata); int pid = -getpid(); if (ioctl(client, SIOCSPGRP, (char *) &pid) Selecting Specific Protocols If the third argument of the socket(3SOCKET) call is 0, socket(3SOCKET) selects a default protocol to use with the returned socket of the type requested. The default protocol is usually correct, and alternate choices are not usually available. When using raw sockets to communicate directly with lower-level protocols or lower-level hardware interfaces, set up de-multiplexing with the protocol argument. Using raw sockets in the Internet family to implement a new protocol on IP ensures that the socket only receives packets for the specified protocol. To obtain a particular protocol, determine the protocol number as defined in the protocol family. For the Internet family, use one of the library routines that are discussed in Standard Routines, such as getprotobyname(3SOCKET). #include #include #include #include ... pp = getprotobyname(\"newtcp\"); s = socket(AF_INET6, SOCK_STREAM, pp->p_proto); Using getprotobyname results in a socket s by using a stream-based connection, but with a protocol type of newtcp instead of the default tcp. Address Binding For addressing, TCP and UDP use a 4-tuple of: Local IP address Local port number Foreign IP address Foreign port number TCP requires these 4-tuples to be unique. UDP does not. User programs do not always know proper values to use for the local address and local port, because a host can reside on multiple networks. The set of allocated port numbers is not directly accessible to a user. To avoid these problems, leave parts of the address unspecified and let the system assign the parts appropriately when needed. Various portions of these tuples can be specified by various parts of the sockets API: bind(3SOCKET) Local address or local port or both connect(3SOCKET) Foreign address and foreign port A call to accept(3SOCKET) retrieves connection information from a foreign client. This causes the local address and port to be specified to the system even though the caller of accept(3SOCKET) did not specify anything. The foreign address and foreign port are returned. A call to listen(3SOCKET) can cause a local port to be chosen. If no explicit bind(3SOCKET) has been done to assign local information, listen(3SOCKET) assigns an ephemeral port number. A service that resides at a particular port can bind(3SOCKET) to that port. Such a service can leave the local address unspecified if the service does not require local address information. The local address is set to in6addr_any, a variable with a constant value in . If the local port does not need to be fixed, a call to listen(3SOCKET) causes a port to be chosen. Specifying an address of in6addr_any or a port number of 0 is known as wildcarding. For AF_INET, INADDR_ANY is used in place of in6addr_any. The wildcard address simplifies local address binding in the Internet family. The following sample code binds a specific port number that was returned by a call to getaddrinfo(3SOCKET) to a socket and leaves the local address unspecified: #include #include ... struct addrinfo *aip; ... if (bind(sock, aip->ai_addr, aip->ai_addrlen) == -1) { perror(\"bind\"); (void) close(sock); return (-1); } Each network interface on a host typically has a unique IP address. Sockets with wildcard local addresses can receive messages that are directed to the specified port number. Messages that are sent to any of the possible addresses that are assigned to a host are also received by sockets with wildcard local addresses. To allow only hosts on a specific network to connect to the server, a server binds the address of the interface on the appropriate network. Similarly, a local port number can be left unspecified, in which case the system selects a port number. For example, to bind a specific local address to a socket, but to leave the local port number unspecified, you could use bind as follows: bzero (&sin, sizeof (sin)); (void) inet_pton (AF_INET6, \"::ffff:127.0.0.1\", sin.sin6_addr.s6_addr); sin.sin6_family = AF_INET6; sin.sin6_port = htons(0); bind(s, (struct sockaddr *) &sin, sizeof sin); The system uses two criteria to select the local port number: Internet port numbers less than 1024 (IPPORT_RESERVED) are reserved for privileged users. Nonprivileged users can use any Internet port number that is greater than 1024. The largest Internet port number is 65535. The port number is not currently bound to some other socket. The port number and IP address of the client are found through either accept(3SOCKET) or getpeername(3SOCKET). In certain cases, the algorithm used by the system to select port numbers is unsuitable for an application due to the two-step creation process for associations. For example, the Internet file transfer protocol specifies that data connections must always originate from the same local port. However, duplicate associations are avoided by connecting to different foreign ports. In this situation, the system would disallow binding the same local address and local port number to a socket if a previous data connection's socket still existed. To override the default port selection algorithm, you must perform an option call before address binding: int on = 1; ... setsockopt(s, SOL_SOCKET, SO_REUSEADDR, &on, sizeof on); bind(s, (struct sockaddr *) &sin, sizeof sin); With this call, local addresses already in use can be bound. This binding does not violate the uniqueness requirement. The system still verifies at connect time that any other sockets with the same local address and local port do not have the same foreign address and foreign port. If the association already exists, the error EADDRINUSE is returned. Socket Options You can set and get several options on sockets through setsockopt(3SOCKET) and getsockopt(3SOCKET). For example, you can change the send or receive buffer space. The general forms of the calls are in the following list: setsockopt(s, level, optname, optval, optlen); and getsockopt(s, level, optname, optval, optlen); The operating system can adjust the values appropriately at any time. The arguments of setsockopt(3SOCKET) and getsockopt(3SOCKET) calls are in the following list: s: Socket on which the option is to be applied level: Specifies the protocol level, such as socket level, indicated by the symbolic constant SOL_SOCKET in sys/socket.h optname: Symbolic constant defined in sys/socket.h that specifies the option optval: Points to the value of the option optlen: Points to the length of the value of the option For getsockopt(3SOCKET), optlen is a value-result argument. This argument is initially set to the size of the storage area pointed to by optval. On return, the argument's value is set to the length of storage used. When a program needs to determine an existing socket's type, the program should invoke inetd(1M) by using the SO_TYPE socket option and the getsockopt(3SOCKET) call: #include #include int type, size; size = sizeof (int); if (getsockopt(s, SOL_SOCKET, SO_TYPE, (char *) &type, &size) After getsockopt(3SOCKET), type is set to the value of the socket type, as defined in sys/socket.h. For a datagram socket, type would be SOCK_DGRAM. inetd Daemon The inetd(1M) daemon is invoked at startup time and gets the services for which the daemon listens from the /etc/inet/inetd.conf file. The daemon creates one socket for each service that is listed in /etc/inet/inetd.conf, binding the appropriate port number to each socket. See the inetd(1M) man page for details. The inetd(1M) daemon polls each socket, waiting for a connection request to the service corresponding to that socket. For SOCK_STREAM type sockets, inetd(1M) accepts (accept(3SOCKET)) on the listening socket, forks (fork(2)), duplicates (dup(2)) the new socket to file descriptors 0 and 1 (stdin and stdout), closes other open file descriptors, and executes (exec(2)) the appropriate server. The primary benefit of using inetd(1M) is that services not in use do not consume machine resources. A secondary benefit is that inetd(1M) does most of the work to establish a connection. The server started by inetd(1M) has the socket connected to its client on file descriptors 0 and 1. The server can immediately read, write, send, or receive. Servers can use buffered I/O as provided by the stdio conventions, as long as the servers use fflush(3C) when appropriate. The getpeername(3SOCKET) routine returns the address of the peer (process) connected to a socket. This routine is useful in servers started by inetd(1M). For example, you could use this routine to log the Internet address such as fec0::56:a00:20ff:fe7d:3dd2, which is conventional for representing the IPv6 address of a client. An inetd(1M) server could use the following sample code: struct sockaddr_storage name; int namelen = sizeof (name); char abuf[INET6_ADDRSTRLEN]; struct in6_addr addr6; struct in_addr addr; if (getpeername(fd, (struct sockaddr *) &name, &namelen) == -1) { perror(\"getpeername\"); exit(1); } else { addr = ((struct sockaddr_in *)&name)->sin_addr; addr6 = ((struct sockaddr_in6 *)&name)->sin6_addr; if (name.ss_family == AF_INET) { (void) inet_ntop(AF_INET, &addr, abuf, sizeof (abuf)); } else if (name.ss_family == AF_INET6 && IN6_IS_ADDR_V4MAPPED(&addr6)) { /* this is a IPv4-mapped IPv6 address */ IN6_MAPPED_TO_IN(&addr6, &addr); (void) inet_ntop(AF_INET, &addr, abuf, sizeof (abuf)); } else if (name.ss_family == AF_INET6) { (void) inet_ntop(AF_INET6, &addr6, abuf, sizeof (abuf)); } syslog(\"Connection from %s\\n\", abuf); } Broadcasting and Determining Network Configuration Broadcasting is not supported in IPv6. Broadcasting is supported only in IPv4. Messages sent by datagram sockets can be broadcast to reach all of the hosts on an attached network. The network must support broadcast because the system provides no simulation of broadcast in software. Broadcast messages can place a high load on a network because broadcast messages force every host on the network to service the broadcast messages. Broadcasting is usually used for either of two reasons: To find a resource on a local network without having its address For functions that require information to be sent to all accessible neighbors To send a broadcast message, create an Internet datagram socket: s = socket(AF_INET, SOCK_DGRAM, 0); Bind a port number to the socket: sin.sin_family = AF_INET; sin.sin_addr.s_addr = htonl(INADDR_ANY); sin.sin_port = htons(MYPORT); bind(s, (struct sockaddr *) &sin, sizeof sin); The datagram can be broadcast on only one network by sending to the network's broadcast address. A datagram can also be broadcast on all attached networks by sending to the special address INADDR_BROADCAST, which is defined in netinet/in.h. The system provides a mechanism to determine a number of pieces of information about the network interfaces on the system. This information includes the IP address and broadcast address. The SIOCGIFCONF ioctl(2) call returns the interface configuration of a host in a single ifconf structure. This structure contains an array of ifreq structures. Every address family supported by every network interface to which the host is connected has its own ifreq structure. The following example shows the ifreq structures defined in net/if.h. Example 8–14 net/if.h Header File struct ifreq { #define IFNAMSIZ 16 char ifr_name[IFNAMSIZ]; /* if name, e.g., \"en0\" */ union { struct sockaddr ifru_addr; struct sockaddr ifru_dstaddr; char ifru_oname[IFNAMSIZ]; /* other if name */ struct sockaddr ifru_broadaddr; short ifru_flags; int ifru_metric; char ifru_data[1]; /* interface dependent data */ char ifru_enaddr[6]; } ifr_ifru; #define ifr_addr ifr_ifru.ifru_addr #define ifr_dstaddr ifr_ifru.ifru_dstaddr #define ifr_oname ifr_ifru.ifru_oname #define ifr_broadaddr ifr_ifru.ifru_broadaddr #define ifr_flags ifr_ifru.ifru_flags #define ifr_metric ifr_ifru.ifru_metric #define ifr_data ifr_ifru.ifru_data #define ifr_enaddr ifr_ifru.ifru_enaddr }; The call that obtains the interface configuration is: /* * Do SIOCGIFNUM ioctl to find the number of interfaces * * Allocate space for number of interfaces found * * Do SIOCGIFCONF with allocated buffer * */ if (ioctl(s, SIOCGIFNUM, (char *)&numifs) == -1) { numifs = MAXIFS; } bufsize = numifs * sizeof(struct ifreq); reqbuf = (struct ifreq *)malloc(bufsize); if (reqbuf == NULL) { fprintf(stderr, \"out of memory\\n\"); exit(1); } ifc.ifc_buf = (caddr_t)&reqbuf[0]; ifc.ifc_len = bufsize; if (ioctl(s, SIOCGIFCONF, (char *)&ifc) == -1) { perror(\"ioctl(SIOCGIFCONF)\"); exit(1); } After this call, buf contains an array of ifreq structures. Every network to which the host connects has an associated ifreq structure. The sort order these structures appear in is: Alphabetical by interface name Numerical by supported address families The value of ifc.ifc_len is set to the number of bytes used by the ifreq structures. Each structure has a set of interface flags that indicate whether the corresponding network is up or down, point-to-point or broadcast, and so on. The following example shows ioctl(2) returning the SIOCGIFFLAGS flags for an interface specified by an ifreq structure. Example 8–15 Obtaining Interface Flags struct ifreq *ifr; ifr = ifc.ifc_req; for (n = ifc.ifc_len/sizeof (struct ifreq); --n >= 0; ifr++) { /* * Be careful not to use an interface devoted to an address * family other than those intended. */ if (ifr->ifr_addr.sa_family != AF_INET) continue; if (ioctl(s, SIOCGIFFLAGS, (char *) ifr) ifr_flags & IFF_UP) == 0 || (ifr->ifr_flags & IFF_LOOPBACK) || (ifr->ifr_flags & (IFF_BROADCAST | IFF_POINTOPOINT)) == 0) continue; } The following example uses the SIOGGIFBRDADDR ioctl(2) command to obtain the broadcast address of an interface. Example 8–16 Broadcast Address of an Interface if (ioctl(s, SIOCGIFBRDADDR, (char *) ifr) ifr_broadaddr, sizeof ifr->ifr_broadaddr); You can also use SIOGGIFBRDADDR ioctl(2) to get the destination address of a point-to-point interface. After the interface broadcast address is obtained, transmit the broadcast datagram with sendto(3SOCKET): sendto(s, buf, buflen, 0, (struct sockaddr *)&dst, sizeof dst) Use one sendto(3SOCKET) for each interface to which the host is connected, if that interface supports the broadcast or point-to-point addressing. 参考 鸿篇巨制: Chapter 8 Socket Interfaces 总目录 "},"notes/c_protobuf介绍.html":{"url":"notes/c_protobuf介绍.html","title":"序列化: protobuf介绍","keywords":"","body":" 先关注一下GPB怎么序列化和反序列化 Parsing and Serialization(C++) 举例 encoding介绍 官方手册: https://developers.google.com/protocol-buffers 先关注一下GPB怎么序列化和反序列化 Parsing and Serialization(C++) Finally, each protocol buffer class has methods for writing and reading messages of your chosen type using the protocol buffer binary format. These include: bool SerializeToString(string* output) const;: serializes the message and stores the bytes in the given string. Note that the bytes are binary, not text; we only use the string class as a convenient container. bool ParseFromString(const string& data);: parses a message from the given string. bool ParseFromArray(const void * data, int size): Parse a protocol buffer contained in an array of bytes. bool SerializeToOstream(ostream* output) const;: writes the message to the given C++ ostream. bool ParseFromIstream(istream* input);: parses a message from the given C++ istream. These are just a couple of the options provided for parsing and serialization. Again, see the Message API reference for a complete list. 举例 比如定义了一个proto是AddressBook, protoc生成了对应的类. # 声明这个类的对象 tutorial::AddressBook address_book; # 从os.in读一个对象出来 fstream input(argv[1], ios::in | ios::binary); address_book.ParseFromIstream(&input) # 写到os.out fstream output(argv[1], ios::out | ios::trunc | ios::binary); address_book.SerializeToOstream(&output) encoding介绍 是binary模式的编码, int是变长的, 叫varints 简明列表:https://developers.google.com/protocol-buffers/docs/encoding Type Meaning Used For 0 Varint int32, int64, uint32, uint64, sint32, sint64, bool, enum 1 64-bit fixed64, sfixed64, double 2 Length-delimited string, bytes, embedded messages, packed repeated fields 3 Start group groups (deprecated) 4 End group groups (deprecated) 5 32-bit fixed32, sfixed32, float "},"notes/c_进程间通信_共享文件和共享内存.html":{"url":"notes/c_进程间通信_共享文件和共享内存.html","title":"进程间通信: 共享文件和共享内存(网摘)","keywords":"","body":"Inter-process communication in Linux: Shared storage Learn how processes synchronize with each other in Linux. This is the first article in a series about interprocess communication (IPC) in Linux. The series uses code examples in C to clarify the following IPC mechanisms: Shared files Shared memory (with semaphores) Pipes (named and unnamed) Message queues Sockets Signals This article reviews some core concepts before moving on to the first two of these mechanisms: shared files and shared memory. Core concepts A process is a program in execution, and each process has its own address space, which comprises the memory locations that the process is allowed to access. A process has one or more threads of execution, which are sequences of executable instructions: a single-threaded process has just one thread, whereas a multi-threaded process has more than one thread. Threads within a process share various resources, in particular, address space. Accordingly, threads within a process can communicate straightforwardly through shared memory, although some modern languages (e.g., Go) encourage a more disciplined approach such as the use of thread-safe channels. Of interest here is that different processes, by default, do not share memory. There are various ways to launch processes that then communicate, and two ways dominate in the examples that follow: A terminal is used to start one process, and perhaps a different terminal is used to start another. The system function fork is called within one process (the parent) to spawn another process (the child). The first examples take the terminal approach. The code examples are available in a ZIP file on my website. Shared files Programmers are all too familiar with file access, including the many pitfalls (non-existent files, bad file permissions, and so on) that beset the use of files in programs. Nonetheless, shared files may be the most basic IPC mechanism. Consider the relatively simple case in which one process (producer) creates and writes to a file, and another process (consumer) reads from this same file: writes +-----------+ reads producer-------->| disk file | The obvious challenge in using this IPC mechanism is that a race condition might arise: the producer and the consumer might access the file at exactly the same time, thereby making the outcome indeterminate. To avoid a race condition, the file must be locked in a way that prevents a conflict between a write operation and any another operation, whether a read or a write. The locking API in the standard system library can be summarized as follows: A producer should gain an exclusive lock on the file before writing to the file. An exclusive lock can be held by one process at most, which rules out a race condition because no other process can access the file until the lock is released. A consumer should gain at least a shared lock on the file before reading from the file. Multiple readers can hold a shared lock at the same time, but no writer can access a file when even a single reader holds a shared lock. A shared lock promotes efficiency. If one process is just reading a file and not changing its contents, there is no reason to prevent other processes from doing the same. Writing, however, clearly demands exclusive access to a file. The standard I/O library includes a utility function named fcntl that can be used to inspect and manipulate both exclusive and shared locks on a file. The function works through a file descriptor, a non-negative integer value that, within a process, identifies a file. (Different file descriptors in different processes may identify the same physical file.) For file locking, Linux provides the library function flock, which is a thin wrapper around fcntl. The first example uses the fcntl function to expose API details. Example 1. The producer program #include #include #include #include #include #define FileName \"data.dat\" #define DataString \"Now is the winter of our discontent\\nMade glorious summer by this sun of York\\n\" void report_and_exit(const char* msg) { perror(msg); exit(-1); /* EXIT_FAILURE */ } int main() { struct flock lock; lock.l_type = F_WRLCK; /* read/write (exclusive versus shared) lock */ lock.l_whence = SEEK_SET; /* base for seek offsets */ lock.l_start = 0; /* 1st byte in file */ lock.l_len = 0; /* 0 here means 'until EOF' */ lock.l_pid = getpid(); /* process id */ int fd; /* file descriptor to identify a file within a process */ if ((fd = open(FileName, O_RDWR | O_CREAT, 0666)) The main steps in the producer program above can be summarized as follows: The program declares a variable of type struct flock, which represents a lock, and initializes the structure's five fields. The first initialization: lock.l_type = F_WRLCK; /* exclusive lock */ makes the lock an exclusive (read-write) rather than a shared (read-only) lock. If the producer gains the lock, then no other process will be able to write or read the file until the producer releases the lock, either explicitly with the appropriate call to fcntl or implicitly by closing the file. (When the process terminates, any opened files would be closed automatically, thereby releasing the lock.) The program then initializes the remaining fields. The chief effect is that the entire file is to be locked. However, the locking API allows only designated bytes to be locked. For example, if the file contains multiple text records, then a single record (or even part of a record) could be locked and the rest left unlocked. The first call to fcntl: if (fcntl(fd, F_SETLK, &lock) tries to lock the file exclusively, checking whether the call succeeded. In general, the fcntl function returns -1 (hence, less than zero) to indicate failure. The second argument F_SETLK means that the call to fcntl does not block: the function returns immediately, either granting the lock or indicating failure. If the flag F_SETLKW (the W at the end is for wait) were used instead, the call to fcntl would block until gaining the lock was possible. In the calls to fcntl, the first argument fd is the file descriptor, the second argument specifies the action to be taken (in this case, F_SETLK for setting the lock), and the third argument is the address of the lock structure (in this case, &lock). If the producer gains the lock, the program writes two text records to the file. After writing to the file, the producer changes the lock structure's l_type field to the unlock value: lock.l_type = F_UNLCK; and calls fcntl to perform the unlocking operation. The program finishes up by closing the file and exiting. Example 2. The consumer program #include #include #include #include #define FileName \"data.dat\" void report_and_exit(const char* msg) { perror(msg); exit(-1); /* EXIT_FAILURE */ } int main() { struct flock lock; lock.l_type = F_WRLCK; /* read/write (exclusive) lock */ lock.l_whence = SEEK_SET; /* base for seek offsets */ lock.l_start = 0; /* 1st byte in file */ lock.l_len = 0; /* 0 here means 'until EOF' */ lock.l_pid = getpid(); /* process id */ int fd; /* file descriptor to identify a file within a process */ if ((fd = open(FileName, O_RDONLY)) 0) /* 0 signals EOF */ write(STDOUT_FILENO, &c, 1); /* write one byte to the standard output */ /* Release the lock explicitly. */ lock.l_type = F_UNLCK; if (fcntl(fd, F_SETLK, &lock) The consumer program is more complicated than necessary to highlight features of the locking API. In particular, the consumer program first checks whether the file is exclusively locked and only then tries to gain a shared lock. The relevant code is: lock.l_type = F_WRLCK; ... fcntl(fd, F_GETLK, &lock); /* sets lock.l_type to F_UNLCK if no write lock */ if (lock.l_type != F_UNLCK) report_and_exit(\"file is still write locked...\"); The F_GETLK operation specified in the fcntl call checks for a lock, in this case, an exclusive lock given as F_WRLCK in the first statement above. If the specified lock does not exist, then the fcntl call automatically changes the lock type field to F_UNLCK to indicate this fact. If the file is exclusively locked, the consumer terminates. (A more robust version of the program might have the consumer sleep a bit and try again several times.) If the file is not currently locked, then the consumer tries to gain a shared (read-only) lock (F_RDLCK). To shorten the program, the F_GETLK call to fcntl could be dropped because the F_RDLCK call would fail if a read-write lock already were held by some other process. Recall that a read-only lock does prevent any other process from writing to the file, but allows other processes to read from the file. In short, a shared lock can be held by multiple processes. After gaining a shared lock, the consumer program reads the bytes one at a time from the file, prints the bytes to the standard output, releases the lock, closes the file, and terminates. Here is the output from the two programs launched from the same terminal with % as the command line prompt: % ./producer Process 29255 has written to data file... % ./consumer Now is the winter of our discontent Made glorious summer by this sun of York In this first code example, the data shared through IPC is text: two lines from Shakespeare's play Richard III. Yet, the shared file's contents could be voluminous, arbitrary bytes (e.g., a digitized movie), which makes file sharing an impressively flexible IPC mechanism. The downside is that file access is relatively slow, whether the access involves reading or writing. As always, programming comes with tradeoffs. The next example has the upside of IPC through shared memory, rather than shared files, with a corresponding boost in performance. Shared memory Linux systems provide two separate APIs for shared memory: the legacy System V API and the more recent POSIX one. These APIs should never be mixed in a single application, however. A downside of the POSIX approach is that features are still in development and dependent upon the installed kernel version, which impacts code portability. For example, the POSIX API, by default, implements shared memory as a memory-mapped file: for a shared memory segment, the system maintains a backing file with corresponding contents. Shared memory under POSIX can be configured without a backing file, but this may impact portability. My example uses the POSIX API with a backing file, which combines the benefits of memory access (speed) and file storage (persistence). The shared-memory example has two programs, named memwriter and memreader, and uses a semaphore to coordinate their access to the shared memory. Whenever shared memory comes into the picture with a writer, whether in multi-processing or multi-threading, so does the risk of a memory-based race condition; hence, the semaphore is used to coordinate (synchronize) access to the shared memory. The memwriter program should be started first in its own terminal. The memreader program then can be started (within a dozen seconds) in its own terminal. The output from the memreader is: This is the way the world ends... Each source file has documentation at the top explaining the link flags to be included during compilation. Let's start with a review of how semaphores work as a synchronization mechanism. A general semaphore also is called a counting semaphore, as it has a value (typically initialized to zero) that can be incremented. Consider a shop that rents bicycles, with a hundred of them in stock, with a program that clerks use to do the rentals. Every time a bike is rented, the semaphore is incremented by one; when a bike is returned, the semaphore is decremented by one. Rentals can continue until the value hits 100 but then must halt until at least one bike is returned, thereby decrementing the semaphore to 99. A binary semaphore is a special case requiring only two values: 0 and 1. In this situation, a semaphore acts as a mutex: a mutual exclusion construct. The shared-memory example uses a semaphore as a mutex. When the semaphore's value is 0, the memwriter alone can access the shared memory. After writing, this process increments the semaphore's value, thereby allowing the memreader to read the shared memory. Example 3. Source code for the memwriter process /** Compilation: gcc -o memwriter memwriter.c -lrt -lpthread **/ #include #include #include #include #include #include #include #include #include \"shmem.h\" void report_and_exit(const char* msg) { perror(msg); exit(-1); } int main() { int fd = shm_open(BackingFile, /* name from smem.h */ O_RDWR | O_CREAT, /* read/write, create if needed */ AccessPerms); /* access permissions (0644) */ if (fd Here's an overview of how the memwriter and memreader programs communicate through shared memory: The memwriter program, shown above, calls the shm_open function to get a file descriptor for the backing file that the system coordinates with the shared memory. At this point, no memory has been allocated. The subsequent call to the misleadingly named function ftruncate: ftruncate(fd, ByteSize); /* get the bytes */ allocates ByteSize bytes, in this case, a modest 512 bytes. The memwriter and memreader programs access the shared memory only, not the backing file. The system is responsible for synchronizing the shared memory and the backing file. The memwriter then calls the mmap function: caddr_t memptr = mmap(NULL, /* let system pick where to put segment */ ByteSize, /* how many bytes */ PROT_READ | PROT_WRITE, /* access protections */ MAP_SHARED, /* mapping visible to other processes */ fd, /* file descriptor */ 0); /* offset: start at 1st byte */ to get a pointer to the shared memory. (The memreader makes a similar call.) The pointer type caddr_t starts with a c for calloc, a system function that initializes dynamically allocated storage to zeroes. The memwriter uses the memptr for the later write operation, using the library strcpy (string copy) function. At this point, the memwriter is ready for writing, but it first creates a semaphore to ensure exclusive access to the shared memory. A race condition would occur if the memwriter were writing while the memreader was reading. If the call to sem_open succeeds: sem_t* semptr = sem_open(SemaphoreName, /* name */ O_CREAT, /* create the semaphore */ AccessPerms, /* protection perms */ 0); /* initial value */ then the writing can proceed. The SemaphoreName (any unique non-empty name will do) identifies the semaphore in both the memwriter and the memreader. The initial value of zero gives the semaphore's creator, in this case, the memwriter, the right to proceed, in this case, to the write operation. After writing, the memwriter increments the semaphore value to 1: if (sem_post(semptr) with a call to the sem_post function. Incrementing the semaphore releases the mutex lock and enables the memreader to perform its read operation. For good measure, the memwriter also unmaps the shared memory from the memwriter address space: munmap(memptr, ByteSize); /* unmap the storage * This bars the memwriter from further access to the shared memory. Example 4. Source code for the memreader process /** Compilation: gcc -o memreader memreader.c -lrt -lpthread **/ #include #include #include #include #include #include #include #include #include \"shmem.h\" void report_and_exit(const char* msg) { perror(msg); exit(-1); } int main() { int fd = shm_open(BackingFile, O_RDWR, AccessPerms); /* empty to begin */ if (fd In both the memwriter and memreader programs, the shared-memory functions of main interest are shm_open and mmap: on success, the first call returns a file descriptor for the backing file, which the second call then uses to get a pointer to the shared memory segment. The calls to shm_open are similar in the two programs except that the memwriter program creates the shared memory, whereas the memreader only accesses this already created memory: int fd = shm_open(BackingFile, O_RDWR | O_CREAT, AccessPerms); /* memwriter */ int fd = shm_open(BackingFile, O_RDWR, AccessPerms); /* memreader */ With a file descriptor in hand, the calls to mmap are the same: caddr_t memptr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0); The first argument to mmap is NULL, which means that the system determines where to allocate the memory in virtual address space. It's possible (but tricky) to specify an address instead. The MAP_SHARED flag indicates that the allocated memory is shareable among processes, and the last argument (in this case, zero) means that the offset for the shared memory should be the first byte. The size argument specifies the number of bytes to be allocated (in this case, 512), and the protection argument indicates that the shared memory can be written and read. When the memwriter program executes successfully, the system creates and maintains the backing file; on my system, the file is /dev/shm/shMemEx, with shMemEx as my name (given in the header file shmem.h) for the shared storage. In the current version of the memwriter and memreader programs, the statement: shm_unlink(BackingFile); /* removes backing file */ removes the backing file. If the unlink statement is omitted, then the backing file persists after the program terminates. The memreader, like the memwriter, accesses the semaphore through its name in a call to sem_open. But the memreader then goes into a wait state until the memwriter increments the semaphore, whose initial value is 0: if (!sem_wait(semptr)) { /* wait until semaphore != 0 */ Once the wait is over, the memreader reads the ASCII bytes from the shared memory, cleans up, and terminates. The shared-memory API includes operations explicitly to synchronize the shared memory segment and the backing file. These operations have been omitted from the example to reduce clutter and keep the focus on the memory-sharing and semaphore code. The memwriter and memreader programs are likely to execute without inducing a race condition even if the semaphore code is removed: the memwriter creates the shared memory segment and writes immediately to it; the memreader cannot even access the shared memory until this has been created. However, best practice requires that shared-memory access is synchronized whenever a write operation is in the mix, and the semaphore API is important enough to be highlighted in a code example. Wrapping up The shared-file and shared-memory examples show how processes can communicate through shared storage, files in one case and memory segments in the other. The APIs for both approaches are relatively straightforward. Do these approaches have a common downside? Modern applications often deal with streaming data, indeed, with massively large streams of data. Neither the shared-file nor the shared-memory approaches are well suited for massive data streams. Channels of one type or another are better suited. Part 2 thus introduces channels and message queues, again with code examples in C. [Download the complete guide to inter-process communication in Linux] 转自: https://opensource.com/article/19/4/interprocess-communication-linux-storage "},"notes/c_pthread_condition和mutex.html":{"url":"notes/c_pthread_condition和mutex.html","title":"并发 任务 事件 和锁.md","keywords":"","body":" 关于pthread条件变量 错误的例子 改进, 用mutex保护predicate 用condition变量 此时有两种写法来signal这个条件变量, 我看第一个写法更好. 总结 关于pthread条件变量 在pthread_cond_wait(pthread_cond_t *restrict cond,pthread_mutex_t *restrict mutex)中, 第二个参数时个mutex锁, 它是干嘛的? 为什么条件变量还要个锁呢? 首先看一下pthread_cond_wait手册的说明: 这个函数自动释放muxte锁, 并导致调用线程在cond变量上阻塞; 成功返回的时候, mutex锁已经被获取, 即调用线程拥有该锁. 这个函数返回不一定是条件满足了, 也可能是被信号打断(信号也不是一定会导致这个线程wakeup, 具体要看系统调度). 所以出来以后, 还要检查条件(predicate) 注意这里说的条件(predicate)和条件变量(pthread_cond_t *restrict cond)是两个东西 predicate是个判断条件 详细的问答在这里: https://stackoverflow.com/questions/14924469/does-pthread-cond-waitcond-t-mutex-unlock-and-then-lock-the-mutex 这里简单解读一下: 错误的例子 // 这里的fSet就是predicate, 即程序的\"业务\"逻辑判断条件; // 但这样判断不对, 因为看到(!fSet)为真, 到sleep()之间, fSet可以被其他线程修改, 导致可能到sleep时, 条件不成立了; 多sleep()一会 // 多睡一会也不要紧, 但sleep()过程中, fSet可能又有变化, 但没办法被当前线程感知到. bool fSet = false; int WaitForTrue() { while (!fSet) { sleep(n); } } 改进, 用mutex保护predicate // 用mutex保护fSet, 当拥有mutex锁的时候, 进程可以放心的查看, 并睡觉了. 不会有人再改fSet // 但问题是, 除了当前线程, 没人能够改fSet, 导致一直退不出循环. pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; bool fSet = false; int WaitForTrue() { pthread_mutex_lock(&mtx); while (!fSet) sleep(n); pthread_mutex_unlock(&mtx); } // 下面的版本多加了一个加锁, 解锁对, 这下别人可以改predicate了. pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; bool fSet = false; int WaitForTrue() { pthread_mutex_lock(&mtx); while (!fSet) { pthread_mutex_unlock(&mtx); // XXXXX sleep(n); // YYYYY pthread_mutex_lock(&mtx); } pthread_mutex_unlock(&mtx); } 用condition变量 上面版本依然存在sleep期间, 对predicate不能感知的问题, 这时候就要用到条件变量了: int WaitForPredicate() { // lock mutex (means:lock access to the predicate) pthread_mutex_lock(&mtx); // we can safely check this, since no one else should be // changing it unless they have the mutex, which they don't // because we just locked it. while (!predicate) { // predicate not met, so begin waiting for notification // it has been changed *and* release access to change it // to anyone wanting to by unlatching the mutex, doing // both (start waiting and unlatching) atomically pthread_cond_wait(&cv,&mtx); // upon arriving here, the above returns with the mutex // latched (we own it). The predicate *may* be true, and // we'll be looping around to see if it is, but we can // safely do so because we own the mutex coming out of // the cv-wait call. } // we still own the mutex here. further, we have assessed the // predicate is true (thus how we broke the loop). // take whatever action needed. // You *must* release the mutex before we leave. Remember, we // still own it even after the code above. pthread_mutex_unlock(&mtx); } 此时有两种写法来signal这个条件变量, 我看第一个写法更好. mutex只保护predicatepthread_mutex_lock(&mtx); TODO: change predicate state here as needed. pthread_mutex_unlock(&mtx); pthread_cond_signal(&cv); mutex也包含cond_signalpthread_mutex_lock(&mtx); TODO: change predicate state here as needed. pthread_cond_signal(&cv); pthread_mutex_unlock(&mtx); 总结 Never change, nor check, the predicate condition unless the mutex is locked. Ever. "},"notes/kernel_user_space_howto.html":{"url":"notes/kernel_user_space_howto.html","title":"kernel space和user space交互(网摘, linux2.6)","keywords":"","body":" Introduction Preamble How to Use This Document and Intended Audience Requirements to Use This Document Obtain Example Source Code Getting More Information Disclaimer Copyright Authors Most recent version Procfs, Sysfs, and Similar Kernel Interfaces Introduction Procfs Description Implementation 1. Legacy procfs API 2. Seq_file API Further Reading and Resources Sysfs Description Implementation Module Parameter API Standard Sysfs API Configfs Description Resources and Further Reading Debugfs Description Implementation Sysctl Description Implementation Resources and Further Reading Character Devices Description Implementation Resources and Further Reading Socket Based Mechanisms Introduction UDP Sockets Description Implementation Netlink Sockets Description Implementation User Space Kernel Module Resources and Further Reading Ioctl Introduction Implementation Resources and Further Reading Kernel System Calls Introduction Implementation Resources and Further Reading Sending Signals from the Kernel to the User Space Introduction Implementation Resources and Further Reading Upcall Introduction Implementation Resources and Further Reading Mmap Introduction Implementation my_open my_close my_mmap Resources and Further Reading Kernel Space, User Space Interfaces This document looks at the numerous and interesting ways the Linux kernel 2.6 interacts with user space programs. We explain sockets, procfs (and similar virtual filesystems), creating new Linux system calls, as well as mundane file and memory handling. Revision History First wiki revision- 2008-09-28 Wiki created: Ariane Keller Revision 0.1 - 2008-10-04 Chapter one and two: Ariane Keller Revision 0.2 - 2008-10-04 Added all chapters, needs format reviewing: Ariane Keller Introduction Preamble This how-to aims to provide an overview over all existing communication mechanisms between Linux user and kernel space. The goal is to present a starting point for a developer who needs to transfer data between Linux kernel modules and user space programs. Each mechanism is described in its own section, with each section further divided into Description, Implementation, and Resources & Further Reading. In the description section we describe the basic mechanism and intended use of the mechanism. The implementation provides an example source code along with a short description. The Resources & Further reading section provides a list of useful articles and book chapters. All source code is tested on Linux kernel 2.6.23. Therefore it may not run on other (earlier or more recent) kernels. However, I try to keep it up to date. If you find a bug or if you know a communication mechanism that is not covered in this how-to please send an email to the author or update this wiki yourself. Be warned: playing with your kernel may cause it to crash, necessitating a system reboot! This means: save all your files and close all programs before you insert a kernel module! You may consider to run experimental kernels in a virtual machine so your actual host system does not get affected by any crashes. How to Use This Document and Intended Audience This document is written for programmers with some experience in system programming. The focus is on a short explanation of individual mechanisms, and providing examples for them. It is assumed that the reader is able to understand the source code (with the documentation provided) and that he can use the examples as a basis for its own modules. It is assumed that a programmer reads first the description section, then has a look at the source code and finally compares the source code with the explanation in the implementation section. The examples are deliberately kept simple, and real life modules will be much more complex. Requirements to Use This Document Linux Distribution with all tools needed to build a new Linux kernel. The example code is tested on 2.6.23. Root privileges (in order to execute the example modules) Knowledge of C programming Basic knowledge of operating system concepts Obtain Example Source Code Throughout this document we show a lot of example source code. This code can be downloaded as a tar.gz. probably better upload the code in the LDP wiki like this: test.tgz Getting More Information Each section contains a list to articles with further information. Personally, I like the following resources: There are some excellent books from O'Reilly Linux Device Driver, 3rd edition, Jonathan Corbet, Alessandro Rubini, Greg Kroah-Hartman, February 2005 Understanding the Linux Kernel, 3rd edition, By Daniel P. Bovet, Marco Cesati, November 2005 Understanding the Linux Network Internals, 1rd edition, By Christian Benvenuti, December 2005 The directory Documentation in the Linux kernel source code And of course the Linux source code itself. An excellent web page for browsing the source code is http://lxr.linux.no/linux/Documentation/. It offers the possibility to search for keywords such as function names, defines, variables etc. It shows all the files that contain this data and provides links to the source code where the variable is used. Disclaimer Use the information in this document at your own risk. I disavow any potential liability for the contents of this document. Use of the concepts, examples, and/or other content of this document is entirely at your own risk. Inserting a kernel module may cause your computer to be unresponsive and you may need to perform a \"hard-reset\". Therefore save any information you want to keep before you insert a kernel module. All copyrights are owned by their owners, unless specifically noted otherwise. Use of a term in this document should not be regarded as affecting the validity of any trademark or service mark. Naming of particular products or brands should not be seen as endorsements. You are strongly recommended to take a backup of your system before major installation and backups at regular intervals. Copyright Copyright © 2008 Ariane Keller. Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.1 or any later version published by the Free Software Foundation with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. GNU Free Documentation License Authors This document was written by Ariane Keller. To contact me you can search with google for my email address. The document was reviewed by Steven to improve its readability. Most recent version The most recent version of this mini-HOWTO will be found on http://wiki.tldp.org/kernel_user_space_howto Procfs, Sysfs, and Similar Kernel Interfaces Introduction These file systems are optional to the Linux kernel, and may not be enabled on your system. The file /lib/modules/uname -r/build/.config will tell you how your kernel is configured. In order to exchange data between user space and kernel space the Linux kernel provides a couple of RAM based file systems. These interfaces are, themselves, based on files. Usually a file represents a single value, but it may also represent a set of values. The user space can access these values by means of the standard read(2) and write(2) functions. For most file systems the read and write function results in a callback function in the Linux kernel which has access to the corresponding value. Despite offering similar functionality, the different RAM based file systems are all designed for separate purposes. However it is easy to use these file systems for other purposes as well. Questions such as \"Which file system should be used?\" or \"Why is there a need for the different file systems?\" often arise on the Linux kernel mailing list. The arguments are controversial and each developer seems to have a unique view. The benefit of using the read and write function in comparison to, for example, socket based approaches, is that the user space has a lot of tools available to send data to the kernel space (e.g. cat(1), echo (1)). These programs are well known to users and they can be used in scripts. Procfs Description The procfs, located in /proc, is the best known interface of this class. It was originally designed to export all kind of process information such as the current status of the process, or all open file descriptors to the user space. Despite its initial purposes, the procfs has been used for a lot of other purposes: provide information about the running system such as cpu information, information about interrupts, about the available memory or the version of the kernel. information about \"ide devices\", \"scsi devices\" and \"tty's\". networking information such as the arp table, network statistics or lists of used sockets There is a special subdirectory: /proc/sys. It allows to configure a lot of parameters of the running system. Usually each file consists of a single value. This value may correspond to: a limit (e.g. maximum buffer size) turn on or off a given functionality (for example routing) or represent some other kernel variable All directories and files below /proc/sys/ are not implemented with the procfs interface. Instead they use a mechanism called sysctl. See section sysctl for further details about sysctl. Note, despite the wide use of the procfs, it is deprecated and should only be used to export information related to a process itself. Implementation In order to use the procfs it needs to be compiled with the Linux kernel source code. This is done by setting the parameter CONFIG_PROC_FS=y. In most standard configurations this is enabled by default Procfs supports two different APIs for kernel modules: The legacy procfs API: It is easy to use as long as the amount of data to be handled is small. In this context small means smaller than one page size (PAGE_SIZE), which is in i386 systems 4096 bytes. The seq_file API: Seq_file was designed to facilitate the handling of read requests. It supports read requests for more than PAGE_SIZE bytes and it provides mechanism to traverse a list, collect the elements of the list, and send all elements to user space. 1. Legacy procfs API procfs.c legacy procfs API The legacy procfs API allows for the creation of files and directories. For each file you have to specify two callback functions: One which is executed when a user reads the file and the other when a user writes to the file. The use of this API is well described in the \"Linux Kernel Procfs Guide\" distributed with the Linux kernel source code. Therefore we give here only a very basic example: A module which creates a directory as well as a file. If your file provides more than PAGE_SIZE bytes of data it is easy to get things wrong. This is due to the API of the read function: read(char page, char **start, off_t off, int count, int eof, void *data) The first parameter of this function is a buffer with the size corresponding to one page. Hence, if there is more data, the read has to be split in multiple pieces. 2. Seq_file API The seq_file API is concerned with read requests solely - no writes. It hides the PAGE_SIZE boundary from the developer and it provides an API to step through a series of objects, collect the data from each of them and put all those data in the file. An example module can be found at http://lwn.net/Articles/22359/. Further Reading and Resources http://lwn.net/Articles/22355/ lwn article \"Driver porting: The seq_file interface\". http://lwn.net/Articles/22359/ Example module that uses seq_file in relation with the lwn article. http://www.linux-mag.com/id/2739?r=s Linux magazine article \"Manipulating \"Seq\" Files\" covers legacy API as well as seq_file. http://kernelnewbies.org/Documents/SeqFileHowTo Documents/SeqFileHowTo seqfile how-to on kernelnewbies Linux Kernel Procfs Guide available in the Linux kernel source code Documentation/DocBook/procfs-guide.tmpl. Description of the legacy procfs API T H E /proc F I L E S Y S T E M Description of entries in /proc, available in the Linux kernel source code Documentation/filesystems/proc.txt Sysfs Description Sysfs was designed to represent the whole device model as seen from the Linux kernel. It contains information about devices, drivers and buses and their interconnections. In order to represent the hierarchy and the interconnections sysfs is heavily structured and contains a lot of links between the individual directories. As for kernel 2.6.23 it contains the following 9 top-level directories: sys/block/ all known block devices such as hda/ ram/ sda/ sys/bus/ all registered buses. Each directory below bus/ holds by default two subdirectories: o device/ for all devices attached to that bus o driver/ for all drivers assigned with that bus. sys/class/ for each device type there is a subdirectory: for example /printer or /sound sys/device/ all devices known by the kernel, organised by the bus they are connected to sys/firmware/ files in this directory handle the firmware of some hardware devices sys/fs/ files to control a file system, currently used by FUSE, a user space file system implementation sys/kernel/ holds directories (mount points) for other filesystems such as debugfs, securityfs. sys/module/ each kernel module loaded is represented with a directory. sys/power/ files to handle the power state of some hardware Implementation In order to use sysfs it needs to be compiled with the Linux kernel source code. This is done by setting the parameter CONFIG_SYSFS=y. The philosophy behind sysfs is to represent each value with a dedicated file. In addition each file has a maximum size of PAGE_SIZE bytes. For a kernel module there are three possibilities to use a file below /sys: module parameter register new subsystem debugfs: debugfs, mounted in /sys/kernel/debug. More information about debugfs. Module Parameter API Similar to command line arguments for applications, Linux kernel modules may allow a set of parameters. These parameters can not only be specified upon module insertion but also during module run time. A module parameter can be defined with the following macro: module_param_named(name, value, type, perm) This macro creates a parameter called \"name\" which corresponds to the variable with name \"value\" of type \"type\". There are many predefined types such as byte (for a single character), int (for an integer) or charp (for a string). It is also possible to add new types. The file include/linux/stat.h provides all predefined types as well as an introduction how to define new types. The module_param macro creates a file called /sys/modules/module_name/name with the access rights specified by perm. Depending on the specified access rights, the file - and thereby the parameter value - can be read or written. If perm is set to 0 the file is not created, and therefore the parameter cannot be accessed during run time. The module does not receive a notification when a user reads or writes a given parameter, but the value is silently changed. Therefore it is not possible to do some additional stuff when a parameter changes its value. This may be acceptable in some circumstances as for changing a debug level, but in most circumstances the module wants to do some additional stuff such as sanity checks or manipulating a data structure. Standard Sysfs API The standard sysfs API uses a dedicated terminology: A file is called an attribute, the function executed upon reading an attribute is called show and the one for writing an attribute store. Before starting with the implementation of a module which uses sysfs you have to figure out which subdirectory it belongs to. If you deal with a bus, it belongs to bus/, with a file system it belongs to fs/ or with a block device it belongs to block/. The API to use depends on the given subdirectory. We first show an example which uses the low level sysfs functions to add a new directory to fs/, and in a second example we show how to add a new entry to the bus/ directory. 1. new fs/ entry sysfs_ex.c creates the directory /sys/fs/myfs/ along with two files first and second. Both containing one single integer value. The first step is to declare our subsystem. This can be done with the use of the decl_subsys macro (on top of the file). This macro creates a struct kset with the name myfs_subsys. The module_init() function performs the proper registration of our subsystem: The macro kobj_set_kset_s initializes myfs_subsys so that it will be part of the fs_subsys. The field myfs_subsys.kobj.ktype points to a structure which holds all the attributes as well as the functions to read and write the attributes. And finally a call to register_subsystem() registers our subsystem. Files are generally represented by a struct attribute. This struct holds the name as well as the access permission for the corresponding file, but no data. Therefore you have to create your own attribute type which consists of at least the struct attribute and the value corresponding to that file. By design all attributes share the same show and store functions. Each time one of these two functions is invoked it gets the corresponding struct attribute as an argument. Therefore in the show and store functions you can obtain the value corresponding to the file being read/written and you can manipulate it accordingly. For this purpose you need the macro container_of(ptr, type, member). ptr is a pointer to the member of the struct. type is the type of the struct this member is emeded in and member is the name of the member within the struct. 2. new bus/ entry sysfs_ex2.c use of sysfs in combination with a bus. It provides the possibility to read and write one value with the help of \"my_pseude_bus\". First of all we define our bus my_pseudo_bus. Then we create our attribute with the help of the BUS_ATTR macro. In the init function we register our pseudo bus and we create a file (attribute). If we would like more than one attribute we would have to use BUS_ATTR several times and provide for each attribute its own store and show function. This example is similar to the debugging facility of the scsi bus, which is implemented in drivers/scsi/scsi_debug.c Resources and Further Reading Understanding the Linux Kernel p. 527 Linux Device Driver p. 362, 377 for the bus directory Linux kernel source code: drivers/scsi/scsi_debug.c Configfs Description The configfs is somewhat the counterpart of the sysfs. It can be seen as a filesystem based manager of kernel objects. An important difference between configfs and sysfs is that in configfs all objects are created from user space with a call to mkdir(2). The kernel responds with creating the attributes (files) and then they can be read and written by the user. If the user no longer needs the files, he calls rmdir(2) and everything gets deleted. Therefore the life cycle of a configfs object is fully controlled by user space. Each time mkdir is invoked a new \"config_item\" is created by the kernel implementation. This config_item represents the files (attributes), the show and store callback functions as well as the associated value. Therefore each mkdir creates a new directory along with new files which represent new values. Configfs has the same limitations than sysfs: each file should represent only one value and it should be smaller than PAGE_SIZE bytes. Implementation In order to use configfs it needs to be compiled with the Linux kernel source code. This is done by setting the parameter CONFIG_CONFIGFS_FS=y. In order to access configfs it has to be mounted with the following command: mount -t configfs none /config The Linux kernel documentation provides a good manual for configfs along with an example module. Therefore we do not describe the configfs implementation aspects. Resources and Further Reading Linux kernel source code: Documentation/filesystems/configfs Debugfs Description Debugfs is a simple to use RAM based file system especially designed for debugging purposes. Developers are encouraged to use debugfs instead of procfs in order to obtain some debugging information from their kernel code. Debugfs is quite flexible: it provides the possibility to set or get a single value with the help of just one line of code but the developer is also allowed to write its own read/write functions, and he can use the seq_file interface described in the procfs section. Implementation In order to use debugfs it needs to be compiled with the Linux kernel source code. This is done by setting the parameter CONFIG_DEBUG_FS=y. Before having access to the debugfs it has to be mounted with the following command. mount -t debugfs none /sys/kernel/debug debugfs.c kernel module that implements the \"one line\" API for a variable of type u8 as well as the API with which you can specify your own read and write functions. All the \"one line\" APIs start with debugfscreate and are listed in include/linux/debugfs.h The API with which you can provide your own read and write functions is similar to the one of procfs. In contrast to sysfs, you may create directories and files without having to care about a given hierarchy. Resources and Further Reading Kernel source code: include/linux/debugfs.h List of all one line APIs. http://kerneltrap.org/node/4394 announcement of the debugfs by Greg KH. http://lwn.net/Articles/115405/ article on lwn about debugfs Sysctl Description The sysctl infrastructure is designed to configure kernel parameters at run time. The sysctl interface is heavily used by the Linux networking subsystem. It can be used to configure some core kernel parameters; represented as files in /proc/sys/*. The values can be accessed by using cat(1), echo(1) or the sysctl(8) commands. If a value is set by the echo command it only persists as long as the kernel is running, but gets lost as soon as the machine is rebooted. In order to change the values permanently they have to be written to the file /etc/sysctl.conf. Upon restarting the machine all values specified in this file are written to the corresponding files in /proc/sys/. Implementation sysctl.c sysctl example module: write an integer to /proc/sys/net/test/value1 and value2 respectively Each entry in the /proc/sys directory is represented by an entry in a table maintained by the Linux kernel, arranged in a hierarchy. A directory is represented by an entry pointing to a subtable. A file is represented by an entry of type struct ctl_table. This entry consists of the data represented by this file along with some access rules. New files and directories can be added by expanding one of the subtables. In this example we add a new directory called test below the /proc/sys/net/ directory. Our directory has got two files: value1 and value2. Each of these files hold an integer variable which can have a value between 10 and 20. The user root is allowed to change the entries whereas normal user are allowed to read the entries. Each file is represented with an entry in the test_table[] array: static ctl_table test_table[] = { { .ctl_name = CTL_UNNUMBERED, .procname = \"value1\", .data = &value1, .maxlen = sizeof(int), .mode = 0644, .proc_handler = &proc_dointvec_minmax, .strategy = &sysctl_intvec, .extra1 = &min, .extra2 = &max }, ... } The struct ctl_table entries are: .ctl_name: For new entries this has to be CTL_UNNUMBERED (according to Documentation/sysctl/ctl_unnumbered.txt). .procname: The name of the file. .data: A reference to the data we want to be shown in the file. maxlen: The size of the data. mode: Access permissions (read, write, execute for user, group, others) proc_handler: The routine which handles read and write requests. There is a set of default routines declared near the end of include/linux/sysctl.h strategy: Some routine that enforces additional access control. In this example it checks that the value to be written is between min and max. static ctl_table test_net_table[] = { { .ctl_name = CTL_UNNUMBERED, .procname = \"test\", .mode = 0555, .child = test_table }, { .ctl_name = 0 } }; This table represents our test directory. The entry .child says that the elements below this directory are represented by the table test_table, discussed above. static ctl_table test_root_table[] = { { .ctl_name = CTL_UNNUMBERED, .procname = \"net\", .mode = 0555, .child = test_net_table }, { .ctl_name = 0 } }; This table represents the directory to which we want to attach our new directory. In this example, this is the net directory. In the module_init() function we have to register this root table with a call to register_sysctl_table(test_root_table); Resources and Further Reading Linux kernel source code: net/core/sysctl_net_core.c Linux kernel Documentation: Documentation/sysctl/ctl_unnumbered.txt Character Devices Description As the name suggests, this interface was designed for character device drivers, and is commonly used for communication between uer and kernel space. (For example, users with sufficient privileges my write directly to the virtual terminal 1 with echo \"hi there\" > /dev/tty1). Each module can register itself as a character device and provide some read and write functions which handle the data. Files representing character devices are located within the /dev directory (where you will also find block devices, but we will not be describing them further). Usually these files correspond to a hardware device. Implementation cdev.c kernel module that prints its majorNumber to the system log. The minorNumber can be chosen to be 0. As with all file system based approaches the module has to specify a read and a write callback function. Therefore, we have to register ourself with the function register_chrdev(unsigned int major, const char name, struct file_operations ops);. major is the major number of this device. We can set it to 0 to let the kernel choose an appropriate number. name is the name of this character device, as it will be shown below the /dev directory. ops is a pointer to the read and write functions. In contrast to most file system based approaches seen so far, the user has to create the device file explicitly with a call to: mknod /dev/arbitrary_name c majorNumber minorNumber Resources and Further Reading man mknod(1) Linux Device Driver chapter 3. Socket Based Mechanisms Introduction Beside the file system based mechanism described in the last section, there are mechanisms based on the socket interface. Sockets allow the Linux kernel to send notifications to a user space application. This is in contrast to the file system mechanisms, where the kernel can alter a file, but the user program does not know of any change until it choses to access that file. The socket based mechanisms allow the applications to listen on a socket, and the kernel can send them messages at any time. This leads to a communication mechanism in which user space and kernel space are equal partners. There are different socket families which can be used to achieve this goal: AF_INET: designed for network communication, but UDP sockets can also be used for the communication between a kernel module and the user space. The use of UDP sockets for node local communication involves a lot of overhead. AF_PACKET: allows the user to define all packet headers. AF_NETLINK (netlink sockets): They are especially designed for the communication between the kernel space and the user space. There are different netlink socket types currently implemented in the kernel, all of which deal with a specific subset of the networking part of the Linux kernel. In this section we briefly cover UDP sockets, as they are the most flexible communication mechanism. Afterwards we look at netlink sockets, with a special focus on \"generic netlink sockets\". UDP Sockets Description We briefly describe UDP sockets, since their usage in user space is well known and they provide a lot of flexibility. With UDP sockets it is possible to have communication between a kernel module on one system and a user space application on an other machine. Implementation udpRecvCallback.c kernel module that provides a callback function to receive messages on port number 5555. udpUser.c user space program that sends a message to the kernel which sends the message back. The user space program sends the string \"hello world\" to the kernel, which listens on port 5555. The module prints the payload to the system log file and sends it back to the application. The application receives the message and prints the payload to stdout. The user space code does not need any explanation, since a lot of documentation is available for socket programming. The module implementation requires some understanding of the Linux kernel. The module_init function performs three actions: Create a socket to receive UDP packets. As is well known from user space, we first create the socket and then we bind it. In addition we specify a callback function, which is executed every time a packet is received on that socket. This callback function is executed in \"interrupt context\". This implies that only a restricted set of operations can be performed in that function, and it is not allowed to send any messages. Therefore we define a work_queue with which we can delay the sending of the answer until we have left interrupt context. Finally, we create a socket to send the answer back to the application. Each time the module receives a packet, the callback function cb_data() is executed. The only thing this callback function does is to submit the send_answer() function to the work_queue. If the kernel decides that there is no better work to do, it executes the send_answer function. The send_answer() function receives the packets, by dequeuing them from the sockets-receive-queue, with skb_dequeue. Since more than one packet can be received by the socket between two consecutive executions of send_answer(), this function may need to dequeue multiple messages. The number of messages in the socket queue can be obtained with a call to skb_queue_len(). After having dequeued the packet the message is printed to the system log and a message is send back to the application with the sock_sendmsg() function. Since this function assumes to be executed from user space, we have to adjust the boundaries for the allowed memory region with the help of the set_fs macro. Netlink Sockets Description Netlink is a special IPC used for transferring information between kernel and user space processes, and provides a full-duplex communication link between the Linux kernel and user space. It makes use of the standard socket APIs for user-space processes, and a special kernel API for kernel modules. Netlink sockets use the address family AF_NETLINK, as compared to AF_INET used by a TCP/IP socket. Netlink sockets have the following advantages over other communication mechanisms: It is simple to interact with the standard Linux kernel as only a constant has to be added to the Linux kernel source code. There is no risk to pollute the kernel or to drive it in instability, since the socket can immediately be used. Netlink sockets are asynchronous as they provide queues, meaning they do not disturb kernel scheduling. This is in contrast to system calls which have to be executed immediately. Netlink sockets provide the possibility of multicast. Netlink sockets provide a truly bidirectional communication channel: A message transfer can be initiated by either the kernel or the user space application. They have less overhead (header and processing) compared to standard UDP sockets. Beside these advantages netlink sockets have two drawbacks: Each entity using netlink sockets has to define its own protocol type (family) in the kernel header file include/linux/netlink.h, necessiating a kernel re-compilation before it can be used. The maximum number of netlink families is fixed to 32. If everyone registers its own protocol this number will be exhausted. To eliminate these drawbacks the \"Generic Netlink Family\" was implemented. It acts as a Netlink multiplexer, in a sense that different applications may use the generic netlink address family. Generic Netlink communications are essentially a series of different communication channels which are multiplexed on a single Netlink family. Communication channels are uniquely identified by channel numbers which are dynamically allocated by the Generic Netlink controller. Kernel or user space users which provide services, establish new communication channels by registering their services with the Generic Netlink controller. Users of the service, then query the controller to see if the service exists and to determine the correct channel number. Each generic netlink family can provide different \"attributes\" and \"commands\". Each command has its own callback function in the kernel module and may receive messages with different attributes. Both commands and attributes, are \"addressed\" by an identifier. Implementation gnKernel.c kernel module that implements a generic netlink family (CONTROL_EXMPL) gnUser.c user space program that sends a message to the kernel and receives an answer back Here we describe how to use this generic netlink protocol to exchange data between user space and kernel space. User Space Although the user space application could be written just with the help of the well known socket operations it is not reasonable to do so. For convenient user space programming there exists the libnl netlink library. It provides functions dedicated to be used for generic netlink socket communication. The libnetlink library supports generic netlink as of version 1.1, so probably you need to download the actual version from http://people.suug.ch/~tgr/libnl/. A program that uses this library needs to be compiled with -lnl specified. User Space Sending Phase:: nl_handle_alloc: create a socket genl_connect: connect to the NETLINK_GENERIC socket family. genl_ctrl_resolve: resolve the ID for the particular generic netlink family we want to talk with. In this example we have called the family CONTROL_EXMPL. genlmsg_put: create the generic netlink message header. In most cases you can leave all the arguments as in the example except the DOC_EXMPL_C_ECHO argument. This specifies which callback function of your kernel module gets executed. In this example there is just one callback function. nlaput*: put the data into the message. All the possibilities are listed in the file attr.c of libnl. The second argument is used by the kernel module to distinguish which attribute was sent. In this example we have only one attribute: DOC_EXMPL_A_MSG which is a null terminated string. nl_send_auto_complete: send the message to the kernel Receiving Phase: nlsocket_modify_cb: Add a callback function to the socket. This callback function gets executed when the socket receives a message. In the callback function the message needs to be decoded. We use nla_parse for this. Using genlmsg_parse would be more specific, but I could not link genlmsg_parse with my program. The nla_get functions are the counterpart of the nlaput functions, and are used to get a specific attribute from the message. nl_recvmsgs_default: wait until a message is received. Kernel Module In the module_init function we need to register our generic netlink family with a call to genl_register_family. As an argument we have to specify a struct genl_family which holds the name of our family (CONTROL_EXMPL). In a second step we have to register the functions that get executed upon receiving a message from the user space. This is done with genl_register_ops which takes as an argument the family to which this function belongs and a struct genl_ops. This struct specifies the actual callback function as well as a security policy checked before the actual callback function gets executed. This means that if you want to receive, for example an integer, but the user space program sends a string, your callback function does not get invoked. The actual callback function is doc_exmpl_echo. It performs two things: prints the received message, and sends a message back to the user space process. The callback function has as an argument a struct genl_info *info which holds the already parsed message. This struct contains an array which has an entry for each possible attribute. Our example has only one attribute (DOC_EXMPL_A_MSG). The data related to this attribute is saved in info->attrs[DOC_EXMPL_A_MSG];. In order to obtain the data for a given attribute there is a simple function: nla_data. The sending process is very similar to the user space sending process. genlmsg_new: this allocates a new skb that will be sent to the user space. Since we do not yet know the final size, we use the macro NLMSG_GOODSIZE. genlmsg_put: fills the generic netlink header. All messages sent by kernel have pid = 0 nla_put_string: write a string into the message. genlmsg_end: finalize the message genlmsg_unicast: send the message back to the user space program. Resources and Further Reading http://www.linuxjournal.com/article/7356 Kernel Korner - Why and How to Use Netlink Socket, Kevin Kaichuan He man 3 netlink man 7 netlink man 3 rtnetlink man 7 rtnetlink http://www.linux-foundation.org/en/Net:Generic_Netlink_HOWTO http://lwn.net/Articles/208755/ Patch: Generic Netlink HOW-TO based on Jamal's original doc http://people.suug.ch/~tgr/libnl/ libnl - netlink library: the library along with a doxygen documentation. Ioctl Introduction In addition to the read and write functions described in the section Procfs, Sysfs, and Similar Kernel Interfaces all file based mechanisms offer the possibility of additional control commands which are supported by the ioctl method.. The ioctl mechanism is implemented as a single system call which multiplexes the different commands to the appropriate kernel space function. A call to ioctl has three arguments: a file (or socket) descriptor, a number identifying the command, and a data argument. The multiplexing is done based on a) the file descriptor and b) the number of the command. Conceptually it would be possible to use any number for your new ioctl command, but this is strongly discouraged, and a system wide unique number should be used instead. This ensures that it is not possible to execute an ioctl on a wrong device leading to unexpected behavior. The exact mechanism to obtain a unique command number is described in the Linux kernel file Documentation/ioctl-number.txt. There are different argument types for an ioctl. The command does not require any data. The command writes some data to the kernel. The command reads some data from the kernel. The kernel module reads the data and exchanges it with some new data. The argument type can be specified when the command number is generated as described in the Linux kernel file Documentation/ioctl-number.txt. Implementation ioctl.c kernel module that uses ioctl in combination with a character device.The ioctl allows to send a message of up to 200 bytes. ioctl_user.c user space program that uses ioctl to send a message to the kernel In order to demonstrate the use of the ioctl mechanism we extend the character device example from section Character Device. Despite sending some data with the read and write system calls, we can now also use ioctl. We have implemented two ioctl commands one for sending data to the kernel and the other for reading data from the kernel. This extension is quite straight forward: Define your ioctl handler function in the struct file_operations (as was already done for the read and write handler function). The callback function consists mainly of a switch statement, which parses the different commands. If the specified command number does not exist ENOTTY is returned, which translates to \"Inappropriate ioctl for device\". Resources and Further Reading Linux Device Driver p. 135 Linux kernel source code: Documentation/ioctl-number.txt Kernel System Calls Introduction System calls are used when a user space program wants to use some data or some service provided by the Linux kernel. In current Linux kernel (2.6.23) there exist 324 system calls. All system calls provide a function which is useful for many application programs (such as file operations, network operations or process related operations). There is no point in adding a very specific system call which can be used only by a specific program. Usually the system call gets invoked by a wrapper function provided by glibc (for example open(), socket(), getpid()). Internally each system call is identified by a unique number. When a user space process invokes a system call, the CPU switches to kernel mode and executes the appropriate kernel function. In order to actually do the switch from user mode to kernel mode there are some assembly instructions. For x86 architectures there are two possibilities: \"int $0x80\", or the newer \"sysenter\". Both possibilities cause: the CPU to switch to kernel mode, the necessary registers to be saved some validity checks invoke the system call corresponding to the number provided by the user space process. If the system call service routine has finished, the system_call function checks whether there is some other work to do (such as rescheduling or signal processing) and it finally restores the user mode process context. Implementation System calls are a low level construct, therefore they are heavily integrated in the Linux kernel. In order to add a new system call to the Linux kernel, you have to modify a number of Linux kernel files: include/asm-i386/unistd.h This file defines the system call numbers provided by the user space to identify the system call. Add your system call at the end and increment the total number of system calls. #define __NR_mysyscall 325 #define NR_syscalls 326 include/linux/syscalls.h This file contains the declaration of all system calls. Add your system call at the end. asmlinkage long sys_mysyscall(char __user *data, int len); arch/i386/kernel/syscall_table.S This file is a table with all the available system calls. In order to make your call accessible add the following line at the bottom of this file: .long sys_mycall add a new directory mysyscall Since the system call has to be integrated in the kernel we add a new directory \"mysyscall\" for our system call in the top level directory of the Linux source code. Makefile Add the new directory to the variable core-y (search for core-y.*+=) in the top level Makefile. Write your new system call function in the file mysyscall/mysyscall.c This example prints the message provided by the user space to the system log file and exchanges it with \"hello world\". #include #include asmlinkage long sys_mysyscall(char __user *buf, int len) { char msg [200]; if(strlen_user(buf) > 200) return -EINVAL; copy_from_user(msg, buf, strlen_user(buf)); printk(\"mysyscall: %s\\n\", msg); copy_to_user(buf, \"hello world\", strlen(\"hello world\")+1); return strlen(\"hello world\")+1; } Add a Makefile to your directory with the following line: obj-y:= mysyscall.o With this you have completed the kernel implementation. After having compiled and installed the new kernel, user space applications can make use of the new system call. The following is an example code which uses the new system call: #include #include #include #include #define MYSYSCALL 325 int main(){ char *buf [20]; memcpy(buf, \"hi kernel\", strlen(\"hi kernel\") +1); syscall(MYSYSCALL, buf, 10); printf(\"kernel said %s\\n\", buf); return 0; } Resources and Further Reading Linux Device Driver, p. 398 Sending Signals from the Kernel to the User Space Introduction This approach is somewhat different from the others, since only the kernel can send a signal to the user space, but not vice versa. Additionally, the amount of data to be sent is quite limited. There are two types of signal APIs in user space: \"normal\" signals which do not have any data, and \"realtime\" signals which carry 32 bits of data. The main difference between them is that real time signals are queued, whereas normal signals are not. This means that if more than one normal signal is sent to a process before it is able to process it, it receives this signal only once, whereas he receives all real time signals. The user space process registers a signal handler function with the kernel. This adds the address of the signal handler function to the process descriptor. This function gets executed each time a certain signal is delivered. The sending phase of a signal consists of two parts: Update the process descriptor with the new signal. If this process is to be rescheduled, or if it returns from an interrupt, it first checks whether there is a signal pending. If yes, it executes first the signal handler and only then does it continue with the rest of the program. Implementation signal_kernel.c kernel module that sends a signal to a user space process. The kernel needs to know the PID of the user space process. Therefore the user space process writes its PID in the debugfs file signalconfpid. signal_user.c user space program that receives the signal. In order to be able to send a signal from kernel space to user space, the kernel needs to know the pid of the user space process. Therefore in this dummy example the user space process sends its pid to the kernel. As soon as the kernel module receives the pid, it looks for the corresponding process descriptor, and sends a signal to it. All information related to the signal is saved in a struct siginfo. We are able to send 32 bit of data in si_int. In order that the user space interprets this signal as a real time signal and that we can therefore access the data part, we need to set si_code to SI_QUEUE. Otherwise our data is not received by the user space function. Resources and Further Reading man 7 signal Understanding the Linux Kernel chapter 11 Upcall Introduction The upcall functionality of the Linux kernel allows a kernel module to invoke a function in user space. It is possible to start a program in user space, and give it some command line arguments, as well as setting environment variables. Implementation usermodehelper.c kernel module that starts a process callee.c user space program that will be executed on behalf of the kernel Our example consists of a kernel module usermodehelper and a user space program callee. Since callee is not started within a shell, we cannot use printf to verify its correct execution, instead we let him run the beep command and let the kernel module specify how many times to beep. The callee gets invoked by the following function prototype: int call_usermodehelper(char path, char *argv, char envp, enum umh_wait wait) With the following arguments: path: The path to the user space program argv: The arguments for the user space program envp: A set of environment varialbes umh_wait: Enum that says whether the kernel module has to wait or whether it can continue with the execution: UMH_NO_WAIT: don't wait at all UMH_WAIT_EXEC: wait for the exec, but not the process UMH_WAIT_PROC: wait for the process to complete Resources and Further Reading Understanding the Linux Network Internals Chapter 5 Mmap Introduction Memory mapping is the only way to transfer data between user and kernel spaces that does not involve explicit copying, and is the fastest way to handle large amounts of data. There is a major difference between the conventional read(2) and write(2) functions and mmap. While data is transfered with mmap no \"control\" messages are exchanged. This means that a user space process can put data into the memory, but that the kernel does not know that new data is available. The same holds for the opposite scenario: The kernel puts its data into the shared memory, but the user space process does not get a notification of this event. This characteristic implies that memory mapping has to be used with some other communication means that transfers control messages, or that the shared memory needs to be checked in regular intervals for new content. Similar to the read and write function calls mmap can be used with different file systems and with sockets. Implementation mmap_simple_kernel.c kernel module that provides the mmap system call based on debugfs. mmap_user.c user space program that will share a memory area with the kernel module Of course we need some memory that we want to map between user space and kernel space. In this example we share some RAM but if you are writing a device driver, this could be the memory of your device. We use debugfs and attach the memory area to a file. This allows the user space process to access the shared memory area with the help of a file descriptor. The user space program uses the system calls open, mmap, memcpy and close, which are all documented in the Linux man pages. The kernel module is more challenging. Please note that the discussed module offers only the most basic functionality, and that usually mmap is just one of the functions provided to handle a file. In the module_init function we create the file as discussed in section debugfs. Since it is an example module, our file_operations struct contains only three entries: my_open, my_close and my_mmap. my_open In this function we allocate the memory that will later be shared with the user space process. Since memory mapping is done on a PAGE_SIZE basis we allocate one page of memory with get_zeroed_page(GFP_KERNEL) We initialize the memory with a message form the kernel that states the name of this file. We set the private_data pointer of this file to the allocated memory in order to access it later in the my_mmap and my_close function my_close This function frees the memory allocated during my_open. my_mmap This function initializes the vm_area_struct to point to the mmap functions specific to our implementation. mmap_open und mmap_close are used only for bookkeeping. mmap_nopages is called when the user space process references a memory area that is not in its memory. Therefore mmap_nopages does the real mapping between user space and kernel space. The most important function is virt_to_page which takes the memory area to be shared as an argument and returns a struct page * that can be used by the user space to access this memory area. Resources and Further Reading man(2) mmap Linux Device Driver, Chapter 15 "},"notes/CentOS_系统性能优化配置.html":{"url":"notes/CentOS_系统性能优化配置.html","title":"CentOS 性能优化系统配置","keywords":"","body":" System Level performance tuning CPU frequency limits disable firewall disable selinux disable auditd disable swap System Level performance tuning CPU frequency Where performance \"score\" is the key index for benchmarks/applications tests, make sure CPU frequency is set to \"performance\", which is usually set to \"conservative\" by default. sudo cpupower frequency-set -g performance to check the current settings and CPU frequency: sudo cpupower frequency-info sudo cpupower -c all frequency-info | grep \"current CPU frequency\" limits add below to /etc/security/limits.conf and reboot * soft noproc 65535 * hard noproc 65535 * soft nofile 655350 * hard nofile 655350 * soft memlock 60397977 * hard memlock 60397977 disable firewall sudo systemctl stop firewalld.service sudo systemctl disable firewalld.service disable selinux sestatus setenforce 0 sudo vi /etc/sysconfig/selinux SELINUX=disabled disable auditd sudo systemctl stop auditd.service sudo systemctl disable auditd.service disable swap sudo swapoff "},"notes/as_title_driver.html":{"url":"notes/as_title_driver.html","title":"内核 设备和驱动相关","keywords":"","body":"如题 "},"notes/platform_device_driver.html":{"url":"notes/platform_device_driver.html","title":"平台驱动杂记","keywords":"","body":" 关于软中断 关于各种地址 关于smp_processor_id() 问题现象 分析 关于initrd 关于struct file A. inode, inode也有fop指针(struct file_operations) A1. inode_operations B. file_operations 关于vm, 内存管理 A. vm_operations_struct A1. vm_fault A11. 物理页框 物理页与线性地址 arm / arm64 ia64 x86 历史 分配页框 使用 关于module 头文件 MODULE_AUTHOR MODULE_DESCRIPTION宏 导出符号 声明模块参数 如何修改参数 为初始化函数建立别名init_module 如何编译module 如何编译module完全版 reboot相关 CPLD之平台驱动 platform相关驱动 cpld_probe platform_driver核心结构 关于uio 注册 uio_device uio info相关 resource 结构体 uio内核操作 用户态程序读是读中断, 读写uio用mmap uio注册设备, 在cpld_probe里面有注册uio设备 用户态uio lib 用户态使用uio UIO总结: 平台设备初始化 设备驱动如何注册的? 现在的问题是, dev哪里来的? 关于device_add和sysfs 很多地方都会调device_add(), 比如: 那么device_add()干了什么? octeon-platform.c dev都是在这里生成的? 驱动相关 driver的初始化位置 platform驱动注册流程 I2C是个字符设备 i2c设备初始化 很多地方都会调用i2c_add_driver() 关于device_create 原型及主要流程 i2c中的使用例子 uio中的使用例子 如何创建设备? 方法1: 在父节点调用 方法2: 直接在dts里声明 mips kernel mips 64bit空间 mips ioremap ioread8的入参应该是个CPU地址, 而不是物理地址 编译时的条件检测，条件为真则导致编译错误 运行时的条件检测，条件为真则触发运行时exception jiffies jiffies_64和HZ local_irq_disable() 关中断 current 表示当前进程 preempt_disable() 禁止抢占 自旋锁spin_lock() ARRAY_SIZE(arr) 各种内核编译宏, 见compiler.h dump_stack() 驱动打印调用栈 linux中断 中断注册request_threaded_irq() 关于中断号 信号量 debugfs 在/sys/kernel/debug/创建文件 关于notify_chain 创建proc的entry, 并绑定相关的文件操作 在sys目录下创建个class 驱动中使用工作队列轮询 在设备相关结构体内添加work work的处理函数 在初始化的时候开始工作队列 在rmmod时删除这个work 关于软中断 硬件上也有软中断: 也叫做编程异常, 就是一个指令可以触发的异常 linux里面的软中断: 这其实是个软件的概念, 也叫做可延迟函数, 下面说的软中断特指linux软中断. 软中断是静态分配的, 能够并发的运行在多个CPU上, 必须用可重入的函数, 必须用自旋锁保护. 而同类型的tasklet是串行执行的, 不可能出现在两个CPU上同时执行一个tasklet的情况. 深入理解linux内核里面说, 触发和执行软中断都是在同一个CPU上做的. --是否RPS CPU解决了这个问题? 软中断的触发: raise_softirq(), 本质上是调用wakeup_softirq()去唤醒本地CPU的ksoftirqd内核线程. 内核在几个固定点上, 会主动的调用do_softiq()去处理一些已经在等待处理的软中断, 那里处理不过来的, 再交给ksoftirqd线程. local_bh_enable()激活软中断时 do_IRQ()的irq_exit()时 多处理器的核间中断 每个CPU都有自己的ksoftirqd线程, 用来处理那些需要频繁激活自己的软中断, 比如网络. 关于各种地址 有好几种地址: CPU虚拟地址: kmalloc(), vmalloc()得到的地址, 可以表示为void * CPU物理地址: 不能直接使用, 表示为phys_addr_t或resource_size_t, 需要用ioremap()转成虚拟地址后使用 在/proc/iomem里面有体现. bus地址: 从外设角度来看的地址, 主要对DMA来说的 CPU CPU Bus Virtual Physical Address Address Address Space Space Space +-------+ +------+ +------+ | | |MMIO | Offset | | | | Virtual |Space | applied | | C +-------+ --------> B +------+ ----------> +------+ A | | mapping | | by host | | +-----+ | | | | bridge | | +--------+ | | | | +------+ | | | | | CPU | | | | RAM | | | | Device | | | | | | | | | | | +-----+ +-------+ +------+ +------+ +--------+ | | Virtual |Buffer| Mapping | | X +-------+ --------> Y +------+ 在初始化阶段, kernel知道一个PCI设备的bar地址(A), 然后转成物理地址(B), 并做为资源struct resource 保存在/proc/iomem. 驱动使用ioremap()获得(B)的虚拟地址(C), 然后就可以用ioread32(C)来访问总线地址(A). 对一个支持DMA的设备来说, 驱动使用kmalloc()或类似的接口获得虚拟地址(X), 通过TLB对应到物理地址(Y). 驱动可以直接使用地址(X)来操作这个buffer, 但设备不可以. 设备的DMA不认CPU的虚拟地址. 在一些简单的系统下面, 设备可以使用物理地址(Y), 这些系统没有IOMMU, 设备看到的地址和CPU的物理地址是一致的; 但其他系统下, 这么做是不行的--这里面设备访问的地址需要经过IOMMU, 比如把总线地址(Z)(其实也就是这个设备看到的地址), 转换成物理地址(Y). 这个过程是dma_map_single()完成的, 它把输入的虚拟地址(X)映射到总线地址(Z), 填在一个表里, 这个表由IOMMU来解析. 使用完了用dma_unmap_single()来取消映射. 那么什么地址是可以DMA的呢? 我理解首先要在物理上地址连续, 比如kmalloc()可以DMA, 而vmalloc()就不行 关于smp_processor_id() 这个函数的作用是获得当前CPU ID. 问题现象 但在调LSI驱动的时候, 经常打印: BUG: using smp_processor_id() in preemptible [00000000] code: systemd-udevd/1318 Call trace: 或 BUG: using smp_processor_id() in preemptible [00000000] code: mount/1973 Call trace: 每个对这个RAID卡操作的命令都会有很多这样的打印, 但基本功能还OK, 只是有这些打印 分析 在打开了CONFIG_DEBUG_PREEMPT选项之后, smp_processor_id是个宏, 实际调用debug_smp_processor_id() #ifdef CONFIG_DEBUG_PREEMPT extern unsigned int debug_smp_processor_id(void); # define smp_processor_id() debug_smp_processor_id() 而debug_smp_processor_id()会做一些检查, 检查什么呢? 调用smp_processor_id是否安全. 首先, 在以下场景下是安全的: 当前这个内核thread(或者说thread这个进程在内核态执行)不可抢占, 即current_thread_info()->preempt_count不为0时.--为什么pteempt count不为0就不可抢占了呢? 肯定有地方把这个值++了, 比如一些中断或中断下半部调了什么禁止抢占的函数 我们明确知道这个thread只绑定在当前这个CPU上, 即这个thread不会被调度到其他CPU上 为什么要在不能抢占的情况下调用呢? 整个系统干的活可以认为是以(thread+核)为单位的, 比如在一个多核系统下, 一个核C1正运行thread A, 此时如果发生抢占, 即运行任务A的核C1被安排去干其他活了, 比如去搞thread B了, 等B运行完了回来可就不一定是A了, 也许A被安排到其他core上去了. 结合smp_processor_id()这个函数, 比如是thread A调的, 如果调的时候还是core C1, 但中间发生内核抢占, 结果是C1去干其他事了, 现在是C2来接手, 但返回C1的CPU ID, 不就乱了吗? 关于initrd 在init/initramfs.c中，populate_rootfs(), 会首先调用unpack_to_rootfs(__initramfs_start, __initramfs_size)来从kernel内置的rootfs解压. 其次，会从initrd_start这个变量地址解压，此时需要两个变量，一个就是前面的initrd_start，还有就是initrd_end。 那么initrd_start和initrd_end是哪里设的呢? 关于struct file 内核中文件表示为struct file结构体, 有fop, 也保存了inode指针 struct file { /* * fu_list becomes invalid after file_free is called and queued via * fu_rcuhead for RCU freeing */ union { struct list_head fu_list; struct rcu_head fu_rcuhead; } f_u; struct path f_path; #define f_dentry f_path.dentry //指向inode的指针 struct inode *f_inode; /* cached value */ //见A const struct file_operations *f_op; //见B /* * Protects f_ep_links, f_flags, f_pos vs i_size in lseek SEEK_CUR. * Must not be taken from IRQ context. */ spinlock_t f_lock; #ifdef CONFIG_SMP int f_sb_list_cpu; #endif atomic_long_t f_count; unsigned int f_flags; fmode_t f_mode; loff_t f_pos; struct fown_struct f_owner; const struct cred *f_cred; struct file_ra_state f_ra; u64 f_version; #ifdef CONFIG_SECURITY void *f_security; #endif /* needed for tty driver, and maybe others */ void *private_data; #ifdef CONFIG_EPOLL /* Used by fs/eventpoll.c to link all the hooks to this file */ struct list_head f_ep_links; struct list_head f_tfile_llink; #endif /* #ifdef CONFIG_EPOLL */ struct address_space *f_mapping; #ifdef CONFIG_DEBUG_WRITECOUNT unsigned long f_mnt_write_state; #endif #ifdef CONFIG_FUMOUNT atomic_t f_getcount; struct list_head fumount_list; #endif }; A. inode, inode也有fop指针(struct file_operations) inode 处理\"实体\" /* * Keep mostly read-only and often accessed (especially for * the RCU path lookup and 'stat' data) fields at the beginning * of the 'struct inode' */ struct inode { umode_t i_mode; unsigned short i_opflags; kuid_t i_uid; kgid_t i_gid; unsigned int i_flags; #ifdef CONFIG_FS_POSIX_ACL struct posix_acl *i_acl; struct posix_acl *i_default_acl; #endif const struct inode_operations *i_op; //见A1 struct super_block *i_sb; struct address_space *i_mapping; #ifdef CONFIG_SECURITY void *i_security; #endif /* Stat data, not accessed from path walking */ unsigned long i_ino; /* * Filesystems may only read i_nlink directly. They shall use the * following functions for modification: * * (set|clear|inc|drop)_nlink * inode_(inc|dec)_link_count */ union { const unsigned int i_nlink; unsigned int __i_nlink; }; dev_t i_rdev; loff_t i_size; struct timespec i_atime; struct timespec i_mtime; struct timespec i_ctime; spinlock_ i_lock; /* i_blocks, i_bytes, maybe i_size */ unsigned short i_bytes; unsigned int i_blkbits; blkcnt_t i_blocks; #ifdef __NEED_I_SIZE_ORDERED seqcount_t i_size_seqcount; #endif /* Misc */ unsigned long i_state; struct mutex i_mutex; unsigned long dirtied_when; /* jiffies of first dirtying */ struct hlist_node i_hash; struct list_head i_wb_list; /* backing dev IO list */ struct list_head i_lru; /* inode LRU list */ struct list_head i_sb_list; union { struct hlist_head i_dentry; struct rcu_head i_rcu; }; u64 i_version; atomic_t i_count; atomic_t i_dio_count; atomic_t i_writecount; //重要!!! 指向fop的指针. const struct file_operations *i_fop; /* former ->i_op->default_file_ops */ struct file_lock *i_flock; struct address_space i_data; #ifdef CONFIG_QUOTA struct dquot *i_dquot[MAXQUOTAS]; #endif struct list_head i_devices; union { struct pipe_inode_info *i_pipe; struct block_device *i_bdev; struct cdev *i_cdev; }; __u32 i_generation; #ifdef CONFIG_FSNOTIFY __u32 i_fsnotify_mask; /* all events this inode cares about */ struct hlist_head i_fsnotify_marks; #endif #ifdef CONFIG_IMA atomic_t i_readcount; /* struct files open RO */ #endif void *i_private; /* fs or device private pointer */ }; A1. inode_operations inode的操作主要针对\"实体存在\"的操作, 比如创建, 删除, 重命名等 struct inode_operations { struct dentry * (*lookup) (struct inode *,struct dentry *, unsigned int); void * (*follow_link) (struct dentry *, struct nameidata *); int (*permission) (struct inode *, int); struct posix_acl * (*get_acl)(struct inode *, int); int (*readlink) (struct dentry *, char __user *,int); void (*put_link) (struct dentry *, struct nameidata *, void *); int (*create) (struct inode *,struct dentry *, umode_t, bool); int (*link) (struct dentry *,struct inode *,struct dentry *); int (*unlink) (struct inode *,struct dentry *); int (*symlink) (struct inode *,struct dentry *,const char *); int (*mkdir) (struct inode *,struct dentry *,umode_t); int (*rmdir) (struct inode *,struct dentry *); int (*mknod) (struct inode *,struct dentry *,umode_t,dev_t); int (*rename) (struct inode *, struct dentry *, struct inode *, struct dentry *); int (*setattr) (struct dentry *, struct iattr *); int (*getattr) (struct vfsmount *mnt, struct dentry *, struct kstat *); int (*setxattr) (struct dentry *, const char *,const void *,size_t,int); ssize_t (*getxattr) (struct dentry *, const char *, void *, size_t); ssize_t (*listxattr) (struct dentry *, char *, size_t); int (*removexattr) (struct dentry *, const char *); int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start, u64 len); int (*update_time)(struct inode *, struct timespec *, int); int (*atomic_open)(struct inode *, struct dentry *, struct file *, unsigned open_flag, umode_t create_mode, int *opened); } ____cacheline_aligned; B. file_operations file_operations表述的是系统和\"实体存在\"之间的交互, 比如读写, mmap, 锁等 struct file_operations { struct module *owner; loff_t (*llseek) (struct file *, loff_t, int); ssize_t (*read) (struct file *, char __user *, size_t, loff_t *); ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *); ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t); ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t); int (*readdir) (struct file *, void *, filldir_t); unsigned int (*poll) (struct file *, struct poll_table_struct *); long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long); long (*compat_ioctl) (struct file *, unsigned int, unsigned long); int (*mmap) (struct file *, struct vm_area_struct *); //见下节, 内存管理 int (*open) (struct inode *, struct file *); int (*flush) (struct file *, fl_owner_t id); int (*release) (struct inode *, struct file *); int (*fsync) (struct file *, loff_t, loff_t, int datasync); int (*aio_fsync) (struct kiocb *, int datasync); int (*fasync) (int, struct file *, int); int (*lock) (struct file *, int, struct file_lock *); ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int); unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long); int (*check_flags)(int); int (*flock) (struct file *, int, struct file_lock *); ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int); ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int); int (*setlease)(struct file *, long, struct file_lock **); long (*fallocate)(struct file *file, int mode, loff_t offset, loff_t len); int (*show_fdinfo)(struct seq_file *m, struct file *f); } __do_const; 关于vm, 内存管理 一个进程可以有很多vm_area_struct, 它们用红黑树来管理include/linux/mm_types.h /* * This struct defines a memory VMM memory area. There is one of these * per VM-area/task. A VM area is any part of the process virtual memory * space that has a special rule for the page-fault handlers (ie a shared * library, the executable area etc). */ struct vm_area_struct { /* The first cache line has the info for VMA tree walking. */ unsigned long vm_start; /* Our start address within vm_mm. */ unsigned long vm_end; /* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; /* * Largest free memory gap in bytes to the left of this VMA. * Either between this VMA and vma->vm_prev, or between one of the * VMAs below us in the VMA rbtree and its ->vm_prev. This helps * get_unmapped_area find a free area of the right size. */ unsigned long rb_subtree_gap; /* Second cache line starts here. */ struct mm_struct *vm_mm; /* The address space we belong to. */ pgprot_t vm_page_prot; /* Access permissions of this VMA. */ unsigned long vm_flags; /* Flags, see mm.h. */ /* * For areas with an address space and backing store, * linkage into the address_space->i_mmap interval tree, or * linkage of vma in the address_space->i_mmap_nonlinear list. */ union { struct { struct rb_node rb; unsigned long rb_subtree_last; } linear; struct list_head nonlinear; } shared; /* * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma * list, after a COW of one of the file pages. A MAP_SHARED vma * can only be in the i_mmap tree. An anonymous MAP_PRIVATE, stack * or brk vma (with NULL file) can only be in an anon_vma list. */ struct list_head anon_vma_chain; /* Serialized by mmap_sem & * page_table_lock */ struct anon_vma *anon_vma; /* Serialized by page_table_lock */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; //见A /* Information about our backing store: */ unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units, *not* PAGE_CACHE_SIZE */ struct file * vm_file; /* File we map to (can be NULL). */ void * vm_private_data; /* was vm_pte (shared mem) */ #ifndef CONFIG_MMU struct vm_region *vm_region; /* NOMMU mapping region */ #endif #ifdef CONFIG_NUMA struct mempolicy *vm_policy; /* NUMA policy for the VMA */ #endif struct vm_area_struct *vm_mirror;/* PaX: mirror vma or NULL */ }; A. vm_operations_struct include/linux/mm.h /* * These are the virtual MM functions - opening of an area, closing and * unmapping it (needed to keep files on disk up-to-date etc), pointer * to the functions called when a no-page or a wp-page exception occurs. */ struct vm_operations_struct { void (*open)(struct vm_area_struct * area); void (*close)(struct vm_area_struct * area); int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf); //见A1 /* notification that a previously read-only page is about to become * writable, if an error is returned it will cause a SIGBUS */ int (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf); /* called by access_process_vm when get_user_pages() fails, typically * for use by special VMAs that can switch between memory and hardware */ ssize_t (*access)(struct vm_area_struct *vma, unsigned long addr, void *buf, size_t len, int write); #ifdef CONFIG_NUMA /* * set_policy() op must add a reference to any non-NULL @new mempolicy * to hold the policy upon return. Caller should pass NULL @new to * remove a policy and fall back to surrounding context--i.e. do not * install a MPOL_DEFAULT policy, nor the task or system default * mempolicy. */ int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new); /* * get_policy() op must add reference [mpol_get()] to any policy at * (vma,addr) marked as MPOL_SHARED. The shared policy infrastructure * in mm/mempolicy.c will do this automatically. * get_policy() must NOT add a ref if the policy at (vma,addr) is not * marked as MPOL_SHARED. vma policies are protected by the mmap_sem. * If no [shared/vma] mempolicy exists at the addr, get_policy() op * must return NULL--i.e., do not \"fallback\" to task or system default * policy. */ struct mempolicy *(*get_policy)(struct vm_area_struct *vma, unsigned long addr); int (*migrate)(struct vm_area_struct *vma, const nodemask_t *from, const nodemask_t *to, unsigned long flags); #endif /* called by sys_remap_file_pages() to populate non-linear mapping */ int (*remap_pages)(struct vm_area_struct *vma, unsigned long addr, unsigned long size, pgoff_t pgoff); }; typedef struct vm_operations_struct __no_const vm_operations_struct_no_const; A1. vm_fault /* * vm_fault is filled by the the pagefault handler and passed to the vma's * ->fault function. The vma's ->fault is responsible for returning a bitmask * of VM_FAULT_xxx flags that give details about how the fault was handled. * * pgoff should be used in favour of virtual_address, if possible. If pgoff * is used, one may implement ->remap_pages to get nonlinear mapping support. */ struct vm_fault { unsigned int flags; /* FAULT_FLAG_xxx flags */ pgoff_t pgoff; /* Logical page offset based on vma */ void __user *virtual_address; /* Faulting virtual address */ struct page *page; //见A11 /* ->fault handlers should return a * page here, unless VM_FAULT_NOPAGE * is set (which is also implied by * VM_FAULT_ERROR). */ }; A11. 物理页框 /* * Each physical page in the system has a struct page associated with * it to keep track of whatever it is we are using the page for at the * moment. Note that we have no way to track which tasks are using * a page, though if it is a pagecache page, rmap structures can tell us * who is mapping it. * * The objects in struct page are organized in double word blocks in * order to allows us to use atomic double word operations on portions * of struct page. That is currently only used by slub but the arrangement * allows the use of atomic double word operations on the flags/mapping * and lru list pointers also. */ struct page { /* First double word block */ unsigned long flags; /* Atomic flags, some possibly * updated asynchronously */ struct address_space *mapping; /* If low bit clear, points to * inode address_space, or NULL. * If page mapped as anonymous * memory, low bit is set, and * it points to anon_vma object: * see PAGE_MAPPING_ANON below. */ /* Second double word */ struct { union { pgoff_t index; /* Our offset within mapping. */ void *freelist; /* slub/slob first free object */ bool pfmemalloc; /* If set by the page allocator, * ALLOC_NO_WATERMARKS was set * and the low watermark was not * met implying that the system * is under some pressure. The * caller should try ensure * this page is only used to * free other pages. */ }; union { #if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) && defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE) /* Used for cmpxchg_double in slub */ unsigned long counters; #else /* * Keep _count separate from slub cmpxchg_double data. * As the rest of the double word is protected by * slab_lock but _count is not. */ unsigned counters; #endif struct { union { /* * Count of ptes mapped in * mms, to show when page is * mapped & limit reverse map * searches. * * Used also for tail pages * refcounting instead of * _count. Tail pages cannot * be mapped and keeping the * tail page _count zero at * all times guarantees * get_page_unless_zero() will * never succeed on tail * pages. */ atomic_t _mapcount; struct { /* SLUB */ unsigned inuse:16; unsigned objects:15; unsigned frozen:1; }; int units; /* SLOB */ }; atomic_t _count; /* Usage count, see below. */ }; }; }; /* Third double word block */ union { struct list_head lru; /* Pageout list, eg. active_list * protected by zone->lru_lock ! */ struct { /* slub per cpu partial pages */ struct page *next; /* Next partial slab */ #ifdef CONFIG_64BIT int pages; /* Nr of partial slabs left */ int pobjects; /* Approximate # of objects */ #else short int pages; short int pobjects; #endif }; struct list_head list; /* slobs list of pages */ struct slab *slab_page; /* slab fields */ }; /* Remainder is not double word aligned */ union { unsigned long private; /* Mapping-private opaque data: * usually used for buffer_heads * if PagePrivate set; used for * swp_entry_t if PageSwapCache; * indicates order in the buddy * system if PG_buddy is set. */ #if USE_SPLIT_PTLOCKS # ifndef CONFIG_PREEMPT_RT_FULL spinlock_t ptl; # else spinlock_t *ptl; # endif #endif struct kmem_cache *slab_cache; /* SL[AU]B: Pointer to slab */ struct page *first_page; /* Compound tail pages */ }; /* * On machines where all RAM is mapped into kernel address space, * we can simply calculate the virtual address. On machines with * highmem some memory is mapped into kernel virtual memory * dynamically, so we need a place to store that address. * Note that this field could be 16 bits on x86 ... ;) * * Architectures with slow multiplication can define * WANT_PAGE_VIRTUAL in asm/page.h */ #if defined(WANT_PAGE_VIRTUAL) void *virtual; /* Kernel virtual address (NULL if not kmapped, ie. highmem) */ #endif /* WANT_PAGE_VIRTUAL */ #ifdef CONFIG_WANT_PAGE_DEBUG_FLAGS unsigned long debug_flags; /* Use atomic bitops on this */ #endif #ifdef CONFIG_KMEMCHECK /* * kmemcheck wants to track the status of each byte in a page; this * is a pointer to such a status block. NULL if not tracked. */ void *shadow; #endif #ifdef LAST_NID_NOT_IN_PAGE_FLAGS int _last_nid; #endif } 物理页与线性地址 arm / arm64 arch/arm/include/asm/memory.h #define PAGE_OFFSET UL(CONFIG_PAGE_OFFSET) #define __virt_to_phys(x) ((x) - PAGE_OFFSET + PHYS_OFFSET) #define __phys_to_virt(x) ((x) - PHYS_OFFSET + PAGE_OFFSET) arch/arm64/include/asm/memory.h #define PAGE_OFFSET UL(0xffffc00000000000) //这里的PHYS_OFFSET应该是内存的起始物理地址 #define __virt_to_phys(x) (((phys_addr_t)(x) - PAGE_OFFSET + PHYS_OFFSET)) #define __phys_to_virt(x) ((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET)) /* * These are *only* valid on the kernel direct mapped RAM memory. * Note: Drivers should NOT use these. They are the wrong * translation for translating DMA addresses. Use the driver * DMA support - see dma-mapping.h. */ static inline phys_addr_t virt_to_phys(const volatile void *x) { return __virt_to_phys((unsigned long)(x)); } ia64 /* * The top three bits of an IA64 address are its Region Number. * Different regions are assigned to different purposes. */ #define RGN_SHIFT (61) #define RGN_BASE(r) (__IA64_UL_CONST(r) x86 /* * This handles the memory map. * * A __PAGE_OFFSET of 0xC0000000 means that the kernel has * a virtual address space of one gigabyte, which limits the * amount of physical memory you can use to about 950MB. * * If you want more physical memory than this then see the CONFIG_HIGHMEM4G * and CONFIG_HIGHMEM64G options in the kernel configuration. */ #define __PAGE_OFFSET _AC(CONFIG_PAGE_OFFSET, UL) 历史 x86历史上, 内存被划分为: ZONE_DMA: 物理内存低16M; 受限于ISA总线 ZONE_NORMAL: 物理内存高于16M但低于896M; 被线性的映射到\"第四个GB\"; 内核可以直接访问; \"内核页表\" ZONE_HIGHMEM: 物理内存高于896M; 内核不能直接使用; 在64位模式下总是空的. 分配页框 内核函数alloc_page()或__get_free_page()用来获得物理页框, 可以接受一些mask include/linux/gfp.h #define __GFP_DMA ((__force gfp_t)___GFP_DMA) //从DMA区分页框 #define __GFP_HIGHMEM ((__force gfp_t)___GFP_HIGHMEM) //从HIGHMEM分页框 如果没有以上标记, 则从ZONE_NORMAL分配页框 另外还有一些标记: #define __GFP_WAIT ((__force gfp_t)___GFP_WAIT) /* Can wait and reschedule? */ #define __GFP_HIGH ((__force gfp_t)___GFP_HIGH) /* Should access emergency pools? */ #define __GFP_IO ((__force gfp_t)___GFP_IO) /* Can start physical IO? */ #define __GFP_FS ((__force gfp_t)___GFP_FS) /* Can call down to low-level FS? */ #define __GFP_ZERO ((__force gfp_t)___GFP_ZERO) /* Return zeroed page on success */ 组合标记类型: #define GFP_ATOMIC(__GFP_HIGH) #define GFP_NOIO (__GFP_WAIT) #define GFP_NOFS (__GFP_WAIT | __GFP_IO) #define GFP_KERNEL (__GFP_WAIT | __GFP_IO | __GFP_FS) //一般使用这个标记的是从NORMAL区分的 #define GFP_TEMPORARY (__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_RECLAIMABLE) #define GFP_USER (__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL) #define GFP_HIGHUSER (__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL | __GFP_HIGHMEM) #define GFP_HIGHUSER_MOVABLE (__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL | __GFP_HIGHMEM | __GFP_MOVABLE) #define GFP_IOFS (__GFP_IO | __GFP_FS) #define GFP_TRANSHUGE (GFP_HIGHUSER_MOVABLE | __GFP_COMP | __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | __GFP_NO_KSWAPD) 使用 # define __pa(x) ((x) - PAGE_OFFSET) 推荐使用 static inline unsigned long virt_to_phys (volatile void *address) # define __va(x) ((x) + PAGE_OFFSET) 推荐使用 static inline void* phys_to_virt (unsigned long address) #define page_to_phys(page) (page_to_pfn(page) > PAGE_SHIFT) #define pfn_to_kaddr(pfn) __va((pfn) 关于module 头文件 #include /* Needed by all modules */ #include /* Needed for KERN_INFO */ MODULE_AUTHOR MODULE_DESCRIPTION宏 类似的宏, 会编译在特殊的段\".modinfo\" MODULE_LICENSE(\"GPL\"); MODULE_INFO(license, \"GPL\") #ifdef MODULE static const char __mod_license__LINE__[] __attribute__((section(\".modinfo\"),unused)) = \"license=GPL\" #else 空 #define MODULE_PARM_DESC(_parm, desc) \\ __MODULE_INFO(parm, _parm, #_parm \":\" desc) 导出符号 在__ksymtab段, 定义一个{地址, 名字}的结点 EXPORT_SYMBOL(sym) __EXPORT_SYMBOL(sym, sec) __EXPORT_SYMBOL(sym, \"\") extern typeof(sym) sym; static const char __kstrtab_##sym[] __attribute__((section(\"__ksymtab_strings\"), aligned(1))) = #sym static const struct kernel_symbol __ksymtab_##sym __attribute__((section(\"__ksymtab\" sec), unused)) = { (unsigned long)&sym, __kstrtab_##sym } 其中, struct kernel_symbol { unsigned long value; const char *name; }; 声明模块参数 参数类型byte, short, ushort, int, uint, long, ulong, charp, bool or invbool也可以用module_param_array声明数组 在段__param保存一个下面的结构体 struct kernel_param { const char *name; u16 perm; u16 flags; param_set_fn set; param_get_fn get; union { void *arg; const struct kparam_string *str; const struct kparam_array *arr; }; }; module_param(name, type, perm) module_param(panic_counter, ulong, 0644); module_param_named(name, name, type, perm) module_param_call(name, param_set_##type, param_get_##type, &value, perm); static struct kernel_param __moduleparam_const __param_##name __attribute__ ((unused,__section__ (\"__param\"),aligned(sizeof(void *)))) \\ = { __param_str_##name, perm, isbool ? KPARAM_ISBOOL : 0, set, get, { arg } } __MODULE_INFO(parmtype, name##type, #name \":\" _type) 参数的权限如下: #define S_IRWXU 00700 #define S_IRUSR 00400 #define S_IWUSR 00200 #define S_IXUSR 00100 #define S_IRWXG 00070 #define S_IRGRP 00040 #define S_IWGRP 00020 #define S_IXGRP 00010 #define S_IRWXO 00007 #define S_IROTH 00004 #define S_IWOTH 00002 #define S_IXOTH 00001 如何修改参数 加载module时: 例1: insmod hello.ko msg_buf=veryCD n_arr=1,2,3,4,5,6 例2: modprobe usbcore blinkenlights=1 模块加载以后, 还可以使用sysfs文件系统动态修改: 例: /sys/module/modulexxx/parameters/xxx 模块编进内核时, 在启动linux时加选项: 例: usbcore.blinkenlights=1 为初始化函数建立别名init_module 可能系统调用要用到这个函数, SYSCALL_DEFINE3(init_module ...), 前提是这个module已经被insmod进内核空间?--不是的, SYSCALL_DEFINE3只是定义一个系统调用sys_init_module. 那这个init_module的别名有什么用? #include module_init(reboot_helper_init); module_init(initfn) static inline initcall_t __inittest(void) {return initfn;} int init_module(void) __attribute__((alias(#initfn))); 如何编译module obj-m := hello.o kDIR := /lib/modules/2.6.18-53.el5/build make –C $(KDIR) M=$(PWD) modules obj-m := megaraid_sas.o megaraid_sas-objs := megaraid_sas_base.o megaraid_sas_fusion.o megaraid_sas_fp.o megaraid_sas_raptor.o 如何编译module完全版 https://www.kernel.org/doc/Documentation/kbuild/modules.txt reboot相关 reboot_helper_init() request_mem_region(mem_address, mem_size, \"reset_safe_reboot_info\"); reboot_info = ioremap(mem_address,mem_size); node = of_find_node_by_name(NULL, \"cpld\"); prop = of_get_address(node, 0, &cpld_size, NULL); cpld_start = of_translate_address(node, prop); of_node_put(node); request_mem_region(cpld_start, cpld_size, \"cpld\"); _cpld = ioremap(cpld_start, cpld_size); atomic_notifier_chain_register(&panic_notifier_list, &panic_helper_notifier_block); register_reboot_notifier(&reboot_helper_notifier_block); board_specific_init(reboot_info); reboot_helper_exit() iounmap(_cpld); release_mem_region(cpld_start, cpld_size); unregister_reboot_notifier(&reboot_helper_notifier_block); atomic_notifier_chain_unregister(&panic_notifier_list, &panic_helper_notifier_block); iounmap(reboot_info); release_mem_region(mem_address, mem_size); CPLD之平台驱动 驱动常用函数 //可移植性好的io读, 需#include ioread8(off_reg) // irq disable_irq(dev_info->irq) enable_irq(dev_info->irq); //#include // of与dts对应的处理函数 of_get_property() //#include //驱动错误输出 dev_warn() dev_dbg() dev_err() dev_warn(&pdev->dev, \"Device node %s has missing or invalid \" \"cell-index property. Using 0.\\n\", pdev->dev.of_node->full_name); // 驱动申请内存 platdata = devm_kzalloc(&pdev->dev,sizeof(*platdata), GFP_KERNEL); platform相关驱动 struct platform_device { const char * name; int id; struct device dev; u32 num_resources; struct resource * resource; struct platform_device_id *id_entry; /* arch specific additions */ struct pdev_archdata archdata; }; struct platform_driver { int (*probe)(struct platform_device *); int (*remove)(struct platform_device *); void (*shutdown)(struct platform_device *); int (*suspend)(struct platform_device *, pm_message_t state); int (*resume)(struct platform_device *); struct device_driver driver; struct platform_device_id *id_table; }; cpld_probe static int __devinit cpld_probe(struct platform_device *pdev) //获得cpld序号, 参见dts indexp = of_get_property(pdev->dev.of_node, \"cell-index\", &len); index = be32_to_cpup(indexp); //申请数据结构内存, platdata = devm_kzalloc(&pdev->dev,sizeof(*platdata), GFP_KERNEL); uioinfo_nmi = devm_kzalloc(&pdev->dev,sizeof(*uioinfo_nmi), GFP_KERNEL); uioinfo_com = devm_kzalloc(&pdev->dev,sizeof(*uioinfo_com), GFP_KERNEL); uioinfo_fqm = devm_kzalloc(&pdev->dev,sizeof(*uioinfo_fqm), GFP_KERNEL); //申请name内存 name = devm_kzalloc(&pdev->dev,UIO_NAME_SIZE, GFP_KERNEL); name_nmi = devm_kzalloc(&pdev->dev,UIO_NAME_SIZE, GFP_KERNEL); name_fqm = devm_kzalloc(&pdev->dev,UIO_NAME_SIZE, GFP_KERNEL); //填结构体 uioinfo_com->name = \"cpld0\"; uioinfo_com->version = \"0.01\"; uioinfo_com->irq = UIO_IRQ_NONE; //memory map, 是给usr看的吗? of_address_to_resource(pdev->dev.of_node, 0, &res); struct uio_mem *uiomem = &uioinfo_com->mem[0]; uiomem->name = \"cpld memory map\"; uiomem->memtype = UIO_MEM_PHYS; uiomem->addr = res.start; uiomem->size = res.end - res.start + 1; //内部调用ioremap, 映射到内核空间 uiomem->internal_addr = of_iomap(pdev->dev.of_node, 0); //中断 int irq = of_irq_to_resource(pdev->dev.of_node, cpld_com_irq, NULL); uioinfo_com->irq = irq; uioinfo_com->handler = cpld_com_irq_handler; uioinfo_com->irq_flags = IRQF_SHARED; /* Good practice to support sharing interrupt lines */ //注册platform_device的私有成员, 就是本驱动的核心结构platdata platdata->uioinfo_nmi = uioinfo_nmi; platdata->uioinfo_com = uioinfo_com; platdata->uioinfo_fqm = uioinfo_fqm; platform_set_drvdata(pdev, platdata); //注册UIO uio_register_device(&pdev->dev, uioinfo_nmi); uio_register_device(&pdev->dev, uioinfo_com); uio_register_device(&pdev->dev, uioinfo_fqm); // 和dts对应的 static const struct of_device_id cpld_ids[] = { { .compatible = \"alu,cpld\", }, {}, }; platform_driver核心结构 static struct platform_driver cpld_driver = { .driver = { .owner = THIS_MODULE, .name = \"cpld\", .of_match_table = cpld_ids, }, .probe = cpld_probe, .remove = __devexit_p(cpld_remove), }; 关于uio uio是个字符设备, 设备文件挂在devtmpfs下面, read和write是针对中断来说的. 而mmap用于对地址空间的访问. 注册 uio_register_device(&pdev->dev, uioinfo_com) uio_major = register_chrdev(0, \"uio\", &uio_fops) static const struct file_operations uio_fops = { .owner = THIS_MODULE, .open = uio_open, .release = uio_release, .read = uio_read, .write = uio_write, .mmap = uio_mmap, .poll = uio_poll, .fasync = uio_fasync, }; uio_device struct uio_device { struct module *owner; struct device *dev; int minor; atomic_t event; struct fasync_struct *async_queue; wait_queue_head_t wait; int vma_count; struct uio_info *info; struct kobject *map_dir; struct kobject *portio_dir; }; uio info相关 struct uio_info { struct uio_device *uio_dev; const char *name; const char *version; struct uio_mem mem[MAX_UIO_MAPS]; struct uio_port port[MAX_UIO_PORT_REGIONS]; long irq; unsigned long irq_flags; void *priv; irqreturn_t (*handler)(int irq, struct uio_info *dev_info); int (*mmap)(struct uio_info *info, struct vm_area_struct *vma); int (*open)(struct uio_info *info, struct inode *inode); int (*release)(struct uio_info *info, struct inode *inode); int (*irqcontrol)(struct uio_info *info, s32 irq_on); }; struct uio_mem { const char *name; phys_addr_t addr; unsigned long size; int memtype; void __iomem *internal_addr; struct uio_map *map; }; resource 结构体 struct resource { resource_size_t start; resource_size_t end; const char *name; unsigned long flags; struct resource *parent, *sibling, *child; }; uio内核操作 static int uio_open(struct inode *inode, struct file *filep) //调用device自己的open ret = idev->info->open(idev->info, inode); // 这个read阻塞的. 读出中断个数? static ssize_t uio_read(struct file *filep, char __user *buf, size_t count, loff_t *ppos) DECLARE_WAITQUEUE(wait, current); add_wait_queue(&idev->wait, &wait); do while 1 循环: set_current_state(TASK_INTERRUPTIBLE); if 某个条件 //先从idev->event里面读出evnet个数, 再拷贝给用户空间的buffer copy_to_user(buf, &event_count, count) schedule() __set_current_state(TASK_RUNNING) remove_wait_queue(&idev->wait, &wait) //read用了waitqueue, 那么肯定有地方调用wake_up函数, 见下: static irqreturn_t uio_interrupt(int irq, void *dev_id) //调用device自己的handler ret = idev->info->handler(irq, idev->info) uio_event_notify(idev->info) //对atomic_unchecked_t event这个变量增1 wake_up_interruptible(&idev->wait) static unsigned int uio_poll(struct file *filep, poll_table *wait) poll_wait(filep, &idev->wait, wait) poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p) p->_qproc(filp, wait_address, p) if (listener->event_count != atomic_read_unchecked(&idev->event)) return POLLIN | POLLRDNORM; return 0 static int uio_mmap(struct file *filep, struct vm_area_struct *vma) case UIO_MEM_PHYS: uio_mmap_physical(vma) remap_pfn_range() 用户态程序读是读中断, 读写uio用mmap 上面的uio_interrupt函数, 被注册到irq上, irq号由dts得来 request_irq(info->irq, uio_interrupt, info->irq_flags, info->name, idev); uio_interrupt()会调用uio设备注册时提供的handler(), 只有该handler()返回IRQ_HANDLED时, 才会更新uio文件, 此时用户态会发现这个uio文件可读, 有中断来了. 所以, uio中断有两步, 先是uio.ko的内核态中断, 然后是uio用户态从uio设备文件\"读\"到的中断. uio注册设备, 在cpld_probe里面有注册uio设备 uio_register_device(struct device *parent, struct uio_info *info) __uio_register_device(THIS_MODULE, parent, info) struct uio_device *idev init_uio_class() //申请uio_device结点 idev = kzalloc(sizeof(*idev), GFP_KERNEL) idev->owner = owner; idev->info = info; init_waitqueue_head(&idev->wait); atomic_set(&idev->event, 0); //申请新的minor号 uio_get_minor(idev) //创建设备文件 idev->dev = device_create(uio_class->class, parent, MKDEV(uio_major, idev->minor), idev, \"uio%d\", idev->minor) dev = kzalloc(sizeof(*dev), GFP_KERNEL) //一些初始化结构变量的填写 //创建\"dev\" 文件 device_register(dev) uio_dev_add_attributes(idev) info->uio_dev = idev if (idev->info->irq >= 0) request_irq(idev->info->irq, uio_interrupt, idev->info->irq_flags, idev->info->name, idev) uio设备注册的时候, 有个irq_handler, 在中断上下文中执行的. 用户态uio lib 提供了从名字找info, 以及用户态open等操作(open时自动mmap) 用户态使用uio //获得设备首地址 void *uiodrv_get_base(DeviceNbr dev) //真正的首地址是从mmap而来, 详见libuio.c void *uio_mmap(struct uio_info_t* info, int map_num) //drv_cpld.c uio_hww_cpld_early_init(void) for (i = 0; i 在uio hww初始化时, 会起一个进程, 处理uio中断 uiodrv_start_interrupt_task(253) xt_create (\"UIOI\", prio, 0x4000, 0, 0, &task_id); xt_start (task_id, T_PREEMPT | T_NOASR | T_TSLICE, interrupt_task_body, args); while (1) uiodrv_process_devices(); for each dev: 中断速率限制, 在currSet中, FD_CLR中断太频繁的dev do { ret = select(maxSocket + 1, &currSet, NULL, NULL, timeout); //在select收到EINTR时重试 } while ((ret fd在select列表内, 则handle=1 uiodrv_handle_fd(inst, timestamp, handle); read(inst->fd) inst->handler(inst->device_nr, inst->user_arg); //向这个文件fd写0是禁止中断, 写1是使能中断 如果设置了auto unmask标记, 则向fd写1 UIO总结: 在cpld内核驱动probe里面创建UIO设备, 此时在/dev下面会有uio%d的字符设备; 在用户态open的时候mmap, 并保存在info->maps [i].map, 后可通过uio_get_mem_map()获得 平台设备初始化 cpld_init(void) platform_driver_register(&cpld_driver); 设备驱动如何注册的? int platform_driver_register(struct platform_driver *drv) drv->driver.bus = &platform_bus_type; drv->driver.probe = platform_drv_probe; drv->driver.remove = platform_drv_remove; drv->driver.shutdown = platform_drv_shutdown; //这里注册用的是device_driver, 后面可以用to_platform_driver(drv)把platform_driver找回来 driver_register(&drv->driver); bus_add_driver(drv); driver_attach(drv) bus_for_each_dev(drv->bus, NULL, drv, __driver_attach); //以下是__driver_attach干的活, 优先从fdt里面match, 然后依次是ACPI, id table, name platform_match(struct device *dev, struct device_driver *drv) //如果match driver_probe_device(drv, dev) really_probe(dev, drv) //暂时把dev的driver指针置为当前drv dev->driver = drv driver_sysfs_add(dev) //platform的bus没有probe方法, 不调用dev->bus->probe(dev) drv->probe(dev) //在这里就是platform_drv_probe(struct device *_dev) //把platform的drv和dev通过指针偏移找回来 drv = to_platform_driver(_dev->driver); dev = to_platform_device(_dev); //调用具体drv的probe, 比如CPLD, cpld_probe(struct platform_device *pdev) drv->probe(dev); //执行过程见上面的cpld_probe() driver_bound(dev) //把dev的p->knode_driver加到drv的p->klist_devices链表尾部 klist_add_tail(&dev->p->knode_driver, &dev->driver->p->klist_devices); 现在的问题是, dev哪里来的? 可能是通过调用of_device_add(dev), 在解析dtb时得来. 关于device_add和sysfs device_add()就会生成sysfs和/dev下面的文件 很多地方都会调device_add(), 比如: device_register() device_initialize(dev); device_add(dev); pci_bus_add_device() of_device_add() dev_name(&ofdev->dev) set_dev_node() device_add(&ofdev->dev) platform_device_add(struct platform_device *pdev) pdev->dev.parent = &platform_bus pdev->dev.bus = &platform_bus_type device_add(&pdev->dev) spi_add_device() 那么device_add()干了什么? device_private_init(dev) setup_parent(dev, parent) //加到/sys?--yes kobject_add(&dev->kobj, dev->kobj.parent, NULL) platform_notify(dev) device_create_file(dev, &uevent_attr) if (MAJOR(dev->devt)) device_create_file(dev, &devt_attr) device_create_sys_dev_entry(dev) //通过devtmpfs创建设备结点 devtmpfs_create_node(dev) //查找/dev? vfs_path_lookup(dev_mnt->mnt_root, dev_mnt, nodename, LOOKUP_PARENT, &nd) dentry = lookup_create(&nd, 0) //mknod??? vfs_mknod(nd.path.dentry->d_inode, dentry, mode, dev->devt) device_add_class_symlinks(dev) device_add_attrs(dev) //把dev加到bus的dev链表里 bus_add_device(dev) dpm_sysfs_add(dev) device_pm_add(dev) blocking_notifier_call_chain() //通知用户态有ADD事件, mdev会处理, 并创建设备文件 kobject_uevent(&dev->kobj, KOBJ_ADD) bus_probe_device(dev) device_attach(dev) if (dev->driver) device_bind_driver(dev) else //遍历驱动 bus_for_each_drv(dev->bus, NULL, dev, __device_attach) driver_probe_device(drv, dev) octeon-platform.c static struct of_device_id __initdata octeon_ids[] = { { .compatible = \"simple-bus\", }, { .compatible = \"cavium,octeon-6335-uctl\", }, { .compatible = \"cavium,octeon-3860-bootbus\", }, { .compatible = \"cavium,mdio-mux\", }, { .compatible = \"gpio-leds\", }, {}, }; dev都是在这里生成的? device_initcall(octeon_publish_devices) octeon_publish_devices() of_platform_bus_probe(NULL, octeon_ids, NULL) of_platform_bus_create(child, matches, parent) of_platform_device_create(bus, NULL, parent); for_each_child_of_node(bus, child) of_platform_bus_create(child, matches, &dev->dev) of_device_add(dev) 驱动相关 driver的初始化位置 start_kernel ...最后 rest_init() kernel_thread(kernel_init, NULL, CLONE_FS | CLONE_SIGHAND) //新内核线程 do_basic_setup() init_workqueues(); cpuset_init_smp(); usermodehelper_init(); init_tmpfs(); driver_init(); devtmpfs_init(); devices_init(); buses_init(); classes_init(); firmware_init(); hypervisor_init(); platform_bus_init(); system_bus_init(); cpu_dev_init(); memory_dev_init(); init_irq_proc(); do_ctors(); do_initcalls(); cpu_idle() i2c驱动 spi驱动 platform驱动都会调用driver_register() platform驱动注册流程 kernel_init()中do_basic_setup()->driver_init()->platform_bus_init()->...初始化platform bus(虚拟总线) 设备向内核注册的时候platform_device_register()->platform_device_add()->...内核把设备挂在虚拟的platform bus下 驱动注册的时候platform_driver_register()->driver_register()->bus_add_driver()->driver_attach()->bus_for_each_dev() 对每个挂在虚拟的platform bus的设备做__driver_attach()->driver_probe_device()->drv->bus->match()==platform_match()比较strncmp(pdev->name, drv->name, BUS_ID_SIZE)，如果相符就调用platform_drv_probe()->driver->probe()，如果probe成功则绑定该设备到该驱动. I2C是个字符设备 ssize_t i2cdev_write (struct file *file, const char __user *buf, size_t count, loff_t *offset) struct i2c_client *client = (struct i2c_client *)file->private_data copy_from_user() i2c_master_send(client,tmp,count) struct i2c_adapter *adap=client->adapter; struct i2c_msg msg; msg.addr = client->addr; msg.flags = client->flags & I2C_M_TEN; msg.len = count; msg.buf = (char *)buf; i2c_transfer(adap, &msg, 1) adap->algo->master_xfer(adap, msgs, num) static const struct file_operations i2cdev_fops = { .owner = THIS_MODULE, .llseek = no_llseek, .read = i2cdev_read, .write = i2cdev_write, .unlocked_ioctl = i2cdev_ioctl, .open = i2cdev_open, .release = i2cdev_release, }; static struct i2c_driver i2cdev_driver = { .driver = { .name = \"dev_driver\", }, .attach_adapter = i2cdev_attach_adapter, .detach_adapter = i2cdev_detach_adapter, }; i2c设备初始化 i2c_dev_init(void) register_chrdev(I2C_MAJOR, \"i2c\", &i2cdev_fops) i2c_dev_class = class_create(THIS_MODULE, \"i2c-dev\"); i2c_add_driver(&i2cdev_driver); i2c_register_driver(THIS_MODULE, driver) driver->driver.bus = &i2c_bus_type; driver_register(&driver->driver); bus_for_each_dev(&i2c_bus_type, NULL, driver, __attach_adapter); 很多地方都会调用i2c_add_driver() i2c_init(void) i2c_add_driver(&dummy_driver); i2c_dev_init(void) i2c_add_driver(&i2cdev_driver); tmp421_init(void) i2c_add_driver(&tmp421_driver); eeprom_init(void) i2c_add_driver(&eeprom_driver); 关于device_create 原型及主要流程 struct device *device_create(struct class *class, struct device *parent, dev_t devt, void *drvdata, const char *fmt, ...) device_register(dev) device_initialize(dev) kobject kset 互斥锁 自旋锁 链表初始化 device_add(dev) ... //sysfs添加文件 device_create_file(dev, &uevent_attr); device_create_file(dev, &devt_attr) device_create_sys_dev_entry(dev); devtmpfs_create_node(dev); init_completion() // 向内核进程devtmpfsd发送request, 创建设备结点 wake_up_process(devtmpfsd) wait_for_completion() device_add_class_symlinks(dev); device_add_attrs(dev); bus_add_device(dev); dpm_sysfs_add(dev); device_pm_add(dev); blocking_notifier_call_chain(&dev->bus->p->bus_notifier, BUS_NOTIFY_ADD_DEVICE, dev); kobject_uevent(&dev->kobj, KOBJ_ADD); bus_probe_device(dev); i2c中的使用例子 i2c_dev->dev = device_create(i2c_dev_class, &adap->dev, MKDEV(I2C_MAJOR, adap->nr), NULL, \"i2c-%d\", adap->nr); uio中的使用例子 idev->dev = device_create(&uio_class, parent, MKDEV(uio_major, idev->minor), idev, \"uio%d\", idev->minor); 如何创建设备? 有两种方法: 方法1: 在父节点调用 of_platform_bus_probe(pdev->dev.of_node, cpld_child_ids, &pdev->dev) 这个函数是在cpld probe阶段调用的, 会根据cpld_child_ids来匹配其子节点, 并为匹配上的节点及其子节点创建device. static struct of_device_id cpld_child_ids[] = { { .compatible = \"cpld-leds\", }, {}, }; 创建好device以后, 对应的driver的probe函数才能被调用. 比如在cpld这一级调用of_platform_bus_probe(), 那么cpld的子节点leds的driver才能找到leds的device 在leds的driver里面调用 led_classdev_register(&pdev->dev, &led_dat->cdev) 来创建led. 成功后在/sys/class/leds里能看到led. 方法2: 直接在dts里声明 把simple-bus加进cpld的compatible compatible = \"alu,cpld\", \"simple-bus\"; 原理: 还是在arch/mips/cavium-octeon/octeon-platform.c里面有个octeon_ids列表, 里面有 static struct of_device_id __initdata octeon_ids[] = { { .compatible = \"simple-bus\", }, { .compatible = \"cavium,octeon-6335-uctl\", }, { .compatible = \"cavium,octeon-5750-usbn\", }, { .compatible = \"cavium,octeon-3860-bootbus\", }, { .compatible = \"cavium,mdio-mux\", }, { .compatible = \"gpio-leds\", }, { .compatible = \"cavium,octeon-7130-usb-uctl\", }, { .compatible = \"cavium,octeon-7130-sata-uctl\", }, {}, }; 并在device_initcall的时候调用, 负责创建全部的of_device static int __init octeon_publish_devices(void) { return of_platform_bus_probe(NULL, octeon_ids, NULL); } device_initcall(octeon_publish_devices); mips kernel mips 64bit空间 见arch/mips/include/asm/mach-cavium-octeon/spaces.h #ifdef CONFIG_64BIT /* They are all the same and some OCTEON II cores cannot handle 0xa8.. */ #define CAC_BASE _AC(0x8000000000000000, UL) #define UNCAC_BASE _AC(0x8000000000000000, UL) #define IO_BASE _AC(0x8000000000000000, UL) #endif /* CONFIG_64BIT */ mips ioremap /* * ioremap - map bus memory into CPU space * @offset: bus address of the memory * @size: size of the resource to map * * ioremap performs a platform specific sequence of operations to * make bus memory CPU accessible via the readb/readw/readl/writeb/ * writew/writel functions and the other mmio helpers. The returned * address is not guaranteed to be usable directly as a virtual * address. */ #define ioremap(offset, size) \\ __ioremap_mode((offset), (size), _CACHE_UNCACHED) plat_ioremap(offset, size, flags) base = (u64) IO_BASE return (void __iomem *) (unsigned long) (base + offset) 综上: 相当于把物理地址最高位或上1 使用时 flash_map.virt = ioremap(flash_map.phys, flash_map.size); ioread8的入参应该是个CPU地址, 而不是物理地址 ioread8() readb(addr) return *(const volatile u8 __force *) addr 编译时的条件检测，条件为真则导致编译错误 #include linux/kernel.h BUILD_BUG_ON(sizeof(struct lock_class_key) > sizeof(struct lockdep_map)); 原理： #define BUILD_BUG_ON(condition) ((void)sizeof(char[1 - 2*!!(condition)])) 运行时的条件检测，条件为真则触发运行时exception #include asm/bug.h BUG_ON(i != pos); 或直接调用BUG() 原理：利用mips断点和自陷指令break和tne做运行时检测 __asm__ __volatile__(\"tne $0, %0, %1\" : : \"r\" (condition), \"i\" (BRK_BUG)); __asm__ __volatile__(\"break %0\" : : \"i\" (BRK_BUG)); jiffies jiffies_64和HZ 在jiffies.h中有 jiffies和jiffies_64声明 extern u64 __jiffy_data jiffies_64; extern unsigned long volatile __jiffy_data jiffies; linux中，用jiffies记录系统启动以来的时钟滴答数。HZ是一秒钟的时钟滴答数，被定义为CONFIG_HZ，这是menuconfig时指定的值。 所以，HZ为100时，jiffies一秒钟增大100，32位机下，497天溢出。 为了防止jiffies溢出导致问题，linux定义了jiffies_64，这样能保证几百年都不溢出。 jiffies_64是在/kernel/timer.c中定义的 u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES; 而jiffies并没有定义，只有个声明。但代码里却大量使用它，那么这个变量到底在哪里呢？ 在链接脚本vmlinux.lds.S中，有 jiffies = jiffies_64 + 4; 注意，链接脚本的=号并不是赋值，而是\"赋地址\"，在这里，jiffies的地址是jiffies_64地址向后偏移4字节，在大端CPU上，就是jiffies_64的低32位。 所以，jiffies就是jiffies_64的低32位，几乎可以认为是同一个变量。 每次时钟中断到来，会调用do_timer函数，jiffies就会增加了。 void do_timer(unsigned long ticks) { jiffies_64 += ticks; update_times(ticks); } 与jiffies有关的宏：用于确定时间关系，常和另外一个宏cpu_relax()一起，用于忙等待。 time_after(a,b) time_before(a,b) local_irq_disable() 关中断 关中断，并保存SR到flags #include linux/irqflags.h local_irq_save(flags) 原理：\"di\"，汇编指令，关中断，相当于SR(IE) = 0 current 表示当前进程 mips用$28(gp)保存当前进程的thread_info而current宏返回当前进程的进程描述符 current = $28->task 代码： register struct thread_info *__current_thread_info __asm__(\"$28\"); #define current_thread_info() __current_thread_info static inline struct task_struct * get_current(void) { return current_thread_info()->task; } #define current get_current() preempt_disable() 禁止抢占 #include linux/preempt.h 原理：每个thread都有个preempt_count，只有为0的时候才能抢占 current_thread_info()->preempt_count += 1 自旋锁spin_lock() #include linux/spinlock.h 原型： typedef struct { raw_spinlock_t raw_lock; } spinlock_t; 定义 static DEFINE_SPINLOCK(logbuf_lock); //默认初值为unlock 初始化 spin_lock_init(&logbuf_lock); //初始化为unlock 使用时配对 spin_lock()/spin_unlock() //禁止抢占，用于只和其他cpu互斥 spin_lock_irq()/spin_unlock_irq()， //禁止抢占，关中断，用于和其他cpu互斥+和中断互斥 spin_lock_irqsave/spin_unlock_irqrestore() //禁止抢占，关中断，并保存SR寄存器，用于和其他cpu互斥+和中断互斥 spin_lock_bh()/spin_unlock_bh() //禁止抢占，关下半部，用于和其他cpu互斥+和bh互斥 原理：linux提供spin_lock用于SMP环境下保护临界区，换句话说，用于多CPU互斥。在没有配置CONFIG_SMP时，没有真正的锁操作，只是调用了preempt_disable()。 SMP时，spin_lock()会先调用preempt_disable()来禁止抢占，最后会调用__raw_spin_lock(raw_spinlock_t *lock)来完成真正的获取锁操作。 注意： spin_lock()只是禁止抢占，并没有关中断 持有锁期间不能主动让出cpu 锁被持有时, 持有者不允许再次尝试获取该锁. 在必须获取多个锁时, 始终以相同的顺序获得. ARRAY_SIZE(arr) #include linux/kernel.h 各种内核编译宏, 见compiler.h #ifdef __CHECKER__ # define __user __attribute__((noderef, address_space(1))) # define __force_user __force __user # define __kernel __attribute__((address_space(0))) # define __force_kernel __force __kernel # define __safe __attribute__((safe)) # define __force __attribute__((force)) # define __nocast __attribute__((nocast)) # define __iomem __attribute__((noderef, address_space(2))) # define __force_iomem __force __iomem # define __must_hold(x) __attribute__((context(x,1,1))) # define __acquires(x) __attribute__((context(x,0,1))) # define __releases(x) __attribute__((context(x,1,0))) # define __acquire(x) __context__(x,1) # define __release(x) __context__(x,-1) # define __cond_lock(x,c) ((c) ? ({ __acquire(x); 1; }) : 0) # define __percpu __attribute__((noderef, address_space(3))) # define __force_percpu __force __percpu #ifdef CONFIG_SPARSE_RCU_POINTER # define __rcu __attribute__((noderef, address_space(4))) # define __force_rcu __force __rcu #else # define __rcu # define __force_rcu #endif dump_stack() 驱动打印调用栈 比如, ubifs在ubi_io_read()中, 调用底层读出错时, ubi_err(\"error %d%s while reading %d bytes from PEB %d:%d, read %zd bytes\", err, errstr, len, pnum, offset, read); dump_stack(); linux中断 中断注册request_threaded_irq() //这个函数用来由硬件中断号获取linux irq号. root->irq = irq_create_mapping(NULL, root->hwint) rv = request_threaded_irq(root->irq, NULL,octeon_hw_status_irq, IRQF_ONESHOT,\"octeon-hw-status\", root) /* * request_threaded_irq()用来注册中断. * 第2个参数为handler, 在中断上下文执行, 第3个参数为thread_fn, 在内核线程执行, 相当于下半部. * 如果这两个参数同时存在, 则在do_IRQ()中, 先调用handler(), 然后根据handler()的返回值, 判断是否再起内核进程调用thread_fn() * handler()可能为, IRQ_WAKE_THREAD或IRQ_HANDLED. * 如果入参handler为NULL, 则默认用 irq_default_primary_handler(), 这个函数直接return IRQ_WAKE_THREAD * * thread_fn()是在内核进程执行的, 在request_threaded_irq()里面, 会创建内核进程 */ t = kthread_create(irq_thread, new, \"irq/%d-%s\", irq, new->name); 关于中断号 中断号是个软件概念, 一般都是芯片相关的, 不同的芯片中断号也不一样. //下面的函数用来从硬件中断号找linux中断号 unsigned int irq_find_mapping(struct irq_domain *domain,irq_hw_number_t hwirq) //只有linux irq号是不够的, 这时就需要转为irq描述符 struct irq_desc *desc = irq_to_desc(irq) struct irq_data *irq_data = irq_desc_get_irq_data(desc); 信号量 #include // 定义信号量 static DEFINE_SEMAPHORE(hwstat_sem); // 获取信号量 down(&hwstat_sem); // 释放信号量 up(&hwstat_sem); debugfs 在/sys/kernel/debug/创建文件 debugfs有宏开关 #ifdef CONFIG_DEBUG_FS debugfs一般在/sys/kernel/debug/ 创建一个\"文件\", 需要传入文件操作集,支持.open .read .llseek .releasedebugfs_create_file(\"hwstat\", S_IFREG | S_IRUGO, NULL, NULL, &hwstat_operations); 关于notify_chain 内核用notify机制来提供注册-通知机制, 内部用带优先级的链表实现. 有四种notify方式, 下面以raw方式为例 先声明链表头, 以后的操作都通过这个链表头来访问.static RAW_NOTIFIER_HEAD(octeon_hw_status_notifiers); 在初始化或者适当的地方注册一个call-back, 或者叫notifier_blockraw_notifier_chain_register(&octeon_hw_status_notifiers, nb);在实现这个nb时, 如果返回NOTIFY_STOP, 则链表后面的nb都不执行.比如static int octeon_error_tree_hw_status(struct notifier_block *nb, unsigned long val, void *v)先判断事件类型, 这个类型一般保存在val里面. 然后干活, 成功就返回NOTIFY_DONE, 失败返回NOTIFY_STOP 在事件产生时, 调用这个链表, 一般后两个参数是事件类型和一个指针raw_notifier_call_chain(&octeon_hw_status_notifiers,OCTEON_HW_STATUS_SOURCE_ASSERTED, &ohsd); 创建proc的entry, 并绑定相关的文件操作 比如logbuffer_operations static const struct file_operations logbuffer_operations = { .open = logbuffer_open, .read = seq_read, .llseek = seq_lseek, .release = single_release, }; //seq_read, single_open等函数在fs/seq_file.c entry = create_proc_entry(\"logbuffer\", 0, NULL); if (entry) entry->proc_fops = &logbuffer_operations; 在sys目录下创建个class reborn_class = class_create(THIS_MODULE, \"reborn\"); 驱动中使用工作队列轮询 在驱动中使用工作队列过程如下: 在设备相关结构体内添加work struct platdata { struct uio_info *uioinfo_nmi; spinlock_t nmi_lock; unsigned long nmi_flags; struct uio_info *uioinfo_com; struct mtd_writeprotection_handler writeprotect_handler; void __iomem *cpld_base; struct delayed_work dwork; }; work的处理函数 在最后再次注册自身, 达到周期调用的效果. 这里使用内核的默认工作队列. HZ是1秒一次 void cpld_dworker(struct work_struct *work) { int i = 0; unsigned char val = 0; static unsigned char last_martini_status[2] = {0, 0}; struct delayed_work *dwork = to_delayed_work(work); struct platdata *platdata = container_of(dwork, struct platdata, dwork); //printk(\"Get cpld base:0x%llx\\n\", (unsigned long long)platdata->cpld_base); for (i = 0; i cpld_base, MAR1_UP, &val); else rd_cpld_field(platdata->cpld_base, MAR2_UP, &val); if (val ^ last_martini_status[i]) { if (val) { printk(\"Martini%d status changed up. Opening SHPI bus.\\n\", i); } else { printk(KERN_ERR \"Martini%d status changed down! Closing SHPI bus.\\n\", i); } if (i == 0) wr_cpld_field(platdata->cpld_base, MAR1_SHPI, val); else wr_cpld_field(platdata->cpld_base, MAR2_SHPI, val); last_martini_status[i] = val; } } schedule_delayed_work(dwork, HZ); } 在初始化的时候开始工作队列 cpld_probe /* Start a worker to check martini status */ INIT_DELAYED_WORK(&platdata->dwork, cpld_dworker); schedule_delayed_work(&platdata->dwork, HZ); 在rmmod时删除这个work cpld_remove cancel_delayed_work(&platdata->dwork); "},"notes/device_driver_杂记.html":{"url":"notes/device_driver_杂记.html","title":"驱动调试杂记","keywords":"","body":" 用户态使用设备物理地址方法 iomem冲突问题 现象 driver代码 dts kernel的iomem机制 修改 nand问题分析 Nand访问 ONFI 接口的nand器件访问 闪存命令 闪存寻址 时序 跟踪octeon_nand_wait 加printk信息的结果 nand写相关函数 perf probe 相关函数 perf script原始记录 ls cp 后台进程 写nand调用栈分析 一次ubifs_writepage, 并没有真正写nand 这次真正有nand操作 nand buserror nand的timing mode 网口中断 core0 CPU占用率持续100%的情况下, 调试网口不通. 用户态使用设备物理地址方法 he traditional answer here is to use dma_alloc_coherent() in a kernel driver, then share that memory with userspace, typically via mmap(). If you need more than a few megs of memory, you will run into some default size limits, which can be worked around by tweaking various kernel settings. The \"modern\" solution is called CMA - Contiguous Memory Allocator - which was merged into mainline linux kernel 3.5 (IIRC). 参考http://lwn.net/Articles/447405/http://lwn.net/Articles/486301/http://mina86.com/2012/06/10/deep-dive-into-contiguous-memory-allocator-part-i/ https://events.linuxfoundation.org/images/stories/pdf/lceu2012_nazarwicz.pdf Sample driver: http://thread.gmane.org/gmane.linux.kernel/1263136 iomem冲突问题 现象 CPLD初始化失败 [ 45.424249] (c01 1178 modprobe) cpld 100001a000000.cpld: cpld0: Failed to reserve physical memory 0 at 0x100001a000000 (size 0x10000) [ 45.436819] (c01 1178 modprobe) cpld 100001a000000.cpld: cpld0: unable to reserve memory for device (rc -16) [ 45.447447] (c01 1178 modprobe) cpld 100001a000000.cpld: cannot activate cpld driver代码 注意这里的IRQ_CONFIG_FLAG_MEMRESERVE dts 确实和bootbus冲突了. kernel的iomem机制 kernel给driver提供了资源注册的函数:在include/linux/ioport.h devm_request_region() devm_request_mem_region() 主要作用是给driver在初始化时, 声明自己的资源, 一般都是driver从dts里读出其设备的地址范围, 然后调用devm_request_mem_region()来\"注册\".devm_request_mem_region里面会做冲突检查, 有冲突打印错误.该机制主要是防止driver用的资源有冲突, 手段是在driver初始化时检查. 但不调用这个devm_request_mem_region函数, 对driver的实际功能并没有影响. 体现在/proc目录下的ioports和iomem ~ # cat /proc/ioports 00001000-ffffffff : OCTEON PCIe-0:0 IO ~ # cat /proc/iomem 00400000-007fffff : System RAM 00800000-012e5fff : System RAM 00800000-00f84207 : Kernel code 00f84208-012e5fff : Kernel data 012e6000-0212ffff : System RAM 02130000-022c2fff : System RAM 02130000-022c2daf : Kernel bss 02500000-0fcfffff : System RAM 20300000-79efffff : System RAM 80300000-efffffff : System RAM f0001000-ffefffff : System RAM 100001000-10effffff : System RAM 100001a006000-100001a00600f : serial 1070000000800-10700000008ff : /soc@0/gpio-controller@1070000000800 1070000001000-10700000010ff : /soc@0/spi@1070000001000 1180000000800-118000000083f : serial 1180000000c00-1180000000c3f : serial 1180000001000-11800000011ff : /soc@0/i2c@1180000001000 1180000001200-11800000013ff : /soc@0/i2c@1180000001200 1180000001800-118000000183f : /soc@0/mdio@1180000001800 1180040000000-118004000000f : octeon_rng 11b00f0000000-11b0fffffffff : OCTEON PCIe-0:0 MEM 11b00f0000000-11b00f00fffff : PCI Bus 0000:01 11b00f0000000-11b00f000ffff : 0000:01:00.0 11b00f0010000-11b00f00103ff : 0000:01:00.0 11b00f0100000-11b00f010ffff : 0000:00:00.0 11b0100000000-11b0100003fff : 0000:00:00.0 1400000000000-1400000000007 : octeon_rng 修改 硬件不用uart2的, 就在dts里删掉 或者在dts里面, 避免冲突 uart2: reg = cpld: 这里改小一点 nand问题分析 Nand访问 对Octeon的CPU来说:nand控制器挂在bootbus上, 和其他bootbus器件共用信号;nand读写时序都是多周期的, data线只有8个, 而需要通过这8根线, 给nand器件发送命令字, 地址, 和数据.在CLE阶段发CMD, 在ALE阶段发行地址, 列地址; 在WE/OE阶段发数据或读数据. ONFI 接口的nand器件访问 参考: https://www.ssdfans.com/blog/2018/05/17/%E9%97%AA%E5%AD%98%E5%AE%9E%E6%88%98%E6%8C%87%E5%8D%97/ 闪存实战指南: 闪存接口有同步异步之分，一般来说，异步传输速率慢，同步传输速率快。异步接口没有时钟，每个数据读由一次REn信号触发，每个数据写由一次WE_n信号触发。同步接口有一个时钟信号，数据读写和时钟同步。![](img/device_driver杂记20220920104639.png)异步写:![](img/device_driver杂记_20220920104652.png)这张图里有5个信号： CLE：Command Latch Enable，CLE有效时IOx发送命令； CE_n：Chip Enable，这个信号来选通一个逻辑上的芯片——Target。为什么说是逻辑上的芯片？因为物理芯片里面封装了很多Target，每个Target是完全独立的，只是有可能共享数据信号，所以通过CE_n来选择当前数据传输的是哪个Target，业内一般把Target叫做CE； WE_n：Write Enable，写使能，这个信号是用户发给闪存的，有效时意味着用户发过来的写数据可以采样了； ALE：Address Latch Enable，ALE有效时IOx发送地址； IOx：数据总线。 同时有很多时间参数，这里只介绍几个关键的参数： tWP是WE_n低电平脉冲的宽度，tWH是WE_n高电平保持时间，合起来一个周期的时间就是tWC； tDS是数据建立时间，意思就是8比特数据要都达到稳定状态，最多这么长时间； tDH是数据稳定时间，这段时间里数据信号稳定，可以来采样； 这样我们来看上面的时序图，数据写入的时候，数据总线不能传输地址和命令，所以ALE和CLE无效。这个Target有数据传输，所以CE_n有效。每一个WE_n周期对应一次有效的数据传输。 再来看看的异步数据读出时序图，多了两个信号： RE_n：读使能。这个信号是用户发给闪存的，每发一个读使能，闪存就在数据总线上准备好数据，等用户采样； R/B_n：Ready/Busy。闪存正在进行内部读的时候，Busy_n有效，当操作完成数据准备好之后，Ready有效，用户可以来读了。 所以，图就是用户向闪存发了读命令之后，Ready信号拉高，意味着数据准备好了。接着，用户发RE_n信号去读数据，每个RE_n周期，闪存发送一个有效数据到数据总线上，供用户采样。 闪存命令 nand控制器和nand存储器件通过如下命令交互:有的命令有两个时序周期, 比如 Page Program第一个周期发80h, 第二个周期发10h 闪存寻址 举例: 一个nand, 有2个LUN(Logical Unit), 每个LU有2个Plane, 每个Plane有N个Block, 每个Block有N个Page.行地址编码: LUN Address + Block Address + Page Address列地址: 就是Page内部的地址擦除以Block为单位. 时序 读: 先发CMD 00h, 接着发列地址, 再发行地址, 再发CMD 30h; 然后等待状态寄存器变为ready, 就可以开始读数据了. 写: 每次写一个Page, 先发CMD 80h, 再发列地址, 行地址, 然后发数据, 发完数据发CMD 10h, 然后等待状态寄存器为ready. 擦: 擦以Block为单位, 不涉及数据传输. 再CMD 60h和D0h之间发行地址即可. 跟踪octeon_nand_wait 加printk信息的结果 nand写相关函数 #并不完整 /repo/yingjieb/ms/linux-octeon-sdk3.1/drivers/mtd/nand/nand_base.c nand_write_page chip->cmdfunc(mtd, NAND_CMD_PAGEPROG, -1, -1); status = chip->waitfunc(mtd, chip); nand_write_oob_std chip->cmdfunc(mtd, NAND_CMD_SEQIN, mtd->writesize, page); chip->write_buf(mtd, buf, length); chip->cmdfunc(mtd, NAND_CMD_PAGEPROG, -1, -1); status = chip->waitfunc(mtd, chip); /repo/yingjieb/ms/linux-octeon-sdk3.1/arch/mips/cavium-octeon/octeon-nand.c octeon_nand_write_oob_std chip->cmdfunc(mtd, NAND_CMD_PAGEPROG, -1, -1); chip->waitfunc(mtd, chip); perf probe 相关函数 perf probe octeon_nand_wait perf probe octeon_nand_cmdfunc perf record -e probe:octeon_nand_wait -e probe:octeon_nand_cmdfunc -aR -g --no-children sleep 60 perf record -e probe:octeon_nand_wait -e probe:octeon_nand_cmdfunc -aR -g sleep 60 perf report > perf.report perf script > perf.script grep -E \"^\\S\" perf.script | less perf script原始记录 ls 全是读操作. cp cp是先读后写.读一个特定文件时, 这个文件一般并没有被文件系统的page cache缓存, 所以读是直接读nand器件.同时page cache也会缓存, 后续的读会快.写是先写到page cache, 由内核线程把cache page的内容真正写入nand器件.先是要nandcheck_wp, 看名字是检查写保护 ![](img/device_driver杂记20220920105305.png)接着是连续的几个nand_write_page, 比如这次是连续两次write_page![](img/device_driver杂记_20220920105328.png) 再然后是wait, 也就是本文的重点要调查的函数. 后台进程 主要是erase 写nand调用栈分析 mm/page-writeback.c #walk the list of dirty pages of the given address space and write all of them write_cache_pages() fs/ubifs/file.c do_writepage(struct page *page, int len) fs/ubifs/journal.c ubifs_jnl_write_data() 这之间是有锁的. make_reservation() ... release_head() fs/ubifs/io.c ubifs_leb_write() drivers/mtd/ubi/eba.c #writes data to logical eraseblock @lnum of a dynamic volume @vol ubi_eba_write_leb() #每个logical eraseblock 都有锁保护 leb_write_lock() ubi_io_write_data() ubi_io_write() #drivers/mtd/nand/nand_base.c #[MTD Interface] NAND write with ECC nand_write() #一个nand controller可以挂多个chip, 用spin lock来保证当前的chip被选中. #用状态机来保证controller的状态是对的, 状态不对说明其他进程没有准备好状态, 就释放spin lock, 搭配wait_queue让出控制权. nand_get_device(mtd, FL_WRITING); ret = nand_do_write_ops(mtd, to, &ops); nand_write_page() #octeon有硬件ecc, 但似乎有问题, 我们没使用. #我们用的是NAND_ECC_SOFT和NAND_ECC_BCH, ecc.write_page是nand_write_page_swecc #每个page写之前算ecc, 见nand_calculate_ecc() #每个page都有oob, 里面放ecc和坏块信息 chip->ecc.write_page() chip->cmdfunc(mtd, NAND_CMD_PAGEPROG, -1, -1); status = chip->waitfunc(mtd, chip); nand_release_device(mtd); leb_write_unlock() 一次ubifs_writepage, 并没有真正写nand 这次真正有nand操作 nand buserror 操作命令记录: #典型错误打印 PAGEPROG failed with -4 #相关打印 recovery completed UBIFS: mounted UBI device 1, volume 2, name \"nand-persistent\" #调试记录 #uboot setenv kernel_extra_args config_overlay=reboot=0 octeon-nand.debug=1 setenv kernel_extra_args config_overlay=reboot=0 loglevel=8 debug ignore_loglevel #linux mount_ubi start dd if=/dev/zero of=/mnt/nand-persistent/testnand bs=4M count=100;sync;mount_ubi stop mount_ubi stop ubidetach -p /dev/mtd ubidetach -p \"$mtddev\" ubiformat -yq \"$mtddev\" mount -t ubifs ubi$ubidevnr:$volname $mountpoint #编成ko加载 modprobe octeon-nand.ko # perf和ftrace相关 perf probe -V cvmx_nand_page_write perf probe -V cvmx_nand_page_write perf probe --del probe:cvmx_nand_page_write perf probe -x vmlinux -F | grep cvmx_nand_page_write zcat /proc/config.gz | grep CONFIG_FRAME_POINTER perf probe cvmx_nand_page_write perf record -e probe:cvmx_nand_page_write -aR sleep 30 #相关代码 linux-octeon-sdk3.1/fs/ubifs/super.c arch/mips/cavium-octeon/octeon-nand.c arch/mips/cavium-octeon/executive/cvmx-nand.c nand的timing mode kernel4.9配的timing mode是4改为0 ONFI的timingmode有6种: 见ONFI规范 mode 0是最慢的. 网口中断 目前调试网口中断默认全部由core0处理 通过smpaffinity可以修改让四个核都处理网口中断下半部: echo f > smp_affinity ![](img/device_driver杂记_20220920110139.png) core0 CPU占用率持续100%的情况下, 调试网口不通. 默认core 0处理全部eth0的中断, 当core 0 100%被用户占用时(通常时100%用户态占用), 网口不通.目前我的解释是: 该用户态进程优先级过高, 导致中断不能被调度处理.解决方法:用tasetset把占CPU的进程绑定到其他核, 留0核处理网口中断.taskset -c 1,2,3 xxxx程序 "},"notes/device_localbus_16bit读写.html":{"url":"notes/device_localbus_16bit读写.html","title":"CPLD做8bit到16bit转换","keywords":"","body":"背景 octeon的localbus每次读写的基本单位是字节(8bit), 即对一个地址的读写会在bus产生CE/ALE/8bit DATA等硬件信号. 而我们在localbus下挂的一个器件(Pineer)是按16bit位宽设计的, 驱动中对16bit的读写, octeon CPU会首先传输低地址(addr), 然后传输高地址(addr+1). 这里要求addr必须2字节对齐. 我们在CPLD中做这个8bit到16biy的转换. verilog伪代码如下(多年未写verilog了...): "},"notes/as_title_driver1.html":{"url":"notes/as_title_driver1.html","title":"nand","keywords":"","body":"如题 "},"notes/device_driver_octeon_nand.html":{"url":"notes/device_driver_octeon_nand.html","title":"octeon nand flash驱动","keywords":"","body":" dts配置 硬件图 关于mtd 分层 初始化 octeon-nand.c 调试 初始化脚本化简 attach结果 错误1 错误2 错误3 底层读函数调用栈 dts配置 硬件图 关于mtd 分层 其中, FTL是个虚拟块设备层, 用于把nand flash转换成块设备 初始化 octeon-nand.c 知识点: late_initcall(), 定义在init.h中, 定义linux一个初始化的函数.见late_initcall和module_init 知识点: pr_debug(fmt, ...), 在定义了DEBUG后, 这是个KERN_DEBUG级别的printk 知识点: DEV_DBG, 这是octeon驱动定义的宏, 内部调用dev_info(), 类似的, 还有dev_dbg(), dev_err(), dev_warn()该宏用 static int debug 变量控制, 而这个变量是模块参数static int debug; module_param(debug, int, 0644); MODULE_PARM_DESC(debug, \"Debug bit field. -1 will turn on all debugging.\"); 知识点: 模块参数修改, linux 模块笔记 知识点: 驱动中申请内存 devm_kzalloc(&pdev->dev, sizeof(struct octeon_nand), GFP_KERNEL) 申请的内存会在drv/dev dettach时自动释放 内存会被全部清零 知识点: 内核中的字节序转换宏, 比如le32_to_cpu(data) 代码梳理 late_initcall(octeon_nand_driver_init) platform_driver_register(&octeon_nand_driver) driver_register(&drv->driver) //先find, 找到了就不需要重新注册了 driver_find(drv->name, drv->bus) bus_add_driver(drv) //申请driver_private结构*priv, 并和drv关联 //并加入kset drv->p = priv priv->kobj.kset = bus->p->drivers_kset //在sysfs文件系统下在总线所在的drivers_kset目录下添加该驱动的目录 kobject_init_and_add(&priv->kobj, &driver_ktype, NULL,\"%s\", drv->name) //drv添加到总线驱动链表上 klist_add_tail(&priv->knode_bus, &bus->p->klist_drivers) //开始匹配驱动和设备 driver_attach(drv) //对bus下面的每个设备调用__driver_attach bus_for_each_dev(drv->bus, NULL, drv, __driver_attach) __driver_attach(struct device *dev, void *data) struct device_driver *drv = data driver_match_device(drv, dev) //bus->match即platform_match //优先从fdt里面match, 然后依次是ACPI, id table, name drv->bus->match(dev, drv) //match即platform_match(dev, drv) //如果match, 绑定drv和dev driver_probe_device(drv, dev) really_probe(dev, drv) //暂时把dev的driver指针置为当前drv dev->driver = drv //sysfs中绑定drv和dev driver_sysfs_add(dev) //调用BUS_NOTIFY_BIND_DRIVER call chain blocking_notifier_call_chain(&dev->bus->p->bus_notifier, BUS_NOTIFY_BIND_DRIVER, dev) //在sysfs中, drv和dev互相创建链接 sysfs_create_link(&dev->driver->p->kobj, &dev->kobj, kobject_name(&dev->kobj)) sysfs_create_link(&dev->kobj, &dev->driver->p->kobj, \"driver\") //platform的bus没有probe方法, 不调用dev->bus->probe(dev) drv->probe(dev) //在这里就是platform_drv_probe(struct device *_dev) //把platform的drv和dev通过指针偏移找回来 drv = to_platform_driver(_dev->driver); dev = to_platform_device(_dev); //调用具体drv的probe, 比如octeon_nand_probe(struct platform_device *pdev) drv->probe(dev); //执行过程见下比如octeon_nand_probe() driver_bound(dev) //把dev的p->knode_driver加到drv的p->klist_devices链表尾部 klist_add_tail(&dev->p->knode_driver, &dev->driver->p->klist_devices); //添加drv的module module_add_driver(drv->owner, drv) //在sysfs的目录下创建文件uevent属性文件 driver_create_file(drv, &driver_attr_uevent) //给driver添加bus上的所有属性 driver_add_attrs(bus, drv) //添加绑定文件，driver_attr_bind 和 driver_attr_unbind add_bind_files(drv) driver_add_groups(drv, drv->groups) //产生一个KOBJ_ADD uevent kobject_uevent(&drv->p->kobj, KOBJ_ADD) octeon_nand_probe(struct platform_device *pdev) //从dts中解析所有nand chip //read nand id, onfi读nand芯片参数 cvmx_nand_initialize() //申请priv结构, 对每个底层驱动来说, 这个结构都至关重要 priv = devm_kzalloc(&pdev->dev, sizeof(struct octeon_nand), GFP_KERNEL) /* ecc mode 默认为NAND_ECC_SOFT_BCH, 其他可选值为 ** NAND_ECC_NONE,NAND_ECC_SOFT,NAND_ECC_HW,NAND_ECC_HW_SYNDROME,NAND_ECC_HW_OOB_FIRST, */ //填priv结构 priv->mtd.owner = THIS_MODULE priv->mtd.priv = &priv->nand memset(priv->data, 0xff, sizeof(priv->data)) priv->dev = &pdev->dev priv->selected_chip = chip priv->nand.ecc.mode = NAND_ECC_SOFT_BCH priv->nand.ecc.size = 512 priv->nand.ecc.bytes = 7 priv->nand.read_byte = octeon_nand_read_byte priv->nand.read_word = octeon_nand_read_word priv->nand.write_buf = octeon_nand_write_buf priv->nand.read_buf = octeon_nand_read_buf priv->nand.select_chip = octeon_nand_select_chip //几乎所有的活都由octeon_nand_cmdfunc完成 priv->nand.cmdfunc = octeon_nand_cmdfunc priv->nand.init_size = octeon_nand_init_size octeon_nand_scan_onfi(priv) //确定ecc模式, 实际为NAND_ECC_HW_SYNDROME, 即hardware ECC syndrome //因为onfi_params.ecc_bits=4, 并且有OCTEON_FEATURE_BCH //octeon_nand_scan_onfi NAND chip 1 using hw_bch ECC for 4 bits of correction per 512 byte block. ECC size is 8 bytes octeon_nand_hw_bch_init(priv) //octeon_nand_calc_ecc_layout layout eccbytes: 32, free offset: 2, free length: 30 //扫描每个page的oob, 建立bbt:bad block table, 及其他mtd/chip 结构 nand_scan(&priv->mtd, 1) //mtd的name设为octeon_nand%d priv->mtd.dev.parent = &pdev->dev //static const char *part_probes[] = { \"cmdlinepart\", NULL } mtd_device_parse_register(&priv->mtd, part_probes, NULL, NULL, 0) parse_mtd_partitions(mtd, types, &real_parts, parser_data) add_mtd_device(mtd) octeon_nand_open_mtd[chip] = priv 调试 初始化脚本化简 ~ # cat /etc/ubi_devices.config # See mount_ubi for details. ubidevnr=0 mtddev=/dev/mtd_nand ~ # ~ # cat /etc/ubi_volumes.config # See mount_ubi for details ubidevnr=0 volname=meta mountpoint= size=lebs:1 ubidevnr=0 volname=data mountpoint=/mnt/nand size= ~ # ~ # cat /etc/ubi_symlinks.config # See mount_ubi for details. linkdst=/mnt/nand linksrc=/mnt/persistent #attach MTD device 6 (mtd6) to UBI and create UBI device number 0 (ubi0) ubiattach -p /dev/mtd6 -d 0 #leb:logical eraseblocks, 这里nand的擦写单位是block, 128k ubimkvol /dev/ubi0 -N meta --lebs=1 ubimkvol /dev/ubi0 -N data --maxavsize #mount mkdir -p /mnt/nand mount -t ubifs ubi0:data /mnt/nand #datach ubidetach -p /dev/mtd6 #不用attach就可以format ubiformat -yq /dev/mtd6 attach结果 /user # ubiattach -p /dev/mtd6 -d 0 [ 222.657944] UBI: attaching mtd6 to ubi0 [ 223.012711] UBI: scanning is finished [ 223.038015] UBI: attached mtd6 (name \"nand\", size 128 MiB) to ubi0 [ 223.056902] UBI: PEB size: 131072 bytes (128 KiB), LEB size: 129024 bytes [ 223.076382] UBI: min./max. I/O unit sizes: 2048/2048, sub-page size 512 [ 223.095681] UBI: VID header offset: 512 (aligned 512), data offset: 2048 [ 223.115068] UBI: good PEBs: 1024, bad PEBs: 0, corrupted PEBs: 0 [ 223.133761] UBI: user volume: 0, internal volumes: 1, max. volumes count: 128 [ 223.153584] UBI: max/mean erase counter: 0/0, WL threshold: 4096, image sequence number: 1121128680 [ 223.175315] UBI: available PEBs: 1000, total reserved PEBs: 24, PEBs reserved for bad PEB handling: 20 [ 223.197328] UBI: background thread \"ubi_bgt0d\" started, PID 1210 UBI device number 0, total 1024 LEBs (132120576 bytes, 126.0 MiB), available 1000 LEBs (129024000 bytes, 123.0 MiB), LEB size 129024 bytes (126.0 KiB) 错误1 /user # ubiattach -p /dev/mtd6 -d 0 [ 246.687282] UBI: attaching mtd6 to ubi0 [ 247.041904] UBI: scanning is finished [ 247.061929] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 271:2048, read only 22528 bytes, retry [ 247.090222] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 271:2048, read only 22528 bytes, retry [ 247.118534] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 271:2048, read only 22528 bytes, retry [ 247.146826] UBI error: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 271:2048, read 22528 bytes [ 247.170303] CPU: 0 PID: 1140 Comm: ubiattach Tainted: G O 3.10.20-rt14-Cavium-Octeon #37 [ 247.191948] Stack : 0000000040808000 ffffffff80171db8 0000000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 000000000000000d 0000000000000000 0000000000000001 0000000000000000 0000000000000010 0000000000000000 0000000000000000 0000000000000000 ffffffff816e0000 ffffffff816d0000 ffffffff80667c98 ffffffff80715477 ffffffff816c8d80 800000008c3a7858 0000000000000474 0000000000000000 ffffffff80597150 ffffffff80698e60 800000008c232000 ffffffff8054213c 800000008985bad8 800000008985b9d0 ffffffff80710448 ffffffff8040e654 800000008c3a7520 ffffffff80667c98 0000000000000000 0000000000000474 0000000000000000 ffffffff8014fa48 0000000000000000 0000000000000000 ... [ 247.929641] Call Trace: [ 247.944784] [] show_stack+0xc0/0xe0 [ 247.962528] [] ubi_io_read+0x1fc/0x3d8 [ 247.980533] [] ubi_read_volume_table+0x168/0xae0 [ 247.999402] [] ubi_attach+0xa80/0x14b0 [ 248.017401] [] ubi_attach_mtd_dev+0x710/0xcd0 [ 248.036006] [] ctrl_cdev_ioctl+0xf8/0x1b8 [ 248.054265] [] compat_sys_ioctl+0xa4/0xe30 [ 248.072612] [] handle_sys+0x134/0x160 错误2 /user # ubiattach -p /dev/mtd6 -d 0 [ 246.687282] UBI: attaching mtd6 to ubi0 [ 247.041904] UBI: scanning is finished [ 247.061929] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 271:2048, read only 22528 bytes, retry [ 247.090222] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 271:2048, read only 22528 bytes, retry [ 247.118534] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 271:2048, read only 22528 bytes, retry [ 247.146826] UBI error: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 271:2048, read 22528 bytes [ 247.170303] CPU: 0 PID: 1140 Comm: ubiattach Tainted: G O 3.10.20-rt14-Cavium-Octeon #37 [ 247.191948] Stack : 0000000040808000 ffffffff80171db8 0000000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 000000000000000d 0000000000000000 0000000000000001 0000000000000000 0000000000000010 0000000000000000 0000000000000000 0000000000000000 ffffffff816e0000 ffffffff816d0000 ffffffff80667c98 ffffffff80715477 ffffffff816c8d80 800000008c3a7858 0000000000000474 0000000000000000 ffffffff80597150 ffffffff80698e60 800000008c232000 ffffffff8054213c 800000008985bad8 800000008985b9d0 ffffffff80710448 ffffffff8040e654 800000008c3a7520 ffffffff80667c98 0000000000000000 0000000000000474 0000000000000000 ffffffff8014fa48 0000000000000000 0000000000000000 ... [ 247.929641] Call Trace: [ 247.944784] [] show_stack+0xc0/0xe0 [ 247.962528] [] ubi_io_read+0x1fc/0x3d8 [ 247.980533] [] ubi_read_volume_table+0x168/0xae0 [ 247.999402] [] ubi_attach+0xa80/0x14b0 [ 248.017401] [] ubi_attach_mtd_dev+0x710/0xcd0 [ 248.036006] [] ctrl_cdev_ioctl+0xf8/0x1b8 [ 248.054265] [] compat_sys_ioctl+0xa4/0xe30 [ 248.072612] [] handle_sys+0x134/0x160 [ 248.090526] [ 248.108337] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 272:2048, read only 22528 bytes, retry [ 248.136635] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 272:2048, read only 22528 bytes, retry [ 248.164927] UBI warning: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 272:2048, read only 22528 bytes, retry [ 248.193219] UBI error: ubi_io_read: error -77 (ECC error) while reading 22528 bytes from PEB 272:2048, read 22528 bytes [ 248.216694] CPU: 1 PID: 1140 Comm: ubiattach Tainted: G O 3.10.20-rt14-Cavium-Octeon #37 [ 248.238338] Stack : 0000000040808000 ffffffff80171db8 0000000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 0000000000000000 000000000000000d 0000000000000000 0000000000000001 0000000000000000 0000000000000010 0000000000000000 0000000000000000 0000000000000000 ffffffff816e0000 ffffffff816d0000 ffffffff80667c98 ffffffff80715477 ffffffff816c8d80 800000008c3a7858 0000000000000474 0000000000000001 ffffffff80597150 ffffffff80698e60 800000008c232000 ffffffff8054213c 800000008985bad8 800000008985b9d0 ffffffff80710448 ffffffff8040e654 800000008c3a7520 ffffffff80667c98 0000000000000001 0000000000000474 0000000000000000 ffffffff8014fa48 0000000000000000 0000000000000000 ... [ 248.976052] Call Trace: [ 248.991191] [] show_stack+0xc0/0xe0 [ 249.008933] [] ubi_io_read+0x1fc/0x3d8 [ 249.026939] [] ubi_read_volume_table+0x168/0xae0 [ 249.045806] [] ubi_attach+0xa80/0x14b0 [ 249.063804] [] ubi_attach_mtd_dev+0x710/0xcd0 [ 249.082409] [] ctrl_cdev_ioctl+0xf8/0x1b8 [ 249.100668] [] compat_sys_ioctl+0xa4/0xe30 [ 249.119015] [] handle_sys+0x134/0x160 [ 249.136927] [ 249.153204] UBI: attached mtd6 (name \"nand\", size 128 MiB) to ubi0 [ 249.172084] UBI: PEB size: 131072 bytes (128 KiB), LEB size: 129024 bytes [ 249.191563] UBI: min./max. I/O unit sizes: 2048/2048, sub-page size 512 [ 249.210865] UBI: VID header offset: 512 (aligned 512), data offset: 2048 [ 249.230252] UBI: good PEBs: 1024, bad PEBs: 0, corrupted PEBs: 0 [ 249.248944] UBI: user volume: 2, internal volumes: 1, max. volumes count: 128 [ 249.268768] UBI: max/mean erase counter: 15/12, WL threshold: 4096, image sequence number: 1553858720 [ 249.290673] UBI: available PEBs: 0, total reserved PEBs: 1024, PEBs reserved for bad PEB handling: 20 [ 249.312684] UBI: background thread \"ubi_bgt0d\" started, PID 1144 UBI device number 0, total 1024 LEBs (132120576 bytes, 126.0 MiB), available 0 LEBs (0 bytes), LEB size 129024 bytes (126.0 KiB) /user # [ 249.340960] Unhandled kernel unaligned access[#1]: [ 249.361326] CPU: 0 PID: 0 Comm: swapper/0 Tainted: G O 3.10.20-rt14-Cavium-Octeon #37 [ 249.382703] task: ffffffff80715560 ti: ffffffff806d8000 task.ti: ffffffff806d8000 [ 249.402862] $ 0 : 0000000000000000 ffffffff80548d6c 0000000000010000 0000000000000003 [ 249.486964] $ 4 : 80000000034827b8 8000000003482808 8000000003482858 0000000000000000 [ 249.571066] $ 8 : 00000045a98a140b 0000000000000c50 0000000000000000 0000000000000000 [ 249.655167] $12 : ffffffff806dba58 000000001000001f ffffffff8027c2a8 800000008c554000 [ 249.739267] $16 : 7fffffffffffffff 8000000003482280 0000000000000000 ffffffff8078b910 [ 249.823367] $20 : ffffffff8078b938 ffffffff806dd380 000000008f001160 ffffffff8074f080 [ 249.907467] $24 : 0000000010016980 ffffffff8014c4f8 [ 249.991568] $28 : ffffffff806d8000 ffffffff806dba70 80000000034826e0 ffffffff8019c7d8 [ 250.075667] Hi : 0000000000000004 [ 250.091917] Lo : 0000000000000006 [ 250.108177] epc : ffffffff801b6c34 ktime_get_update_offsets+0xc/0x128 [ 250.127467] Tainted: G O [ 250.143896] ra : ffffffff8019c7d8 hrtimer_interrupt+0xc8/0x408 [ 250.162664] Status: 14101ce2 KX SX UX KERNEL EXL [ 250.256151] Cause : 00800014 [ 250.271705] BadVA : 00000045a989d776 [ 250.287955] PrId : 000d9600 (Cavium Octeon III) [ 250.305247] Modules linked in: hxdrv(O) martini(O) fglt_b_reboot_helper(O) fglt_b_cpld(O) reborn_class(O) [ 250.403603] Process swapper/0 (pid: 0, threadinfo=ffffffff806d8000, task=ffffffff80715560, tls=0000000000000000) [ 250.426453] Stack : 0000000000000001 ffffffff803ce864 ffffffff81760f58 80000000034826e0 8000000088702e18 ffffffff81761088 ffffffff81761088 0000000000000003 80000000034827b8 8000000003482808 8000000003482858 0000000000000023 ffffffff80710448 0000000000000000 0000000000000000 ffffffff8078b910 ffffffff8078b938 ffffffff806dd380 000000008f001160 ffffffff8074f080 ffffffff806d8000 ffffffff801528d8 0000000000000008 ffffffff801e3494 0000000000000008 ffffffff8014a8c0 ffffffff806dd380 ffffffff807176a8 ffffffff81630000 0000000000000018 ffffffff8066c7b8 ffffffff80790000 000000008f001160 ffffffff80790000 ffffffff806d8000 ffffffff801e7740 0000000000000000 ffffffff81630000 ffffffff81630000 ffffffff801e2984 ... [ 251.163781] Call Trace: [ 251.178904] [] ktime_get_update_offsets+0xc/0x128 [ 251.197850] [] hrtimer_interrupt+0xc8/0x408 [ 251.216278] [] c0_compare_interrupt+0x70/0xa0 [ 251.234877] [] handle_irq_event_percpu+0xac/0x338 [ 251.253823] [] handle_percpu_irq+0x98/0xc8 [ 251.272159] [] generic_handle_irq+0x44/0x60 [ 251.290585] [] do_IRQ+0x24/0x30 [ 251.307969] [] plat_irq_dispatch+0xa0/0xc0 [ 251.326305] [] ret_from_irq+0x0/0x4 [ 251.344033] [] __r4k_wait+0x20/0x40 [ 251.361762] [] cpu_startup_entry+0x98/0x320 [ 251.380188] [] start_kernel+0x4b0/0x4d0 [ 251.398264] [ 251.412428] Code: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 [ 251.561874] Unhandled kernel unaligned access[#2]: [ 251.561882] ---[ end trace 852c8e19d7dea6e0 ]--- [ 251.561885] Kernel panic - not syncing: Fatal exception in interrupt [ 251.561891] reboot_helper: stored panic_counter = 1 [ 251.561893] reboot_helper: isam_reboot_type='warm' [ 251.561900] reboot-helper: Enabling preserved ram [ 251.561902] flush l2 cache. [ 251.561973] reboot_helper: continuing standard linux reboot [ 251.701767] CPU: 1 PID: 1144 Comm: ubi_bgt0d Tainted: G D O 3.10.20-rt14-Cavium-Octeon #37 [ 251.723405] task: 800000008c414040 ti: 8000000089574000 task.ti: 8000000089574000 [ 251.743564] Rebooting in 5 seconds.. [ 251.743564] $ 0 : 0000000000000000 ffffffff80548d6c 0000000000010000 0000000000000003 [ 251.831227] $ 4 : 800000000351a7b8 800000000351a808 800000000351a858 0000000000000007 [ 251.915328] $ 8 : 0000000000000000 00000000000005f0 0000000000000000 0000000000000000 [ 251.999428] $12 : 8000000089577688 000000001000001f 0000000000000017 0000000000000017 [ 252.083528] $16 : 7fffffffffffffff 800000000351a280 0000000000000000 ffffffff8078b910 [ 252.167628] $20 : ffffffff8078b938 ffffffff806dd380 0000000000000840 ffffffff8074f080 [ 252.251729] $24 : 0000000000000004 ffffffff816e0000 [ 252.335829] $28 : 8000000089574000 80000000895776a0 800000000351a6e0 ffffffff8019c7d8 [ 252.419931] Hi : 0000000000000000 [ 252.436180] Lo : 0000000007270e00 [ 252.452440] epc : ffffffff801b6c34 ktime_get_update_offsets+0xc/0x128 [ 252.471731] Tainted: G D O [ 252.488159] ra : ffffffff8019c7d8 hrtimer_interrupt+0xc8/0x408 [ 252.506928] Status: 14101ce2 KX SX UX KERNEL EXL [ 252.600415] Cause : 00800014 [ 252.615969] BadVA : ffffffffffffc36b [ 252.632218] PrId : 000d9600 (Cavium Octeon III) [ 252.649510] Modules linked in: hxdrv(O) martini(O) fglt_b_reboot_helper(O) fglt_b_cpld(O) reborn_class(O) [ 252.747868] Process ubi_bgt0d (pid: 1144, threadinfo=8000000089574000, task=800000008c414040, tls=0000000000000000) [ 252.770978] Stack : 0000000000000001 ffffffff80548bf8 8000000003277198 800000000351a6e0 0000000000000001 ffffffff8015e894 80000000899d5a98 0000000000000003 800000000351a7b8 800000000351a808 800000000351a858 ffffffffffffffff ffffffff80710450 0000000000000000 0000000000000000 ffffffff8078b910 ffffffff8078b938 ffffffff806dd380 0000000000000840 ffffffff8074f080 00000000021ee000 ffffffff801528d8 0000000000000008 ffffffff801e3494 0000000000000008 ffffffff8013fc54 ffffffff806dd380 ffffffff807176a8 ffffffff81630000 0000000000000018 00000000893905f8 ffffffff816c6458 0000000000000840 ffffffff816c0000 00000000021ee000 ffffffff801e7740 0000000000000018 ffffffff81630000 ffffffff81630000 ffffffff801e2984 ... [ 253.508306] Call Trace: [ 253.523429] [] ktime_get_update_offsets+0xc/0x128 [ 253.542377] [] hrtimer_interrupt+0xc8/0x408 [ 253.560803] [] c0_compare_interrupt+0x70/0xa0 [ 253.579402] [] handle_irq_event_percpu+0xac/0x338 [ 253.598349] [] handle_percpu_irq+0x98/0xc8 [ 253.616685] [] generic_handle_irq+0x44/0x60 [ 253.635112] [] do_IRQ+0x24/0x30 [ 253.652497] [] plat_irq_dispatch+0xa0/0xc0 [ 253.670833] [] ret_from_irq+0x0/0x4 [ 253.688565] [] cvmx_nand_page_read+0xe4c/0xfb8 [ 253.707250] [] octeon_nand_cmdfunc+0x1fc/0x540 [ 253.725936] [] nand_do_read_ops+0x130/0x4a8 [ 253.744359] [] nand_read+0x64/0xa0 [ 253.762001] [] part_read+0x40/0x90 [ 253.779644] [] mtd_read+0x6c/0xb0 [ 253.797200] [] ubi_io_read+0xd0/0x3d8 [ 253.815103] [] ubi_eba_copy_leb+0x2b0/0x5b0 [ 253.833527] [] wear_leveling_worker+0x36c/0x798 [ 253.852299] [] do_work+0xa4/0x148 [ 253.869853] [] ubi_thread+0x118/0x1a8 [ 253.887757] [] kthread+0xb0/0xb8 [ 253.905224] [] ret_from_kernel_thread+0x14/0x1c [ 253.923993] [ 253.938158] Code: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 [ 254.087610] Kernel unaligned instruction access[#3]: [ 254.105262] CPU: 3 PID: 0 Comm: swapper/3 Tainted: G D O 3.10.20-rt14-Cavium-Octeon #37 [ 254.126640] task: 800000008c1394a0 ti: 800000008c13c000 task.ti: 800000008c13c000 [ 254.146799] $ 0 : 0000000000000000 ffffffff801bc80c 0000000000000001 ffffffff816e0000 [ 254.230901] $ 4 : ffffffff816e0000 ffffffff816e0000 00000009532a8285 0000000000000000 [ 254.315001] $ 8 : 00000045a98a140b 0000000000000c50 0000000000000000 0000000000000000 [ 254.399100] $12 : 800000008c13fb98 000000001000001f ffffffff80172878 8000000089670000 [ 254.483200] $16 : 0000000000000000 ffffffff80710460 800000000364a928 ffffffff806e0000 [ 254.567298] $20 : ffffffff806e0000 00000009532a8285 000000008f001160 ffffffff80790000 [ 254.651399] $24 : 00000000100c4eb0 ffffffff8014c4f8 [ 254.735496] $28 : 800000008c13c000 800000008c13fbc0 800000008c13c000 0000000000000003 [ 254.819597] Hi : 0000000000000004 [ 254.835846] Lo : 0000000000000006 [ 254.852099] epc : 0000000000000003 0x3 [ 254.868696] Tainted: G D O [ 254.885120] ra : 0000000000000003 0x3 [ 254.901717] Status: 14109ce2 KX SX UX KERNEL EXL [ 254.995201] Cause : 40808010 [ 255.010755] BadVA : 0000000000000003 [ 255.027005] PrId : 000d9600 (Cavium Octeon III) [ 255.044296] Modules linked in: hxdrv(O) martini(O) fglt_b_reboot_helper(O) fglt_b_cpld(O) reborn_class(O) [ 255.142646] Process swapper/3 (pid: 0, threadinfo=800000008c13c000, task=800000008c1394a0, tls=0000000000000000) [ 255.165495] Stack : 800000000364a928 ffffffff801bd444 0000000000000000 0000000000000003 ffffffff81630000 ffffffff81630000 0000000000000018 ffffffff8066c7b8 ffffffff80790000 ffffffff80179080 ffffffff81630000 ffffffff805496ec 0000000000000008 0000000000000018 ffffffff8066c7b8 ffffffff80106a20 ffffffff80710000 0000000000000000 ffffffff8070d018 ffffffff8078c1a8 ffffffff80670000 ffffffff8014a560 0000000000000000 ffffffff801b42b8 0000000000000000 0000000000000001 800000000364ab98 0000000000000000 0000000000000140 0000000000000000 800000000364a720 800000000364a730 800000000364a860 0000000000000000 0000000000000000 0000000000009c00 ffffffff80172878 8000000089670000 ffffffff80710000 ffffffff8070d018 ... [ 255.902816] Call Trace: [ 255.930618] [] tick_check_idle+0xe4/0x190 [ 255.948871] [] plat_mem_setup+0x9c/0x234 [ 255.967036] [] irq_enter+0x80/0xb0 [ 255.984678] [] do_IRQ+0x1c/0x30 [ 256.002059] [] plat_irq_dispatch+0xa0/0xc0 [ 256.020396] [] ret_from_irq+0x0/0x4 [ 256.038125] [] cpu_startup_entry+0x98/0x320 [ 256.056552] [] SyS_syslog+0x0/0x30 [ 256.074194] [] plat_mem_setup+0x9c/0x234 [ 256.092357] [] plat_mem_setup+0x9c/0x234 [ 256.110520] [] r4k_wait+0x0/0x30 [ 256.127988] [] cpu_startup_entry+0x98/0x320 [ 256.146415] [] account_idle_time+0x50/0xc8 [ 256.164751] [] __r4k_wait+0x20/0x40 [ 256.182482] [] kernel_entry+0x0/0x17c [ 256.200387] [ 256.214551] Code: (Bad address in epc) [ 256.243837] 错误3 [40.39] [ 2451.230273] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page octeon_nand_hw_bch_read_page(800000008c580018, 800000008c580300, c00000000020c000, 0, 65) [40.39] [ 2451.258910] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_read_buf len=2048 [40.39] [ 2451.279788] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_read_buf len=64 [40.39] [ 2451.300486] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page Correcting block offset 0, ecc offset 0 [40.39] [ 2451.324752] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page Correcting block offset 512, ecc offset 8 [40.39] [ 2451.349183] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page Correcting block offset 1024, ecc offset 16 [40.39] [ 2451.373792] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page Correcting block offset 1536, ecc offset 24 [40.39] [ 2451.398401] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_cmdfunc READ0 page_addr=0x42 [40.39] [ 2451.420540] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page octeon_nand_hw_bch_read_page(800000008c580018, 800000008c580300, c00000000020c800, 0, 66) [40.39] [ 2451.449135] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_read_buf len=2048 [40.39] [ 2451.470002] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_read_buf len=64 [40.39] [ 2451.490697] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page Correcting block offset 0, ecc offset 0 [40.40] [ 2451.514958] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page Correcting block offset 512, ecc offset 8 [40.40] [ 2451.539393] octeon-nand 1070001000000.nand-flash-interface: octeon_nand_hw_bch_read_page Correcting block ooffffset 110024, eecccc ooffffsseet 166 [40.40] [40.40] [ 2445511.5399443311]] CCPPUU 30 UUnnaabble ttoo handdllee kkeerrnneell paggiinng reeqquuest aatt virttuuaal addddrress 000000000000000000001158, eeppcc ==== fffffffffffff88001155dd77668, rraa == fffffffffffff880014a556644 [40.40] [ 2451.539461] CPU 1 Unable to handle kernel paging request at virtual address 00000000000001d0, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539476] CPU 0 Unable to handle kernel paging request at virtual address 000000008f0012b8, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539492] CPU 0 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539515] CPU 0 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539530] CPU 1 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539567] CPU 0 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539592] CPU 3 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539607] CPU 0 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539620] CPU 1 Unable to handle kernel paging request at virtual address 0000000000000101, epc == ffffffff80194080, ra == ffffffff8019e980 [40.40] [ 2451.539664] CPU -1 Unable to handle kernel paging request at virtual address 0000000000000100, epc == ffffffff80194080, ra == ffffffff8019e980 [40.40] [ 2451.539764] CPU 15 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539855] CPU 15 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539871] CPU 13 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539896] CPU 30 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.539922] CPU 59 Unable to handle kernel paging request at virtual address 0000000000000230, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.548618] CPU 12 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.548688] CPU 0 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.586537] CPU 7 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.586555] CPU 0 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.586616] CPU 7 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.586632] CPU 7 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.40] [ 2451.586649] CPU 20 Unable to handle kernel paging request at virtual address 0000000000000158, epc == ffffffff8015d768, ra == ffffffff8014a564 [40.52] [40.52] =====================ISAM BOOT======================= 底层读函数调用栈 ubi_io_read mtd_read part_read nand_read nand_do_read_ops //把整个page+oob都读出到内部buffer, 即pirv->data octeon_nand_cmdfunc(mtd=0x80000000885dc018, command=0, column=0, page_addr=0) case NAND_CMD_READ0: cvmx_nand_page_read //分4组校正读到的page数据 octeon_nand_hw_bch_read_page #0 octeon_nand_read_buf (mtd=0x80000000886b0018, buf=0xc000000000091000 \"\\377\", len=) at arch/mips/cavium-octeon/octeon-nand.c:176 #1 0xffffffff8014710c in octeon_nand_hw_bch_read_page (mtd=0x80000000886b0018, chip=0x80000000886b0300, buf=0xc000000000091000 \"\\377\", oob_required=, page=) at arch/mips/cavium-octeon/octeon-nand.c:217 #2 0xffffffff803ffbf8 in nand_do_read_ops (mtd=mtd@entry=0x80000000886b0018, from=from@entry=33949696, ops=ops@entry=0x800000008df7bac0) at drivers/mtd/nand/nand_base.c:1473 #3 0xffffffff80400294 in nand_read (mtd=0x80000000886b0018, from=33949696, len=22528, retlen=0x800000008df7bb70, buf=0xc000000000091000 \"\\377\") at drivers/mtd/nand/nand_base.c:1578 #4 0xffffffff803e4758 in part_read (mtd=0x800000008c57f000, from=, len=, retlen=, buf=) at drivers/mtd/mtdpart.c:68 #5 0xffffffff803e1b24 in mtd_read (buf=, retlen=, len=, from=, mtd=0x800000008c57f000) at drivers/mtd/mtdcore.c:808 #6 mtd_read (mtd=0x800000008c57f000, from=from@entry=33949696, len=len@entry=22528, retlen=retlen@entry=0x800000008df7bb70, buf=buf@entry=0xc000000000091000 \"\\377\") at drivers/mtd/mtdcore.c:793 #7 0xffffffff8040e528 in ubi_io_read (ubi=ubi@entry=0x800000008dfb4000, buf=0xc000000000091000, pnum=, offset=, len=) at drivers/mtd/ubi/io.c:167 #8 0xffffffff804047c8 in ubi_io_read_data (buf=, ubi=0x800000008dfb4000, len=, offset=0, pnum=) at drivers/mtd/ubi/ubi.h:946 #9 process_lvol (av=, ai=0x800000008c1baa80, ubi=0x800000008dfb4000) at drivers/mtd/ubi/vtbl.c:418 #10 ubi_read_volume_table (ubi=ubi@entry=0x800000008dfb4000, ai=ai@entry=0x800000008c1baa80) at drivers/mtd/ubi/vtbl.c:821 #11 0xffffffff80414710 in ubi_attach (ubi=ubi@entry=0x800000008dfb4000, force_scan=force_scan@entry=0) at drivers/mtd/ubi/attach.c:1438 #12 0xffffffff804085c0 in ubi_attach_mtd_dev (mtd=mtd@entry=0x800000008c57f000, ubi_num=0, vid_hdr_offset=, max_beb_per1024=20) at drivers/mtd/ubi/build.c:988 #13 0xffffffff80408e58 in ctrl_cdev_ioctl (file=, cmd=, arg=2139068992) at drivers/mtd/ubi/cdev.c:1014 #14 0xffffffff802bcc64 in compat_sys_ioctl (fd=, cmd=, arg=2139068992) at fs/compat_ioctl.c:1590 #15 0xffffffff80159c34 in handle_sys () at arch/mips/kernel/scall64-n32.S:57 "},"notes/device_driver_nand概率写失败问题分析.html":{"url":"notes/device_driver_nand概率写失败问题分析.html","title":"Nand flash概率写失败问题分析","keywords":"","body":" 问题现象 根因分析 nand访问逻辑 控制器读写序列的关键区 再上一层的软件有锁吗? 根本原因 问题解决 补充 问题现象 启动的时候概率出现\"PAGEPROG failed with -4\", 并常伴随Data bus error.概率较高. 根因分析 Octeon底层nand驱动缺少临界区保护, 导致Nand控制器的命令字不配对, 产生超时错误. ubifs后台进程ubi_bgt1d是普通进程, 优先级低, 容易被抢占, 增大了问题出现的概率. nand访问逻辑 nand的一般操作序列为: 读: 先发CMD 00h, 接着发列地址, 再发行地址, 再发CMD 30h; 然后等待状态寄存器变为ready, 就可以开始读数据了. 写: 每次写一个Page, 先发CMD 80h, 再发列地址, 行地址, 然后发数据, 发完数据发CMD 10h, 然后等待状态寄存器为ready. 擦: 擦以Block为单位, 不涉及数据传输. 再CMD 60h和D0h之间发行地址即可. 对Octeon的CPU来说:nand控制器挂在bootbus上, 和其他bootbus器件共用信号;nand读写时序都是多周期的, data线只有8个, 而需要通过这8根线, 给nand器件发送命令字, 地址, 和数据.在CLE阶段发CMD, 在ALE阶段发行地址, 列地址; 在WE/OE阶段发数据或读数据. 控制器读写序列的关键区 如上所述, 读写nand需要一系列的操作序列, 应该保证在该操作序列的执行期间, bootbus上的引脚变化必须是一次完整的Nand 操作. 在硬件层面上, Octeon用NDF_CMD_Queue来保证, 它定义了一些列控制器命令字: 所有的操作序列从BUS_ACQ开始, 以BUS_REL结束. 对应代码是__cvmx_nand_build_pre_cmd和__cvmx_nand_build_post_cmd 这之间的若干命令组成了nand操作命令序列. Nand控制器会参与bootbus总线仲裁, 为了保证不产生死锁, 比如获取了bootbus控制权, 但软件没来得及发release命令, 进程被调度出去了.控制器给出的机制是doorbell, 其实现的核心是写doorbell寄存器: cvmx_write_csr(CVMX_NDF_DRBELL, 1) 那么, 为什么还有关键区竞争问题呢?arch/mips/cavium-octeon/executive/cvmx-nand.c所提供的底层nand函数, 都没有锁保护. 配对的命令字BUS_ACQ和BUS_REL之间, 也没有锁保护.函数__cvmx_nand_low_level_read里, __cvmx_nand_build_pre_cmd和__cvmx_nand_build_post_cmd之间是临界区.那么, 试想一下, 两个进程同时操作nand, 比如本文出现问题的场景, 进程opkg和ubi_bgt1d, 如果上层控制逻辑没有锁, 那么到cvmx-nand.c的__cvmx_nand_low_level_read时, 临界区控制器命令字可能会交叉, 导致nand控制器无法理解这个操作序列. 再上一层的软件有锁吗? 它的上一层接口, arch/mips/cavium-octeon/octeon-nand.c里面, 部分函数有octeon_bootbus_sem锁.但kernel4.9中, 新增的octeon_nand_wait没有这个锁, 但它会调用底层接口cvmx_nand_get_status(), 进而调用__cvmx_nand_low_level_read来获取nand的状态 注: down(&octeon_bootbus_sem)和up(&octeon_bootbus_sem)是新加的, 用以bug fix这个问题.截图里注释掉是为了复现调试问题. 根本原因 问题就出在这里.如前所述, 在写nand flash后, 调用这个接口来等待nand flash状态变为ready, cvmx_nand_get_status会在bootbus上发送一些列的操作序列, 这些序列包括ALE CLE 等关键信号的变化; 虽然nand控制器硬件上有doorbell机制, 但软件实现上, 在BUS_ACQ和BUS_REL之间并没有保护临界区.可以在上一层软件上加更大的锁, 比如octeon_bootbus_semkernel3.10 octeon驱动在mtd层就加了这个锁, 不会出问题. 但kernel4.9新增的这个的函数没有锁保护. ubifs后台进程ubi_bgt1d是普通进程, 在attach mtd分区时启动, 优先级是20, 优先级低, 容易被s6脚本启动的其他进程(优先级是实时RR -31)抢占了, 被抢占时, 命令字没配对; 同时, 另一个对nand操作的进程的nand驱动也在写nand命令字到nand控制器, 导致Nand命令字序列非法, 软件在等待超时后错误打印. 此时, 如果: 有CPLD访问, 则会出现 bus error 错误. 这两个互相干扰的nand写操作进程, 会出现 PAGEPROG failed with -4 问题解决 给kernel4.9新增函数加octeon_bootbus_sem锁经验证可以解决, 但: ubi_bgt1d很容易被抢占, 如果持有锁时被抢占, 则此期间 CPLD的访问会阻塞 Nand flash读写会阻塞 bootbus上, 所有需要octeon_bootbus_sem的访问都阻塞 提高ubi相关进程优先级 提高到实时优先级, 与其他s6启动的进程同级. 理论上会降低概率, 但不能完全避免.未验证. 在BUS_ACQ和BUS_REL之间加锁.理论上, 能解决PAGEPROG failed with -4问题, 但不能与其他bootbus访问互斥.-- 更正: 我认为可以保证和其他bootbus访问互斥, 因为这是硬件保证的, nand控制器会参与bootbus总线仲裁.patch如下:```diff changeset: 317:32127fd74f84 branch: linux-4.9-preparation user: Bai Yingjie date: Thu Aug 01 12:12:05 2019 +0800 files: arch/mips/cavium-octeon/executive/cvmx-nand.c description: nand: use mutex to protect the DMA operation and command sequence in NDF_CMD queue Octeon nand controller uses NDF_CMD queue to receive nand command sequence, and then drives the bootbus pins to operate with the Nand flash chip. One nand flash operation includes several commands which form the command sequence, software writes the sequence wrapping with BUS_ACQ and BUS_REL pair and then rings the doorbell to inform nand controller which in turn arbitrates for and releases the boot-bus pins when it executes the BUS_ACQ/BUS_REL commands, respectively. The sequence between BUS_ACQ and BUS_REL pair should in order. Without proper lock, the command sequence into NDF_CMD queue may become out of order, which results nand flash access failure. Typical error message: octeon-nand 1070001000000.nand-flash-interface: PAGEPROG failed with -4 Because of the nand failure, bootbus may occur Data Bus Error when other bootbus devices trying to access through bootbus right after the nand failure. Previously in kernel 3.10, higher level of software uses a bigger lock \"octeon_bootbus_sem\" to do the job, but in kernel 4.9, the new added functions like \"octeon_nand_wait\" does not acquire the lock. We could add \"down(&octeon_bootbus_sem)\" to such functions, follow the previous lock implementation. But \"octeon_bootbus_sem\" is a big lock, other bootbus devices like CPLD also acquire the lock. This patch provides another option that uses smaller lock granularity that only locks NDF_CMD queue and DMA operation. This approach passes the reboot test over 100 times. diff --git a/arch/mips/cavium-octeon/executive/cvmx-nand.c b/arch/mips/cavium-octeon/executive/cvmx-nand.c --- a/arch/mips/cavium-octeon/executive/cvmx-nand.c +++ b/arch/mips/cavium-octeon/executive/cvmx-nand.c @@ -273,6 +273,23 @@ static CVMX_SHARED const char *cvmx_nand static void __cvmx_nand_hex_dump(uint64_t buffer_address, int buffer_length); +static DEFINE_MUTEX(cmdq_mutex); + +/** ensure the comands put into the NDF_CMD queue are ordered this lock protects the sequence between BUS_ACQ and BUS_REL command pair and also protects the DMA operation in conjunction with NDF_CMD queue. */ +void __cvmx_nand_cmdq_lock(void) +{ mutex_lock(&cmdq_mutex); +} + +void __cvmx_nand_cmdq_unlock(void) +{ mutexunlock(&cmdq_mutex); +} + / Compute the CRC for the ONFI parameter page. Adapted from sample code ** in the specification. / @@ -1456,7 +1473,6 @@ static inline cvmx_nand_status_t __cvmx if (result) CVMX_NAND_RETURN(result); } - CVMX_NAND_RETURN(CVMX_NAND_SUCCESS); } @@ -1640,6 +1656,8 @@ static inline int _cvmx_nand_low_level if (!buffer_length) CVMX_NAND_RETURN(CVMX_NAND_INVALID_PARAM); cvmx_nand_cmdq_lock(); + nand_selected = cvmx_nand_select(1); / Build the command and address cycles / @@ -1740,10 +1758,12 @@ static inline int _cvmx_nand_low_level __cvmx_nand_select(nand_selected); __cvmx_nand_cmdq_unlock(); CVMX_NAND_RETURN(bytes); error: __cvmx_nand_select(nand_selected); __cvmx_nand_cmdq_unlock(); CVMX_NAND_RETURN(status); } @@ -1887,6 +1907,8 @@ cvmx_nand_status_t cvmx_nand_page_write( if (buffer_address & 7) CVMX_NAND_RETURN(CVMX_NAND_INVALID_PARAM); cvmx_nand_cmdq_lock(); + nand_selected = cvmx_nand_select(1); nand_address = __cvmx_nand_adjust_address(chip, nand_address); @@ -1975,6 +1997,7 @@ cvmx_nand_status_t cvmx_nand_page_write( done: __cvmx_nand_select(nand_selected); __cvmx_nand_cmdq_unlock(); CVMX_NAND_RETURN(status); } EXPORT_SYMBOL(cvmx_nand_page_write); @@ -2001,6 +2024,8 @@ cvmx_nand_status_t cvmx_nand_block_erase if (!cvmx_nand_state[chip].page_size) CVMX_NAND_RETURN(CVMX_NAND_INVALID_PARAM); cvmx_nand_cmdq_lock(); + nand_selected = cvmx_nand_select(1); / Build the command and address cycles / status = __cvmx_nand_build_pre_cmd(chip, NAND_COMMAND_ERASE, @@ -2030,6 +2055,7 @@ cvmx_nand_status_t cvmx_nand_block_erase done: __cvmx_nand_select(nand_selected); __cvmx_nand_cmdq_unlock(); CVMX_NAND_RETURN(status); } EXPORT_SYMBOL(cvmx_nand_block_erase); @@ -2230,6 +2256,8 @@ cvmx_nand_status_t cvmx_nand_set_feature if (feature == NULL) CVMX_NAND_RETURN(CVMX_NAND_INVALID_PARAM); cvmx_nand_cmdq_lock(); + nand_selected = cvmx_nand_select(1); status = __cvmx_nand_build_pre_cmd(chip, NAND_COMMAND_SET_FEATURES, 1, feat_num, 0); @@ -2278,6 +2306,7 @@ cvmx_nand_status_t cvmx_nand_set_feature } done: __cvmx_nand_select(nand_selected); __cvmx_nand_cmdq_unlock(); CVMX_NAND_RETURN(status); } EXPORT_SYMBOL(cvmx_nand_set_feature); @@ -2380,6 +2409,8 @@ cvmx_nand_status_t cvmx_nand_reset(int c if (!cvmx_nand_state[chip].page_size) CVMX_NAND_RETURN(CVMX_NAND_INVALID_PARAM); cvmx_nand_cmdq_lock(); + nand_selected = cvmx_nand_select(1); status = __cvmx_nand_build_pre_cmd(chip, NAND_COMMAND_RESET, 0, 0, 0); if (status) @@ -2396,6 +2427,7 @@ cvmx_nand_status_t cvmx_nand_reset(int c goto done; done: __cvmx_nand_select(nand_selected); __cvmx_nand_cmdq_unlock(); CVMX_NAND_RETURN(status); } EXPORT_SYMBOL(cvmx_nand_reset); ``` 补充 "},"notes/octeon_remote_pci.html":{"url":"notes/octeon_remote_pci.html","title":"octeon remote-pci.c阅读","keywords":"","body":"host/remote-lib/octeon-remote-pci.c阅读 总体pcie操作集, 提供上层使用的回调 int octeon_remote_pci(octeon_remote_funcs_t *remote_funcs) { remote_funcs->open = pci_open; remote_funcs->close = pci_close; remote_funcs->read_csr = pci_read_csr; remote_funcs->write_csr = pci_write_csr; remote_funcs->read_csr32 = pci_read_csr32; remote_funcs->write_csr32 = pci_write_csr32; remote_funcs->read_mem = pci_read_mem; remote_funcs->write_mem = pci_write_mem; remote_funcs->get_model = pci_get_model; remote_funcs->start_cores = pci_start_cores; remote_funcs->stop_cores = pci_stop_cores; remote_funcs->get_running_cores = pci_get_running_cores; remote_funcs->get_num_cores = pci_get_num_cores; remote_funcs->get_core_state = pci_get_core_state; remote_funcs->set_core_state = pci_set_core_state; remote_funcs->reset = pci_reset; remote_funcs->get_sample = pci_get_sample; if (getenv(\"OCTEON_PCI_DEBUG\")) remote_funcs->debug++; return 0; } pci_open int pci_open(const char *remote_spec) pci_get_device(int device) in = fopen(\"/proc/bus/pci/devices\", \"r\"); //这个文件里面每行都是个pcie设备 while (!feof(in)) //用fscanf读出相关信息 fscanf(in, \"%2x%2x %8x %x %Lx %Lx %Lx %Lx %Lx %Lx %Lx %Lx %Lx %Lx\",省略) //找到对应的id, 得到br0和br1的地址和大小(物理地址) octeon_pci_bar0_address octeon_pci_bar1_address octeon_pci_bar0_size octeon_pci_bar1_size //检查是否打开了配置空间的master位, 注意写/proc/bus/pci/devices这个文件可以直接修改配置空间 //使用/dev/mem来mmap64得到虚拟地址 octeon_pci_bar0_ptr = octeon_remote_map(octeon_pci_bar0_address, octeon_pci_bar0_size, &bar0_cookie); mmap64(NULL, alength, PROT_READ|PROT_WRITE, MAP_SHARED, file_handle, physical_address & (int64_t)pagemask); //同样的, 获得bar1虚拟地址 //这里是一些有关pcie变量的初始化? setup_globals() pci_read_csr uint64_t pci_read_csr(uint64_t physical_address) octeon2 OCTEON_SLI_ADDR直接访问 其他通过窗口寄存器(octeon_pci_bar0_win_rd_addr)转 octeon3 都通过octeon_pci_bar0_win_rd_addr转 pci_write_mem void pci_write_mem(uint64_t physical_address, const void *buffer_ptr, int length) //原则上按照4M一块来写(还记得吗, bar1是16*4M的访问模式) //但这里有个问题, 就是physical_address应该是个任意地址, 而且length也应该是任意的 //所以是这样访问, 把physical_address...length分成三大部分 //不到4M的头+n*4M中间的+不到4M的尾 char *buffer = (char*)buffer_ptr; uint32_t block_mask = (1 (physical_address & ~(uint64_t)block_mask) + block_mask + 1) length = block_mask + 1 - (physical_address & block_mask); else length = end_address - physical_address; //只用到16个窗口的index0, 修改其基址 pci_bar1_setup(physical_address); //根据地址对齐方式来拷贝 fast_memcpy(ptr, buffer, length); buffer += length; physical_address += length; } while (physical_address "},"notes/Device_VFIO_notes.html":{"url":"notes/Device_VFIO_notes.html","title":"VFIO简介","keywords":"","body":" 什么是VFIO 绑定驱动 VFIO要解决的问题 VFIO是个device驱动 什么是VFIO 用户态驱动框架 Virtual Function IO 基于IOMMU的DMA和中断隔离 完整的device的访问(MMIO, IO port, PCI config)绑定驱动 比如在跑dpdk之前, 先要绑定用户态驱动框架, 比如sudo usertools/dpdk-devbind.py --status sudo usertools/dpdk-devbind.py -b uio_pci_generic 04:00.0 和这里的uio框架并列的, 能被status show出来的driver, 就有vfio: vfio-pci. -- 虽然我没用过... 通过VFIO, 可以: 给device assign VFIO driver(bind), 或者unbind 有更好的安全模式 device隔离 移植性好 VFIO要解决的问题 PCI-assign的问题: 资源的访问和secure boot不兼容 IOMMU的颗粒度不满足 device的所有权模型不好 PCI onlyVFIO是个device驱动 支持模块化的设备驱动后端 vfio-pci可以bind到非桥类的pci device pci-stub是个\"桩\"driver, 无法访问 被VFIO接管的device无法被其他host driver使用 "},"notes/as_title_driver2.html":{"url":"notes/as_title_driver2.html","title":"智能网卡和DPDK","keywords":"","body":"如题 "},"notes/smartNIC_智能网卡对比.html":{"url":"notes/smartNIC_智能网卡对比.html","title":"智能网卡对比","keywords":"","body":" AWS Nitro Niro hypervisor 趋势 原理 virtio和vf互转 软件vf Netronome 智能网卡, 基于网络流处理器 OVS offload NFP core BCM智能网卡 BCM5880X pairing model 解读 软件 MLX 智能网卡 Innova 2 BlueField EMC EMC处理 EMC细节 miniflow OVS设计与实现 OVS的tuple space classifier cache的invalidate Aliyun smartNIC 零拷贝 Azure FPGA SDN pdf 问题 用NIC来卸载流表来解决这些问题: 微软选择FPGA 性能 ppt AWS Nitro 网络卸载 存储卸载 Niro hypervisor The launch of C5 instances introduced a new hypervisor for Amazon EC2, the Nitro Hypervisor. As a component of the Nitro system, the Nitro Hypervisor primarily provides CPU and memory isolation for EC2 instances. VPC networking and EBS storage resources are implemented by dedicated hardware components, Nitro Cards that are part of all current generation EC2 instance families. The Nitro Hypervisor is built on core Linux Kernel-based Virtual Machine (KVM) technology, but does not include general-purpose operating system components. 趋势 原理 AWS提供的信息有限, 根据 https://www.twosixlabs.com/running-thousands-of-kvm-guests-on-amazons-new-i3-metal-instances 的解读: Niro基于KVM, 但去掉了qemu it is clear that the Nitro firmware includes a stripped-down version of the KVM hypervisor that forgoes the QEMU emulator and passes hardware directly to the running instance. In this sense, Nitro is more properly viewed as partitioning firmware that uses hardware self-virtualization features, including support for nested virtualization on the i3.metal instances. 基于Nitro的i3.metal实例, 可以直接运行自己的hypervisor virtio和vf互转 https://github.com/Netronome/virtio-forwarder 软件vf https://www.usenix.net/legacy/events/wiov08/tech/full_papers/levasseur/levasseur_html Standardized but Flexible I/O for Self-Virtualizing Devices 基本原理是使用pf驱动, 来模拟pcie总线, 对pf下面的逻辑功能做pcie配置空间的模拟, 使kernel认为它们是VF, 称软件VF.PF驱动对上是pcie设备, 对下是pcie总线.软件VF使用PF的mem空间和MSI-X中断, PF驱动负责分配这些资源. Netronome 智能网卡, 基于网络流处理器 Agilio CX 2x10GbE: $440 Agilio CX 2x25GbE: $527 另提供软件: OVS offload https://www.netronome.com/media/documents/WP_Agilio_SW.pdf NFP core https://www.netronome.com/media/documents/eBPF_HW_OFFLOAD_HNiMne8_2_.pdf 片上有72 - 120个微核(ME), 12个一组叫flow processing cores. 一个ME有4线程或8线程, 共256个寄存器. 这些寄存器16个一组可以map到ebpf的寄存器(共11个) BCM智能网卡 BCM5880X 支持: DPDK enhancement (mostly in poll mode driver) for SmartNIC representor pairing RoCE 128 VF on host; 64 VF on SOC pairing model 在SOC的DPDK PMD里面, 用mbuf的metadata来标记接收端口和添加发送端口标记. pairing会加tunnel头在报文里, 是由hw switch(或者叫internal loopback)来处理吗? 解读 A72 + eth core架构, A72跑OVS-DPDK, eth core支持HW switch, 提供VF接口给HOST的VM.A72跑的OVS没有vhost接口, 应该全是dpdk的物理口pmd. 这么看和virtio没有关系了. 软件 配置命令从host通过bcm kernel驱动发送到NIC上. MLX 智能网卡 Innova 2 mlx5+FPGA Open CAPI是(Open Coherent Accelerator Processor Interface), Power系列CPU才有. mlx5里面集成了PCIe switch, 使得mlx5core和FPGA能够被HOST分别识别为PCIe设备.Innova一代, mlx4core用私有总线和FPGA连接, FPGA对HOST不直接可见, 必须通过kernel module来交互. BlueField mlx core + ARM core(up to 16个)主打: 存储: NVME SSD阵列; NVME over Fabric 网络: OVS NFV offload; VXLAN NVGRE等overlay网络offload; HPC offload EMC https://software.intel.com/en-us/articles/the-open-vswitch-exact-match-cache EMC处理 //dpif-netdev.c emc_processing( IN/OUT: dp_packet_batch, OUT: netdev_flow_key[] OUT: packet_batch_per_flow[]) foreach packet in batch flowkey.miniflow = miniflow_extract (pkt) flowkey.hash = dpif_netdev_packet_get_rss_hash (pkt) dp_netdev_flow = emc_lookup(flowkey) if dp_netdev_flow /* EMC hit \\0/ */ Add packet to entry in packet_batch_per_flow[] else //EMC miss Write packet back to dp_packet_batch First, incoming packets have their netdev_flow_key calculated. Conceptually, the netdev_flow_key is a concatenation all the packet's header fields that OVS knows how to decode. Then, this key is used to look up the EMC to find the corresponding dp_netdev_flow, which may (EMC hit) or may not (EMC miss) exist. The dp_netdev_flow structure contains an actions structure that determines how the packet should be processed, without having to perform a more expensive up-call to the datapath classifier. Once the action for a packet has been determined—which is to say once a dp_netdev_flow for the packet has been found—that action is not executed immediately. Instead, for performance reasons, all the packets for each action are batched together and returned via the packet_batch_per_flow argument. All the packets that missed the EMC are also batched together and returned via the dp_packet_batch argument to dp_netdev_input(). There, the packets that missed the EMC are passed to the datapath classifier with fast_path_processing(), and finally, the packet batches are dispatched using packet_batch_per_flow_execute(). By batching packets in this manner, egressing packets to devices becomes more efficient as the cost of slow memory-mapped input/output (MMIO) operations is spread across multiple packets. netdev_flow_key是把所有报文头字段连起来的key emc查找后, 返回dp_netdev_flow, 包含action, 但不马上action emc miss后, 报文被保存到dp_packet_batch, 交给dp_netdev_input()处理, 后者执行fast_path_processing()来做datapath classifier:dpcls_lookup() 带action的报文集中一起, 调用packet_batch_per_flow_execute() EMC细节 EM_FLOW_HASH_SHIFT来设置EMC的大小, 默认13位, 即8192 每个entry有570个字节, 一共4.4MB 默认是2 way 8192 set结构的cache, 可由EM_FLOW_HASH_SEGS配置N的个数 由报文头算hash, 可以硬件NIC算,也可以CPU算, 见dpif_netdev_packet_get_rss_hash() 比如算出来hash是0x00B177EE, 而EM_FLOW_HASH_SEGS配成3, 那么在cache里, 这三个位置都可以: 0xB1, 0x77, or 0xEE 即使openflow有通配规则, 比如dst的mac是01:FC:09:xx:xx:xx则转发到某个端口, EMC也不会有通配的match, 还是每个匹配到的dst mac还是会有不同的entry 另外一个例子 As another example, the IPv4 time-to-live (TTL) field is a known field to OVS, and is extracted into the miniflow structure described above. If a packet in a TCP connection takes a slightly different route before traversing OVS that packet will fail to match the existing EMC entry for the other packets in that TCP connection (as it will have a different TTL value); this will result in an up-call to the datapath classifier and a new entry into the EMC just for that packet. entry的插入不会fail, 报文的hash决定是哪(几)个可能的slot, 如果该slot空则插入, 如果不空, 则随机选一个(比如2选1)插入, 那么被插入的这个slot原来的值就没了. 不是基于least-recently used or least-frequently used算法. 可以指定, 多少个packet在一个flow里才会被插入到EMC, 默认是1? 下面命令指定至少有5个packet的flow才会被EMC $ ovs-vsctl --no-wait set Open_vSwitch . other_config:emc-insert-inv-prob=5 每个entry指向一个dp_netdev_flow, 后者是个在datapath classifier里面的项, 它有生存时间的, 过期了就会被datapath classifier移除, 那同时也会移除EMC里相应的entry 每个PMD都有自己的EMC HASH key是根据OVS知道的报文头里的所有字段生成的; 也可以是NIC生成, 就和NIC做RSS的算法一样. 这样MLX网卡就不能对mac做hash了??? miniflow miniflow由miniflow_extract()算出, 包括了相对大而全的struct flow结构体来说, 较少的信息, 而且它是内存efficient的. OVS设计与实现 The Design and Implementation of Open vSwitch-OVS设计与实现.pdf OVS+DPDK Datapath 包分类技术 OVS从一开始(inception)就支持openflow 用flow cache提高性能 开源并且多平台 上游controller通过openflow协议控制ovs-vswitchd的报文转发, 通过ovsdb协议和ovsdb-server通信, 来控制ovs的创建, 销毁, 端口添加/删除. ovs-vswitchd通过和上游的openflow控制器通信, 得到flow怎么转发, 有什么action, 比如修改报文, 报文采样和统计, 报文丢弃等 data patch负责执行flow的action, 遇到没有命中的flow, 则上送给ovs-vswitchd flow可以包含报文的任意字段, 比如eht头, ip头, 端口等信息, 也可以是报文外的metadata, 比如ingress端口号.OVS的tuple space classifier 对同一个format的flow, 分类器为其创建一个hash表; 比如一个只看src mac和dst mac的flow, 对应一个hash表; 一个看src ip的flow, 对应另一个hash表 对一个进入分类器的报文, 分类器会对所有的hash表进行搜索匹配, 如果有多个匹配结果, 则取优先级最高的一个. 最开始的datapath是个内核模块, classifier代码都在内核态, 但upstream社区不接受. 后来的方案是内核模块实现microflow cache, 这是个hash表; 未被该表匹配到的报文会送到userspace, 查openflow流表, 这个过程是性能瓶颈, 报文不仅要在内核态和用户态传递, 还要走tuple classifier流程. 一个方法是批处理模式和多线程, 比如一次上送32个报文给多个用户态thread的classifier. 但该方法治标不治本 所以, 后来用megaflow cache替代了microflow cache, 它也是一个flow查找表, 但和后者相比, megaflow支持任意字段匹配; 同样的, megaflow对不同format的flow有不同的hash表, 一个报文的平均查找hash表的个数为(n+1)/2, 这也是不小的开销; 所以, 再后来, microflow cache当做第一级, megaflow cache当做第二级 openflow有200+个bit来做匹配, 但可以只看其中的一部分field, 比如L2 mac learning, 只看mac地址, 那么从它产生的megaflow也只关心mac字段, 其他字段是wildcard; 此时, 比如新增一个flow, 要看TCP的dst port, 那么是否所有的报文都要开始检查dst port了? 好像是的... 下面是一些针对这个问题的优化: tuple优先级排序: 这里tuple是指某个format(或者说mask)的hash表, 即对这些format的hash表排序. staged查表: 对一次全匹配来说, 包括metadata, L2, L3, L4, 分解成与操作, 即metadata && L2 && L3 && L4都必须匹配, 有一个stage不匹配就提前退出 Prefix Tracking: 对ip地址/掩码形式的flow用单词查找树结构做优化cache的invalidate cache在很多时候需要更新, 比如openflow的流表的改变需要更新megaflow, 但能够精确定位到改变哪些吗? 似乎不太现实, 那么就invalidate整个datapath. 后面来的报文走first packet流程. 再后面的优化包括多线程来setup flow, 以及多线程去多flow的eviction(回收, 驱逐) Aliyun smartNIC https://www.alibabacloud.com/blog/zero-copy-optimization-for-alibaba-cloud-smart-nic-solution_593986 virtio + sriov = AVS(Application Virtual Switch), AVS和OVS功能类似, 最开始是纯软件实现 纯软件的AVS需要加速, 所以有了后面的HW smartNIC 这个smartNIC是个标准的NIC + CPU, slow path和fast path分离 有个dpdk-based的程序跑在host上, 负责VF到virtio的转换. dpdk数据结构 收包流程 零拷贝 NIC直接DMA到VM的内存, 在host上跑的转换进程只负责VF格式到virtio格式的转换. 性能提高40% Azure FPGA SDN pdf Azure Accelerated Networking: SmartNICs in the Public Cloud 问题 cpu的单流性能不好 We show that FPGAs are the best current platform for offloading our networking stack as ASICs do not provide sufficient programmability, and embedded CPU cores do not provide scalable performance, especially on single network flows. AccelNet: providing consistent Azure的虚拟网络功能基于在host/hyperviser上运行的软件, vSwitch, 微软叫Virtual Filtering Platform (VFP) vSwitch类的实现, 最大的问题是要消耗HOST资源来打通NIC到VM路径, 包括拷贝报文, 注入中断给VM. SRIOV的性能和隔离好, 但问题在于, 它不能处理SDN的策略, 通常这些策略都是在host上的vSwitch跑的. 用NIC来卸载流表来解决这些问题: 微软的流表Generic Flow Table(GFT)是match-action的一张大表, 每条流就是L2/L3/L4的元组 查不到表的flow, 通常是这个flow的首报文, 被上送到host上的VFP软件, 软件生成规则添加到到GFT表, 然后可以用硬件处理 总的来说, host软件处理首报文, 后面的报文交给SmartNIC查exact-match找action 即使很短的flow, 至少也有7-10个报文, 除了首报文, 能被加速的报文比例也挺大的. 数据中心3-5年服务器换代, CPU应该更多的被卖个用户, 相对于卖给用户的收益($900/year)来说, 用CPU来跑vSwitch太贵了. ASIC周期太长, multicore SOC在10G节点还可以, 但单核速度跟不上40G以后的节点, 而且核数也需要大大增加. 这带来了比较高的latency和抖动. 因为通常一个流会被通一个core处理, 是run to complete模式. 以DPDK为代表的PMD提供了不错的性能和可编程性, 但上文说了, 消耗host上的CPU就是原罪. 微软选择FPGA 人们对FPGA通常的问题以及微软的解答: 1. 比ASIC大 -- 没那么大 2. 贵 -- 没那么贵, 因为量大 3. 编程难 -- 确实难, 多招5个人搞FPGA开发, 软硬件协同开发, 搞敏捷. 4. 不好部署 -- 自己写工具(FPGA供应商提供一部分) 5. 和FPGA厂家锁定 -- 一共就两家, 用system verilog保证移植. FPGA的定位: 是NIC的增强, 而不是替代NIC, 主要用于offload SDN. FPGA的driver被放在NIC的driver里, 叫GFT Lightweight Filter (LWF), 对外来说就是一个带GFT卸载功能的NIC 首报文叫做exception packet, 被发往host的VFP. -- slow path 为了知道TCP的状态, 当FPGA收到TCP终结报文, 比如有SYN, RST or FIN 标记的, FPGA复制报文, 一份走正常match-action路径(fast path), 一份发给host的VFP, 后者就知道这个tcp连接结束了, 从而删除这个规则. 下图表示exception packet的处理流程 -- slow path. FPGA里面offload的核心: GFT engine 一共两个报文处理单元, 每个单元都有四个组件, 如下图所示 parser负责解析报文头, 生成唯一的flow key 在match阶段, 根据flow key算hash, 从而确定cache index. 一级cache在片内, 能保存2K个flow; 二级cache在片上的DDR里, 支持1M个flow; 剩下的flow table在DDR里, 不限个数. 在action阶段, 根据查到的规则修改报文头. action执行的是微码. host的VFP通过pcie同步流表(GFT), GFT有个更新计数器用于同步 性能 在一个tcp连接里, 发送1M个ping, 普通的accel net平均延时50us, 99.9%在300us内; smartNIC方案下, 平均延时17us, 99.9%在80us内. 在DB query场景下, 平均延时从1ms下降到300us. ppt "},"notes/octeon_pci_NIC.html":{"url":"notes/octeon_pci_NIC.html","title":"octeon PCI NIC","keywords":"","body":" pci NIC支持两种模式 PEM in EP mode in RC mode DPI & SLI DPI(以下以61xx为例) input output DPI DMA DMA的命令字 DMA的命令queue DMA engine 0..5 PCI-BASE packages overview input rings output rings dma dir tree host driver device configuration input ring initialization output ring initialization dma initialization other initialization linux ko input ring processing 发送报文 发送报文属性 DMA支持的四种模式 软件发送接口 response格式 octeon设备文件 output ring processing SE都做了什么 test pci NIC支持两种模式 Packet Mode: 类似网卡 Coprocessor Mode: host发请求, octeon给response PEM RC mode: type1 configuration space header EP mode: type 0 configuration space header pcie2.0 port0: RC or EP port 1/2: RC in EP mode no I/O space bar0: CN71xx SLI CSR; 16KBSLIWIN* enable indirect access to all other CPU CSRs(or mem?) bar1: 64M, indirect access to L2/DRAM EP: pcie bar1(64MB window) 38-bit L2/DRAM address referenced by the PCIe read/write operations in BAR1 space: 37 22|21 0 ------------------------------------------------------------ + PEM0_BAR1_INDEXn[ADDR_IDX] | EntryOff(from BAR1 address) + ------------------------------------------------------------ | ^ |\\ \\ | | | \\ \\ | | | \\ \\ | | | \\ \\ | | | v v | | | pcie address from X86_64(lower 26bit) | | | 25 22 21 0 | | | ------------------------------------- | | | +Entry| EntryOff(4M) + | | | ------------------------------------- | | | | | +-+-----------+-----+ v v v PEM0_BAR1_INDEXn(0..15) CSR 19 4 3 2 1 0 ------------------------------------------------ + ADDR_IDX |CA|END_SWP|ADDR_V+ ------------------------------------------------ 64M window = 16 block * 4M bar2: direct access to L2/DRAM; 512G/2T capability;PEM0_BAR_CTL[BAR2_ENB ]: default is disable --how to enable as default? cat /proc/iomem rom bar: to bootbus; 64KB in RC mode RC mode: does not use the base address in type 1 header. RC mode: use internal bar0 bar1 bar2 to access octeon from EP device internal bar0: 16K, --\"just like in EP mode\"PEM0_P2N_BAR0_START make no sense? only expose SLI CSRs to device? internal bar1: 64M, --\"just like in EP mode\", but can change base? --PEM1_P2N_BAR0_START internal bar2: 2T, --\"just like in EP mode\", but can change base? --PEM2_P2N_BAR0_START ? DPI & SLI DPI负责DMA SLI负责IO BUS 为什么DMA能把X86的内存拷贝到OCTEON? 反向看X86的内存? 用户态中断怎么处理?UIO? VFIO? 可DMA的buffer怎么申请? 尤其是用户态?参考 搜索Mastering the DMA and IOMMU APIs http://lxr.free-electrons.com/source/Documentation/dma-buf-sharing.txt http://stackoverflow.com/questions/11137058/from-the-kernel-to-the-user-space-dma http://stackoverflow.com/questions/5539375/linux-kernel-device-driver-to-dma-from-a-device-into-user-space-memory http://minghuasweblog.wordpress.com/2013/03/25/mapping-dma-buffers-to-user-space-on-linux-with-mmap/ 零拷贝 http://www.linuxjournal.com/article/6345?page=0,1 stack over flow QA ``` The technical basics are mostly summarized on a stackoverflow QA: Mapping DMA buffers to userspace. The key steps of different alternatives, are copied below: LDD chapter 15, page 435, Direct IO operations. Use kernel call of get_user_pages. Asynchronous IO may achieve the same result, but without userspace application having to wait for the read to finish. Check the infiband drivers which goes to much effort to make zero-copy DMA and RDMA to user space work. The person also commented: Doing DMA directly to user space memory mappings is full of problems. Copying DMA’d data into the userspace buffers will save much of the grief. Preallcate n buffers with myAddr[i] = pci_alloc_consistent(blah,size,&pci_addr[i]) until it fails. On a machine with 4G space usually it gets 2.5G of buffers each 4MiB. Cat /proc/buddyinfo to verify. Tell the device to DMA data into the buffer and send interrupt to tell the driver which buffer has been filled. In user space mmap the buffer, then wait on read or ioctl till the driver tells it which buffer is usable. get_user_pages to pin the user pages and to get an array of struct page . dma_map_page on each struct page to get the DMA addresses, this also creates an IOMMU mapping. Tell the device to do the DMA. dma_sync_single_for_cpu to flush etc. Then dma_unmap_page and put_page. Sometimes, all the normal operations as above will not work. The vendor (like on iMX51) provides special API for SDMA. Though the topic is about DMA memory used by device operation, the approach is the same if a memory mapped memory is shared by multiple processes on different virtual mappings. # DPI(以下以61xx为例) ![](img/octeon_pci_NIC_20220922231139.png) * 32 instruction input ring: 8 ring each port * DPI input ports in PIP/IPD: 32 33 34 35 ## input ![](img/octeon_pci_NIC_20220922231201.png) 这里面DPTR要么 1. 直接模式: 和DPTR1 2 3 联用, 指向data 2. 间接模式: 指向一个链表, 每个component如下, 64bit对齐 ![](img/octeon_pci_NIC_20220922231234.png) * DPI 根据instruction构造packet ![](img/octeon_pci_NIC_20220922231434.png) * 4种DPTR format ![](img/octeon_pci_NIC_20220922231502.png) ![](img/octeon_pci_NIC_20220922231625.png) ![](img/octeon_pci_NIC_20220922231535.png) ![](img/octeon_pci_NIC_20220922231643.png) ![](img/octeon_pci_NIC_20220922231700.png) ## output * 32 output ring: 8 ring each port * 4 output port of PKO: 32 33 34 35 * ring里面的每个项: ![](img/octeon_pci_NIC_20220922231731.png) 对每个ring SLI_PKT_DPADDR[DPTR]=1时, 这两个pointer的format是上面的DPTR0 SLI_PKT_DPADDR[DPTR]=0时, 这两个pointer的format是上面的DPTR1 * output有两种mode SLI_PKT_IPTR[IPTR]=1时, Info-Pointer Mode 当报文小于一个buffer size的时候, 很简单, DPI把报文写入buffer pointer的地方, 同时把大小写入info pointer的packet length; 但报文太大的时候, DPI把报文依次写在buffer pointer里面, 但只用第一个对应的Info pointer, 这里面会记录整个packet的size info的格式 ![](img/octeon_pci_NIC_20220922231908.png) 注: 这里面的info bytes由SLI_PKT(0..31)_OUT_SIZE[ISIZE]决定, 最多120 同时SLI_PKT(0..31)_OUT_SIZE[BSIZE]决定了buffer size, 最多65536 SLI_PKT_IPTR[IPTR]=0时, Buffer-Pointer-Only Mode 先写packet size, 紧跟着是data ## DPI DMA * 8个命令queue, 给6个DMA engine ## DMA的命令字 ![](img/octeon_pci_NIC_20220922232120.png) ![](img/octeon_pci_NIC_20220922232141.png) HDR里面会指明DMA的种类 ![](img/octeon_pci_NIC_20220922232206.png) pointer的格式 这里面的size只有12位, 说明每个PTR只能管4K byte ![](img/octeon_pci_NIC_20220922232303.png) ## DMA的命令queue * 是个链表, 在ram里. 软件写链表头, 硬件读链表尾 ![](img/octeon_pci_NIC_20220922232325.png) ## DMA engine 0..5 * DPI_DMA_CONTROL[PKT_EN]=1 ==> DPI use engine 5; 其他情况从8 queue 到6 engine任意map * 其他情况下, DMA和DPI有鸟关系? 可能只是给通用DMA用的? 从core到host mem, 通用方式? # PCI-BASE components/driver/OCT-PCI-BASE-GUIDE-2.3.pdf ## packages * OCTEON-PCI-BASE: x86 pci driver && octeon se * OCTEON-PCI-NIC: x86 network device; on top of OCTEON-PCI-BASE * OCTEON-PCI-CNTQ: x86 app; on top of OCTEON-PCI-BASE ## overview ![](img/octeon_pci_NIC_20220922232710.png) ![](img/octeon_pci_NIC_20220922232727.png) ## input rings ![](img/octeon_pci_NIC_20220922232756.png) ## output rings ![](img/octeon_pci_NIC_20220922232810.png) ## dma ![](img/octeon_pci_NIC_20220922232824.png) ## dir tree ![](img/octeon_pci_NIC_20220922232859.png) ## host driver * Initialization of various OCTEON PCI blocks including Input/Output rings and DMA engines. * Receiving requests from user and kernel space applications and forwarding them to OCTEON. * Managing different requests types while they wait being fetched by OCTEON or till a response arrives from OCTEON. * Receiving packets from OCTEON via the output rings and dispatch them to kernel applications. * Managing various hooks that other kernel modules (like NIC) can call to change the default packet processing behavior. ### device configuration See OCTEON_config.h/OCTEON_main.c ### input ring initialization See `request_manager.c` - `OCTEON_init_instr_queue()` 默认配置: * 32字节type, 1024 entries for ring0, 128 entries for other rings * for all input rings: 64-bit endian swapping and disables relaxed ordering and NoSnoop operations ### output ring initialization See `OCTEON_droq.c` - `OCTEON_init_droq()` 默认配置: * 1024 entries for ring0, 128 entries for other rings * info pointer mode * pre-allocated buffer(1024 bytes) for each entry in descriptor ring * 100ms timer interrupt ### dma initialization 完成通知: * Issue a work-queue entry to the POW * Write 0 to a memory location ### other initialization * request list: from app * poll list: in kernel thread * dispatch: opcode for packages arriving on output rings ### linux ko `components/driver/host/driver/linux/octeon_linux.c` ## input ring processing ![](img/octeon_pci_NIC_20220922233859.png) ![](img/octeon_pci_NIC_20220922233919.png) ### 发送报文 ![](img/octeon_pci_NIC_20220922234031.png) * `OCTEON_send_request()` from kernel space * `ioctl` from user space #### 发送报文属性 ![](img/octeon_pci_NIC_20220922234105.png) ![](img/octeon_pci_NIC_20220922234155.png) #### DMA支持的四种模式 ![](img/octeon_pci_NIC_20220922234221.png) 注: 即使对app支持多buffer模式, 但从user space拷贝到kernel space时, 驱动会拷贝到一个buffer里面 #### 软件发送接口 `OCTEON_soft_request()`, 用到的结构体OCTEON_soft_request_t在`components/driver/host/include/cavium_defs.h` #### response格式 ![](img/octeon_pci_NIC_20220922234425.png) ![](img/octeon_pci_NIC_20220922234536.png) ![](img/octeon_pci_NIC_20220922234554.png) ## octeon设备文件 `/dev/OCTEON_device`, user api 库(liboctapi.a)依赖它 api在`components/driver/host/api/octeon_user.h` ## output ring processing * 100ms中断 * 从ring0开始依次处理 ![](img/octeon_pci_NIC_20220922234715.png) 可以根据opcode调用相应的回调函数 可以根据不同的ring做fast path, 用`OCTEON_register_droq_ops()`来注册fast path处理函数 默认走slow path: 即所有来的package都查opcode做相应处理, 没有处理函数则driver就把buffer回收了 output ring 重填可以update新的buffer info ## SE都做了什么 比如初始化DMA cvm_drv_init() cvm_drv_local_init() cvm_drv_setup_app_mode() cvm_drv_start() ``` 从pcie进来的报文进WQE, 和其他接口进来的报文类似.data前有24字节的头? octeon发报文给x86时, 就像发给SGMII一样, 给PKO写命令, 此时, 硬件完成 从output ring获取下一个报文buffer 把PKO来的报文分片, 以写入一个或多个buffer 写info pointer, 比如要写报文长度 发中断(报文数或者时间片) DMA不要求octeon和x86的buffer数是一样的, 在DMA传输时, 只要总字节数一样就OKcore driver会填DMA命令字来做DMA双向传输 test oct_req是用户态程序 req_resp是内核态程序 droq_test是内核态程序, 在output ring上注册了一个收包函数 cvmcs:SE 在applications/pci-core-app oct_dbg:一个带menu的dgb工具 oct_stats:显示input output DMA queue的工具 "},"notes/as_title_driver3.html":{"url":"notes/as_title_driver3.html","title":"octeon liquidIO","keywords":"","body":"如题 "},"notes/smartNIC_liquidIO_代码阅读app篇.html":{"url":"notes/smartNIC_liquidIO_代码阅读app篇.html","title":"PCI-NIC 代码阅读 --app篇","keywords":"","body":" 编译 用户态使用 load 代码梳理 共享内存里面保存的结构体 每个进程一个的test_stats 处理SIGINT的handler 主函数流程 重要结构体 这个测试程序的默认配置 子进程 编译 hostcd OCTEON-SDK/components/driver make corecd OCTEON-SDK/applications/pci-core-app/base make testcd OCTEON-SDK-2.3.0/components/driver/host/test make 用户态使用 load cd /home/yingjie/repos/OCTEON-SDK-2.3.0/target/bin OCTEON_REMOTE_DEBUG=1 oct-pci-boot u-boot-octeon_nic10e_66.bin OCTEON_REMOTE_DEBUG=1 oct-pci-load 0 ../../components/driver/bin/cvmcs.strip OCTEON_REMOTE_DEBUG=1 oct-pci-bootcmd \"bootoct 0 coremask=f\" cd /home/yingjie/repos/OCTEON-SDK-2.3.0/components/driver/bin insmod octeon_drv.ko mknod /dev/octeon_device c 127 0 ./oct_req 0 -ubsy 对应代码OCTEON-SDK/components/driver/host/test/oct_req.c [root@cvmx bin]# ./oct_req 0 -ubsy Octeon Test utility version: PCI BASE RELEASE 2.3.0 build 84 Starting operation in silent mode with Octeon id: 0 response order = UNORDERED (1) response mode = BLOCKING (0) dma mode = DIRECT (0) Max in bufs = 1 Max out bufs = 1 Inbuf size: 1024 Outbuf size: 1024 Shared memory id is 851990 --- Press Ctrl-C to stop the test --- Main thread pid: 20804 Thread (index: 0 pid: 20805) starting execution Request Failed with status 4: 0x105d4a--C- Test Thread 0 (pid: 20805)stopping now.... Main thread stopping... Test completed: 19770 requests sent. Verification: 19769 passed 0 failed Tested with Input buffers [buffer count/requests sent] [ 1/19770 ] Tested with Output buffers [buffer count/requests sent] [ 1/19770 ] Max data sent: 1024 bytes Max data received: 1024 bytes 代码梳理 共享内存里面保存的结构体 struct test { volatile int ok_to_send; pid_t main_pid; OCTEON_RESPONSE_ORDER resp_order; OCTEON_RESPONSE_MODE resp_mode; OCTEON_DMA_MODE dma_mode; //每个子进程都有一个test_stats struct test_stats perthread[TEST_THREAD_COUNT]; struct test_stats total; time_t start, end; }; 每个进程一个的test_stats struct test_stats { void *sh_mem; struct request_list *nbreqs; pid_t pid; volatile int running; volatile int reqs_pending; unsigned long incntfreq[MAX_INBUFS+1]; unsigned long outcntfreq[MAX_OUTBUFS+1]; unsigned long maxdatasent; unsigned long maxdatareceived; unsigned long request_count; unsigned long verify_failed; unsigned long verify_passed; }; 处理SIGINT的handler 注意SIGINT会向前台进程组里面的所有进程发signal所以这个函数里面会判断只有主进程才干活在这里是置一个标志, 各个子进程会把当前的活干完, 才根据这个标志退出 void signal_handler(int x) pid_t my_pid = getpid(); /* Just clear the ok_to_send flag. When the signal handler returns in the main process, it will wait for the children to complete its processing. */ if(t_main && (my_pid == t_main->main_pid) ) { t_main->ok_to_send = 0; return; } } 主函数流程 main octeon_initialize //打开字符设备 oct_dev_handle = open(\"/dev/octeon_device\", 0) print_test_setup //打印req的信息 //共享内存, 用于多进程 shmid = shmget(0, sizeof(struct test), IPC_CREAT | IPC_EXCL); //attach共享内存, 得到地址 t_main = (struct test *)shmat(shmid, NULL, 0); //安装signal处理SIGINT prev_sig_handler = signal(signal_to_catch, signal_handler); //起多线程 for TEST_THREAD_COUNT 个: pid = fork() //子进程 case 0: oct_request_thread(q_no, count, i); //fork失败 case -1: //对已经成功fork的子进程kill, 发SIGINT kill(t_main->perthread[i].pid, signal_to_catch); //父进程 //记录子进程的pid t_main->perthread[i].pid = pid //开始 t_main->ok_to_send = 1; t_main->main_pid = getpid(); //等待子进程完工 wait_for_thread_completion() //在循环里sleep(1)等待所有子进程的t->perthread[i].running全为0 //为什么不用waitpid呢? //统计并打印信息 add_thread_stats(t_main); print_test_stats(t_main); //detach共享内存 shmdt(t_main); //恢复signal signal(signal_to_catch, prev_sig_handler ); 重要结构体 components/driver/host/include/cavium_defs.h //这个共用体的好处是不用强转地址了 /** Use this type to pass buffer address to the driver in ioctls. Use the addr field to copy your buffer's address. */ typedef union { uint64_t addr64; uint8_t *addr; } cavium_ptr_t; //这里面MAX_BUFCNT是16 /** Structure for passing input and output buffers in the request structure. */ typedef struct { /** number of buffers */ uint32_t cnt; uint32_t rsvd; /** buffer pointers */ cavium_ptr_t ptr[MAX_BUFCNT]; /** their data sizes*/ uint32_t size[MAX_BUFCNT]; } octeon_buffer_t; /** Information about the request sent to driver. This structure * points to the input data buffer(s), to the output buffer(s) (if any) * if a response is expected. It also keep information about the type of * DMA, mode of operation (response order, mode etc). * Several MACROS are defined to help access the fields. */ typedef struct { /** The input buffers and their sizes. */ octeon_buffer_t inbuf; /** The output buffer pointers and the size allocated at each pointer. */ octeon_buffer_t outbuf; /** The instruction header to be sent with this request to Octeon. */ //和手册里面input ring里面DPI_INST_HDR一致 octeon_instr_ih_t ih; /** The Input Request Header to be sent with the request to Octeon. */ octeon_instr_irh_t irh; /** The extra headers (upto 4 64-bit words) for a 64-bytes instruction. */ uint64_t exhdr[4]; /** Information about the formatting to be done to each extra header. */ octeon_exhdr_info_t exhdr_info; /** Additional information required for processing this request. Also driver returns an id identifying the request and the current status of the request in its fields.*/ union { uint64_t addr64; octeon_request_info_t *ptr; } req_info; } octeon_soft_request_t; 这个测试程序的默认配置 //以下决定了往哪个dev上发 #define OCTEON_ID 0 #define REQ_IQ_NO 0 //一共用几个进程 #define TEST_THREAD_COUNT 1 //buffer数 #define MAX_INBUFS 1 #define MAX_OUTBUFS 1 //buffer大小 #define INBUF_SIZE (1 * 1024) #define OUTBUF_SIZE (1 * 1024) #define REQUEST_TIMEOUT 500 #define MAX_NB_REQUESTS 256 /*DMA可以是以下几种: **OCTEON_DMA_DIRECT; OCTEON_DMA_GATHER; **OCTEON_DMA_SCATTER; OCTEON_DMA_SCATTER_GATHER. */ OCTEON_DMA_MODE TEST_DMA_MODE = OCTEON_DMA_DIRECT; //以下模式可以通过命令行传入 /*response的oder类型, 可以是: **OCTEON_RESP_NORESPONSE; OCTEON_RESP_ORDERED; OCTEON_RESP_UNORDERED; **用户态app不支持ORDERED模式 */ OCTEON_RESPONSE_ORDER TEST_RESP_ORDER = OCTEON_RESP_NORESPONSE; /*response的阻塞模式, 可以是: **OCTEON_RESP_NON_BLOCKING; OCTEON_RESP_BLOCKING; */ OCTEON_RESPONSE_ORDER TEST_RESP_MODE = OCTEON_RESP_NON_BLOCKING; 子进程 oct_request_thread(int q_no, int count, int tidx) //注: google以后发现子进程应该会继承shmat的segment, 但这里为什么又自己attach一遍呢? t = (struct test *)shmat(shmid, NULL, 0) s = (struct test_stats *)&(t->perthread[tidx]); s->sh_mem = t; /*nbregs是个数组, 元素是request_list struct request_list { octeon_soft_request_t *sr; int status; uint32_t outsize; uint32_t verify_size; }; */ //MAX_NB_REQUESTS是256 s->nbreqs = malloc(sizeof(struct request_list) * MAX_NB_REQUESTS) //等着主进程发开始 do { sleep(1); } while(!t->ok_to_send); time(&t1); srandom(t1); /*主体循环, 条件是上次发送成功&&t_main->ok_to_send = 1 **这个循环没有什么sleep操作, 全速运转 */ /*response的order类型, 好像用户态不支持ORDERED OCTEON_RESP_ORDERED=0, OCTEON_RESP_UNORDERED=1, OCTEON_RESP_NORESPONSE=2 response的block类型 OCTEON_RESP_BLOCKING=0, OCTEON_RESP_NON_BLOCKING */ //所以req分几种: //不需要response的应该最简单, 这里传入的tag是固定的0x101011 req_status = noresponse_request(q_no, tag, s, t->dma_mode); //随机生成incnt outcnt insize outsize generate_data_sizes() //根据DMA mode, 创建no response, 非阻塞的请求 soft_req = create_soft_request() //注意: 这里第二个malloc写错了! //给以下两个结构malloc空间 octeon_soft_request_t *soft_req=malloc() octeon_request_info_t *req_info=malloc() //raw为0则其后面的位会被忽略 soft_req->ih.raw = 1; soft_req->ih.qos = 0; soft_req->ih.grp = 0; soft_req->ih.rs = 0; soft_req->ih.tagtype = 1; soft_req->ih.tag = tag; //DMA模式里面有gather则置1 soft_req->ih.gather = 1; //irh是instruction response header soft_req->irh.opcode = CVMCS_REQRESP_OP; soft_req->irh.param = 0x10; soft_req->irh.dport = 32; //根据DMA置1 soft_req->irh.scatter = 1; //这个req_info是给谁看的? req_info->octeon_id = 0; req_info->request_id = 0xff; req_info->req_mask.dma_mode = dma_mode; req_info->req_mask.resp_mode = resp_mode; req_info->req_mask.resp_order = resp_order; req_info->req_mask.iq_no = q_no; req_info->timeout = REQUEST_TIMEOUT; //这里要申请(malloc)真正的inbuf outbuf, 因为上面的结构体里面只有buffer指针和大小 set_buffers(soft_req, inbuf_cnt, outbuf_cnt) //这里的status 3是什么意思? SOFT_REQ_INFO(soft_req)->status = 3; return soft_req; req_status = send_request(oct_id, s, soft_req); /*octeon*是用户态下的api, 基本上都是对ioctl的封装 **这部分代码在components/driver/host/api/octeon_user.c */ retval = octeon_send_request(oct_id, sr); //raw模式下不支持ORDERED模式 //非raw模式不支持response ioctl(oct_dev_handle, IOCTL_OCTEON_SEND_REQUEST, soft_req) //发送成功则各种count++ //最后free这个soft_req free_soft_request(soft_req); /*上面详细看了noresponse的发送 **剩下是unordered阻塞, 这里采用轮询的策略, 调octeon_query_request()来查询 */ req_status = unordered_blocking_request(q_no, tag, s, t->dma_mode); //也是调这两个函数, 但阻塞在ioctl里面? create_soft_request() send_request() //因为是阻塞方式, 现在就可以free这个soft_req了 free_soft_request(soft_req); /*再剩下是unordered非阻塞 **这种情况下, 要看nbreqs[nbidx].status **可以有三种:REQ_NONE REQ_PEND REQ_DONE */ switch nbreqs[nbidx].status: //这里的status是下面置的 //发出去的报文有回应, 但还没有处理 case REQ_PEND: r = check_req_status(nbreqs[nbidx].sr); //调用api octeon_query_request() /*如果此时r为OCTEON_REQUEST_PENDING **说明respond还在queue里, 那本次不处理 */ /*其他情况要么respond done, 要么出错了*/ if r == OCTEON_REQUEST_DONE verify_output() //释放buffer, 传入octeon_soft_request_t free_soft_request(nbreqs[nbidx].sr); //释放input buffer, output buffer和这个soft_req本身 nbreqs[nbidx].status = REQ_NONE; case REQ_NONE: /*上面看了阻塞式的带response的发送 **这个是非阻塞有response的发送, 入参多了一个参数struct request_list *nb **还记得吗, struct request_list *nb是个数组, 里面有256项 **上面的case说的是已经发完了请求 **这里的case是说要开始发送 */ req_status = unordered_nonblocking_request(q_no, tag,s,&nbreqs[nbidx], t->dma_mode); //同样是 create_soft_request() send_request() //因为是非阻塞, 所以不能在这里free, 因此在这里做REQ_PEND标记, 在上面的case用 nb->status = REQ_PEND; nb->outsize = outsize; nb->verify_size = verify_size; nb->sr = soft_req; if(nbreqs[nbidx].status == REQ_PEND) nbidx++;//简化, 实际会处理回环 //主循环结束,如果还有pending if(s->reqs_pending) wait_for_unordered_requests(s); //清理每个s->nbreqs[i] free(s->nbreqs); //dettach共享内存 shmdt(t); "},"notes/smartNIC_liquidIO_代码阅读api篇.html":{"url":"notes/smartNIC_liquidIO_代码阅读api篇.html","title":"PCI-NIC 代码阅读 --api篇","keywords":"","body":" api ioctl的宏是怎么定义的? api components/driver/host/api/octeon_user.c这里面都是对ioctl的封装, api包括:在octeon_user.h里面有详细说明最主要的发送 octeon_send_request 检查发送后的情况 octeon_query_request octeon_get_stats 还有pcie相关的接口:config空间 octeon_read_pcicfg_register octeon_write_pcicfg_register mem空间, 得到BAR0 BAR1的mapping地址 octeon_get_mapping_info octeon_read32 octeon_write32 直接访问octeon芯片地址的接口 octeon_read_core_direct octeon_write_core_direct 都是x86直接读写octeon, 以下两个和上面两个有什么区别? octeon_read_core octeon_write_core ioctl的宏是怎么定义的? components/driver/host/include/octeon_ioctl.h 比如定义 #define IOCTL_OCTEON_SEND_REQUEST \\ _IOWR(OCTEON_MAGIC, OCTEON_SEND_REQUEST_CODE, octeon_soft_request_t) 这里面magic是 #define OCTEON_MAGIC 0xC1 而__IOWR在内核树里面 linux/include/uapi/asm-generic/ioctl.h /* used to create numbers */ #define _IO(type,nr) _IOC(_IOC_NONE,(type),(nr),0) #define _IOR(type,nr,size) _IOC(_IOC_READ,(type),(nr),(_IOC_TYPECHECK(size))) #define _IOW(type,nr,size) _IOC(_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size))) #define _IOWR(type,nr,size) _IOC(_IOC_READ|_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size))) #define _IOR_BAD(type,nr,size) _IOC(_IOC_READ,(type),(nr),sizeof(size)) #define _IOW_BAD(type,nr,size) _IOC(_IOC_WRITE,(type),(nr),sizeof(size)) #define _IOWR_BAD(type,nr,size) _IOC(_IOC_READ|_IOC_WRITE,(type),(nr),sizeof(size)) #ifndef __KERNEL__ #define _IOC_TYPECHECK(t) (sizeof(t)) #endif 而IOC在 #define _IOC(dir,type,nr,size) \\ (((dir) dir是direction的意思 #ifndef _IOC_NONE # define _IOC_NONE 0U #endif #ifndef _IOC_WRITE # define _IOC_WRITE 1U #endif #ifndef _IOC_READ # define _IOC_READ 2U #endif 还有一些宏可以在驱动里面用 /* used to decode ioctl numbers.. */ #define _IOC_DIR(nr) (((nr) >> _IOC_DIRSHIFT) & _IOC_DIRMASK) #define _IOC_TYPE(nr) (((nr) >> _IOC_TYPESHIFT) & _IOC_TYPEMASK) #define _IOC_NR(nr) (((nr) >> _IOC_NRSHIFT) & _IOC_NRMASK) #define _IOC_SIZE(nr) (((nr) >> _IOC_SIZESHIFT) & _IOC_SIZEMASK) "},"notes/smartNIC_liquidIO_代码阅读driver篇之结构体.html":{"url":"notes/smartNIC_liquidIO_代码阅读driver篇之结构体.html","title":"PCI-NIC 代码阅读 --driver篇之结构体","keywords":"","body":" insmod log 主结构体octeon_device_t A.描述PCIE空间的结构体 B.间接访问寄存器, SLI_WIN* C.用来适配不同的octeon型号 D.表示一个input ring E.对应主结构体的plist域 E1.octeon_pending_entry_t E11.这个就是和硬件最接近的结构? E111.octeon_request_info_t E112.对应到硬件寄存器DPI_INST_HDR E113.Input Request Header E114.这个就是input ring的一个instruction E115.extra header F.头指针, 表示一个response链表 G.表示output ring描述符 G1.octeon_droq_ops_t G2.cvm_kthread_t G3.真正的符合硬件的定义的结构体 G4.octeon_droq_info_t G41.octeon_resp_hdr_t G5.驱动用这个来表示buffer, 用的是虚拟地址 G6. oct_droq_stats_t H. octeon_dma_ops_t I. octeon_range_table_t J.收到报文后按opcode分发 K. buffer pool L. buffer pool的fragment M.USE_DDOQ_THREADS octeon_module_handler_t octeon_cn6xxx_t 2A.cn6xxx_config_t insmod log octeon_drv: module license 'Cavium Networks' taints kernel. Disabling lock debugging due to kernel taint -- OCTEON: Loading Octeon PCI driver (base module) OCTEON: Driver Version: PCI BASE RELEASE 2.3.0 build 84 OCTEON: System is Little endian (1000 ticks/sec) OCTEON: PCI Driver compile options: DEBUG OCTEON: Found device 177d:92..Initializing... OCTEON: Setting up Octeon device 0 Octeon 0000:08:00.0: PCI INT A -> GSI 16 (level, low) -> IRQ 16 Octeon 0000:08:00.0: setting latency timer to 64 OCTEON[0]: CN66XX PASS1.2 OCTEON[0]: Using PCIE Port 0 OCTEON[0]: BIST enabled for soft reset OCTEON[0]: Reset completed OCTEON[0] Poll Function (Module Starter arg: 0x0) registered OCTEON[0]: Enabling PCI-E error reporting.. OCTEON: Initializing droq tasklet OCTEON: Initializing completion on interrupt tasklet alloc irq_desc for 146 on node 0 alloc kstat_irqs on node 0 alloc irq_2_iommu on node 0 Octeon 0000:08:00.0: irq 146 for MSI/MSI-X OCTEON[0]: MSI enabled OCTEON: Octeon device 0 is ready -- OCTEON: Octeon Poll Thread starting execution now! -- OCTEON: Octeon PCI driver (base module) is ready! 主结构体octeon_device_t 在components/driver/host/driver/osi/octeon_device.c中定义了octeon_device[]数组, 最多支持MAX_OCTEON_DEVICES(4)同时存在 octeon_device_t *octeon_device[MAX_OCTEON_DEVICES]; typedef struct _OCTEON_DEVICE octeon_device_t; 每个octeon device被抽象为一个octeon_device_t结构体, 注意这个主结构是osi的 /** The Octeon device. * Each Octeon device has this structure to represent all its * components. */ struct _OCTEON_DEVICE { /** Lock for this Octeon device */ cavium_spinlock_t oct_lock; //实际是spinlock_t /** OS dependent PCI device pointer */ cavium_pci_device_t *pci_dev; //linux下面是struct pci_dev /** Chip specific information. */ void *chip; /** Octeon Chip type. */ uint16_t chip_id; uint16_t rev_id; /** This device's id - set by the driver. */ uint16_t octeon_id; /** This device's PCIe port used for traffic. */ uint16_t pcie_port; /** The state of this device */ cavium_atomic_t status; //是atomic_t /** memory mapped io range */ octeon_mmio mmio[OCT_MEM_REGIONS]; //3个, 结构体定义见A struct octeon_reg_list reg_list; //见B struct octeon_fn_list fn_list; //见C cavium_atomic_t interrupts; cavium_atomic_t in_interrupt; int num_iqs; /** The 4 input instruction queues */ octeon_instr_queue_t *instr_queue[MAX_OCTEON_INSTR_QUEUES]; //32个, 结构体定义在D,很关键 int pend_list_size; octeon_pending_list_t *plist; //见E /** The doubly-linked list of instruction response */ octeon_response_list_t response_list[MAX_RESPONSE_LISTS]; //3个, 一个ordered, 一个unordered-blocking, 一个unordered-nonblocking, 结构体见F int num_oqs; /** The 4 output queues */ octeon_droq_t *droq[MAX_OCTEON_OUTPUT_QUEUES]; //32个, droq是Descriptor Ring Output Queue, 结构体见G #if !defined(USE_DROQ_THREADS) /** Tasklet structures for this device. */ struct tasklet_struct droq_tasklet; #endif struct tasklet_struct comp_tasklet; uint32_t napi_mask; void *poll_list; cavium_spinlock_t poll_lock; struct tasklet_struct cntq_tasklet; uint32_t cntq_ready; /* The 2 Octeon DMA Counter Queues */ void *cntq[MAX_OCTEON_DMA_QUEUES]; //2个 /* The DDOQ lookup table */ void *ddoq_list; /** Operations on the DMA queues */ octeon_dma_ops_t dma_ops; //见H /** A table maintaining maps of core-addr to BAR1 mapped address. */ octeon_range_table_t range_table[MAX_OCTEON_MAPS]; //最多32个map, 见I /** Total number of core-address ranges mapped (Upto 32). */ uint32_t map_count; octeon_io_enable_t io_qmask; //三个uint32_t, iq, oq, iq64B /** List of dispatch functions */ octeon_dispatch_list_t dispatch; //驱动收到报文后,会依次调这个链表里面的函数, 注意里面的opcode, 见J #ifdef USE_BUFFER_POOL //如何打开buffer pool??? /** The buffer pool implementation */ cavium_buffer_t buf[BUF_POOLS]; //6个pool, 分别是32k,16k,8k,4k,2k,1k大小的pool, 见K cavium_frag_buf_t fragments[MAX_BUFFER_CHUNKS]; //1500个, 见L uint16_t fragment_free_list[MAX_BUFFER_CHUNKS]; uint16_t fragment_free_list_index; cavium_spinlock_t fragment_lock; #endif /** The /proc file entries */ void *proc_root_dir; /** Statistics for this octeon device. Does not include IQ, DROQ stats */ oct_dev_stats_t stats; //统计信息, 都是64位, interrupts poll_count comp_tasklet_count droq_tasklet_count cntq_tasklet_count /** IRQ assigned to this device. */ int irq; int msi_on; /** The core application is running in this mode. See octeon-drv-opcodes.h for values. */ int app_mode; #ifdef CVMCS_DMA_IC /* When DMA interrupt raised we have these many packets DMAed by Octeon */ cavium_atomic_t dma_cnt_to_process; #endif #ifdef USE_DDOQ_THREADS cvm_ddoq_thread_t ddoq_thread[CVM_MAX_DDOQ_THREADS]; //8个或16个, 见M #endif /** The name given to this device. */ char device_name[32]; }; A.描述PCIE空间的结构体 /** PCI address space mapping information. * Each of the 3 address spaces given by BAR0, BAR2 and BAR4 of * Octeon gets mapped to different physical address spaces in * the kernel. */ typedef struct { /** PCI address to which the BAR is mapped. */ unsigned long start; /** Length of this PCI address space. */ unsigned long len; /** Length that has been mapped to phys. address space. */ unsigned long mapped_len; /** The physical address to which the PCI address space is mapped. */ void *hw_addr; /** Flag indicating the mapping was successful. */ int done; }octeon_mmio; B.间接访问寄存器, SLI_WIN* struct octeon_reg_list { uint32_t *pci_win_wr_addr_hi; uint32_t *pci_win_wr_addr_lo; uint64_t *pci_win_wr_addr; uint32_t *pci_win_rd_addr_hi; uint32_t *pci_win_rd_addr_lo; uint64_t *pci_win_rd_addr; uint32_t *pci_win_wr_data_hi; uint32_t *pci_win_wr_data_lo; uint64_t *pci_win_wr_data; uint32_t *pci_win_rd_data_hi; uint32_t *pci_win_rd_data_lo; uint64_t *pci_win_rd_data; }; C.用来适配不同的octeon型号 最基本的设置寄存器的接口 struct octeon_fn_list { void (* setup_iq_regs)(struct _OCTEON_DEVICE *, int); void (* setup_oq_regs)(struct _OCTEON_DEVICE *, int); cvm_intr_return_t (* interrupt_handler)(void *); int (* soft_reset)(struct _OCTEON_DEVICE *); int (* setup_device_regs)(struct _OCTEON_DEVICE *); void (* reinit_regs)(struct _OCTEON_DEVICE *); void (* bar1_idx_setup)(struct _OCTEON_DEVICE *, uint64_t, int, int); void (* bar1_idx_write)(struct _OCTEON_DEVICE *, int, uint32_t); uint32_t (* bar1_idx_read)(struct _OCTEON_DEVICE *, int); uint32_t (* update_iq_read_idx)(octeon_instr_queue_t *); void (* enable_oq_pkt_time_intr)(octeon_device_t *, int ); void (* disable_oq_pkt_time_intr)(octeon_device_t *, int ); void (* enable_interrupt)(void *); void (* disable_interrupt)(void *); void (* enable_io_queues)(struct _OCTEON_DEVICE *); void (* disable_io_queues)(struct _OCTEON_DEVICE *); }; D.表示一个input ring /** The instruction (input) queue. The input queue is used to post raw (instruction) mode data or packet data to Octeon device from the host. Each input queue (upto 4) for a Octeon device has one such structure to represent it. */ typedef struct { /** A spinlock to protect access to the input ring. */ cavium_spinlock_t lock; /** Flag that indicates if the queue uses 64 byte commands. */ uint32_t iqcmd_64B:1; /** Queue Number. */ uint32_t iq_no:5; uint32_t rsvd:18; uint32_t status:8; /** Maximum no. of instructions in this queue. */ uint32_t max_count; #if !defined(DISABLE_PCIE14425_ERRATAFIX) /** Count of packets that were not sent due to backpressure. */ uint32_t bp_hits; #endif /** Index in input ring where the driver should write the next packet. */ uint32_t host_write_index; /** Index in input ring where Octeon is expected to read the next packet. */ uint32_t octeon_read_index; /** This index aids in finding the window in the queue where Octeon has read the commands. */ uint32_t flush_index; /** This field keeps track of the instructions pending in this queue. */ cavium_atomic_t instr_pending; uint32_t reset_instr_cnt; /** Pointer to the Virtual Base addr of the input ring. */ uint8_t *base_addr; octeon_noresponse_list_t *nrlist; //int buftype, void * buf, 表示NORESPONSE的请求被octeon执行了, 但驱动还没有把资源释放掉 struct oct_noresp_free_list nr_free; //octeon_noresponse_list_t *q, int put_idx, get_idx;表示需要free的NORESPONSE list? /** Octeon doorbell register for the ring. */ void *doorbell_reg; /** Octeon instruction count register for this ring. */ void *inst_cnt_reg; /** Number of instructions pending to be posted to Octeon. */ uint32_t fill_cnt; /** The max. number of instructions that can be held pending by the driver. */ uint32_t fill_threshold; /** The last time that the doorbell was rung. The unit is OS-dependent. */ unsigned long last_db_time; /** The doorbell timeout. If the doorbell was not rung for this time and fill_cnt is non-zero, ring the doorbell again. */ unsigned long db_timeout; /** Statistics for this input queue. */ oct_iq_stats_t stats; /** DMA mapped base address of the input descriptor ring. */ unsigned long base_addr_dma; } octeon_instr_queue_t; E.对应主结构体的plist域 pending list是用来处理需要response的request的, 和output ring有关吗? --not only, but also input ring /** Pending list implementation for each Octeon device. */ typedef struct { /** Pending list for input instructions */ octeon_pending_entry_t *list; //见E1 /** A list which indicates which entry in the pending_list above is free */ uint32_t *free_list; //free_list是个数组, 有count个uint32 /** The next location in pending_free_list where an index into pending_list can be saved */ uint32_t free_index; /** Number of pending list entries. */ uint32_t entries; /** Count of pending instructions */ cavium_atomic_t instr_count; /** A lock to control access to the pending list */ cavium_spinlock_t lock; } octeon_pending_list_t; E1.octeon_pending_entry_t /** Structure of an entry in pending list. */ typedef struct { /** Used to add/delete this entry to one of the 3 response lists. */ cavium_list_t list; //双向链表 /** Index in the input queue where this request was posted */ uint16_t queue_index; /** Queue into which request was posted. */ uint16_t iq_no; /** Index into pending_list that is returned to the user (for polling) */ uint32_t request_id; /** Status of this entry */ OCTEON_PENDING_ENTRY_STATUS status; //有4种状态, FREE USED TIMEOUT REMOVE /** The instruction itself (not in the format that Octeon sees it)*/ octeon_soft_instruction_t *instr; //见E11, 非常重要 }octeon_pending_entry_t; E11.这个就是和硬件最接近的结构? --no, 还只是个开始 /** Format of a instruction presented to the driver. This structure has the values that get posted to Octeon in addition to other fields that are used by the driver to keep track of the instruction's progress. */ typedef struct { cavium_list_t list; #define COMPLETION_WORD_INIT 0xffffffffffffffffULL /** Pointer to the completion status word */ volatile uint64_t *status_word; /** The timestamp (in ticks) till we wait for a response for this instruction. */ unsigned long timeout; /**How the response for the instruction should be handled.*/ octeon_request_info_t req_info; //见E111 /** Input data pointer. It is either pointing directly to input data or to a gather list which is a list of addresses where data is present. */ void *dptr; //可以是直接一个指针地址, 也可以是octeon_sg_entry_t 的链表 /** Response from Octeon comes at this address. It is either pointing to output data buffer directly or to a scatter list which in turn points to output data buffers. */ void *rptr; //可以是soft_instr->rptr = SOFT_REQ_OUTBUF(soft_req, 0), 也可以是octeon_sg_entry_t 的链表 /** The instruction header. All input commands have this field. */ octeon_instr_ih_t ih; //见E112 /** Input request header. */ octeon_instr_irh_t irh; //见E113 /** The PCI instruction to be sent to Octeon. This is stored in the instr to retrieve the physical address of buffers when instr is freed. */ octeon_instr_64B_t command; //见E114, 重要! input ring 的64字节命令字 /** These headers are used to create a 64-byte instruction */ uint64_t exhdr[4]; /** Information about the extra headers. */ octeon_exhdr_info_t exhdr_info; //见E115 /** Flags to indicate memory allocated for this instruction. Used by driver when freeing the soft instruction. */ uint32_t alloc_flags; /** If a gather list was allocated, this ptr points to the buffer used for the gather list. The gather list has to be 8B aligned, so this value may be different from dptr. */ void *gather_ptr; /** Total data bytes transferred in the gather mode request. */ uint32_t gather_bytes; /** If a scatter list was allocated, this ptr points to the buffer used for the scatter list. The scatter list has to be 8B aligned, so this value may be different from rptr. */ void *scatter_ptr; /** Total data bytes to be received in the scatter mode request. */ uint32_t scatter_bytes; }octeon_soft_instruction_t; E111.octeon_request_info_t /** Information about the request sent to driver by kernel mode applications. */ typedef struct { /** The Octeon device to use for this request */ uint32_t octeon_id; /** The request mask */ octeon_request_mask_t req_mask; //一共32bit, resp_mode:2,dma_mode:2,resp_order:2,ignore_signal:2,iq_no:5,rsvd:19 /** timeout for this request */ uint32_t timeout; /** Status of this request */ octeon_req_status_t status; //一个u32 /** The request id assigned by driver to this request */ uint32_t request_id; /** The callback function to call after request completion */ instr_callback_t callback; //原型是void(* instr_callback_t)(octeon_req_status_t, void *); /** Argument passed to callback */ void *callback_arg; } octeon_request_info_t; E112.对应到硬件寄存器DPI_INST_HDR 好像是小端模式 typedef struct { /** Tag Value */ uint64_t tag:32; /** Tag type */ uint64_t tagtype:2; /** Short Raw Packet Indicator 1=short raw pkt */ uint64_t rs:1; /** Core group selection (1 of 16) */ uint64_t grp:4; /** Packet Order / Work Unit selection (1 of 8)*/ uint64_t qos:3; /** Front Data size */ uint64_t fsz:6; /** Data length OR no. of entries in gather list */ uint64_t dlengsz:14; /** Gather indicator 1=gather*/ uint64_t gather:1; /** Raw mode indicator 1 = RAW */ uint64_t raw:1; }octeon_instr_ih_t; E113.Input Request Header /** Input Request Header in LITTLE ENDIAN format */ typedef struct { /** Request ID */ uint64_t rid:16; /** PCIe port to use for response */ uint64_t pcie_port:3; /** Scatter indicator 1=scatter */ uint64_t scatter:1; /** Size of Expected result OR no. of entries in scatter list */ uint64_t rlenssz:14; /** Desired destination port for result */ uint64_t dport:6; /** Opcode Specific parameters */ uint64_t param:8; /** Opcode for the return packet */ uint64_t opcode:16; } octeon_instr_irh_t; E114.这个就是input ring的一个instruction /** 64-byte instruction format. Format of instruction for a 64-byte mode input queue. */ typedef struct { /** Pointer where the input data is available. */ uint64_t dptr; /** Instruction Header. */ uint64_t ih; /** Pointer where the response for a RAW mode packet will be written by Octeon. */ uint64_t rptr; /** Input Request Header. */ uint64_t irh; /** Additional headers available in a 64-byte instruction. */ uint64_t exhdr[4]; }octeon_instr_64B_t; E115.extra header /** Information about each of the extra headers added for a 64-byte instruction. */ typedef struct { /** The number of 64-bit extra header words in this request. */ uint64_t exhdr_count:4; /** Use a value of type OCTEON_EXHDR_FMT */ uint64_t exhdr1_op:2; uint64_t exhdr2_op:2; uint64_t exhdr3_op:2; uint64_t exhdr4_op:2; uint64_t rsvd:52; } octeon_exhdr_info_t; F.头指针, 表示一个response链表 typedef struct { /** List structure to add delete pending entries to */ cavium_list_t head; /** A lock for this response list */ cavium_spinlock_t lock; } octeon_response_list_t; G.表示output ring描述符 /** The Descriptor Ring Output Queue structure. This structure has all the information required to implement a Octeon DROQ. */ typedef struct { /** A spinlock to protect access to this ring. */ cavium_spinlock_t lock; uint32_t q_no; uint32_t fastpath_on; octeon_droq_ops_t ops; //见G1 octeon_device_t *oct_dev; //指向主结构体的指针 #ifdef USE_DROQ_THREADS cvm_kthread_t thread; //见G2 cavium_wait_channel wc; //就是wait_queue_head_t int stop_thread; cavium_atomic_t thread_active; #endif /** The 8B aligned descriptor ring starts at this address. */ octeon_droq_desc_t *desc_ring; //见G3, 这个才是硬件寄存器 /** Index in the ring where the driver should read the next packet */ uint32_t host_read_index; /** Index in the ring where Octeon will write the next packet */ uint32_t octeon_write_index; /** Index in the ring where the driver will refill the descriptor's buffer */ uint32_t host_refill_index; /** Packets pending to be processed - tasklet implementation */ cavium_atomic_t pkts_pending; /** Number of descriptors in this ring. */ uint32_t max_count; /** The number of descriptors pending refill. */ uint32_t refill_count; uint32_t pkts_per_intr; uint32_t refill_threshold; /** The max number of descriptors in DROQ without a buffer. This field is used to keep track of empty space threshold. If the refill_count reaches this value, the DROQ cannot accept a max-sized (64K) packet. */ uint32_t max_empty_descs; /** The 8B aligned info ptrs begin from this address. */ octeon_droq_info_t *info_list; //见G4 /** The receive buffer list. This list has the virtual addresses of the buffers. */ octeon_recv_buffer_t *recv_buf_list; //见G5, 驱动用来保存虚拟buffer地址 /** The size of each buffer pointed by the buffer pointer. */ uint32_t buffer_size; /** Pointer to the mapped packet credit register. Host writes number of info/buffer ptrs available to this register */ void *pkts_credit_reg; /** Pointer to the mapped packet sent register. Octeon writes the number of packets DMA'ed to host memory in this register. */ void *pkts_sent_reg; #if defined(ENABLE_PCIE_2G4G_FIX) void *buffer_block; #endif cavium_list_t dispatch_list; //双向链表 /** Statistics for this DROQ. */ oct_droq_stats_t stats; //见G6 /** DMA mapped address of the DROQ descriptor ring. */ unsigned long desc_ring_dma; /** Info ptr list are allocated at this virtual address. */ unsigned long info_base_addr; /** Allocated size of info list. */ uint32_t info_alloc_size; }octeon_droq_t; G1.octeon_droq_ops_t /** Used by NIC module to register packet handler and to get device * information for each octeon device. */ typedef struct { /** This registered function will be called by the driver with the octeon id, pointer to buffer from droq and length of data in the buffer. The response header gives the port number to the caller. Function pointer is set by caller. */ void (*fptr)(int, void *, uint32_t, octeon_resp_hdr_t *); /* This function will be called by the driver for all NAPI related events. The first param is the octeon id. The second param is the output queue number. The third is the NAPI event that occurred. */ void (*napi_fn)(int, int, int ); int poll_mode; /** Flag indicating if the DROQ handler should drop packets that it cannot handle in one iteration. Set by caller. */ int drop_on_max; uint16_t op_mask; uint16_t op_major; } octeon_droq_ops_t; G2.cvm_kthread_t typedef struct { #if LINUX_VERSION_CODE G3.真正的符合硬件的定义的结构体 /** Octeon descriptor format. The descriptor ring is made of descriptors which have 2 64-bit values: -# Physical (bus) address of the data buffer. -# Physical (bus) address of a octeon_droq_info_t structure. The Octeon device DMA's incoming packets and its information at the address given by these descriptor fields. */ typedef struct { /** The buffer pointer */ uint64_t buffer_ptr; /** The Info pointer */ uint64_t info_ptr; //octeon_droq_info_t的地址, 见G4 }octeon_droq_desc_t; G4.octeon_droq_info_t /** Information about packet DMA'ed by Octeon. The format of the information available at Info Pointer after Octeon has posted a packet. Not all descriptors have valid information. Only the Info field of the first descriptor for a packet has information about the packet. */ typedef struct { /** The Output Response Header. */ octeon_resp_hdr_t resp_hdr; //见G41 /** The Length of the packet. */ uint64_t length; }octeon_droq_info_t; G41.octeon_resp_hdr_t /** Response Header in LITTLE ENDIAN format */ typedef struct { /** The request id for a packet thats in response to pkt sent by host. */ uint64_t request_id:16; /** Reserved. */ uint64_t reserved:4; /** The destination Queue port. */ uint64_t dest_qport:22; /** The source port for a packet thats in response to pkt sent by host. */ uint64_t src_port:6; /** Opcode for this packet. */ uint64_t opcode:16; } octeon_resp_hdr_t; G5.驱动用这个来表示buffer, 用的是虚拟地址 /** Pointer to data buffer. Driver keeps a pointer to the data buffer that it made available to the Octeon device. Since the descriptor ring keeps physical (bus) addresses, this field is required for the driver to keep track of the virtual address pointers. The fields are operated by OS-dependent routines. */ typedef struct { /** Pointer to the packet buffer. Hidden by void * to make it OS independent. */ void *buffer; /** Pointer to the data in the packet buffer. This could be different or same as the buffer pointer depending on the OS for which the code is compiled. */ uint8_t *data; } octeon_recv_buffer_t; G6. oct_droq_stats_t /** Output Queue statistics. Each output queue has four stats fields. */ typedef struct { uint64_t pkts_received; /** H. octeon_dma_ops_t /** Used by CNTQ module to register DMA queue interrupt handler, tasklets * and statistics routines for each octeon device. */ typedef struct { /** Tasklet to be scheduled for CNTQ bottom half processing. */ void (*bh)(unsigned long); /** Interrupt Handler for DMA Queue interrupts. */ int (*intr_handler)(void *, uint64_t); /** Read DMA Counter Queue and DDOQ list statistics into a structure */ int (* read_statsb)(int, oct_stats_t *); /** Format and print DMA Counter Queue and DDOQ list statistics into a buffer */ int (* read_stats)(int, char *); } octeon_dma_ops_t; I. octeon_range_table_t /** Map of Octeon core memory address to Octeon BAR1 indexed space. */ typedef struct { /** Starting Core address mapped */ uint64_t core_addr; /** Physical address (of the BAR1 mapped space) corressponding to core_addr. */ void *mapped_addr; /** Indicator that the mapping is valid. */ int valid; } octeon_range_table_t; J.收到报文后按opcode分发 /** The dispatch list entry. * The driver keeps a record of functions registered for each * response header opcode in this structure. Since the opcode is * hashed to index into the driver's list, more than one opcode * can hash to the same entry, in which case the list field points * to a linked list with the other entries. */ typedef struct { /** List head for this entry */ cavium_list_t list; /** The opcode for which the above dispatch function & arg should be used */ octeon_opcode_t opcode; /** The function to be called for a packet received by the driver */ octeon_dispatch_fn_t dispatch_fn; /** The application specified argument to be passed to the above function along with the received packet */ void *arg; } octeon_dispatch_t; /** The dispatch list structure. */ typedef struct { cavium_spinlock_t lock; /** Count of dispatch functions currently registered */ uint32_t count; /** The list of dispatch functions */ octeon_dispatch_t *dlist; } octeon_dispatch_list_t; K. buffer pool /** Each buffer pool is represented by this structure. */ typedef struct { /** Lock for this pool. */ cavium_spinlock_t buffer_lock; /** Number of chunks in this pool. */ int chunks; /** Size of each chunk available for use after allocation. */ int chunk_size; /** Actual size of each chunk. (includes size of buffer tag)*/ int real_size; uint8_t *base; /** Address of each chunk. */ uint8_t *address[MAX_BUFFER_CHUNKS]; /** Address of usable space in chunk ( chunk - buffer tag) */ uint8_t *address_trans[MAX_BUFFER_CHUNKS]; /** Free list for this pool. */ uint16_t free_list[MAX_BUFFER_CHUNKS]; /** The next location in free list where a buffer is available. */ int free_list_index; /** Start of head for this pool's fragment list. */ cavium_list_t frags_list; } cavium_buffer_t; L. buffer pool的fragment /** List to keep track of fragmented buffers in the buffer pool. */ typedef struct { cavium_list_t list; cavium_list_t alloc_list; uint8_t *big_buf; int frags_count; int index; OCTEON_BUFPOOL p; uint16_t free_list[MAX_FRAGMENTS]; uint8_t *address[MAX_FRAGMENTS]; int free_list_index; int not_allocated; } cavium_frag_buf_t; M.USE_DDOQ_THREADS typedef struct cvm_ddoq_thread_info { int ddoq_id; int req_id; int num_pkts; } octeon_ddoq_thread_info_t; typedef struct cvm_ddoq_thread { octeon_device_t *oct_dev; //主结构体的指针 cvm_kthread_t thread; //见G2 cavium_wait_channel wc; //还是wait_queue_head_t int stop_thread; cavium_atomic_t thread_active; cavium_atomic_t ddoq_pkts_queued; cavium_spinlock_t th_lock; int th_read_idx; int th_write_idx; /* On each interrupt we can handle at most */ octeon_ddoq_thread_info_t th_info[CVM_DDOQ_MAX_THREAD_PKTS]; } cvm_ddoq_thread_t; octeon_module_handler_t 当octeon device被初始化 被复位 被停止时调用的附加函数 /** Structure passed by kernel application when registering a module with the driver. */ typedef struct { /* Application type for which handler is being registered. */ uint32_t app_type; /* Call this routine to perform add-on module related setup activities when a octeon device is being initialized. */ int (*startptr)(int, void *); /* Call this routine to perform add-on module related reset activities when a octeon device is being reset. */ int (*resetptr)(int, void *); /* Call this routine to perform add-on module related shutdown activities when a octeon device is being removed or the driver is being unloaded. */ int (*stopptr)(int, void *); } octeon_module_handler_t; octeon_cn6xxx_t /* Register address and configuration for a CN6XXX devices. */ /* If device specific changes need to be made then add a struct to include device specific fields as shown in the commented section */ typedef struct { /** PCI interrupt summary register */ uint8_t *intr_sum_reg64; /** PCI interrupt enable register */ uint8_t *intr_enb_reg64; /** The PCI interrupt mask used by interrupt handler */ uint64_t intr_mask64; cn6xxx_config_t *conf; //见2A /* Example additional fields - not used currently struct { }cn6xyz; */ } octeon_cn6xxx_t; 2A.cn6xxx_config_t /** Structure to define the configuration for CN61XX,CN63XX,CN66XX & CN68XX Octeon processors. */ typedef struct { /** Common attributes. */ octeon_common_config_t c; //num_iqs num_oqs pending_list_size /** Input Queue attributes. */ /*num_descs: 每个ring有多少个命令 **instr_type: 是32bit还是64bit格式的 **db_min: 发doorbell之前需要准备好多少个command **db_timeout: 在查询pending的command之前的超时时间 */ octeon_iq_config_t iq[CN6XXX_MAX_INPUT_QUEUES]; /** Some of the Output Queue attributes. */ /*num_descs: 每个ring多少个描述符 **info_ptr: 默认是1, 表示使用info指针模式 **buf_size: buffer大小 **pkts_per_intr: 每次调用driver的tasklet要处理多少个packets **refill_threshold: 小于此值, 驱动需要充填(replenish) */ cn6xxx_oq_config_t oq[CN6XXX_MAX_OUTPUT_QUEUES]; /** Interrupt Coalescing (Packet Count). Octeon will interrupt the host only if it sent as many packets as specified by this field. The driver usually does not use packet count interrupt coalescing. All output queues have the same packet count setting. */ //两种中断机制之一, 时间间隔 uint32_t oq_intr_pkt; /** Interrupt Coalescing (Time Interval). Octeon will interrupt the host if atleast one packet was sent in the time interval specified by this field. The driver uses time interval interrupt coalescing by default. The time is specified in microseconds. All output queues have the same time interval setting. */ uint32_t oq_intr_time; #ifdef CVMCS_DMA_IC /** Interrupt Coalescing (Packet Count). Octeon will interrupt the host only if it DMAed as many packets as specified by this field. */ //两种中断机制之二, 已经DMA的报文数 uint32_t dma_intr_pkt; /** Interrupt Coalescing (Time Interval). Octeon will interrupt the host if atleast one packet was DMAed in the time interval specified by this field. */ uint32_t dma_intr_time; #endif } cn6xxx_config_t; "},"notes/smartNIC_liquidIO_代码阅读driver篇.html":{"url":"notes/smartNIC_liquidIO_代码阅读driver篇.html","title":"PCI-NIC 代码阅读 --driver篇","keywords":"","body":" 打开调试打印 代码里随处可见的spin_lock和atomic变量 几种模式的组合 一些主要脉络 pci base driver模块 A. pci驱动 A1. octeon的pci probe函数 A11. 如何osi? 到底哪些是osi的? 哪些又不是? A12.cn66xx的配置表 A13. oct_poll_module_starter A14. check_db_timeout \"Doorbell Timeout\", 周期1个tick A141. octeon_pci_map_single和octeon_pci_unmap_single A142. 内存申请及释放 A15. oct_poll_req_completion, 1个tick A16. oct_poll_check_unordered_list A17. check_droq_refill, 每个q一个 A171. octeon_droq_refill A18. oct_droq_thread, 收包线程, 每个output q一个 A181. octeon_register_dispatch_fn() 注册收包对应的opcode函数 A182. octeon_create_recv_info(), 从驱动层到分发层, 每个来的报文都会对应一个 A19. octeon_droq_bh B. oct_poll_thread, 所有设备和回调共享这一个线程 C. octeon_fops: 这里面主要是ioctl C1. octeon_ioctl C11.oct_poll_module_starter() C12.octeon_ioctl_send_request C121. octeon_copy_input_dma_buffers : 新申请内核态的input buffer, 并把用户态提供的data拷进来 C122. octeon_create_output_dma_buffers : 将用户态提供的output buffer在内核态也申请一份 C123. octeon_create_data_buf C1231. octeon_create_sg_list C124. 主要的查询接口 中断处理 :就是调用具体器件的中断处理函数 INT1. octeon_droq_bh, 见A19 INT2. octeon_request_completion_bh, 和A15几乎一样 打开调试打印 定义CAVIUM_DEBUG PRINT_FLOW或CAVIUM_DEBUG PRINT_ALL还可以用这个函数打印droq信息: oct_dump_droq_state(octeon_droq_t *oq) 代码里随处可见的spin_lock和atomic变量 几乎每个结构体都有spin_lock保护, 比如pending list就有cavium_spin_lock_init(&oct->plist->lock); 接着还有atomic变量cavium_atomic_set(&oct->plist->instr_count, 0); 几种模式的组合 resp_order == OCTEON_RESP_ORDERED : resp_list = OCTEON_ORDERED_LIST : 由process_ordered_list()处理, 见A15 resp_order == OCTEON_RESP_UNORDERED && resp_mode == OCTEON_RESP_BLOCKING : resp_list = OCTEON_UNORDERED_BLOCKING_LIST : 由check_unordered_blocking_list()处理, 见A15 resp_order == OCTEON_RESP_UNORDERED && resp_mode == OCTEON_RESP_NON_BLOCKING : resp_list = OCTEON_UNORDERED_NONBLOCKING_LIST : 由oct_poll_check_unordered_list()处理, 但被注释掉了?, 见A16 一些主要脉络 octeon_ioctl_send_request __do_request_processing() __do_instruction_processing(); oct_test_thread() send_test_packet() octeon_process_request() __do_request_processing() __do_instruction_processing(); perf_test_loop() octeon_process_request() __do_request_processing() __do_instruction_processing(); cn56xx_send_peer_to_peer_map()----注意: nic模块也会调用下面这个函数 octeon_process_instruction() __do_instruction_processing(); octeon_destroy_resources() octeon_send_short_command() __do_instruction_processing(); octeon_hot_reset() octeon_send_short_command() __do_instruction_processing(); octeon_shutdown_output_queue() octeon_send_short_command() __do_instruction_processing(); octeon_restart_output_queue() octeon_send_short_command() __do_instruction_processing(); pci base driver模块 components/driver/host/driver/linux/octeon_linux.c module_init(octeon_base_init_module); //打印一些系统特征信息, 大小端, HZ, 编译宏开关等, 这是很好的架构设计习惯 octeon_state = OCT_DRV_DEVICE_INIT_START; //清零octeon device数组, 共4个 octeon_init_device_list(); //清零octmodhandlers数组, 3个 octeon_init_module_handler_list(); //见PCI-NIC 代码阅读之driver篇(结构体)之octeon_module_handler_t //注册pci驱动, 见A ret = pci_register_driver(&octeon_pci_driver); //简单设置了driver的bus等属性后, 还是调用driver_register(&drv->driver) octeon_state = OCT_DRV_DEVICE_INIT_DONE; //初始化poll进程 octeon_init_poll_thread() //表示kthread的结构体见PCI-NIC 代码阅读之driver篇(结构体)G2 //进程主体是oct_poll_thread, 见B cavium_kthread_setup(&oct_poll_id, oct_poll_thread, NULL, \"Oct Poll Thread\", 1); cavium_kthread_create() t->id = kthread_create(t->fn, t->fn_arg, t->fn_string); if(t->exec_on_create) wake_up_process(t->id); octeon_state = OCT_DRV_POLL_INIT_DONE; //注册字符设备/dev/octeon_device, 见C ret = register_chrdev(OCTEON_DEVICE_MAJOR, DRIVER_NAME,&octeon_fops); octeon_state = OCT_DRV_REGISTER_DONE; A. pci驱动 static struct pci_driver octeon_pci_driver = { .name = \"Octeon\", .id_table = octeon_pci_tbl, //包括了6XXX全系列的PCI ID .probe = octeon_probe, //见A1 .remove = __devexit_p(octeon_remove), }; A1. octeon的pci probe函数 components/driver/host/driver/linux/octeon_main.c octeon_probe(struct pci_dev *pdev, const struct pci_device_id *ent) //首先要hold一个octeon_device_t, 见主结构体 octeon_device_t *oct_dev=NULL; /*这里先打印(uint32_t)pdev->vendor, (uint32_t)pdev->device **这是linux驱动core层匹配好pci设备后传入的 **如果要改为用户态,参考remote-pci的方式找octeon设备 */ /*申请全局变量octeon_device[4]的内存 **这是个osi的接口, 但最后怎么调到linux的?见A11 oct_dev = octeon_allocate_device(pdev->device); oct = octeon_allocate_device_mem(pci_id); /*实际分配的内存包括 **主结构体:sizeof(octeon_device_t)+见结构体:sizeof(octeon_cn6xxx_t)+见结构体J:sizeof(octeon_dispatch_t)*64 **可能是为了省事, 把内存一次全部申请了, 后面也会分开用 */ //这里怎么做到osi的?见A11 buf = cavium_alloc_virt(size); //虚拟地址 oct = (octeon_device_t *)buf; oct->chip = (void *)(buf + octdevsize); oct->dispatch.dlist = (octeon_dispatch_t *)(buf + octdevsize + configsize); //见A18里面的使用 octeon_device[oct_idx] = oct; //不同的octeon的name不一样, \"Octeon%d\" //把oct_dev当作驱动的priv成员 pci_set_drvdata(pdev, oct_dev); //反过来指 oct_dev->pci_dev = (void *)pdev; //具体硬件要在这里配? octeon_device_init(oct_dev) /*这里面有个函数指针, 原型是 **typedef oct_poll_fn_status_t (* octeon_poll_fn_t)(void *, unsigned long); */ octeon_poll_ops_t poll_ops; //这里面一共做了三件事, 在pci-remote里面有对应的 octeon_pci_os_setup(octeon_dev) pci_enable_device(octeon_dev->pci_dev) //使能64bit访问, dev可以访问64bit host空间? pci_set_dma_mask(octeon_dev->pci_dev, PCI_DMA_64BIT) //使能device的master位 pci_set_master(octeon_dev->pci_dev) //使用驱动函数pci_read_config_dword来获得ID,并调用相应函数 octeon_chip_specific_setup(octeon_dev) case OCTEON_CN66XX_PCIID: /*设置主结构体:C **用来适配不同型号的octeon的钩子函数 **这些函数是和硬件最相关的操作, 寄存器和中断 **注意这是个osi的函数 **函数实现在components/driver/host/driver/osi/cn6xxx_common.c */ setup_cn66xx_octeon_device(oct) //其实chip是个void*, 还记得前面多申请的size吗? sizeof(octeon_cn6xxx_t) octeon_cn6xxx_t *cn6xxx = (octeon_cn6xxx_t *)oct->chip; //map bar0地址到oct->mmio[0].start octeon_map_pci_barx(oct, 0, 0) //reserve资源 pci_request_region(oct->pci_dev, baridx*2, DRIVER_NAME) oct->mmio[baridx].start = pci_resource_start(oct->pci_dev, baridx*2); oct->mmio[baridx].len = pci_resource_len(oct->pci_dev, baridx*2); //地址需要ioremap?, 把调试打印打开! oct->mmio[baridx].hw_addr = ioremap(oct->mmio[baridx].start, mapped_len); //map bar1 octeon_map_pci_barx(oct, 1, MAX_BAR1_IOREMAP_SIZE)) cn6xxx->conf = (cn6xxx_config_t *)oct_get_config_info(oct); /*聪明!根据chip id, 直接赋值配置表 **所有的配置表在components/driver/host/include/oct_config_data.h **这里按照octeon_cn6xxx_t来配的, 见A12 */ return (void *)&default_cn66xx_conf; /*设置oct->fn_list **包括setup_iq_regs setup_oq_regs setup_device_regs update_iq_read_idx **bar1_idx_setup bar1_idx_write **interrupt_handler enable_interrupt enable_io_queues **等等, 详见主结构体:C **中断处理函数cn6xxx_interrupt_handler见下:中断处理 */ //下面这个函数设置bar0的window寄存器相关的地址 cn6xxx_setup_reg_address() //map完成了, 配置也完成了, 但还没有写进去 cavium_atomic_set(&octeon_dev->status, OCT_DEV_PCI_MAP_DONE); //先复位一下 octeon_dev->fn_list.soft_reset(octeon_dev) //清空dispatch链表 octeon_init_dispatch_list(octeon_dev) //初始化poll fn list到oct->poll_list, 里面共64个poll函数 octeon_setup_poll_fn_list(octeon_dev); /*注册一个驱动自己用的poll_ops:\"Module Starter\" **函数是oct_poll_module_starter, 1秒一次, 见A13 **注册到上面的oct->poll_list */ octeon_register_poll_fn(octeon_dev->octeon_id, &poll_ops); //至此初始化完成? 但没有写任何的octeon的寄存器??? cavium_atomic_set(&octeon_dev->status, OCT_DEV_DISPATCH_INIT_DONE); //如果使能了buffer pool(USE_BUFFER_POOL), 搜索关键字: HUGE_BUFFER_CHUNKS /*注意这也是个osi的函数 **基本上是调用cavium_malloc_dma来预先分配内存, 按size大小分为6类,32k,16k,依次 **最后是调用kmalloc ----需要适配 **默认是关闭的, ----我准备打开这个功能 **但从get_buffer_from_pool()这个函数来看,不能把要申请的size分片 **比如超过比如32K, 则会调用cavium_malloc_dma分配内存 **也就是说, 这个驱动里面的packet都是假定物理内存连续的 ----这个问题要解决--要结合后面的pci_map_single() */ octeon_init_buffer_pool(octeon_dev, &bufpool_config) //先关闭input ring和output ring octeon_set_io_queues_off(octeon_dev); //初始化input ring octeon_setup_instr_queues(octeon_dev) //对32个ring, 或者说queue //注意这个结构体不需要物理地址连续, 见主结构体D oct->instr_queue[i] = cavium_alloc_virt(sizeof(octeon_instr_queue_t)); //这里才是真正的input ring的内存申请的地方 octeon_init_instr_queue(oct, i) iq = oct->instr_queue[iq_no]; q_size = conf->instr_type * conf->num_descs; //这个input queue的深度 //这个函数底层是调用linux内核的pci_alloc_consistent ----想好怎么替代了吗? 要保证硬件和软件看到的东西同步哦! iq->base_addr = octeon_pci_alloc_consistent(oct->pci_dev, q_size, &iq->base_addr_dma); //见A141, DMA内存 //no response list, 申请虚拟连续的内存, 是结构体D里面的一个成员 octeon_init_nr_free_list(iq, iq->max_count) //命令字的32bit或64bit是根据配置来的 //真正开始干活, 调用钩子函数设置iput ring oct->fn_list.setup_iq_regs(oct, iq_no); //注册poll函数 check_db_timeout, \"Doorbell Timeout\", 周期1个tick??会不会太快了?见A14 octeon_register_poll_fn(oct->octeon_id, &poll_ops); //到这里input ring完工 cavium_atomic_set(&octeon_dev->status, OCT_DEV_INSTR_QUEUE_INIT_DONE); //pending list是用来处理需要response的request的, 见主结构体E, 很霸道的那个结构体 octeon_init_pending_list(octeon_dev) //count是从那个配置表里来的 count = (CHIP_FIELD(oct, cn6xxx, conf))->c.pending_list_size; oct->pend_list_size = count; //给主结构体E分配空间, 虚拟地址连续就行 oct->plist = cavium_alloc_virt(sizeof(octeon_pending_list_t)); //这个list是E1,是个链表, 有count个node oct->plist->list = (octeon_pending_entry_t *) cavium_alloc_virt(OCT_PENDING_ENTRY_SIZE * count); //free_list是个数组, 有count个uint32 oct->plist->free_list = (uint32_t *) cavium_alloc_virt(sizeof(uint32_t) * count); oct->plist->entries = count; cavium_atomic_set(&octeon_dev->status, OCT_DEV_PEND_LIST_INIT_DONE); //现在是应答链表 octeon_setup_response_list(octeon_dev) //对三种类型, 初始化链表头, 实际上, 以下三类的response, 由链表头引领 //1 ordered, 1 unordered-blocking, 1 unordered-nonblocking //对全局变量noresp_buf_free_fn赋NULL, 这是个函数指针的二维数组 noresp_buf_free_fn[oct->octeon_id][i] = NULL; //注册poll函数, oct_poll_req_completion, 1个tick, 见A15 ret = octeon_register_poll_fn(oct->octeon_id, &poll_ops); //注意: 这个函数被注释掉了! 注册poll函数, oct_poll_check_unordered_list, 一秒10次, 见A16 /*ret = octeon_register_poll_fn(oct->octeon_id, &poll_ops);*/ cavium_atomic_set(&octeon_dev->status, OCT_DEV_RESP_LIST_INIT_DONE); //开始初始化output queue, 也叫dpoq? octeon_setup_output_queues(octeon_dev) //给32个droq分配空间 for 32个 //droq见主结构体G oct->droq[i] = cavium_alloc_virt(sizeof(octeon_droq_t)); octeon_init_droq(oct, i) //先从config表中查配置信息 //然后给真正的硬件output ring分配空间 //必须注意以下的申请内存的函数 droq->desc_ring = octeon_pci_alloc_consistent() //DMA内存 //底下调用__get_free_pages(),8字节对齐 droq->info_list = cavium_alloc_aligned_memory() //一个新的页? droq->recv_buf_list = cavium_alloc_virt() //见主结构体G5 octeon_droq_setup_ring_buffers(oct, droq)) //A1i1. 对每个描述符, 申请buffer, 真正和octeon硬件对应的buffer, 另外一处在refill, 见A17 for(i = 0; i max_count; i++) //注意这里的get_new_recv_buffer是osi的, 但实现是linux的skbuf； 这里就已经把收包bufer申请好了 buf = get_new_recv_buffer(droq->buffer_size); //用skb申请buffer?为什么? //recv_buf_list是个G5的数组 droq->recv_buf_list[i].buffer = buf; droq->recv_buf_list[i].data = get_recv_buffer_data(buf); //下面关键是, 简写, 实际上用octeon_pci_map_single转为IOMMU地址, 说明也是DMA内存 desc_ring[i].info_ptr = octeon_pci_map_single(&droq->info_list[i],...) desc_ring[i].buffer_ptr = octeon_pci_map_single(droq->recv_buf_list[i].data,...) octeon_droq_reset_indices(droq); droq->host_read_index = 0; droq->octeon_write_index = 0; droq->host_refill_index = 0; droq->refill_count = 0; cavium_atomic_set(&droq->pkts_pending, 0); //调用真正的设硬件的函数 oct->fn_list.setup_oq_regs(oct, q_no); poll_ops.fn = check_droq_refill; //注册check_droq_refill函数, 见A17, 每个q一个 octeon_register_poll_fn(oct->octeon_id, &poll_ops); //如果打开了USE_DROQ_THREADS, 但好像没开, 收包内核线程 cavium_init_wait_channel(&droq->wc); //底下是init_waitqueue_head() //droq内核线程, 运行oct_droq_thread, 见A18, 每个q一个 cavium_kthread_setup(&droq->thread, oct_droq_thread, droq,\"Oct DROQ Thread\", 0) cavium_kthread_create(&droq->thread) //底下是kthread_create() //绑定到cpu, q和cpu个数求余droq->q_no % cavium_get_cpu_count() cavium_kthread_set_cpu_affinity() //底下是kthread_bind() cavium_kthread_run(&droq->thread); //底下是wake_up_process() //至此output q初始化结束 cavium_atomic_set(&octeon_dev->status, OCT_DEV_DROQ_INIT_DONE); //根据注释, 调用其他的device相关的初始化寄存器函数 ret = octeon_dev->fn_list.setup_device_regs(octeon_dev); //这部分注册/proc/octeon cavium_init_proc(octeon_dev); root = proc_mkdir(octeon_dev->device_name, NULL); octeon_dev->proc_root_dir = (void *)root; node = create_proc_entry(\"debug_level\", 0644, root); node->read_proc = proc_read_debug_level; node->write_proc = proc_write_debug_level; //这里面的create_proc_read_entry()在新的内核里应该是proc_create_data() and seq_file node = create_proc_read_entry(\"stats\",0444, root, proc_read_stats,octeon_dev) //类似的, 创建\"configreg\" \"csrreg\" \"npireg\", 挂的是不同的读写函数 //如果没有打开USE_DROQ_THREADS, 则用tasklet来处理output q, 默认是走这 //octeon_droq_bh见A19, 由中断触发, 见中断处理 cavium_tasklet_init(&octeon_dev->droq_tasklet, octeon_droq_bh, (unsigned long)octeon_dev); //comp_tasklet用来加速ORDERED的请求的处理, 这个下半部函数主要是调用process_ordered_list(), 可以参考A15 cavium_tasklet_init(&octeon_dev->comp_tasklet, octeon_request_completion_bh, (unsigned long)octeon_dev); //注册PCIE中断, 见中断处理 octeon_setup_interrupt(octeon_dev); //打开msi中断 pci_enable_msi(oct->pci_dev) irqret = request_irq(oct->pci_dev->irq, octeon_intr_handler, CVM_SHARED_INTR, \"octeon\", oct); //现在开始使能中断 octeon_dev->fn_list.enable_interrupt(octeon_dev->chip); //使能input output q octeon_dev->fn_list.enable_io_queues(octeon_dev); //send credit是什么意思? for(j = 0; j num_oqs; j++) OCTEON_WRITE32(octeon_dev->droq[j]->pkts_credit_reg, octeon_dev->droq[j]->max_count); /* Packets can start arriving on the output queues from this point. */ octeon_dev->app_mode = CVM_DRV_INVALID_APP; cavium_atomic_set(&octeon_dev->status, OCT_DEV_HOST_OK); //注册dispath函数, 见A181 octeon_setup_driver_dispatches(oct_dev->octeon_id) //要求oct_dev->app_mode不是CVM_DRV_BASE_APP或CVM_DRV_INVALID_APP octeon_start_module(oct_dev->app_mode, oct_dev->octeon_id) //里面调的函数是octeon_register_module_handler()注册的 __octeon_module_action(app_type, OCTEON_START_MODULE, octeon_id) octeon_state = OCT_DRV_ACTIVE; A11. 如何osi? 到底哪些是osi的? 哪些又不是? components/driver/host/driver/osi/octeon_device.c buf = cavium_alloc_virt(size);实际调的是vmalloc, 这是linux kernel的函数, 这里已经不是osi了 os相关的定义在components/driver/host/driver/linux/cvm_linux_types.h #define cavium_alloc_virt(size) vmalloc((size)) 这个头文件被 components/driver/host/driver/linux/linux_sysdep.h 引用进而被 components/driver/host/include/cavium_sysdep.h 引用 #ifdef linux #include \"../driver/linux/linux_sysdep.h\" //现在是这个分支 #elif defined(__FreeBSD__) #include \"../driver/freebsd/freebsd_sysdep.h\" #elif defined (_WIN32) #include \"..\\driver\\windows\\windows_sysdep.h\" #else /* CUSTOM_OS */ #include \"custom_sysdep.h\" #endif A12.cn66xx的配置表 cn66xx_config_t default_cn66xx_conf = { /* num_iqs; num_oqs; pending_list_size; */ { 4, 4, 4096, }, /* Input Queue configuration */ /* num_descs; instr_type; db_min; db_timeout; */ { {1024, 32, 1, 1} , {1024, 32, 1, 1} , {1024, 32, 1, 1} , {1024, 32, 1, 1} }, /* Output Queue configuration */ /* num_descs; info_ptr; bufsize, pkts_per_intr; refill_threshold*/ { {1024, 1, 1536, 128, 128} , {1024, 1, 1536, 128, 128} , {1024, 1, 1536, 128, 128} , {1024, 1, 1536, 128, 128} , }, /* oq_intr_pkt; oq_intr_time; */ 64, 100, #ifdef CVMCS_DMA_IC /* dma_intr_pkt; dma_intr_time; */ 64, 1000, #endif }; A13. oct_poll_module_starter 这个poll的作用是等待core程序运行成功, 并报告它的app type oct_poll_module_starter(void *octptr, unsigned long arg) if(cavium_atomic_read(&oct->status) == OCT_DEV_RUNNING) { return OCT_POLL_FN_FINISHED; } /* If the status of the device is CORE_OK, the core application has reported its application type. Call any registered handlers now and move to the RUNNING state. */ if(cavium_atomic_read(&oct->status) != OCT_DEV_CORE_OK) return OCT_POLL_FN_CONTINUE; cavium_atomic_set(&oct->status,OCT_DEV_RUNNING); A14. check_db_timeout \"Doorbell Timeout\", 周期1个tick /* Called by the Poll thread at regular intervals to check the instruction * queue for commands to be posted and for commands that were fetched by Octeon. */ check_db_timeout(void *octptr, unsigned long iq_no) //先获取对应的input queue iq = oct->instr_queue[iq_no]; //计算是否超时, 这里面的cavium_jiffies是内核变量jiffies //没超时就啥也不干 If cavium_jiffies - last_db_time last_db_time = cavium_jiffies; //记录上次的cavium_jiffies //这里用spin_lock_softirqsave是为了阻止中断(tasklet)--哪个中断也要锁iq->lock呢? cavium_spin_lock_softirqsave(&iq->lock); //fill_cnt表示等待发到octeon的instructions个数, 超时了还有值, 则按门铃 if(iq->fill_cnt != 0) ring_doorbell(iq); //OCTEON_WRITE32也是个osi的接口, 底下是writel OCTEON_WRITE32(iq->doorbell_reg, iq->fill_cnt); iq->fill_cnt = 0; iq->last_db_time = cavium_jiffies; cavium_spin_unlock_softirqrestore(&iq->lock); if(cavium_atomic_read(&iq->instr_pending)) /** 这个注释真好! * Check for commands that were fetched by Octeon. If they were NORESPONSE * requests, move the requests from the per-queue pending list to the * per-device noresponse completion list. */ flush_instr_queue(oct, iq); update_iq_indices(oct, iq); //根据octeon已经读取的commands数更新, octeon_read_index表示octeon已经执行完的index iq->octeon_read_index = oct->fn_list.update_iq_read_idx(iq); /*把flush_index到octeon_read_index之间的 **NORESPONSE的请求移动到completion list(每个device一个) */ if(iq->flush_index != iq->octeon_read_index) inst_processed = __process_iq_noresponse_list(oct, iq); //在循环里把flush_index到octeon_read_index之间的instruction移到freelist //从iq->nrlist移到iq->nr_free.q iq->nr_free.q[put_idx].buf = iq->nrlist[old].buf; iq->nr_free.q[put_idx].buftype = iq->nrlist[old].buftype; iq->nrlist[old].buf = 0; iq->nrlist[old].buftype = 0; //这里表示noresponse的已经处理了inst_processed个 if(inst_processed) cavium_atomic_sub(inst_processed, &iq->instr_pending); iq->stats.instr_processed += inst_processed ; //对iq->nr_free进行释放, 还是irqsave锁iq->lock process_noresponse_list(oct, iq); //在get_idx和put_idx之间 while(get_idx != iq->nr_free.put_idx) switch(iq->nr_free.q[get_idx].buftype) { case NORESP_BUFTYPE_INSTR: //instr是octeon_soft_instruction_t, 见结构体E11 instr = (octeon_soft_instruction_t *)iq->nr_free.q[get_idx].buf; //instr_list是个局部变量, 表示一个链表 cavium_list_add_tail(&instr->list, &instr_list); case NORESP_BUFTYPE_NET: case NORESP_BUFTYPE_NET_SG: case NORESP_BUFTYPE_OHSM_SEND: //这里释放buf noresp_buf_free_fn[oct->octeon_id][iq->nr_free.q[get_idx].buftype](iq->nr_free.q[get_idx].buf); //接下来处理刚生成的instr_list链表 release_soft_instr(oct, instr, instr->req_info.status); //标记A14i1 if(soft_instr->dptr) //DPI_INST_HDR[G]=0时 if(!soft_instr->ih.gather) octeon_pci_unmap_single() //response_manager.c //DPI_INST_HDR[G]=1时, DPTR指向一个链表 else octeon_pci_unmap_sg_list() //注: 以上两个又是osi的接口, 见A141 if(soft_instr->rptr) if(!soft_instr->irh.scatter) octeon_pci_unmap_single() else octeon_pci_unmap_sg_list() //调用callback,是SET_REQ_INFO_CALLBACK()注册的, 见C12里面的调用 soft_instr->req_info.callback() if soft_instr->alloc_flags delete_soft_instr_buffers(octeon_dev, soft_instr); //见A142 cavium_free_buffer(octeon_dev,(uint8_t *)((uint64_t *)soft_instr->status_word - 1)); cavium_free_buffer(octeon_dev, (uint8_t*)soft_instr->scatter_ptr); cavium_free_buffer(octeon_dev, (uint8_t*)soft_instr->gather_ptr); cavium_free_buffer(octeon_dev, (uint8_t*)soft_instr->dptr); cavium_free_buffer(octeon_dev, (uint8_t*)soft_instr); A141. octeon_pci_map_single和octeon_pci_unmap_single #define USE_PCIMAP_CALLS //这里默认是打开的 octeon_pci_map_single(cavium_pci_device_t *pci_dev, void *virt_addr,uint32_t size, int direction) #if defined(USE_PCIMAP_CALLS) physaddr = pci_map_single(pci_dev, virt_addr, size, direction); #else physaddr = virt_to_phys(virt_addr); #endif octeon_pci_unmap_single(cavium_pci_device_t *pci_dev, unsigned long dma_addr, uint32_t size, int direction) #if defined(USE_PCIMAP_CALLS) pci_unmap_single(pci_dev, (dma_addr_t)dma_addr, size, direction); #endif 注:octeon_pci_map_single用来map已经申请的内存 desc_ring[i].info_ptr desc_ring[i].buffer_ptr cmd->dptr cmd->rptr sg_list[i].ptr[k]都会用到 为什么说只是个map呢? 因为这个bufer已经申请好了. 比如在 octeon_droq_setup_ring_buffers中 buf = get_new_recv_buffer(droq->buffer_size); dev_alloc_skb(size + SKB_ADJUST) //实际底下使用skbuf申请的 droq->recv_buf_list[i].data = get_recv_buffer_data(buf); desc_ring[i].buffer_ptr = (uint64_t)octeon_pci_map_single(oct->pci_dev, droq->recv_buf_list[i].data, droq->buffer_size, CAVIUM_PCI_DMA_FROMDEVICE ); 补充:pci_map_single是干啥的?见linux/include/asm-generic/pci-dma-compat.h static inline dma_addr_t pci_map_single(struct pci_dev *hwdev, void *ptr, size_t size, int direction) dma_map_single(hwdev == NULL ? NULL : &hwdev->dev, ptr, size, (enum dma_data_direction)direction); struct dma_map_ops *ops = get_dma_ops(dev); addr = ops->map_page(dev, virt_to_page(ptr), (unsigned long)ptr & ~PAGE_MASK, size, dir, attrs); return addr; 这里需要强调的是: 实际上这里用到了IOMMU, 把ptr转成了IOVA(IO Virtual address)驱动在发DMA的command之前, 需要调用pcimap*()去把地址转成IO虚拟地址IOVA是分domain的, 每个pcie的device都有自己的domain, 在一个p2p桥下面的所有devices共享虚拟地址空间 再补充:octeon_pci_alloc_consistent这个函数被用来申请droq->desc_ring和iq->base_addr的内存 这个函数是否保证了内存一致性??? octeon_pci_alloc_consistent pci_alloc_consistent(pci_dev, size, (dma_addr_t *)dma_addr_ptr); dma_alloc_coherent(hwdev == NULL ? NULL : &hwdev->dev, size, dma_handle, GFP_ATOMIC); struct dma_map_ops *ops = platform_dma_get_ops(dev); caddr = ops->alloc(dev, size, daddr, gfp, attrs); return caddr; A142. 内存申请及释放 在delete_soft_instr_buffers函数中, 调用了很多cavium_free_buffer()函数,这个函数和cavium_alloc_buffer()一起,管理像 soft_instr->dptr soft_instr soft_instr->gather_ptr soft_instr->scatter_ptr 这样的buffer这些是osi的接口 /* * Macros that switch between using the buffer pool and the system-dependent * routines based on compilation flags used. */ static __inline void * cavium_alloc_buffer(octeon_device_t *octeon_dev, uint32_t size) { #ifdef USE_BUFFER_POOL //从预先分配的几个pool中分配, 32k, 16k, 8k, 4k, 2k... //超过32k的size还是用cavium_malloc_dma来分配 return get_buffer_from_pool(octeon_dev, size); #else //其底层实现是kmalloc, 在cvm_linux_types.h return cavium_malloc_dma(size, __CAVIUM_MEM_ATOMIC); #endif } static __inline void cavium_free_buffer(octeon_device_t *octeon_dev, void *buf) { #ifdef USE_BUFFER_POOL put_buffer_in_pool(octeon_dev, (uint8_t *)buf); #else cavium_free_dma(buf); #endif } A15. oct_poll_req_completion, 1个tick 和octeon_request_completion_bh类似 对ordered list, 由链表头&octeon_dev->response_list[OCTEON_ORDERED_LIST]带领 下面链表的元素是octeon_pending_entry_t, 见结构体E1 oct_poll_req_completion(void *octptr, unsigned long arg) process_ordered_list(oct); //这个变量是一次处理的上限值 uint32_t resp_to_process = 4096; /*注意, 如果定义了CVMCS_DMA_IC **如果使能了DMA中断(def CVMCS_DMA_IC), 就可以确切的使用dma_cnt_to_process **dma_cnt_to_process表示已经有多少个被DMA了 */ resp_to_process = cavium_atomic_read(&octeon_dev->dma_cnt_to_process); //见结构体F, 三个response list中的一个头 response_list = &octeon_dev->response_list[OCTEON_ORDERED_LIST]; while instr; //这里的soft_instr->status_word是个地址, 应该是octeon写回status用的 uint64_t status64 = *(soft_instr->status_word); status = (octeon_req_status_t) (status64 & 0xffffffffULL); //已经发送ok了 if(status != OCTEON_REQUEST_PENDING) //从这个response list里面删除节点 release_from_response_list(octeon_dev, pending_entry); //注意这里会调callback, 也就是会把response从内核态考到用户态 release_soft_instr(octeon_dev, soft_instr, status); //见标记A14i1 //从pending list删除节点 release_from_pending_list(octeon_dev,pending_entry); oct->plist->free_index--; oct->plist->free_list[oct->plist->free_index] = pe->request_id; pe->status = OCTEON_PENDING_ENTRY_FREE; else //相当于退出循环了, 这里也正是order模式的精髓! 因为这个模式下的pending list是根据请求的顺序加的, 而处理的结果从status得出, 如果前面的entry没完成, 后面也是不会完成的.所以就此退出 //unordered blocking的ioctl会阻塞, check_unordered_blocking_list(oct); //见结构体F, 三个response list中的一个头 response_list = (octeon_response_list_t *)&(oct->response_list[OCTEON_UNORDERED_BLOCKING_LIST]); //对每个链表节点 //注:这里是unordered, 就不像上面, 遇到个未处理的就退出了. 这里是一直遍历这个链表: 但有个人为的最多16个的限制. cavium_list_for_each_safe(curr, tmp, &response_list->head) pending_entry = (octeon_pending_entry_t *)curr; soft_instr = pending_entry->instr; if *(soft_instr->status_word) != COMPLETION_WORD_INIT uint64_t status64 = *(soft_instr->status_word); else status = OCTEON_REQUEST_TIMEOUT; //不pending表示已经ok了? if(status != OCTEON_REQUEST_PENDING) //这里只调个callback?那么释放资源呢? --是SET_REQ_INFO_CALLBACK()注册的, 见C12里面的调用 soft_instr->req_info.callback() A16. oct_poll_check_unordered_list 这里的注释写的很好, 这个函数是给UNORDERED NONBLOCKING LIST收尸的; 这个类型的response是要app去查询完成状态的: app调用api octeon_query_request_status(query.octeon_id,&query)来查询. 但如果app意外退出了, 有些pending entry就永远不会释放了. A17. check_droq_refill, 每个q一个 check_droq_refill(void *octptr, unsigned long q_no) droq = oct->droq[q_no]; if (droq->refill_count >= droq->refill_threshold) //no dispatch时, buffer会保留, 否则还要申请新的recv buffer //并把droq->refill_count--;见A171 desc_refilled = octeon_droq_refill(oct, droq); //这可能是个刷cache的函数, 重要!怎么在用户态刷cache呢? cavium_flush_write(); OCTEON_WRITE32(droq->pkts_credit_reg, desc_refilled); A171. octeon_droq_refill 这个函数很多地方都在调 octeon_droq_refill(octeon_device_t *octeon_dev, octeon_droq_t *droq) desc_ring = droq->desc_ring; while(droq->refill_count && (desc_refilled max_count)) { /* If a valid buffer exists (happens if there is no dispatch), reuse the buffer, else allocate. */ if(droq->recv_buf_list[droq->host_refill_index].buffer == 0) //buffer为NULL说明已经dispatch了? 此时重新申请一个buffer --参考A182 buf = get_new_recv_buffer(droq->buffer_size); //见A141 /* If a buffer could not be allocated, no point in continuing */ if(!buf) break; droq->recv_buf_list[droq->host_refill_index].buffer = buf; data = get_recv_buffer_data(buf); else data = get_recv_buffer_data(droq->recv_buf_list[droq->host_refill_index].buffer); #if defined(ETHERPCI) data += 8; #endif droq->recv_buf_list[droq->host_refill_index].data = data; desc_ring[droq->host_refill_index].buffer_ptr = (uint64_t)octeon_pci_map_single(octeon_dev->pci_dev, data, droq->buffer_size, CAVIUM_PCI_DMA_FROMDEVICE); /* Reset any previous values in the length field. */ droq->info_list[droq->host_refill_index].length = 0; INCR_INDEX_BY1(droq->host_refill_index, droq->max_count); desc_refilled++; droq->refill_count--; A18. oct_droq_thread, 收包线程, 每个output q一个 但好像没打开; 和A19收包bh二选一 oct_droq_thread(void* arg) //传入的是q号 //打印跑在smp_processor_id()这个核上, 初始化时绑定过的 //如果没有stop, 没有signal, 底下是用kthread_should_stop()检查signal的 //全部的干活主体在这个循环里 while(!droq->stop_thread && !cavium_kthread_signalled()) while(cavium_atomic_read(&droq->pkts_pending)) //干活的主体 octeon_droq_process_packets(droq->oct_dev, droq); //pkts_pending就是待处理的报文数 pkt_count = cavium_atomic_read(&droq->pkts_pending); //何为快转? 好像是调了octeon_register_droq_ops()注册的, base里面没人调; nic模块里面用到了 if droq->fastpath_on pkts_processed = octeon_droq_fast_process_packets(oct, droq, pkt_count); //要不要关注一下? else //如果定义了ETHERPCI, 应该没有吧? octeon_droq_drop_packets(droq, pkt_count); //应该是下面的分支 pkts_processed = octeon_droq_slow_process_packets(oct, droq, pkt_count); for(pkt = 0; pkt info_list[droq->host_read_index]); //从上次的地方开始, 见主结构体G4 resp_hdr = (octeon_resp_hdr_t *)&info->resp_hdr; //见主结构体G41 octeon_swap_8B_data((uint64_t *)info, 2); //这里有dispatch buf_cnt = octeon_droq_dispatch_pkt(oct, droq, resp_hdr, info); //根据buffer_size得到buffer count, 因为每个recvbuf都是固定大小, 而buffer size可以很大 cnt = octeon_droq_get_bufcount(droq->buffer_size, info->length); //根据opcode得到分发函数, 是octeon_register_dispatch_fn()注册的, 见A181 disp_fn = octeon_get_dispatch(oct, (uint16_t)resp_hdr->opcode); //在octeon_dev->dispatch.dlist[idx]里面(注意这可能是个链表), 依次匹配opcode;见主结构体J //注意这里只添加droq->dispatch_list节点, 见A182, 真正处理这个链表在下面 //这里实际上已经从\"底层驱动\"收包层, 上交到分发层处理了 rinfo = octeon_create_recv_info(oct, droq, cnt, droq->host_read_index); rdisp->rinfo = rinfo; rdisp->disp_fn = disp_fn; *((uint64_t *)&rinfo->recv_pkt->resp_hdr) = *((uint64_t *)resp_hdr); cavium_list_add_tail(&rdisp->list, &droq->dispatch_list); /*上面已经dispatch了, 不管是不是有人在处理这些package, 将来总是会被处理和回收 **这里把refill_count加上去, 说明该申请新的buf了 */ droq->refill_count += buf_cnt; //下面判断是否pkt太多, 要丢弃, 和droq->pkts_per_intr配置有关 if((droq->ops.drop_on_max) && (pkts_to_process - pkt)) octeon_droq_drop_packets(droq, (pkts_to_process - pkt)); droq->stats.dropped_toomany += (pkts_to_process - pkt); //减掉已处理的 cavium_atomic_sub(pkts_processed, &droq->pkts_pending); if(droq->refill_count >= droq->refill_threshold) //和A17类似, 那么如果有USE_DROQ_THREADS, 那可能不需要A17了 desc_refilled = octeon_droq_refill(oct, droq); /*接下来遍历droq->dispatch_list链表, 其中每个元素是, 调用里面的disp_fn **调用完即删除 **struct __dispatch { ** cavium_list_t list; ** octeon_recv_info_t *rinfo; ** octeon_dispatch_fn_t disp_fn; **}; */ disp_fn(); //从这就断了???????????????怎么再往上面走?????????, 跟scatter的DMA又怎么联系起来?--不用联系起来 //到这里内层while刚结束, 睡眠等待新的pkts, 中断处理函数会唤醒 cavium_sleep_atomic_cond(&droq->wc, &droq->pkts_pending); //以下是一个常见的wait在wait_queue的操作, 等待droq->pkts_pending不为0 cavium_wait_entry we; cavium_init_wait_entry(&we, current); cavium_add_to_waitq(waitq, &we); set_current_state(TASK_INTERRUPTIBLE); while(!cavium_atomic_read(pcond)) schedule(); set_current_state(TASK_RUNNING); cavium_remove_from_waitq(waitq, &we); A181. octeon_register_dispatch_fn() 注册收包对应的opcode函数 所有的opcode在这里components/driver/common/octeon-drv-opcodes.h 在这里调用 octeon_setup_driver_dispatches(oct_dev->octeon_id); //注册的CORE_DRV_ACTIVE_OP处理函数, 主要作用是收到core OK的报文后, 置oct->status为OCT_DEV_CORE_OK octeon_register_dispatch_fn(oct_id, CORE_DRV_ACTIVE_OP, octeon_core_drv_init, get_octeon_device_ptr(oct_id)); 另外在components/driver/host/test/kernel/droq_test/octeon_droq_test.c 内核测试代码里面也有注册 if(octeon_register_dispatch_fn(OCTEON_ID, DROQ_PKT_OP1, droq_test_dispatch, NULL)) { if(octeon_register_dispatch_fn(OCTEON_ID, DROQ_PKT_OP2, droq_test_dispatch, NULL)) { A182. octeon_create_recv_info(), 从驱动层到分发层, 每个来的报文都会对应一个 /** Receive Packet format used when dispatching output queue packets with non-raw opcodes. The received packet will be sent to the upper layers using this structure which is passed as a parameter to the dispatch function */ typedef struct { /** Number of buffers in this received packet */ uint16_t buffer_count; /** Id of the device that is sending the packet up */ uint8_t octeon_id; /** The type of buffer contained in the buffer ptr's for this recv pkt. */ uint8_t buf_type; /** Length of data in the packet buffer */ uint32_t length; /** The response header */ octeon_resp_hdr_t resp_hdr; /** Pointer to the OS-specific packet buffer */ void *buffer_ptr[MAX_RECV_BUFS]; /** Size of the buffers pointed to by ptr's in buffer_ptr */ uint32_t buffer_size[MAX_RECV_BUFS]; } octeon_recv_pkt_t; /** The first parameter of a dispatch function. For a raw mode opcode, the driver dispatches with the device pointer in this structure. For non-raw mode opcode, the driver dispatches the recv_pkt_t created to contain the buffers with data received from Octeon. --------------------- | *recv_pkt ----|--- |-------------------| | | 0 or more bytes | | | reserved by driver| | |-------------------| octeon_create_recv_info(octeon_device_t *octeon_dev, octeon_droq_t *droq, uint32_t buf_cnt, uint32_t idx) //见G4 octeon_droq_info_t *info; octeon_recv_pkt_t *recv_pkt; //见上 octeon_recv_info_t *recv_info; //见上 uint32_t i, bytes_left; info = &droq->info_list[idx]; recv_info = octeon_alloc_recv_info(sizeof(struct __dispatch)); //以上两个结构体的size加起来再加extra_bytes, 为什么这里要用DMA的内存????? buf = cavium_malloc_dma(OCT_RECV_PKT_SIZE + OCT_RECV_INFO_SIZE + extra_bytes, __CAVIUM_MEM_ATOMIC); recv_info = (octeon_recv_info_t *)buf; recv_info->recv_pkt = (octeon_recv_pkt_t *)(buf + OCT_RECV_INFO_SIZE); recv_info->rsvd = NULL; if(extra_bytes) recv_info->rsvd = buf + OCT_RECV_INFO_SIZE + OCT_RECV_PKT_SIZE; recv_pkt = recv_info->recv_pkt; recv_pkt->resp_hdr = info->resp_hdr; recv_pkt->length = info->length; recv_pkt->buffer_count = (uint16_t)buf_cnt; recv_pkt->octeon_id = (uint16_t)octeon_dev->octeon_id; recv_pkt->buf_type = OCT_RECV_BUF_TYPE_2; i = 0; bytes_left = info->length; //一个大报文可以有多个buf_cnt while(buf_cnt) octeon_pci_unmap_single(octeon_dev->pci_dev, (unsigned long)droq->desc_ring[idx].buffer_ptr, droq->buffer_size, CAVIUM_PCI_DMA_FROMDEVICE); recv_pkt->buffer_size[i] = (bytes_left >= droq->buffer_size)?droq->buffer_size: bytes_left; //这里不是把buffer拷贝过去, 而只是填指针, 为什么要把buffer挪个位置呢? //其实这里是个分层, droq->recv_buf_list[idx].buffer是下层的buffer --就是用skb申请的buffer, 对应硬件DMA的output buffer //recv_pkt->buffer_ptr[i]是上层的buffer, 但从底层到上层不是buffer拷贝, 而只是指针转移 //那么此时, 底层的buffer就可以重新申请了. recv_pkt->buffer_ptr[i] = droq->recv_buf_list[idx].buffer; //底层的buffer已经转移给上层了; 这里赋值0, 在output buffer refill环节重新申请buffer,参考A171 droq->recv_buf_list[idx].buffer = 0; INCR_INDEX_BY1(idx, droq->max_count); bytes_left -= droq->buffer_size; i++; buf_cnt--; return recv_info; A19. octeon_droq_bh octeon_droq_bh(unsigned long pdev) for(q_no = 0; q_no num_oqs; q_no++) reschedule |= octeon_droq_process_packets(oct, oct->droq[q_no]); //参考A18 if(reschedule) //这个函数用来触发tasklet, 一般会在中断处理函数调 cavium_tasklet_schedule(&oct->droq_tasklet); B. oct_poll_thread, 所有设备和回调共享这一个线程 oct_poll_thread(void* arg) while !cavium_kthread_signalled() //对每个octeon设备 oct_process_poll_list(get_octeon_device(i)); poll_list = (octeon_poll_fn_node_t *)oct->poll_list; //对poll_list里面最多64个fn依次调用 //这里的函数由octeon_register_poll_fn()注册 //根据前面分析, 已经注册的函数有:A13 A14 A15 A16 A17 n = (octeon_poll_fn_node_t *)&poll_list[i]; ret = n->fn((void *)oct, n->fn_arg); cavium_sleep_timeout(1); //1个jiffy set_current_state(TASK_INTERRUPTIBLE); schedule_timeout(timeout); set_current_state(TASK_RUNNING); C. octeon_fops: 这里面主要是ioctl static struct file_operations octeon_fops = { open: octeon_open, release: octeon_release, read: NULL, write: NULL, ioctl: octeon_ioctl, //见C1 #if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,11) unlocked_ioctl: octeon_unlocked_ioctl, compat_ioctl: octeon_compat_ioctl, #endif mmap: NULL }; C1. octeon_ioctl int octeon_ioctl (struct inode *inode, struct file *file, unsigned int cmd, unsigned long arg) switch(cmd) { case IOCTL_OCTEON_HOT_RESET: retval = octeon_ioctl_hot_reset(cmd, (void *)arg); //arg是octeon_rw_reg_buf_t //先copy_from_user cavium_copy_in(&rw_buf, arg, OCTEON_RW_REG_BUF_SIZE) //得到主结构体 oct_dev = get_octeon_device(rw_buf.oct_id); octeon_hot_reset(oct_dev); /*我觉得一个典型的重启架构应该是重启函数设置一个全局变量 **真正干活的任务或资源主体在主循环里主动判断是否已经被stop了 **从而干净的清除资源 **这样的好处是功能代码集中在一起使用和释放 **而不用把释放资源的函数全部集中在发起stop的函数里执行 */ //这里面也是, 根据当前的状态字来干活. //总体上是发stop命令给octeon,等待一些活干完, 复位一些变量 //最后注册oct_poll_module_starter()函数到poll进程, 见C11 break; case IOCTL_OCTEON_SEND_REQUEST: retval = octeon_ioctl_send_request(cmd, (void *)arg); //见C12 break; case IOCTL_OCTEON_QUERY_REQUEST: retval = octeon_ioctl_query_request(cmd, (void *)arg); //查询, 结果到octeon_query_request_t break; case IOCTL_OCTEON_STATS: retval = octeon_ioctl_stats(cmd, (void *)arg); break; case IOCTL_OCTEON_READ32: case IOCTL_OCTEON_READ16: case IOCTL_OCTEON_READ8: case IOCTL_OCTEON_READ_PCI_CONFIG: case IOCTL_OCTEON_WIN_READ: retval = octeon_ioctl_read(cmd, (void *)arg); break; case IOCTL_OCTEON_WRITE32: case IOCTL_OCTEON_WRITE16: case IOCTL_OCTEON_WRITE8: case IOCTL_OCTEON_WRITE_PCI_CONFIG: case IOCTL_OCTEON_WIN_WRITE: retval = octeon_ioctl_write(cmd, (void *)arg); break; case IOCTL_OCTEON_CORE_MEM_READ: retval = octeon_ioctl_read_core_mem(cmd, (void *)arg); break; case IOCTL_OCTEON_CORE_MEM_WRITE: retval = octeon_ioctl_write_core_mem(cmd, (void *)arg); break; case IOCTL_OCTEON_GET_DEV_COUNT: retval = octeon_ioctl_get_dev_count(cmd, (void *)arg); break; case IOCTL_OCTEON_GET_MAPPING_INFO: retval = octeon_ioctl_get_mapping_info(cmd, (void *)arg); break; default: cavium_error(\"octeon_ioctl: Unknown ioctl command\\n\"); retval = -ENOTTY; break; } /* switch */ C11.oct_poll_module_starter() C12.octeon_ioctl_send_request typedef struct { /** wait channel head for this request. */ cavium_wait_channel wait_head; /** completion condition */ int condition; /** Status of request. Used in NORESPONSE completion. */ octeon_req_status_t status; } octeon_user_req_complete_t; copy buffer 既管input又管ouput, 这些是在内核空间的拷贝 /** This structure is used to copy the response for a request from kernel space to user space. */ typedef struct { /** The kernel Input buffer. */ uint8_t *kern_inptr; /** Size of the kernel output buffer. */ uint32_t kern_outsize; /** Address of the kernel output buffer. */ uint8_t *kern_outptr; /** Number of user space buffers */ uint32_t user_bufcnt; /** Address of user-space buffers. */ uint8_t *user_ptr[MAX_BUFCNT]; /** Size of each user-space buffer. */ uint32_t user_size[MAX_BUFCNT]; /** The octeon device from which response is awaited. */ octeon_device_t *octeon_dev; /** Wait queue and completion flag for user request. */ octeon_user_req_complete_t *comp; }octeon_copy_buffer_t; int octeon_ioctl_send_request(unsigned int cmd, void *arg) //从内核新申请octeon_soft_request_t结构, 这个结构见app篇 重要结构体 soft_req = (octeon_soft_request_t *)cavium_alloc_buffer(octeon_dev, OCT_SOFT_REQUEST_SIZE); //把用户态的这个结构拷进去 cavium_copy_in(soft_req, (void *)arg, OCT_SOFT_REQUEST_SIZE) //同样的方法把用户的octeon_request_info_t拷进去 req_info = (octeon_request_info_t *)cavium_alloc_buffer(octeon_dev, OCT_REQ_INFO_SIZE); cavium_copy_in((void *)req_info, (void *)user_req_info, OCT_USER_REQ_INFO_SIZE) resp_order = GET_SOFT_REQ_RESP_ORDER(soft_req); resp_mode = GET_SOFT_REQ_RESP_MODE(soft_req); dma_mode = GET_SOFT_REQ_DMA_MODE(soft_req); //获得主结构体 octeon_dev = get_octeon_device(GET_REQ_INFO_OCTEON_ID(req_info)); if (resp_mode == OCTEON_RESP_BLOCKING) //因为block的请求要wait在一个等待队列里, 在linux下是wait_queue_head_t //comp是octeon_user_req_complete_t, 见上面结构体 comp = octeon_alloc_user_req_complete(octeon_dev); //开始拷贝input数据的buf, 总的原则是用户空间的buf考到内核连续空间的buf //并替换相应的指针地址(从指向user的地址变为指向新申请的内核地址) if(soft_req->inbuf.cnt) { if((dma_mode == OCTEON_DMA_DIRECT) || (dma_mode == OCTEON_DMA_SCATTER)) //gather模式下, 用inbuf.ptr[(idx)].addr这个数组 //非gather模式下, 只用inbuf.ptr[0] retval = octeon_copy_input_dma_buffers(octeon_dev, soft_req, 0); //见C121 else retval = octeon_copy_input_dma_buffers(octeon_dev, soft_req, 1); if(retval) goto free_req_info; } else { soft_req->inbuf.cnt = 0; soft_req->inbuf.size[0] = 0; SOFT_REQ_INBUF(soft_req, 0) = NULL; } //申请octeon_copy_buffer_t的buffer, 用来把response拷回user空间 //copy_buf是octeon_copy_buffer_t, 见上面结构体 copy_buf = cavium_alloc_buffer(octeon_dev, sizeof(octeon_copy_buffer_t)); //这里只用0是因为从用户态拷贝到内核态变成了一个整的buffer copy_buf->kern_inptr = SOFT_REQ_INBUF(soft_req, 0); copy_buf->octeon_dev = octeon_dev; copy_buf->comp = comp; //noresponse模式下, 不用申请output buf soft_req->outbuf.cnt = 0; soft_req->outbuf.size[0] = 0; SOFT_REQ_OUTBUF(soft_req, 0) = NULL; copy_buf->user_bufcnt = 0; //以下是response模式, 申请在内核态用的response buffer retval = octeon_create_output_dma_buffers(octeon_dev, soft_req,...); //见C122 //完事还得拷回用户态, 问题是谁调了这个函数呢? --被注释掉的A16和release_soft_instr(), A14和A15都会调 SET_REQ_INFO_CALLBACK(req_info, octeon_copy_user_buffer, copy_buf); //下面是octeon_copy_user_buffer()做的事情 //1. 这个request是阻塞的 if(copy_buf->comp) copy_buf->comp->condition = 1; copy_buf->comp->status = status; //唤醒阻塞的进程 cavium_wakeup(&(copy_buf->comp->wait_head)); 2. 非阻塞的, 在这里拷贝 else /* For non-blocking there is no process sleeping. So do copy to user here and free buffers here. */ if(copy_buf->kern_inptr) { cavium_free_buffer(octeon_dev, copy_buf->kern_inptr); if(copy_buf->user_bufcnt) { //这里就是把copy_buf->kern_outptr + 8的buf考到用户态 //看了这个函数的实现就知道为什么内核态申请的buf都是一整块的了 octeon_user_req_copyout_response(copy_buf, status); if(copy_buf->kern_outptr) { cavium_free_buffer(octeon_dev, copy_buf->kern_outptr); cavium_free_buffer(octeon_dev, copy_buf); //这个重要了 status = __do_request_processing(octeon_dev, soft_req); //先申请重要结构体octeon_soft_instruction_t si, 见E11 si = cavium_alloc_buffer() //开始从soft_req往si里捯饬 cavium_memcpy(&si->exhdr_info, &sr->exhdr_info, OCT_EXHDR_INFO_SIZE); cavium_memcpy(&si->req_info, SOFT_REQ_INFO(sr), OCT_REQ_INFO_SIZE); si->ih = sr->ih; si->irh = sr->irh; //这个函数关键了! 带着问题, 前面已经把内核态的input和output的buffer都搞定了呀(C121, C122)? 而且更底层的output buffer也OK了(A1i1和A17), 这里搞啥? --dptr和rptr octeon_create_data_buf(oct, si, sr) //见C123 //调这个函数之前, input output bufer要准备好, dptr和rptr也要准备好； --所有buffer都要准备好 retval = __do_instruction_processing(oct, si, sr); irh = (uint64_t*)&si->irh; ih = (uint64_t*)&si->ih; si->irh.pcie_port = oct->pcie_port; resp_order = SOFT_INSTR_RESP_ORDER(si); resp_mode = SOFT_INSTR_RESP_MODE(si); iq_no = SOFT_INSTR_IQ_NO(si); //得到iq号 iq = oct->instr_queue[iq_no]; cmd = &si->command; //见主结构体E114 //注意: si->dptr是虚拟地址, 这里用pci_map*函数把这个虚拟地址转为iommu地址, 转到cmd->dptr里 if(si->dptr) //gather模式下, 硬件看到的是octeon_sg_entry_t的数组, 见C1231 cmd->dptr = (uint64_t)octeon_pci_map_single(si->dptr,...) //非gather模式下, 是一个bufer地址 cmd->dptr = (uint64_t)octeon_pci_map_single(si->dptr,...) if(si->rptr) //和上面差不多 //设超时时间 si->timeout = cavium_jiffies + SOFT_INSTR_TIMEOUT(si); if(resp_order != OCTEON_RESP_NORESPONSE) //加到pending list, 这是等待链表 在A15里面处理 pending_entry = add_to_pending_list(oct, si); //从oct->plist->list[]数组里找个空位置, 把这个command加到list //此时free index不能超过总的size if (oct->plist->free_index == oct->pend_list_size)) return ERROR; pl_index = oct->plist->free_list[oct->plist->free_index]; //这个index对应的node应该是free的 if(oct->plist->list[pl_index].status != OCTEON_PENDING_ENTRY_FREE) return ERROR; oct->plist->free_index++; oct->plist->list[pl_index].instr = si; oct->plist->list[pl_index].request_id = pl_index; oct->plist->list[pl_index].status = OCTEON_PENDING_ENTRY_USED; oct->plist->list[pl_index].iq_no = SOFT_INSTR_IQ_NO(si); //instr_count表示pending的命令数 cavium_atomic_inc(&oct->plist->instr_count); //更新一些status si->req_info.status = OCTEON_REQUEST_PENDING; si->irh.rid = pl_index; si->req_info.request_id = pl_index; cmd->ih = *ih; cmd->irh = *irh; //返回值是 index = iq->host_write_index; q_index = post_command64B(oct, iq,...) //实质就是ring_doorbell(iq); __post_command(oct, iq, force_db, (uint8_t *)cmd64); //把cmd拷到iq中去 __copy_cmd_into_iq(iq, cmd); iqptr = iq->base_addr + (cmdsize * iq->host_write_index); cavium_memcpy(iqptr, cmd, cmdsize); //更新host_write_index index = iq->host_write_index; INCR_INDEX_BY1(iq->host_write_index, iq->max_count); iq->fill_cnt++; /* Flush the command into memory. */ cavium_flush_write(); //wmb() //超过fill_threshold才按门铃 if(iq->fill_cnt >= iq->fill_threshold || force_db) ring_doorbell(iq); OCTEON_WRITE32(iq->doorbell_reg, iq->fill_cnt); iq->fill_cnt = 0; iq->last_db_time = cavium_jiffies; cavium_atomic_inc(&iq->instr_pending); if(q_index >= 0) if(resp_order == OCTEON_RESP_NORESPONSE) //谁来处理? --A14 __add_to_nrlist(iq, q_index, si, NORESP_BUFTYPE_INSTR); else pending_entry->queue_index = q_index; //至此这个command已经发送到iq里面了, 然后就是octeon取command执行 retval.s.request_id = si->req_info.request_id; retval.s.error = 0; retval.s.status = si->req_info.status; if(sr) SOFT_REQ_INFO(sr)->request_id = retval.s.request_id; SOFT_REQ_INFO(sr)->status = retval.s.status; if(resp_order != OCTEON_RESP_NORESPONSE) uint32_t resp_list; //三选一, OCTEON_ORDERED_LIST OCTEON_UNORDERED_NONBLOCKING_LIST OCTEON_UNORDERED_BLOCKING_LIST GET_RESPONSE_LIST(resp_order, resp_mode, resp_list); //加到相应的链表里, 这是应答链表. 谁来处理?--难道又是A15????? //每个发送的command都会加到应答链表(response模式下) //这个链表见主结构体F, 是个\"纯的\"链表, 其实它的节点是pending_entry //pending_entry从等待链表来, pending_entry见主结构体E1 push_response_list(&oct->response_list[resp_list], pending_entry); if(cavium_atomic_read(&iq->instr_pending) >= (iq->max_count/2)) flush_instr_queue(oct, iq); //参考A14 //接下来就是等着octeon干活了 //如果是阻塞模式, 注:这个模式不支持ordered if (resp_mode == OCTEON_RESP_BLOCKING) { //就直接在循环里查询等着, 后面等完了以后就地释放资源 do { if(!SOFT_REQ_IGNORE_SIGNAL(soft_req) && signal_pending(current)) query.status = OCTEON_REQUEST_INTERRUPTED; retval = octeon_query_request_status(query.octeon_id,&query);//见C124 /* comp->condition would be set in the callback octeon_copy_user_buffer() called from the request completion tasklet */ if(comp->condition == 0) cavium_sleep_timeout_cond(&comp->wait_head,&comp->condition,1); else query.status = comp->status; } while((query.status == OCTEON_REQUEST_PENDING) && (!retval)); /*### Copy query status to req_info here ###*/ SOFT_REQ_INFO(soft_req)->status = query.status; octeon_user_req_copyout_response(copy_buf, query.status); //把req info考到用户态 cavium_copy_out( user_req_info, SOFT_REQ_INFO(soft_req), OCT_USER_REQ_INFO_SIZE) C121. octeon_copy_input_dma_buffers : 新申请内核态的input buffer, 并把用户态提供的data拷进来 octeon_copy_input_dma_buffers(octeon_device_t *octeon_dev, octeon_soft_request_t *soft_req, uint32_t gather) //计算出态total size for (i = 0,total_size = 0; i inbuf.cnt; i++) total_size += soft_req->inbuf.size[i]; //gather模式最大支持64K, direct模式支持16K //新申请整个buf mem = cavium_alloc_buffer(octeon_dev, total_size); memtmp = mem; //把每个inbuf从用户态拷到刚申请的内核态buf里, 虽然拷成一个整体, 但还是会保留每个inbuf for (i = 0; i inbuf.cnt; i++) { if(cavium_copy_in((void *)memtmp, (void *)SOFT_REQ_INBUF(soft_req, i), soft_req->inbuf.size[i])) { cavium_error(\"OCTEON: copy in failed for inbuf\\n\"); cavium_free_buffer(octeon_dev, mem); soft_req->inbuf.cnt = 0; SOFT_REQ_INBUF(soft_req, 0) = NULL; return -EFAULT; } if(gather) //gather模式下设置每个inbuf的指针 SOFT_REQ_INBUF(soft_req, i) = memtmp; memtmp += soft_req->inbuf.size[i]; } if(!gather) { soft_req->inbuf.cnt = 1; soft_req->inbuf.size[0] = total_size; SOFT_REQ_INBUF(soft_req, 0) = mem; } C122. octeon_create_output_dma_buffers : 将用户态提供的output buffer在内核态也申请一份 octeon_create_output_dma_buffers(octeon_device_t *octeon_dev, octeon_soft_request_t *soft_req, octeon_copy_buffer_t *copy_buf, uint32_t scatter) //首先计算所有的outbuf的total size, 这个size最大16K, 或14*8K(scatter) cnt = soft_req->outbuf.cnt; for (i = 0, total_size=0; i outbuf.size[i]; //total size还要加上sizeof(octeon_resp_hdr_t)再加8, 见主结构体G41 total_size += OCT_RESP_HDR_SIZE + 8; //这个就是在内核空间的output buffer, 但要和refill的那个更底层的buffer区别开 mem = cavium_alloc_buffer(octeon_dev, total_size); //为copy会用户态做的准备 copy_buf->kern_outsize = total_size - OCT_RESP_HDR_SIZE - 8; copy_buf->kern_outptr = mem; copy_buf->user_bufcnt = cnt; for (i = 0; i user_ptr[i] = SOFT_REQ_OUTBUF(soft_req, i); copy_buf->user_size[i] = soft_req->outbuf.size[i]; } //填soft_req->outbuf while (size total_size) buf_size = (total_size - size); else buf_size = OCT_MAX_USER_BUF_SIZE; soft_req->outbuf.size[i] = buf_size; i++; soft_req->outbuf.cnt = i; C123. octeon_create_data_buf 根据DMA的方式, 填soft_instr->dptr(往octeon去)和soft_instr->rptr(从octeon来), 问题, 在哪里用的呢????? 以上两个ptr可以是直接一个指针地址, 也可以是octeon_sg_entry_t 的链表 static uint32_t octeon_create_data_buf(octeon_device_t *oct, octeon_soft_instruction_t *soft_instr, octeon_soft_request_t *soft_req) dma_mode = GET_SOFT_REQ_DMA_MODE(soft_req); resp_order = GET_SOFT_REQ_RESP_ORDER(soft_req); //确认buffer count不大于MAX_BUFCNT, 16 //DMA类型 //第一种: input和output都是direct OCTEON_DMA_DIRECT :soft_req->outbuf.cnt必须为0；最大支持16K的buffer; 可以没有input, 但必须有header的buffer soft_instr->ih.gather = 0; soft_instr->irh.scatter = 0; //申请input buf, 这里好像罗嗦了, 应该不用再申请一次buffer soft_instr->dptr = octeon_process_request_inbuf(oct, soft_instr, soft_req); //是哪个iq iq_no = SOFT_INSTR_IQ_NO(soft_instr); exhdr_size = SOFT_INSTR_EXHDR_COUNT(soft_instr) * 8; //这里在直接DMA模式下, 如果app提供了多个inbuf, 就合并到新的大buf里 if (soft_req->inbuf.cnt > 1) for i in all inbuf.cnt: total_size += soft_req->inbuf.size[i]; //总的size还要加上extra header total_size += exhdr_size; /* Add any extra header bytes present */ //重新申请buf, 底下要么是从buffer池来, 要么是kmalloc(可dma) buf = cavium_alloc_buffer(octeon_dev, total_size); //先拷extra header if(exhdr_size) { cavium_memcpy(tmpbuf, soft_instr->exhdr, exhdr_size); tmpbuf += exhdr_size; } //再拷每个inbuf的内容, 注意这里是dada的第二次拷贝, 第一次发生在从用户态拷到内核态, 见C121 for(i = 0 ; i inbuf.cnt; i++) { cavium_memcpy(tmpbuf, SOFT_REQ_INBUF(soft_req, i),soft_req->inbuf.size[i]); tmpbuf += soft_req->inbuf.size[i]; } soft_instr->dptr = buf; soft_instr->ih.dlengsz = total_size; SET_SOFT_INSTR_ALLOCFLAGS(soft_instr, OCTEON_DPTR_COALESCED); //只有一个inbuf else //只有一个buf就不用拷贝了 soft_instr->dptr = SOFT_REQ_INBUF(soft_req, 0); soft_instr->ih.dlengsz = soft_req->inbuf.size[0]; //填rptr, 因为在直接DMA模式下, soft_req->outbuf.cnt必须为0 if((soft_instr->ih.raw) && (resp_order != OCTEON_RESP_NORESPONSE)) soft_instr->rptr = SOFT_REQ_OUTBUF(soft_req, 0); soft_instr->irh.rlenssz = soft_req->outbuf.size[0]; soft_instr->status_word = outbuf的最后8个字节 //第二种:input是gather, output是direct OCTEON_DMA_GATHER :soft_req->outbuf.cnt必须为0 soft_instr->ih.gather = 1; //因为这个模式下output是直接的 soft_instr->rptr = SOFT_REQ_OUTBUF(soft_req, 0); soft_instr->irh.rlenssz = soft_req->outbuf.size[0]; soft_instr->status_word = (uint64_t *)((uint8_t *)SOFT_REQ_OUTBUF(soft_req, 0) + soft_req->outbuf.size[0] - 8); //上面搞定了rptr, 现在来搞dptr, 见C1231 soft_instr->dptr = octeon_create_sg_list(oct, &soft_req->inbuf , soft_instr, OCTEON_DPTR_GATHER); //第三种:input是direct, output是scatter OCTEON_DMA_SCATTER : 此DMA不支持noresponse, 很简单, 如果不用output,还要scatter干啥? soft_instr->irh.scatter = 1; //这个模式下, 要求inbuf.cnt必须为1???????和上面第一种DMA模式为什么有差别呢? //那么只有一个inbuf, 就直接用作dptr了 soft_instr->dptr = SOFT_REQ_INBUF(soft_req, 0); soft_instr->ih.dlengsz = soft_req->inbuf.size[0]; //上面算是搞定了dptr, 接写来搞rptr, 见C1231 soft_instr->rptr = octeon_create_sg_list(oct, &soft_req->outbuf, soft_instr, OCTEON_RPTR_SCATTER); if(resp_order != OCTEON_RESP_NORESPONSE) { soft_instr->status_word = (uint64_t *) ((uint8_t *)SOFT_REQ_OUTBUF(soft_req, (soft_req->outbuf.cnt-1)) + soft_req->outbuf.size[soft_req->outbuf.cnt-1] - 8); //第四种:input是gather, output是scatter OCTEON_DMA_SCATTER_GATHER :是以上两个模式的综合, 同上, 不支持noresponse //input和output的buf都搞定以后 if(soft_instr->ih.raw) if(IQ_INSTR_MODE_32B(oct, SOFT_INSTR_IQ_NO(soft_instr))) soft_instr->ih.fsz = 16; else soft_instr->ih.fsz = 16 + (SOFT_INSTR_EXHDR_COUNT(soft_instr) * 8); if(soft_instr->status_word) //status置为COMPLETION_WORD_INIT *(soft_instr->status_word) = COMPLETION_WORD_INIT; C1231. octeon_create_sg_list /** The Scatter-Gather List Entry. The scatter or gather component used with a Octeon input instruction has this format. */ typedef struct { /** The first 64 bit gives the size of data in each dptr.*/ union { uint16_t size[4]; uint64_t size64; } u; /** The 4 dptr pointers for this entry. */ uint64_t ptr[4]; }octeon_sg_entry_t; //这个函数并不拷贝data, 只是申请sg_count个上面的结构体, 然后把里面的4个ptr指向outbuf octeon_create_sg_list(octeon_device_t *oct, octeon_buffer_t *buf, octeon_soft_instruction_t *soft_instr, uint32_t flags) //cnt就是buf的个数 //gather链表 //也是先算total size, 加上一些extra header, 最大64K soft_instr->gather_bytes = total_size; soft_instr->ih.dlengsz = cnt; //scatter链表 //同样先算total size, 加上一些extra header, 最大14*8K soft_instr->irh.rlenssz = cnt; soft_instr->scatter_bytes = total_size; //把cnt按4个一组 sg_count = ROUNDUP4(cnt) >> 2; sg_list = octeon_alloc_sglist(oct, soft_instr, sg_count, flags); //结构体见上面octeon_sg_entry_t, sg_list需要8字节对齐 sg_list = cavium_alloc_buffer(octeon_dev,(sg_count*OCT_SG_ENTRY_SIZE)+ 7); if(flags & OCTEON_DPTR_GATHER) soft_instr->gather_ptr = (void *)sg_list; else soft_instr->scatter_ptr = (void *)sg_list; //向sg_list[i].ptr[k]填物理地址(或经过IOMMU的地址) for (i = 0, j = 0; i size[j]; /* Gather list data is swapped and interpreted in Hardware. Scatter data is interpreted by core software. We need to do swapping of scatter list manually. */ if(flags == OCTEON_RPTR_SCATTER) { octeon_swap_2B_data(&size, 1); sg_list[i].u.size[3-k] = size; } else { CAVIUM_ADD_SG_SIZE(&sg_list[i], size, k); } if(flags == OCTEON_DPTR_GATHER) { sg_list[i].ptr[k] = octeon_pci_map_single(oct->pci_dev, OCT_SOFT_REQ_BUFPTR(buf, j), buf->size[j], CAVIUM_PCI_DMA_TODEVICE); } else { sg_list[i].ptr[k] = octeon_pci_map_single(oct->pci_dev, OCT_SOFT_REQ_BUFPTR(buf, j), buf->size[j], CAVIUM_PCI_DMA_FROMDEVICE); octeon_swap_8B_data(&sg_list[i].ptr[k], 1); } } tmpcnt = 0; } return sg_list; //被当作rptr或dptr C124. 主要的查询接口 octeon_query_request_status(uint32_t octeon_id, octeon_query_request_t *query) octeon_device_t *octeon_dev = get_octeon_device(query->octeon_id); process_unordered_poll(octeon_dev, query)) //有request_id就可以找到pending entry pending_entry = &octeon_dev->plist->list[query->request_id]; soft_instr = pending_entry->instr; //COMPLETION_WORD_INIT是最开始的状态, 不是init说明已经在干活了? if(*(soft_instr->status_word) != COMPLETION_WORD_INIT) //得到新的status status = (octeon_req_status_t) (status64 & 0x00000000ffffffffULL); else //是否超时了? if(cavium_check_timeout(cavium_jiffies, soft_instr->timeout)) status = OCTEON_REQUEST_TIMEOUT; //不是pending说明用完了 if(status != OCTEON_REQUEST_PENDING) if(SOFT_INSTR_RESP_MODE(soft_instr) == OCTEON_RESP_BLOCKING) response_list = &octeon_dev->response_list[OCTEON_UNORDERED_BLOCKING_LIST]; else response_list = &octeon_dev->response_list[OCTEON_UNORDERED_NONBLOCKING_LIST]; release_from_response_list(octeon_dev, pending_entry); release_from_pending_list(octeon_dev,pending_entry); release_soft_instr(octeon_dev, soft_instr, status); //本次查询的结果, 一般开始会是OCTEON_REQUEST_PENDING, 最后是完成 query->status = status; 中断处理 :就是调用具体器件的中断处理函数 octeon_intr_handler(int irq, void *dev) return oct->fn_list.interrupt_handler(oct); 比如 cn6xxx_interrupt_handler cn6xxx_interrupt_handler(void *dev) //通过bar0读INT_SUM寄存器, 没有中断则马上退出 intr64 = OCTEON_READ64(cn6xxx->intr_sum_reg64); if !intr64 return //马上关本中断 oct->fn_list.disable_interrupt(oct->chip); if(intr64 & CN63XX_INTR_ERR) cn6xxx_handle_pcie_error_intr(oct, intr64); if(intr64 & CN63XX_INTR_PKT_DATA) //这里是从octeon网口来的报文, 和response没有关系 cn6xxx_droq_intr_handler(oct); droq_time_mask = octeon_read_csr(oct, CN63XX_SLI_PKT_TIME_INT); //对每个q for (oq_no = 0; oq_no num_oqs; oq_no++) //先检查是不是在mask里 if ( !(droq_mask & (1 droq[oq_no]; //一共收了多少包? pkt_count = octeon_droq_check_hw_for_pkts(oct, droq); pkt_count = OCTEON_READ32(droq->pkts_sent_reg) //为什么是加在pkts_pending上呢? 是指收到的还没处理的报文数? cavium_atomic_add(pkt_count, &droq->pkts_pending); //这里为什么还要写回这个寄存器呢? OCTEON_WRITE32(droq->pkts_sent_reg, pkt_count); //还有个poll模式???? 好像没打开 if(droq->ops.poll_mode) { droq->ops.napi_fn(oct->octeon_id, oq_no, POLL_EVENT_INTR_ARRIVED); else //打开了USE_DROQ_THREADS才会调, 也就是说, 即使是用内核线程收包, 也需要中断触发 cavium_wakeup(&droq->wc); //这个任务见A18 //不用USE_DROQ_THREADS时使用tasklet, 见INT1 cavium_tasklet_schedule(&oct->droq_tasklet); if(intr64 & (CN63XX_INTR_DMA0_FORCE|CN63XX_INTR_DMA1_FORCE)) //这里开始让comp_tasklet干活, 见INT2 cavium_tasklet_schedule(&oct->comp_tasklet); //好像没有搜到任何dma_ops下挂的函数 if((intr64 & CN63XX_INTR_DMA_DATA) && (oct->dma_ops.intr_handler)) oct->dma_ops.intr_handler((void *)oct, intr64); /* Clear the current interrupts */ OCTEON_WRITE64(cn6xxx->intr_sum_reg64, intr64); /* Re-enable our interrupts */ oct->fn_list.enable_interrupt(oct->chip); INT1. octeon_droq_bh, 见A19 这里的收包和发送命令字里面的response应该没有关系, 只是收octeon主动发过来的包, 然后分发; 现在走的应该是slow path, 而且没有真正的\"上层\"处理函数注册, 见A181 关于这里使用的tasklet: tasklet工作在软中断上下文 tasklet在多CPU之间是串行化执行的, 多CPU不会同时执行一个tasklet----而同是软中断的softirq可以同时执行 tasklet始终运行在被初始提交的同一处理器上，workqueue不一定 ----待确认 综上, 即使中断来的很快, tasklet_schedule(&oct->droq_tasklet)不断的被调用, 那么droq_tasklet也还是会被串行执行. 而线程收包模式可以利用多CPU一起收包, 只要他们不是处理同一个Q. --最多32个q INT2. octeon_request_completion_bh, 和A15几乎一样 void octeon_request_completion_bh(unsigned long pdev) { octeon_device_t *octeon_dev = (octeon_device_t *)pdev; octeon_dev->stats.comp_tasklet_count++; /* process_ordered_list returns 1 if list is empty. */ #ifdef CVMCS_DMA_IC /* No need to schedule this again, if too many pending, * then we got DMA_COUNT interrupt immediately */ process_ordered_list(octeon_dev); #else if(!process_ordered_list(octeon_dev)) cavium_tasklet_schedule(&octeon_dev->comp_tasklet); #endif check_unordered_blocking_list(octeon_dev); } "},"notes/smartNIC_liquidIO_代码阅读真NIC篇.html":{"url":"notes/smartNIC_liquidIO_代码阅读真NIC篇.html","title":"PCI-NIC 代码阅读 --真NIC篇","keywords":"","body":"这个驱动会把所有octeon的接口注册为host上的网口 结构体 octdev_props_t oct_link_status_resp_t 被用来做net_dev的priv glist A. init_module A1. octnet_init_nic_module A11. octnic_free_netbuf和octnic_free_netsgbuf A12. octnet_setup_nic_device A121. octnet_push_packet B. octnetdevops B1. octnet_open B2. octnet_xmit 网口发送函数 结构体 octdev_props_t /** Octeon device properties to be used by the NIC module. Each octeon device in the system will be represented by this structure in the NIC module. */ struct octdev_props_t { /** Number of interfaces detected in this octeon device. */ int ifcount; /* Link status sent by core app is stored in a buffer at this address. */ oct_link_status_resp_t *ls; /** Pointer to pre-allocated soft instr used to send link status request to Octeon app. */ octeon_soft_instruction_t *si_link_status; /** Flag to indicate if a link status instruction is currently being processed. */ cavium_atomic_t ls_flag; /** The last tick at which the link status was checked. The status is checked every second. */ unsigned long last_check; /** Each interface in the Octeon device has a network device pointer (used for OS specific calls). */ octnet_os_devptr_t *pndev[MAX_OCTEON_LINKS]; //是linux net_device }; oct_link_status_resp_t typedef struct { struct { int octeon_id; cavium_wait_channel wc; int cond; } s; uint64_t resp_hdr; uint64_t link_count; //一个link_info就是octeon的一个接口, 将来会被注册为host的网口 oct_link_info_t link_info[MAX_OCTEON_LINKS]; uint64_t status; } oct_link_status_resp_t; 被用来做net_dev的priv /** Octeon per-interface Network Private Data */ typedef struct { cavium_spinlock_t lock; /** State of the interface. Rx/Tx happens only in the RUNNING state. */ atomic_t ifstate; /** Octeon Interface index number. This device will be represented as oct in the system. */ int ifidx; /** Octeon Input queue to use to transmit for this network interface. */ int txq; //只有一个txq说明octeon的么个interface只能用一个q /** Octeon Output queue from which pkts arrive for this network interface.*/ int rxq; //只有一个rxq说明octeon的么个interface只能用一个q /** Linked list of gather components */ cavium_list_t glist; //比如q的最大深度是n, 则每个格子都一个glist --gather list /** Pointer to the NIC properties for the Octeon device this network interface is associated with. */ struct octdev_props_t *octprops; /** Pointer to the octeon device structure. */ void *oct_dev; octnet_os_devptr_t *pndev; #if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24) struct napi_struct napi; #endif /** Link information sent by the core application for this interface. */ oct_link_info_t linfo; /** Statistics for this interface. */ struct net_device_stats stats; /** Size of Tx queue for this octeon device. */ uint32_t tx_qsize; /** Size of Rx queue for this octeon device. */ uint32_t rx_qsize; /** Copy of netdevice flags. */ uint32_t pndev_flags; /* Copy of the flags managed by core app & NIC module. */ octnet_ifflags_t core_flags; } octnet_priv_t; #define OCTNET_PRIV_SIZE (sizeof(octnet_priv_t)) glist /** Structure of a node in list of gather components maintained by NIC driver for each network device. */ struct octnic_gather { /** List manipulation. Next and prev pointers. */ cavium_list_t list; /** Size of the gather component at sg in bytes. */ int sg_size; /** Number of bytes that sg was adjusted to make it 8B-aligned. */ int adjust; /** Gather component that can accomodate max sized fragment list received from the IP layer. */ octeon_sg_entry_t *sg; }; A. init_module init_module() octeon_module_handler_t nethandler; /* Register handlers with the BASE driver. For each octeon device that runs the NIC core app, the BASE driver would call the functions below for initialization, reset and shutdown operations. */ //会被base调用, 比如在设备重启后 nethandler.startptr = octnet_init_nic_module; //见A1 nethandler.resetptr = octnet_reset_nic_module; nethandler.stopptr = octnet_stop_nic_module; nethandler.app_type = CVM_DRV_NIC_APP; octeon_register_module_handler(&nethandler) A1. octnet_init_nic_module octnet_init_nic_module(int octeon_id, void *octeon_dev) oct_link_status_resp_t *ls = NULL; octeon_soft_instruction_t *si = NULL; int ifidx, retval = 0; //见上面结构体octdev_props_t octprops[octeon_id] = cavium_alloc_virt(sizeof(struct octdev_props_t)); /* Allocate a buffer to collect link status from the core app. */ ls = cavium_malloc_dma(sizeof(oct_link_status_resp_t), __CAVIUM_MEM_GENERAL); octprops[octeon_id]->ls = ls; /* Allocate a soft instruction to be used to send link status requests to the core app. */ si = (octeon_soft_instruction_t *) cavium_alloc_buffer(octeon_dev, OCT_SOFT_INSTR_SIZE); octprops[octeon_id]->si_link_status = si; //用于link status的命令字 octnet_prepare_ls_soft_instr(octeon_dev, si); cavium_memset(si, 0, OCT_SOFT_INSTR_SIZE); si->ih.fsz = 16; si->ih.tagtype = ORDERED_TAG; si->ih.tag = 0x11111111; si->ih.raw = 1; si->irh.opcode = HOST_NW_INFO_OP; si->irh.param = 32; SET_SOFT_INSTR_DMA_MODE(si, OCTEON_DMA_DIRECT); SET_SOFT_INSTR_RESP_ORDER(si, OCTEON_RESP_ORDERED); SET_SOFT_INSTR_RESP_MODE(si, OCTEON_RESP_NON_BLOCKING); SET_SOFT_INSTR_IQ_NO(si, 0); SET_SOFT_INSTR_TIMEOUT(si, 100); /* Since this instruction is sent in the poll thread context, if the doorbell coalescing is > 1, the doorbell will never be rung for this instruction (this call has to return for poll thread to hit the doorbell). So enforce the doorbell ring. */ SET_SOFT_INSTR_ALLOCFLAGS(si, OCTEON_SOFT_INSTR_DB_NOW); si->dptr = NULL; si->ih.dlengsz = 0; /* Send an instruction to get the link status information from core. */ //这个函数会阻塞等待, 注意: 这里是典型的调用发送接口的例子 octnet_get_inittime_link_status(octeon_dev, octprops[octeon_id]) struct octdev_props_t *props; octeon_soft_instruction_t *si; oct_link_status_resp_t *ls; octeon_instr_status_t retval; props = (struct octdev_props_t *)props_ptr; /* Use the link status soft instruction pre-allocated for this octeon device. */ si = props->si_link_status; /* Reset the link status buffer in props for this octeon device. */ ls = props->ls; cavium_memset(ls, 0, OCT_LINK_STATUS_RESP_SIZE); cavium_init_wait_channel(&ls->s.wc); //注意: 这里的rptr是ls的一个成员的地址, ls是上面用cavium_malloc_dma申请的 si->rptr = &(ls->resp_hdr); //这里的size是octeon回复的size, 从ls->resp_hdr地址开始. //是oct_link_status_resp_t去掉s的大小 si->irh.rlenssz = ( OCT_LINK_STATUS_RESP_SIZE - sizeof(ls->s) ); //octeon返回的status si->status_word = (uint64_t *)&(ls->status); *(si->status_word) = COMPLETION_WORD_INIT; ls->s.cond = 0; ls->s.octeon_id = get_octeon_device_id(oct); SET_SOFT_INSTR_OCTEONID(si, ls->s.octeon_id); //这个callback是收包(准确的说是收到response)以后的回调 //作用就是把 ls->s.cond = 1; SET_SOFT_INSTR_CALLBACK(si, octnet_inittime_ls_callback); SET_SOFT_INSTR_CALLBACK_ARG(si, (void *)ls); //发送命令字 retval = octeon_process_instruction(oct, si, NULL); //等待ls->s.cond = 1 /* Sleep on a wait queue till the cond flag indicates that the response arrived or timed-out. */ cavium_sleep_timeout_cond(&ls->s.wc, (int *)&ls->s.cond, 1000); return(ls->status); /* The link count should be swapped on little endian systems. */ octeon_swap_8B_data(&(ls->link_count), 1); octeon_swap_8B_data((uint64_t *)ls->link_info, (ls->link_count * (OCT_LINK_INFO_SIZE >> 3))); //这里是打印从octeon获取到的接口信息 for(ifidx = 0; ifidx link_count; ifidx++) { printk(\"OCTNIC: if%d rxq: %d txq: %d gmx: %d hw_addr: 0x%llx\\n\", ifidx, ls->link_info[ifidx].rxpciq, ls->link_info[ifidx].txpciq, ls->link_info[ifidx].gmxport, CVM_CAST64(ls->link_info[ifidx].hw_addr)); } octprops[octeon_id]->ifcount = ls->link_count; //注册noresponse的释放buf函数, 见driver篇A14 //这两个free的回调函数见A11 octeon_register_noresp_buf_free_fn(octeon_id, NORESP_BUFTYPE_NET, octnic_free_netbuf); octeon_register_noresp_buf_free_fn(octeon_id, NORESP_BUFTYPE_NET_SG, octnic_free_netsgbuf); //注册host上的网口, 见A12 for(ifidx = 0; ifidx link_count; ifidx++) octnet_setup_nic_device(octeon_id, &ls->link_info[ifidx], ifidx); cavium_atomic_set(&octprops[octeon_id]->ls_flag, LINK_STATUS_FETCHED); octprops[octeon_id]->last_check = cavium_jiffies; /* Register a poll function to run every second to collect and update link status. */ octeon_poll_ops_t poll_ops; poll_ops.fn = octnet_get_runtime_link_status; poll_ops.fn_arg = (unsigned long)octprops[octeon_id]; poll_ops.ticks = CAVIUM_TICKS_PER_SEC; strcpy(poll_ops.name, \"NIC Link Status\"); octeon_register_poll_fn(octeon_id, &poll_ops); cavium_print_msg(\"OCTNIC: Network interfaces ready for Octeon %d\\n\", octeon_id); A11. octnic_free_netbuf和octnic_free_netsgbuf octnic_free_netbuf(void *buf) struct sk_buff *skb; struct octnet_buf_free_info *finfo; octnet_priv_t *priv; //其实是sk_buff的一个成员cb finfo = (struct octnet_buf_free_info *)buf; skb = finfo->skb; priv = finfo->priv; octeon_unmap_single_buffer(get_octeon_device_id(priv->oct_dev), finfo->dptr, skb->len, CAVIUM_PCI_DMA_TODEVICE); free_recv_buffer((cavium_netbuf_t *)skb); __check_txq_state(priv); octnic_free_netsgbuf(void *buf) struct octnet_buf_free_info *finfo; struct sk_buff *skb; octnet_priv_t *priv; struct octnic_gather *g; int i, frags; finfo = (struct octnet_buf_free_info *)buf; skb = finfo->skb; priv = finfo->priv; g = finfo->g; frags = skb_shinfo(skb)->nr_frags; octeon_unmap_single_buffer(get_octeon_device_id(priv->oct_dev), g->sg[0].ptr[0], (skb->len - skb->data_len), CAVIUM_PCI_DMA_TODEVICE); i = 1; while(frags--) { struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i-1]; octeon_unmap_page(get_octeon_device_id(priv->oct_dev), g->sg[(i >> 2)].ptr[(i&3)], frag->size, CAVIUM_PCI_DMA_TODEVICE); i++; } octeon_unmap_single_buffer(get_octeon_device_id(priv->oct_dev), finfo->dptr, g->sg_size, CAVIUM_PCI_DMA_TODEVICE); cavium_spin_lock(&priv->lock); //把这个g回收再利用 cavium_list_add_tail(&g->list, &priv->glist); cavium_spin_unlock(&priv->lock); free_recv_buffer((cavium_netbuf_t *)skb); __check_txq_state(priv); A12. octnet_setup_nic_device octnet_setup_nic_device(int octeon_id, oct_link_info_t *link_info, int ifidx) octnet_priv_t *priv; octnet_os_devptr_t *pndev; uint8_t macaddr[6], i; pndev = octnet_alloc_netdev(OCTNET_PRIV_SIZE); octprops[octeon_id]->pndev[ifidx] = pndev; /* Associate the routines that will handle different netdev tasks. */ //这个网口的操作函数, 见B pndev->netdev_ops = &octnetdevops; /* Can checksum all the packets. */ pndev->features = NETIF_F_HW_CSUM; /* Scatter/gather IO. */ pndev->features |= NETIF_F_SG; priv = GET_NETDEV_PRIV(pndev); cavium_memset(priv, 0, sizeof(octnet_priv_t)); priv->ifidx = ifidx; /* Point to the properties for octeon device to which this interface belongs. */ priv->oct_dev = get_octeon_device_ptr(octeon_id); priv->octprops = octprops[octeon_id]; priv->pndev = pndev; cavium_spin_lock_init(&(priv->lock)); /* Record the ethernet port number on the Octeon target for this interface. */ priv->linfo.gmxport = link_info->gmxport; /* Record the pci port that the core app will send and receive packets from host for this interface. */ priv->linfo.ifidx = link_info->ifidx; priv->linfo.hw_addr = link_info->hw_addr; priv->linfo.txpciq = link_info->txpciq; priv->linfo.rxpciq = link_info->rxpciq; //默认是关的, napi见wiz笔记 if(OCT_NIC_USE_NAPI) octnet_setup_napi(priv); netif_napi_add(priv->pndev, &priv->napi, octnet_napi_poll, 64); cavium_print(PRINT_DEBUG, \"OCTNIC: if%d gmx: %d hw_addr: 0x%llx\\n\", ifidx, priv->linfo.gmxport, CVM_CAST64(priv->linfo.hw_addr)); /* 64-bit swap required on LE machines */ octeon_swap_8B_data(&priv->linfo.hw_addr, 1); for(i = 0; i linfo.hw_addr) + 2 + i)); /* Copy MAC Address to OS network device structure */ cavium_memcpy(pndev->dev_addr, &macaddr, ETH_ALEN); priv->linfo.link.u64 = link_info->link.u64; //这里的size是指q的深度 priv->tx_qsize = octeon_get_tx_qsize(octeon_id, priv->txq); priv->rx_qsize = octeon_get_rx_qsize(octeon_id, priv->rxq); octnet_setup_glist(priv) int i; struct octnic_gather *g; CAVIUM_INIT_LIST_HEAD(&priv->glist); //这个q的每个格子都有个glist for(i = 0; i tx_qsize; i++) g = cavium_malloc_dma(sizeof(struct octnic_gather), __CAVIUM_MEM_GENERAL); //OCTNIC_MAX_SG一般是kernel的MAX_SKB_FRAGS, 此值一般为18 //sg entry参考driver篇C1231 g->sg_size = ((ROUNDUP4(OCTNIC_MAX_SG) >> 2)* OCT_SG_ENTRY_SIZE); //为sg entry数组分配空间 g->sg = cavium_malloc_dma(g->sg_size + 8, __CAVIUM_MEM_GENERAL); /* The gather component should be aligned on a 64-bit boundary. */ if( ((unsigned long)g->sg) & 7) g->adjust = 8 - ( ((unsigned long)g->sg) & 7); g->sg = (octeon_sg_entry_t *)((unsigned long)g->sg + g->adjust); //以g的链表元素list为节点, 加到glist链表 cavium_list_add_tail(&g->list, &priv->glist); OCTNET_IFSTATE_SET(priv, OCT_NIC_IFSTATE_DROQ_OPS); /* Register the network device with the OS */ register_netdev(pndev) netif_carrier_off(pndev); if(priv->linfo.link.s.status) netif_carrier_on(pndev); octnet_start_txqueue(pndev); else netif_carrier_off(pndev); /* Register the fast path function pointers after the network device related activities are completed. We should be ready for Rx at this point. */ /* By default all interfaces on a single Octeon uses the same tx and rx queues */ priv->txq = priv->linfo.txpciq; priv->rxq = priv->linfo.rxpciq; octnet_setup_net_queues(octeon_id, priv) octeon_droq_ops_t droq_ops; memset(&droq_ops, 0, sizeof(octeon_droq_ops_t)); droq_ops.fptr = octnet_push_packet; //收包函数, 见A121 if(OCT_NIC_USE_NAPI) droq_ops.poll_mode = 1; droq_ops.napi_fn = octnet_napi_drv_callback; else droq_ops.drop_on_max = 1; /* Register the droq ops structure so that we can start handling packets * received on the Octeon interfaces. */ //注册fast path, 在收包线程和收包bh里面用到；见A18和A19 octeon_register_droq_ops(octeon_id, priv->rxq, &droq_ops) if(OCT_NIC_USE_NAPI) octnet_napi_enable(priv); OCTNET_IFSTATE_SET(priv, OCT_NIC_IFSTATE_REGISTERED); octnet_send_rx_ctrl_cmd(priv, 1); octnic_ctrl_pkt_t nctrl; memset(&nctrl, 0, sizeof(octnic_ctrl_pkt_t)); nctrl.ncmd.s.cmd = OCTNET_CMD_RX_CTL; nctrl.ncmd.s.param1 = priv->linfo.ifidx; nctrl.ncmd.s.param2 = start_stop; nctrl.netpndev = (unsigned long)priv->pndev; octnet_send_nic_ctrl_pkt(priv->oct_dev, &nctrl) si = octnic_alloc_ctrl_pkt_si(oct, nctrl); retval = octeon_process_instruction(oct, si, NULL); octnet_print_link_info(pndev); A121. octnet_push_packet 在收包线程或收包bh里面被调用, 负责把报文向上传递给协议栈 octnet_push_packet(int octeon_id, void *skbuff, uint32_t len, octeon_resp_hdr_t *resp_hdr) struct sk_buff *skb = (struct sk_buff *)skbuff; octnet_os_devptr_t *pndev = (octnet_os_devptr_t *)octprops[octeon_id]->pndev[resp_hdr->dest_qport]; if(pndev) octnet_priv_t *priv = GET_NETDEV_PRIV(pndev); /* Do not proceed if the interface is not in RUNNING state. */ if( !(cavium_atomic_read(&priv->ifstate) & OCT_NIC_IFSTATE_RUNNING)) { free_recv_buffer(skb); priv->stats.rx_dropped++; return; } skb->dev = pndev; skb->protocol = eth_type_trans(skb, skb->dev); skb->ip_summed = CHECKSUM_NONE; //用linux收包接口netif_rx()交到协议栈 if(netif_rx(skb) != NET_RX_DROP) priv->stats.rx_bytes += len; priv->stats.rx_packets++; pndev->last_rx = jiffies; else priv->stats.rx_dropped++; B. octnetdevops const static struct net_device_ops octnetdevops = { .ndo_open = octnet_open, //见B1 .ndo_stop = octnet_stop, .ndo_start_xmit = octnet_xmit, //见B2 .ndo_get_stats = octnet_stats, .ndo_set_mac_address = octnet_set_mac, .ndo_set_multicast_list = octnet_set_mcast_list, .ndo_tx_timeout = octnet_tx_timeout, .ndo_change_mtu = octnet_change_mtu, }; B1. octnet_open octnet_open(struct net_device *pndev) { octnet_priv_t *priv = GET_NETDEV_PRIV(pndev); //告诉上层, 可以往驱动层发包了 netif_start_queue(pndev); //现在是running态 OCTNET_IFSTATE_SET(priv, OCT_NIC_IFSTATE_RUNNING); //注册一个poll函数, octnet_poll_check_txq_status __setup_tx_poll_fn(pndev); octeon_poll_ops_t poll_ops; octnet_priv_t *priv = GET_NETDEV_PRIV(pndev); poll_ops.fn = octnet_poll_check_txq_status; poll_ops.fn_arg = (unsigned long)priv; poll_ops.ticks = 1; poll_ops.rsvd = 0xff; octeon_register_poll_fn(get_octeon_device_id(priv->oct_dev), &poll_ops); CVM_MOD_INC_USE_COUNT; return 0; } B2. octnet_xmit 网口发送函数 /** This structure is used by NIC driver to store information required to free the sk_buff when the packet has been fetched by Octeon. Bytes offset below assume worst-case of a 64-bit system. */ struct octnet_buf_free_info { /** Bytes 1-8. Pointer to network device private structure. */ octnet_priv_t *priv; /** Bytes 9-16. Pointer to sk_buff. */ struct sk_buff *skb; /** Bytes 17-24. Pointer to gather list. */ struct octnic_gather *g; /** Bytes 25-32. Physical address of skb->data or gather list. */ uint64_t dptr; }; octnet_xmit(struct sk_buff *skb, struct net_device *pndev) octnet_priv_t *priv; struct octnet_buf_free_info *finfo; octnic_cmd_setup_t cmdsetup; octnic_data_pkt_t ndata; int status = 0; priv = GET_NETDEV_PRIV(pndev); priv->stats.tx_packets++; if(!OCTNET_IFSTATE_CHECK(priv, OCT_NIC_IFSTATE_TXENABLED)) return OCT_NIC_TX_BUSY; //检查条件 if( !(cavium_atomic_read(&priv->ifstate) & OCT_NIC_IFSTATE_RUNNING) || (!priv->linfo.link.s.status) || (octnet_iq_is_full(priv->oct_dev, priv->txq)) || (skb->len cb to store info used to unmap and free the buffers. */ //这里看不懂; 为什么skb->cb就能是octnet_buf_free_info?? 协议栈按理说不应该知道这个结构体啊???? //解答: 在sk_buff的定义里cb是个48个字节的自由使用区 char cb[48] __aligned(8); //octnet_buf_free_info的定义见上面, 可见它只用了32个字节---标记:B2i1 finfo = (struct octnet_buf_free_info *)skb->cb; finfo->priv = priv; finfo->skb = skb; /* Prepare the attributes for the data to be passed to OSI. */ ndata.buf = (void *)finfo; ndata.q_no = priv->txq; ndata.datasize = skb->len; cmdsetup.u64 = 0; cmdsetup.s.ifidx = priv->linfo.ifidx; if( (is_ipv4(skb) && !is_ip_fragmented(skb) && is_tcpudp(skb)) || (is_ipv6(skb) && is_wo_extn_hdr(skb)) ) { cmdsetup.s.cksum_offset = sizeof(struct ethhdr) + 1; } /*重要!:不管有没有frags, 这里都做到了直接使用skb做为DMA的内存*/ //没有frags, 即只有一个buffer if(skb_shinfo(skb)->nr_frags == 0) cmdsetup.s.u.datasize = skb->len; octnet_prepare_pci_cmd(&(ndata.cmd), &cmdsetup); volatile octeon_instr_ih_t *ih; volatile octeon_instr_irh_t *irh; cmd->ih = 0; ih = (octeon_instr_ih_t *)&cmd->ih; ih->fsz = 16; ih->tagtype = ORDERED_TAG; ih->grp = OCTNET_POW_GRP; ih->tag = 0x11111111 + setup->s.ifidx; ih->raw = 1; if(!setup->s.gather) ih->dlengsz = setup->s.u.datasize; else ih->gather = 1; ih->dlengsz = setup->s.u.gatherptrs; //rptr为0说明是noresponse cmd->rptr = 0; cmd->irh = 0; irh = (octeon_instr_irh_t *)&cmd->irh; if(setup->s.cksum_offset) irh->rlenssz = setup->s.cksum_offset; irh->opcode = OCT_NW_PKT_OP; irh->param = setup->s.ifidx; /* Offload checksum calculation for TCP/UDP packets */ ndata.cmd.dptr = octeon_map_single_buffer(get_octeon_device_id(priv->oct_dev), skb->data, skb->len, CAVIUM_PCI_DMA_TODEVICE); finfo->dptr = ndata.cmd.dptr; ndata.buftype = NORESP_BUFTYPE_NET; //这中情况下是多buffer, 要搞gather链 else int i, frags; struct skb_frag_struct *frag; struct octnic_gather *g; cavium_spin_lock(&priv->lock); //从链表头摘一个元素, 发送完毕会再申请一个glist, 见A11 g = (struct octnic_gather *)cavium_list_delete_head(&priv->glist); cavium_spin_unlock(&priv->lock); cmdsetup.s.gather = 1; cmdsetup.s.u.gatherptrs = (skb_shinfo(skb)->nr_frags + 1); octnet_prepare_pci_cmd(&(ndata.cmd), &cmdsetup); //见上面 memset(g->sg, 0, g->sg_size); //建立gather list g->sg[0].ptr[0] = octeon_map_single_buffer(get_octeon_device_id(priv->oct_dev), skb->data, (skb->len - skb->data_len), CAVIUM_PCI_DMA_TODEVICE); CAVIUM_ADD_SG_SIZE(&(g->sg[0]), (skb->len - skb->data_len), 0); //一共有多少个frags frags = skb_shinfo(skb)->nr_frags; while(frags--) { frag = &skb_shinfo(skb)->frags[i-1]; //建立剩下的gather list g->sg[(i >> 2)].ptr[(i&3)] = octeon_map_page(get_octeon_device_id(priv->oct_dev), frag->page, frag->page_offset, frag->size,CAVIUM_PCI_DMA_TODEVICE); CAVIUM_ADD_SG_SIZE(&(g->sg[(i >> 2)]), frag->size, (i&3)); i++; } //这个cmd的dptr是给octeon DMA的 ndata.cmd.dptr = octeon_map_single_buffer(get_octeon_device_id(priv->oct_dev), g->sg, g->sg_size, CAVIUM_PCI_DMA_TODEVICE); finfo->dptr = ndata.cmd.dptr; finfo->g = g; ndata.buftype = NORESP_BUFTYPE_NET_SG; //发包, 重要! status = octnet_send_nic_data_pkt(priv->oct_dev, &ndata); //这个发送函数更底层, 直接就发送命令字, 不搞什么buffer; buffer在上面已经搞好了 return octeon_send_noresponse_command(oct, ndata->q_no, 1, &ndata->cmd, ndata->buf, ndata->datasize, ndata->buftype); pndev->trans_start = jiffies; return OCT_NIC_TX_OK; "},"notes/networking_dpdk使用_2014.html":{"url":"notes/networking_dpdk使用_2014.html","title":"DPDK使用(2014)","keywords":"","body":" 对内核的要求 hugepage的用法 编译 UIO与VFIO unbind与bind HPET 高精度事件时钟 如何防止业务core还被系统用来跑其他任务 当成一个网口 关于IOMMU 一键运行 对内核的要求 UIO HUGETLBFS PROC_PAGE_MONITOR support HPET and HPET_MMAP hugepage的用法 step1: 通过kernel command line在boot时 //reserve 1024个默认2M的 hugepages=1024 //reserve 4G, 每个1G default_hugepagesz=1G hugepagesz=1G hugepages=4 注意: 如果系统有两个CPU node, 好像是平分 或者在系统起来以后echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages #如果有2个CPU echo 1024 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages echo 1024 > /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages 注意: 1G的页只能在启动时reserve step2: mkdir /mnt/huge mount -t hugetlbfs nodev /mnt/huge #或者在/etc/fstab里加 nodev /mnt/huge hugetlbfs defaults 0 0 #如果要用1G的 nodev /mnt/huge_1GB hugetlbfs pagesize=1GB 0 0 编译 make install T=x86_64*gcc #只配置 make config T=x86_64-native-linuxapp-gcc #编译完了有 $ ls x86_64-native-linuxapp-gcc app build hostapp include kmod lib Makefile UIO与VFIO 以前是用UIOsudo modprobe uio sudo insmod kmod/igb_uio.ko 1.7版本以后有VFIO, 要求kernel版本在3.6.0以上; 还要求系统和bios支持IO虚拟化, 比如Intel VT-dsudo modprobe vfio-pci unbind与bind 一个device要先从其他module unbind, 再bind到igb_uio或vfio-pci上 DPDK用dpdk_nic_bind.py脚本VFIO有几个限制: VF只能作用于VF本身 PF要求所有的VF都bind到VFIO上 如果要用的device在桥后面, 因为这个桥就是一个IOMMU的group或者说domain, 所以这个桥必须从桥的驱动unbind, 然后bind到VFIO上 HPET 高精度事件时钟 用这个命令查看系统是否支持 # grep hpet /proc/timer_list 如何防止业务core还被系统用来跑其他任务 在kenel启动参数加以下参数, 告诉kernel调度任务时不要用哪些core isolcpus=2,4,6 当成一个网口 # insmod kmod/rte_kni.ko 关于IOMMU 内核需要配置以下 IOMMU_SUPPORT IOMMU_API INTEL_IOMMU igb_uio要求在kernel命令行加iommu=pt, 使用pass through模式, 不查DMAR表; 而且, 如果内核没配INTEL_IOMMU_DEFAULT_ON, 还要加intel_iommu=on 而vfio-pci没有这些要求, iommu=pt和iommu=on都能工作 一键运行 setup.sh, 这个脚本完成 Build the Intel® DPDK libraries Insert and remove the Intel ® DPDK IGB_UIO kernel module Insert and remove VFIO kernel modules Insert and remove the Intel ® DPDK KNI kernel module Create and delete hugepages for NUMA and non-NUMA cases View network port status and reserve ports for Intel® DPDK application use Set up permissions for using VFIO as a non-privileged user Run the test and testpmd applications Look at hugepages in the meminfo List hugepages in /mnt/huge Remove built Intel® DPDK libraries 用root运行 user@host:~/rte$ source tools/setup.sh ----------------------------------------------------------------- RTE_SDK exported as /home/user/rte ----------------------------------------------------------------- ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] i686-native-linuxapp-gcc [2] i686-native-linuxapp-icc [3] x86_64-ivshmem-linuxapp-gcc [4] x86_64-ivshmem-linuxapp-icc [5] x86_64-native-bsdapp-gcc [6] x86_64-native-linuxapp-gcc [7] x86_64-native-linuxapp-icc ---------------------------------------------------------- Step 2: Setup linuxapp environment ---------------------------------------------------------- [8] Insert IGB UIO module [9] Insert VFIO module [10] Insert KNI module [11] Setup hugepage mappings for non-NUMA systems [12] Setup hugepage mappings for NUMA systems [13] Display current Ethernet device settings [14] Bind Ethernet device to IGB UIO module [15] Bind Ethernet device to VFIO module [16] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linuxapp environment ---------------------------------------------------------- [17] Run test application ($RTE_TARGET/app/test) [18] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [19] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [20] Uninstall all targets [21] Unbind NICs from IGB UIO driver [22] Remove IGB UIO module [23] Remove VFIO module [24] Remove KNI module [25] Remove hugepage mappings [26] Exit Script "},"notes/device_nvme要点介绍.html":{"url":"notes/device_nvme要点介绍.html","title":"nvme要点介绍","keywords":"","body":" NVMe介绍 NVMe驱动 NVMe介绍 对SSD做了很大优化，明显降低延迟，提高IOPS。解决了相对SSD的速度，SATA接口日益成为瓶颈。 其中一部分的优化来自于软件stack。通过标准的制定，NVMe SSD有通用的驱动，不同厂家兼容，主流操作系统都支持。 NVMe精简了调用方式，执行命令时不需要读取寄存器；而AHCI每条命令则需要读取4次寄存器，一共会消耗8000次CPU循环，从而造成2.5μs的延迟。 但这也不是一劳永逸的，目前SSD使用的NAND闪存本身也会造成约50μs的延迟，也是很大的延迟的因素。 要解决存储介质造成的较高延迟，还需要依赖于未来可能应用的诸如PCM、RRAM、MRAM等新式存储器。 NVMe驱动 参考 https://www.osr.com/nt-insider/2014-issue4/introduction-nvme-technology/ NVMe是基于PCIe的存储接口, 定义了一套寄存器级的接口以及host和NVMe device见的命令协议用于数据传输.NVMe驱动只有一套, 已经集成到kernel里面了. 符合NVMe规范的设备都可以直接用. 和网卡驱动很类似的是, NVMe驱动也是host和device在内存里共享queue的, 而且更有趣的是, 这个共享的内存可以在host上, 也可以在device上. 但存储器件和网卡器件有个本质的不同, 网卡器件的报文是双向触发的, host driver和device都可以发起数据传输; 而存储器件的特性是, host发读写命令, device只负责响应, 所以是host单向发起命令的模式, 这种模式更简单. 所以NVMe在设计上, 用submission queue和completion queue就可以了. 流程见上图. NVMe支持64K个queue, 每个queue支持64K个entry. 通常device不会有这么多, 一般是64个queue(一个管理queue, 63个io queue)和网卡一样, 通常一对queue是要一个core来处理的. 这也是NVMe对多核系统更友好的原因. 另外一点对多核系统友好的是, NVMe用MSI-X做中断机制, 它允许中断被发往特定的core. 这样发送命令和处理device的完成中断都是在同一个core上. NVMe device支持中断聚合(interrupt coalescing), 这样就不用每个complition都发中断了, 可以攒几个一起发. 下图表示一个系统里, 有一个管理队列pair, 三个io队列pair. submission queue的entry是64 byte, 用来描述一次read write或flush操作, 支持最大8K长度的数据. completion queue的entry只有16 byte, 包括了完成状态和到对应的submission queue里的entry的引用, 和原始的command能对应上. NVMe对命令的简化: io命令只有三个: read write flush, 管理命令有create/delete io submission/completion queue, identify, abort, set/get features, async event request.而scsi或者sata的命令, 非常多, 又杂乱. "},"notes/as_title_cpu.html":{"url":"notes/as_title_cpu.html","title":"CPU Arch相关","keywords":"","body":"如题 "},"notes/cache_CPU和cache一致性原理.html":{"url":"notes/cache_CPU和cache一致性原理.html","title":"CPU和cache一致性原理","keywords":"","body":" 内存一致性和内存屏障 内存一致性协议: directory类的协议 内存的一致性协议: MESI协议(用于Pentium II, 1998), 属于snooping类的协议 基本概念 举例: atomic write buffer 带来一致性问题: 用Write memory barrier解决 invalid queue 带来一致性问题 用Read memory barrier解决 内存对齐 pthread的spin lock实现 其他实现方式 内存一致性和内存屏障 内存一致性协议: directory类的协议 目录类的协议, 是在一个表里记录核管理所有cache的状态, 也基本分三大类:Invalid Shared Exclusive CPU的所有读写, 也会发message给这个directory, 来保证一致性. 扩展性好? 内存的一致性协议: MESI协议(用于Pentium II, 1998), 属于snooping类的协议 上图A是指发起改变的cache line. B是监听改变的cache line. 以cache line为基本单位, 有四种状态: Modified Exclusive Shared Invalid 注意cache一致性并不单指L2或L3, 在现代CPU上, 通常一个core有自己的private L1 cache, 这个cache也要遵从cache一致性协议. 可以反证: 在spin lock场景下, 如果CPU的L1 cache是彼此孤立的, 未取得锁的CPU, 因为已经将lock值cache到L1, 每次读都有L1 read hit, 那么每次都将看到同一个值, 永远获取不到锁. 基本概念 read: 读一个cache line的物理地址 read response: 响应前一个read命令, 可以由memory或cache提供; 如果一个cache有这个data, 并且在\"Modified\"状态, 这个cache必须响应read命令 invalidate: 被失效的cache line的物理地址 invalidate acknowledge: 收到invalidate消息的CPU, 必须在移除被失效cache line后, 发invalidate acknowledge消息. read invalidate: 在发read地址的同时, 指示其他cache移除这个数据. 是read和invalidate的组合, 需要read response 和一组invalidate acknowledge writeback: 把\"Modified\"状态的cache line写入到memory, 以及可能的cache; 为其他数据腾空间. 举例: atomic 比如atomic_inc(), 是个read-modify-write的过程, 具体而言: CPU发read invalidate命令, 通过read invalidate来得到data, 同时通知其他持有这个data的CPU的cache line都失效 只有CPU收到完整的invalidate acknowledge消息后, 才能完成这次transition现代CPU通常有原子指令来支持atomic操作, 但其bus上的协议逻辑还是类似的. 再比如下面的while读取current_ticket, 假设此时CPU的cache没有这个data, 那么就是CPU发read命令, 等待read response得到data. 状态从invalid转变到shared. 再比如, 更新current ticket时, 依次完成 本来这个cache line是shared状态, 此时蓝色部分要更改变量, CPU就会发invalidate命令 CPU等待invalidate acknowledge相应, 这里图上画了8个CPU都有这个data, 那要等8个响应回来 此时状态为exclusive CPU把更新的data写到cache line里, 变为modified状态. 蓝色CPU 其他的CPU read这个data, 比如上面那个不断while读current_ticket的CPU 蓝色CPU必须响应红色CPU的read, 用read response把cache line返回给read请求的CPU, 此时也有可能writeback到内存里. 红色CPU得到data的cache line拷贝, 这次值更新了, 红色CPU退出等待. write buffer 如前面所述, 对变量写, 要先invalidate cache line, 要等待其他cache返回invalidate acknowledge, 最后再写cache line. 这个过程比较昂贵. 所以产生了write buffer, 或者叫store buffer. 这样CPU放下数据, 就可以返回, 不用等待. 带来一致性问题: 在下面的情况下, a被写入store buffer, 此时其他cache可能还在refer a=0 在执行b=1时, b是exclusive状态, 是可以直接写cache的;当收到read(b)的请求时, 响应read_reply(b,1)最后CPU1 看到了b的更新, 而没看到a的更新. 导致assert fail 用Write memory barrier解决 写屏障指令smp_wmb()是用来解决上述问题的: 在发生其他的store命令写cache之前, 强制CPU去flush它的store buffer CPU可以stall, 也可以把后续的store命令用store buffer先保存, 直到把之前的store buffer清空. 详细解释: invalid queue invalid消息可能会慢, 比如一个data有很多份cache line的拷贝, 要等到所有的invalidate acknowledgements都完成吗?虽然有store buffer, 但store buffer溢出怎么办?不等cache行不行? 接收CPU用invalid queue保存invalid消息 马上完成acknowledge, 通知发起CPU可以往下走 由invalid queue实施后面的invalid操作 带来一致性问题 这里已经用smp_wmb()来解决了store buffer的一致性问题. 但CPU1收到invalidate(a)后, 是放到队列里的.导致在看到b变为1后的assert(), 对a的访问是cache hit(因为还没有来得及invalidate, 可能因为invalidate queue暂存的invalidate消息比较多.在这种情况下, CPU1读到a还是0 用Read memory barrier解决 smp_rmb(): 标记invalidate queue中的所有entry(即invalidate消息), 使得后续的load(读)操作都要等待, 直到所有的被标记的entry都被成功应用到cache.这样读的时候, 就不会出现\"假的\"cache hit. 详细解释: 在CPU1执行while(b==0)后加入读屏障指令: CPU1必须停下来等待invalidate queue完成 invalidate queue里的invalidate(a)消息, 把a的cache line从cache里移除了 CPU1执行assert, 因为cache里没有a了, 发生cache miss, 产生了read消息 CPU0响应这个read消息, 返回新的a=1 CPU1收到a的值为1, 不会触发断言. 内存对齐 addr & ~(align - 1) // (gdb) p/x ~(8-1) $3 = 0xfffffff8 (gdb) p/x ~(64-1) $5 = 0xffffffc0 (gdb) p/x ~(128-1) $6 = 0xffffff80 pthread的spin lock实现 pthread库实现了spin lock, pthread_spinlock_t值为0表示空闲, 值为1表示被锁. 其实现如下: # 用perf probe能看到代码实现, 需要libc的debug符号 # perf probe -x /lib64/libpthread.so.0 -L pthread_spin_lock pthread_spin_lock (pthread_spinlock_t *lock) { /* atomic_exchange 通常要更省指令, 而 atomic_compare_and_exchange在锁已经被别人持有的时候, 产生的bus traffic更少 这里假设第一次尝试获取锁, 大部分情况下是成功的, 所以第一次用atomic_exchange, 不成功后面再用atomic_compare_and_exchange. */ if (atomic_exchange_acq (lock, 1) == 0) return 0; /* 等待锁释放. 如果没有这个while, 直接用cmpxchg会带来昂贵的核之间的同步开销, 所以这里先等待lock值变为0. 这里也可以加wait--计数, 防止无限等待 */ while (*lock != 0); /* 有人释放了锁, 这里就把锁置1. 也要用循环. 最简单的实现是, 前面都不要, 只要这个while, 也能搞定 代价是多核的同步traffic会变多, 通常这种同步开销比较大 */ while (atomic_compare_and_exchange_val_acq (lock, 1, 0) != 0); return 0; } # perf probe -x /lib64/libpthread.so.0 -L pthread_spin_unlock pthread_spin_unlock (pthread_spinlock_t *lock) { atomic_full_barrier (); //值为0表示释放锁. *lock = 0; return 0; } 其他实现方式 还有用计数器++, 用atomic_inc(计数器)等方式的spin lock实现. 所有这些实现, 都需要atomic_xxx, 来原子的改变lock变量的值. "},"notes/as_title_cpu1.html":{"url":"notes/as_title_cpu1.html","title":"ARM64","keywords":"","body":"如题 "},"notes/CPU_ARM64_thunder_overview.html":{"url":"notes/CPU_ARM64_thunder_overview.html","title":"thunder 概览","keywords":"","body":" 四个运行级别 地址空间 两级地址翻译 hardware walker 49bit VA, 48bit PA thunder overview cache SMMU thunder SMMU PCC NCB RST GIC NIC 收包的CQE格式 收包CQE有几种形态 SQ, send queue TL4:拥塞通告：IEEE 802.1Qau 报文格式 收报文流程 收报文格式解析: 对报文的前254个字节都可以解析? SLI 四个运行级别 --怎么理解secure模式?????好像任何地址都可以配置为secure和unsecure的, secure 地址空间 有三类: 虚拟地址(VA) 中间物理地址(IPA) 物理地址(PA) 同一时刻只能有其中一种地址存在在数据和指令cache中. 下图是物理地址的属性, bit47决定能不能cache. 这和octeon一样 两级地址翻译 两级查找: stage1(VA-->IPA)和stage2(IPA-->PA) uTLB: 缓存从VA到PA的结果 mTLB: 缓存stage1 and stage2 walker cache: 缓存the first three levels of the page table hardware walker 三级单位, 包含关系Page(level 2) > Block(2MB)(level 1) > Granule(64k)(level 0)Hardware-translation lookaside buffers and a page-table walkers deal only with granules or blocks 49bit VA, 48bit PA thunder overview cache L2cache保证CPUs, DMA的一致性. SMMU SMMU用来把PCIe设备的虚拟态transaction地址转为系统物理地址. IOMMU? ----> 只有有DMA的设备才用这个.对thunder来说, 每个内置的block都是pcie设备没错, 但只是看起来是. 而且, 他们bar地址是固定的. 从CPU看起来, 每个设备的寄存器地址也是固定的.这个和octeon一样. 从设备发起的传输是DMA而从CPU发起的的传输不是DMA, 比如读VNIC的寄存器. 因为用户态进程不会搞DMA的事情, 所以IOMMU几乎和内核自己用的空间是一样的.在虚拟化的时候, 这个特点就非常重要, 一个虚拟机操作的设备要看到和这个虚拟机一样的地址空间.SMMU保护的是内存不能被没有权利看到它的设备访问. thunder SMMU 4个SMMU, 每个对应一个ECAM 设备发起的到NCB的read/write transaction有个49位的地址和16位的识别码(用来标识是哪个PCI设备), 这个49位的地址就是每个设备\"隔离\"的地址空间, 通过SMMU查IOTLB(1024个entry), 命中了就好说了, 没命中的话, SMMU会walk整个TABLE去查 SMMU stream ID是个16bit的ID 非ARI设备这16bit就是bus:dev:func 是ARI设备, 是PF, 则是bus:func 是ARI设备, 是VF, 则是bus:func+VF号+1 RAD控制器的没看懂 外部设备就是16bit的requeser ID 可以disable 如果经过SMMU转换后, 一个DMA的写地址落在GIC的中断delivery寄存器, 这个就是MSI用下面的格式来识别发中断的设备 有下面几类的寄存器 ECAM Registers PCC PF Registers PCC VF Registers PCC Bridge Registers PCC PCI Common Configuration unitPCI Enhanced access mechanism(ECAM) 有内置的多个桥: PCI On-Chip Common Bridges (PCCBR) PCCPF/PCCVF就是指具体的device了 支持MSI-X SR-IOV相关的寄存器只在PF里面有 ARI技术: 在PCIE3.0里出现. 见PCIe ARI 在以前一个pci设备是bus(8bit):dev(5bit):func(3bit)来识别的, 一共16位, 所以我们经常说一共有256个bus, 32个dev, 8个func但是, ARI技术的出现, 让dev和func的概念同一叫func, 这样一来, 一个bus下面只有一个dev, 但这个dev可以有256个func NCB each NCBO has PCI configuration ECAM discovery through the PCC each NCBI has address translation routing through the SMMU 4个ECAM RST 内部有个ROM, 可以从spi或eMMC load固件但对image的格式, 每个地址区域存的东西都有要求启动流程: TBL1FW(由ROM load到L2)-->BL2FW-->BL3FW-->UEFI GIC 支持多cpu node 支持deliver to VM NIC 一个NIC有128个VF, 其中127个就像单独的pcie设备(SR-IOV), 地址转换由SMMU完成 QS, queue set, 和VF一对一, QS和VNIC可以自由组合, QS是物理上的, VNIC是逻辑上的QS到VNIC由NIC_PF_QS(0..127)_CFG[VNIC]控制 CQ, SQ, RBDR在内存里 CQ, completion queue, VNIC加entry(CQE, 512byte), core减entry. 每个entry是收包完成或发包完成超过水线可以发中断核处理完一个CQE, 要写NIC_QS(0..127)_CQ(0..7)_DOOR来释放CQE 收包的CQE格式 收包CQE有几种形态 小包直接放在CQE里面 copy报文头到CQE 指示放到RB SQ, send queue 核发包的时候add一个SQE, 再发doorbell, 然后VNIC把这个SQE发出去, 并remove这个entry.SQ必须对应一个CQ(可以多对一), NIC在发包完成时自动创建一个CQE?核发包是要填SQE到SQ, 然后写NIC_QS(0..127)_SQ(0..7)_DOOR发包, VNIC完成后产生一个CQE到CQ 发送命令字 TCP 分片: If NIC_SEND_HDR_S[TSO] is set 发包后的动作? 由VNIC来做? TL4:拥塞通告：IEEE 802.1Qau 拥塞通告属于流量管理，通过指示速率限制器来调整引起拥塞的流量，可将拥塞限制到网络边缘。IEEE 802.1Qau工作组接受了思科的拥塞通告提议，后者定义了一种架构来积极管理流量，以避免流量拥塞。 报文格式 RBDR, receive-buffer descriptor ring, 是个描述符的ring, 表示空闲的收报文buffer.RBDR里面的每项都是个buffer指针, cache line对齐, 大小是NIC_QS(0..127)_RBDR(0..1)_CFG[LINES]RBDR超过水线也可以发中断 RQ, receive queue, VNIC internal structure, 一个RQ也对应到一个CQ(可多对一), VNIC收包的时候会创建CQE. 一个RQ和一个或两个RBDR关联(可多对一).The receive queues (RQ) describe where receive traffic is to be placed. 收报文流程 报文从物理的interface进来, 这个物理口对应一个port号 BGX(0..1)_PORT(0..3)_CH(0..15) 或 TNS_PORT(0..1)_CH(0..127)从TNS来的报文可能加个内部头NIC_RX_HDR_S下一步是区分pkind, 一共16种, NIC_PF_PKIND(0..15)_CFG 典型应用是分三种pkind, one for all TNS interfaces, one for all BGX interfaces, and one for all BGX interfaces with PTP timestamps.然后决定报文的NIC_PF_CHAN(0..255)_RX_CFG, 这个CFG里面有个base和偏移, 决定了报文到哪个NIC_PF_CPI(0..2047)_CFG CPI是channel parse index到了NIC_PF_CPI(0..2047)_CFG里面, 就决定了到那个VNIC, 同时决定了和VNIC对应的RSS和RSSI表的BASE 问题1: 所以从上面过程来看, 关键是pkind和chanel?--TNS可以指示到NIC_PF_CPI(0..2047)_CFG的选择 --必须用TNS? 问题2: NIC_PF_CPI(0..2047)_CFG已经决定了VNIC, 为什么后面NIC_PF_RSSI(0..4095)_RQ还能决定到哪个RQ? --冲突了?--调查哪里用了NIC_PF_CPI(0..2047)_CFG RSS是什么? receive-side scaling NIC_PF_CPI(0..2047)_CFG里面 VNIC: 哪个VNIC收, 同时对应了NIC_VNIC(0..127)_RSS_CFG, 它可以做HASH, 根据IP TCP UDP等可配, 最终决定到哪个RQ? RSSI_BASE: 在RSSI中的基址 NIC_PF_RSSI(0..4095)_RQ: 通过HASH来决定报文到那个QS(0..127), 和一个QS里面的那个IDX(0..7) 收报文格式解析: 对报文的前254个字节都可以解析? mailbox用于在VF和PF之间通信(64bit words) TNSTNS到NIC的报文可以加 PF和VF的pcie接口 SLI 由core发起的请求, S2M: SLI to Mac 配置空间请求 由target发起的请求, M2S; 包括MSI thunder做为EP时, host方也能访问thunder内部寄存器, 通过SLI窗口寄存器, 这和octeon一样. "},"notes/CPU_ARM64_thunder_开发板操作记录.html":{"url":"notes/CPU_ARM64_thunder_开发板操作记录.html","title":"thunder 开发板操作记录","keywords":"","body":" new 如何export git的内容, 不带库 doxygen生成pdf 在fedora上编perf 在板子上直接编kernel 禁用transparent_hugepage 关于audit 分区resize new库 编好以后 做盘 格式化 安文件系统 解压到第二个分区 安装grub等--ubuntu 安装模块 grub错误 ubuntu使用 新版本UEFI CRB更新UEFI qlm配置 old 编译 bdk sdk kernel boot-wrapper --好像没什么用 --有用, dts uboot-build uefi-build atf 垃圾文件 atf的image 制作启动盘 new 如何export git的内容, 不带库 #! /bin/sh sdk_dir=`pwd` target_dir=~/tmp/SDK #export SDK cd $sdk_dir mkdir -p $target_dir git archive HEAD | tar xf - -C $target_dir #export linux cd $sdk_dir/linux/kernel/linux-aarch64 mkdir -p $target_dir/linux/kernel/linux-aarch64 git archive HEAD | tar xf - -C $target_dir/linux/kernel/linux-aarch64 #export uefi cd $sdk_dir/bootloader/edk2 mkdir -p $target_dir/bootloader/edk2 git archive HEAD | tar xf - -C $target_dir/bootloader/edk2 #export grub cd $sdk_dir/bootloader/grub mkdir -p $target_dir/bootloader/grub git archive HEAD | tar xf - -C $target_dir/bootloader/grub #export atf cd $sdk_dir/firmware/atf mkdir -p $target_dir/firmware/atf git archive HEAD | tar xf - -C $target_dir/firmware/atf #export uboot cd $sdk_dir/bootloader/u-boot mkdir -p $target_dir/bootloader/u-boot git archive HEAD | tar xf - -C $target_dir/bootloader/u-boot #export bdk cd $sdk_dir/firmware/bdk svn export . $target_dir/firmware/bdk doxygen生成pdf cd ~/repo/git/thunder/new/bdk/thunder/docs sudo apt-get install asciidoc sudo apt-get install xsltproc sudo apt-get install fop sudo apt-get install doxygen sudo apt-get install lua-ldoc make 在fedora上编perf 需要安装 yum install libdwarf-devel yum install audit-libs-devel yum install elfutils-libelf-devel yum install elfutils-libelf yum install libnuma yum install numactl-libs yum install python-libs yum install slang-devel yum install slang yum install libunwind-devel yum install elfutils-devel yum install gtk2-devel yum install libtool yum install perl-ExtUtils-Embed yum install binutils-devel yum install zlib-static yum install numactl-devel yum install python yum install python-devel cd /usr/src/linux-4.1.12-gentoo/tools/perf make make install 在板子上直接编kernel cd linux make mrproper # cp ../native-config-no-ilp32 .config make olddefconfig make Image -j64 make dtbs //估计要手动拷dtb到/boot, 这个和交叉编出来的一样 make modules -j64 make -s kernelrelease make modules_install -j64 make install -j48 //会在/boot下面安装vmlinuz initramfs和System.map make headers_install make INSTALL_HDR_PATH=/usr/include headers_install make firmware_install cd /repo/thunder/linux/arch/arm64/boot/dts cp thunder-88xx*.dtb /boot cp Image /boot/Image.native 实际上, 这里的Image和vmlinuz是一模一样的 那么现在很可能就是, kernel使用Image(vmlinuz), 不用ramfs, 而是直接从硬盘起. 禁用transparent_hugepage root@Tfedora /sys/kernel/mm/transparent_hugepage # cat enabled [always] madvise never 也可以在kernel commandline里面加 transparent_hugepage=never 关于audit 某次异常重启后有audit打印 # [ 303.397999] audit: type=2404 audit(1429709230.951:78): pid=1277 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=7c:94:0e:bc:be:3c:c9:0e:d7:eb:66:3c: bc:96:00:c0 direction=? spid=1277 suid=0 exe=\"/usr/sbin/sshd\" hostname=? addr=11.1.48.36 terminal=? res=success' [ 303.423078] audit: type=2404 audit(1429709230.981:79): pid=1277 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=cf:6d:fc:af:89:b7:de:5b:67:8e:70:7c:27 :ba:15:4a direction=? spid=1277 suid=0 exe=\"/usr/sbin/sshd\" hostname=? addr=11.1.48.36 terminal=? res=success' [ 303.448021] audit: type=2404 audit(1429709231.001:80): pid=1277 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=94:0c:25:26:08:7b:46:41:85:a3:0c:86:3f :35:09:1f direction=? spid=1277 suid=0 exe=\"/usr/sbin/sshd\" hostname=? addr=11.1.48.36 terminal=? res=success' [ 303.473623] audit: type=2407 audit(1429709231.031:81): pid=1276 uid=0 auid=4294967295 ses=4294967295 msg='op=start direction=from-server cipher=aes128-ctr ksize=128 mac=h mac-md5-etm@openssh.com pfs=curve25519-sha256@libssh.org spid=1277 suid=74 rport=50245 laddr=11.1.48.42 lport=22 exe=\"/usr/sbin/sshd\" hostname=? addr=11.1.48.36 terminal=? res=success' [ 303.505115] audit: type=2407 audit(1429709231.061:82): pid=1276 uid=0 auid=4294967295 ses=4294967295 msg='op=start direction=from-server cipher=aes128-ctr ksize=128 mac=h mac-md5-etm@openssh.com pfs=curve25519-sha256@libssh.org spid=1277 suid=74 rport=50245 laddr=11.1.48.42 lport=22 exe=\"/usr/sbin/sshd\" hostname=? addr=11.1.48.36 terminal=? res=success' 解决办法# auditctl -e 0有人提到了改/etc/audit/audit.rules, 把里面的-D改成-e 0 分区resize e2fsck -f /dev/sdc2 resize2fs /dev/sdc2 100G gdisk /dev/sdc n 几个都是默认的回车 new库 现在又改了 byj@mint ~/repo/git/cavium/thunder/new/sdk-master $ source env-setup git clone git://cagit1.caveonetworks.com/thunder/sdk/sdk-new.git cd sdk-new wget http://toolchain-releases.caveonetworks.com/releases/thunder/gcc47/thunderx-tools-693.tar.bz2 tar -xjf thunderx-tools-693.tar.bz2 ln -s thunderx-tools-693 tools . env-setup BOARD_TYPE=ebb8800 . env-setup BOARD_TYPE=crb-2s ./get_sources.sh -v --prep --gitdepth=10 ./get_sources.sh -v --update ./get_sources.sh -v --stable --prep --gitdepth=10 ./get_sources.sh -v --stable --update make uefi-build make linux-kernel 编好以后 cd ~/tmp/image $ scp yingjie@192.168.1.5:~/repo/git/thunder/thunder-* . $ sudo dd if=thunder-uefi.img of=/dev/mmcblk0 bs=4M $ sync 做盘 cd sdk/bootloader 格式化 ./create_disk.sh --raw-disk /dev/sdc 安文件系统 cd /cavium/share/thunder/linuxfs sudo kpartx -av ThunderX-ubuntu-8G-v3-FAE.img sudo mount /dev/mapper/loop0p2 mnt/ cd mnt sudo tar cjvpf ../ThunderX-ubuntu-8G-v3-FAE-root.tar.bz2 . sudo umount mnt sudo kpartx -dv ThunderX-ubuntu-8G-v3-FAE.img 解压到第二个分区 cd /cavium/share/thunder/linuxfs sudo mount /dev/sdc2 mnt cd mnt sudo tar xvf ../ThunderX-ubuntu-8G-v3-FAE-root.tar.bz2 sudo tar xvf ../ubuntu-shfae-150226.tar.bz2 tar xvf ../F21-aarch64-server-root.tar.bz2 sudo umount mnt 安装grub等--ubuntu ./create_disk.sh --install-grub2 /dev/sdc1 /dev/sdc2 /boot 此时在sdc1里面有 EFI fdt.dtb Image startup.nsh 而sdc2里面有boot/ efi fdt.dtb grub Image 安装grub等--redhat ./create_disk.sh --install-grub2 /dev/sdc1 /dev/sdc2 / 安装模块 ./create_disk.sh --install-modules /dev/sdc2 grub错误 Welcome to GRUB! error: disk `,gpt2' not found. Entering rescue mode... grub rescue> ls (hd0) (hd0,gpt2) (hd0,gpt1) (hd1) (hd2) grub rescue> ls (hd0,gpt2)/ ...显示root分区的内容 grub rescue> set root=(hd0,gpt2) grub rescue> set prefix=(hd0,gpt2)/boot/grub/ grub rescue> insmod /boot/grub/arm64-efi/normal.mod grub rescue> normal ubuntu使用 设时间 date -s \"2015-02-28 15:15:00\" 配网络$ cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo iface lo inet loopback # The primary network interface #auto eth0 #iface eth0 inet dhcp 新版本UEFI Please choose option 1 (97MB) and then answer as follows \\EFI\\BOOT\\BOOTAA64.EFI y Grub. CRB更新UEFI $ scp thunder-uefi.img yingjie@192.168.1.99:/tftpboot $ ssh sysadmin@192.168.1.20 superuser # cd /tmp/ # tftp -gr thunder-uefi.img 192.168.1.99 # /usr/local/bin/CPU_flash_upgrade.sh thunder-uefi.img qlm配置 old 编译 环境: cd sdk . env-setup bdk cd bdk make sdk $ cd scripts $ ./build-wrapper.sh sdk help *. ebb8800 uboot ./build-wrapper.sh --no-uefi --board=ebb8800 --fae sdk build *. ebb8800 uefi ./build-wrapper.sh --uefi --board=ebb8800 --fae sdk build *. ebb8804 uboot ./build-wrapper.sh --no-uefi --board=ebb8804 --fae sdk build *. ebb8804 uefi ./build-wrapper.sh --uefi --board=ebb8804 --fae sdk build *. crb1s uboot ./build-wrapper.sh --no-uefi --board=crb-1s --fae sdk build *. crb1s uefi ./build-wrapper.sh --uefi --board=crb-1s --fae sdk build *. crb2s uboot ./build-wrapper.sh --no-uefi --board=crb-2s --fae sdk build *. crb2s uefi ./build-wrapper.sh --uefi --board=crb-2s --fae sdk build kernel kernel-fae boot-wrapper --好像没什么用 --有用, dts thunder-88xx.dts 指向内核的dts文件 boot-wrapper/Makefile $(IMAGE): boot.o cache.o gic.o mmu.o ns.o smmu.o $(BOOTMETHOD) model.lds fdt.dtb $(KERNEL) $(FILESYSTEM) uboot-build cd $abs_top_builddir/sdk/bootloader/trusted-firmware/atf -->thunder-bootfs.img make with-uboot 只编atf和uboot 在build-sdk.source里面对kernel做mkimage, 然后把linux和dtb也做到thunder-bootfs.img里面去 依赖boot-wrapper/fdt.dtb, 用的是thunder-88xx.dts, 并加了一些启动参数, 比如root= 注: 这里的dts好像只分1s和2s的 uefi-build cd $abs_top_builddir/sdk/bootloader make grub make fdt -->这里用的是原始的thunder-88xx.dts, 没有改动 cd $abs_top_builddir/sdk/bootloader/trusted-firmware/atf make with-uefi uefi build -a AARCH64 -t ARMGCC -p ArmPlatformPkg/ThunderPkg/Thunder_${PLATFORM}.dsc -b ${TARGET} 内存, 默认16G(0x3FF000000) ---->要改 gArmTokenSpaceGuid.PcdSystemMemoryBase|0x00c00000 gArmTokenSpaceGuid.PcdSystemMemorySize|0x3FF000000 启动: gArmPlatformTokenSpaceGuid.PcdDefaultBootDescription|L\"GRUB\" 注: 下面用到的EFI_PART_UUID=E3AE6975-BACE-464E-91E1-BB4FE9954047写死在bootloader/create_disk.sh 默认的SATA port 是 QLM3/Port1, CRB最左边 gArmPlatformTokenSpaceGuid.PcdDefaultBootDevicePath|L\"PciRoot(0x1)/Pci(0x8,0x0)/Sata(0x0,0x0,0x0)/HD(1,GPT,E3AE6975-BACE-464E-91E1-BB4FE9954047,0x800,0x31801)/\\EFI\\BOOT\\BOOTAA64.EFI\" gArmPlatformTokenSpaceGuid.PcdDefaultBootType|0 参考启动: gArmPlatformTokenSpaceGuid.PcdDefaultBootDescription|L\"Linux from SemiHosting\" gArmPlatformTokenSpaceGuid.PcdDefaultBootDevicePath|L\"VenHw(C5B9C74A-6D72-4719-99AB-C59F199091EB)/Image\" gArmPlatformTokenSpaceGuid.PcdDefaultBootArgument|\" root=/dev/sda2 console=ttyAMA0 mem=1024M earlyprintk=pl011,0x87e024000000 debug maxcpus=16 rw\" gArmPlatformTokenSpaceGuid.PcdDefaultBootType|2 gArmPlatformTokenSpaceGuid.PcdFdtDevicePath|L\"VenHw(C5B9C74A-6D72-4719-99AB-C59F199091EB)/fdt.dtb\" 注意: 启动的时候需要\\EFI\\BOOT\\BOOTAA64.EFI, 但我在crb上没看到? --在硬盘的第一个分区上. 需要mount /dev/sdb1, BOOTAA64.EFI 有115k 但这个东西从哪里来的? 貌似是edk代码grub-install里写死的 ./bootloader/edk2/MdePkg/Include/Uefi/UefiSpec.h:2123:#define EFI_REMOVABLE_MEDIA_FILE_NAME_AARCH64 L\"\\\\EFI\\\\BOOT\\\\BOOTAA64.EFI\" ./bootloader/grub2/grub/util/grub-install.c:1107: efi_file = \"BOOTAA64.EFI\" atf with-uboot: ATF u-boot ${ATF_ROOT}/tools/make-bootfs.py --uboot ${UBOOT_IMAGE} -f ${BOOTFS_IMAGE} with-uefi: uefi ATF ${ATF_ROOT}/tools/make-bootfs.py --uefi ${UEFI_IMAGE} -f ${BOOTFS_IMAGE} ATF: bootwrapper fip.bin ${ATF_ROOT}/tools/make-bootfs.py --bs ${BDK_IMAGE} --bl0 ${ATFBS_IMAGE} -f ${BOOTFS_IMAGE} ${ATF_ROOT}/tools/make-bootfs.py --fip ${ATFFIP_IMAGE} --bl1 ${ATFBL1_IMAGE} -f ${BOOTFS_IMAGE} bootwrapper: make -s -C ${ATF_ROOT}/plat/thunder/bootstrap/ -j8 -->bootstrap.bin fip.bin: bl1.bin bl1.bin: make -f Makefile.ATF PLAT=thunder all fip -j8 -s CROSS_COMPILE=${CROSS_COMPILE} u-boot: make -s -C ${UBOOT_ROOT} thunder_asim_config make -s -C ${UBOOT_ROOT} -j8 -s CROSS_COMPILE=${CROSS_COMPILE} uefi: make -s -C ${THUNDER_ROOT}/bootloader uefi -->THUNDER_EFI.fd 垃圾文件 bootloader/trusted-firmware/Makefile, 因为它依赖的一个flash_tool被.py取代 atf的image ATF_BL0_OFFSET = 0x400000 ATF_BL1_OFFSET = 0x500000 ATF_BL2_OFFSET = 0x580000 ATF_BL32_OFFSET = 0x600000 ATF_LINUX_OFFSET = 0x800000 ATF_DTC_OFFSET = 0x700000 ATF_TBL_OFFSET = 0x480000 make-bootfs.py --bs(bdk boot strap) normal-$board.bin --bl0(atf boot strap) ${ATF_ROOT}/plat/thunder/bootstrap/build/bootstrap.bin --bl1(atf boot stage 1) ${ATF_ROOT}/build/thunder/release/bl1.bin --fip(atf boot stage 2 and 3.1) ${ATF_ROOT}/build/thunder/release/fip.bin -u(uboot) ${UBOOT_ROOT}/u-boot.bin ATF_BL32_OFFSET -e(uefi) ${THUNDER_ROOT}/bootloader/edk2/THUNDER_EFI.fd ATF_BL32_OFFSET -l(linux) uboot需要 -d(dtc) uboot需要, 在bootstrap阶段编译的dts 制作启动盘 bootloader/create_disk.sh FI_PART_UUID=E3AE6975-BACE-464E-91E1-BB4FE9954047 --raw-disk /dev/sdb create_disk_raw_disk() RAW_DISK=\"${1:-}\" check_part $RAW_DISK 1 reate_partitions $RAW_DISK 两个分区, 第一个是100M的efi, 第二个是linux program_disk 格式化两个分区fat和ext4 --install-grub2 --install-grub2 /dev/sdb1 /dev/sdb2 / # Red Hat --install-grub2 /dev/sdb1 /dev/sdb2 /boot # Ubuntu install_grub2() $1 $2 $3 __install_grub2 $efi_part $boot_part $boot_dir mkdir -p rootfs/boot mkdir -p rootfs/efi sudo mount -w $boot_part ./rootfs/boot sudo mount -w $efi_part ./rootfs/efi copy_files ./rootfs/efi ./rootfs/boot/$boot_dir ${RAW_DISK} sudo mkdir -p $boot_dir/efi sudo rm -rf $efi_dir/EFI sudo grub2/grub/build/sbin/grub-install \\ --target=arm64-efi --removable \\ --boot-directory=$boot_dir \\ --efi-directory=$efi_dir sudo cp startup.nsh $efi_dir 内容是fs0:\\Image dtb=fdt.dtb root=/dev/sda2 console=ttyAMA0 earlycon=pl011,0x87e024000000 debug rw uefi_debug 拷grub sudo mkdir -p $boot_dir/grub sudo cp ./grub2/grub.cfg $boot_dir/grub/ 默认的grub配置, 第一个是linux /boot/Image root=/dev/sda2 console=ttyAMA0 earlycon=pl011,0x87e024000000 debug rw uefi_debug; devicetree /boot/fdt.dtb 拷dtb sudo cp ./fdt.dtb $boot_dir sudo cp ./fdt.dtb $efi_dir 拷linux Image sudo cp ./Image $boot_dir sudo cp ./Image $efi_dir "},"notes/CPU_ARM64_thunder_bdk.html":{"url":"notes/CPU_ARM64_thunder_bdk.html","title":"thunder BDK","keywords":"","body":" BDK读SATA寄存器 BDK保存文件到flash lua调用c函数 fatfs lua trafficgen 怎么用 require(\"grafficgen\") fat fs over spi 编译 片上ROM BDK头 debug bdk的image构成 启动 重构后的main, normal-boot/boot-stub.c 2s的依赖 内存 bdk有三种 diagnostic 依赖 csr访问, 在lua环境下 lua BDK读SATA寄存器 cavium.csr.GSERX_LANEX_SDS_PIN_MON_2(3,0).display() BDK保存文件到flash 现在flash上实现了一个fatfs文件系统, 所以保存文件是可以的.文件需要用xmodem来传, 在BDK菜单下面的文件菜单下, 使用/xmodem虚拟文件作为源, /fatfs/xxx作为目的来拷贝就可以了. /fatfs/default.cfg /fatfs/lua/xxx.lua /fatfs/stage2.bin -- BDK diag菜单 lua调用c函数 比如在lua里面, 可以直接调用c函数, 怎么做到的呢? local data = cavium.c.bdk_twsix_read_ia(menu.node, twsi_bus, dev_addr, 0, 1, 0) 传统上, 如果一个C函数想被lua调用, 需要按照lua的格式来声明.但这里, 只用一个wrapper函数, 就能达到export所有C函数的效果.首先需要一个列表, 包括所有C函数 typedef struct { const char *name; void *func; } bdk_functions_t; extern const bdk_functions_t bdk_functions[]; 这个工作是一个python脚本完成的.在makefile里面, 生成一个c文件包含这个函数列表 bdk-functions.c: ./create_function_table.py $(objs) ./create_function_table.py $@ $(objs) 在这个脚本里, 主要是用readelf把所有函数列出来, 然后按照bdk_functions_t的格式写到文件里. def read_functions(elf, func_list): file = os.popen(\"aarch64-thunderx-elf-readelf --wide -s %s | grep -v LOCAL | grep FUNC\" % elf) for line in file: parts = line.split() func = parts[-1] if not func in func_list: func_list.append(func) file.close() return func_list all_funcs = [] for elf in sys.argv[2:]: read_functions(elf, all_funcs) def write_function_table(out, table_name, functions): out.write(\"\\n\") for f in functions: out.write(\"extern void %s() __attribute__((weak));\\n\" % f) out.write(\"\\n\") out.write(\"const bdk_functions_t %s[] = {\\n\" % table_name) for f in functions: out.write(\" {\\\"%s\\\", %s},\\n\" % (f, f)) out.write(\" {0, 0}\\n\") out.write(\"};\\n\") out.write(\"\\n\") out = open(sys.argv[1], \"w\") write_function_table(out, \"bdk_functions\", bdk_funcs) 好了, 现在所有C函数(这里只取bdk_开头的)都在bdk-functions.c里面了. 对在这个table里面的每个entry, 其实就是在lua的表(table)里面, 用函数名做索引, 把函数地址作为值. lua_pushlightuserdata(L, bdk_functions[i].func); lua_pushcclosure(L, cavium_c_call, 1); lua_setfield(L, -2, bdk_functions[i].name); 这里面统一注册的是一个叫cavium_c_call的函数, 下面揭开这个万能函数的真面目 static int cavium_c_call(lua_State* L) { //首先, 这个func最多支持8个参数 long (*func)(long arg1, long arg2, long arg3, long arg4, long arg5, long arg6, long arg7, long arg8); long args[8]; //参数个数, 为什么getop能取出来? int num_args = lua_gettop(L); //函数地址 func = lua_topointer(L, lua_upvalueindex(1)); int i; for(i=0; i fatfs /fatfs/default.cfg ./normal-boot/Makefile: $(BDK_ROOT)/bin/fatfs-tool -q -i $(FATFS_IMG) mkfs ./normal-boot/Makefile: $(BDK_ROOT)/bin/fatfs-tool -q -i $(FATFS_IMG) cp $(STAGE1_BIN) / ./normal-boot/Makefile: $(BDK_ROOT)/bin/fatfs-tool -q -i $(FATFS_IMG) cp $(STAGE2_BIN) / ./normal-boot/Makefile: $(BDK_ROOT)/bin/fatfs-tool -q -i $(FATFS_IMG) mkdir /lua ./normal-boot/Makefile: $(BDK_ROOT)/bin/fatfs-tool -q -i $(FATFS_IMG) cp lua-common/*.lua /lua ./normal-boot/Makefile: $(BDK_ROOT)/bin/fatfs-tool -q -i $(FATFS_IMG) cp $(BOARD_ROOT)/lua/*.lua /lua ./normal-boot/Makefile: $(BDK_ROOT)/bin/fatfs-tool -q -i $(FATFS_IMG) cp $(BOARD_ROOT)/default.cfg / ./normal-boot/Makefile: $(BDK_ROOT)/bin/fatfs-tool -q -i $(FATFS_IMG) cp ./safe-mode.cfg / lua trafficgen 怎么用 local tg_pass = true local trafficgen = require(\"trafficgen\") local tg = trafficgen.new() cavium.c.bdk_wait_usec(3 * 1000000) -- wait for links to come up. -- Do 100k packets, 60 bytes each, 50% of gigabit, timeout 5 secs all_pass = tg_run(tg, \"N0.XLAUI1\", 60, 100000, 50, 5) and all_pass -- Do 100k packets, 1499 bytes each, 50% of gigabit, timeout 7 secs all_pass = tg_run(tg, \"N0.XLAUI1\", 1499, 100000, 50, 7) and all_pass -- Do 100k packets, 9212 bytes each, 50% of gigabit, timeout 12 secs all_pass = tg_run(tg, \"N0.XLAUI1\", 9212, 10000, 10, 12) and all_pass -- Do 100k packets, 60 bytes each, 50% of gigabit, timeout 5 secs all_pass = tg_run(tg, \"N0.XFI0.0\", 60, 100000, 50, 5) and all_pass require(\"grafficgen\") 首先, 声明一个对象, 最后就返回它. local TrafficGen = {} new方法:这个方法太牛了! local known_ports = cavium.trafficgen.get_port_names() get_port_names()实际上是C用链表实现的迭代器, 初始化时候构造链表, 用的时候遍历每个元素到lua栈里面, 从而传递给lua. static int get_port_names(lua_State* L) { if (!tg_port_head) tg_init(); int count = 0; lua_newtable(L); for (tg_port_t *tg_port = tg_port_head; tg_port!=NULL; tg_port = tg_port->next) { lua_pushinteger(L, ++count); lua_pushstring(L, bdk_if_name(tg_port->handle)); lua_settable(L, -3); } return 1; } 因为迭代器的存在, 底层C实现也就只需要实现一个迭代的bdk_if_next_port()函数接口 fat fs over spi 在libbdk-os/bdk-fs-fatfs.c里面, 实现了fat fs的封装fatfs_open, fatfs_read等函数是给bdk的\"简单文件系统\"用的. 其实现是使用libfatfs/ff.c库, 调用里面提供的f_read()那f_read怎么知道怎么操作SPI呢? 实际上, fat fs可分为两层, 上层是文件系统, 下层是IO操作. 上层的东西一般是不变的, 变的就是IO层. IO层的驱动是disk_read, disk_write他们利用了设备文件的读和写 total = fwrite(buff, DRV_SECTOR_SIZE(pdrv) * count, 1, DRV_FP(pdrv)); 所以, 只要实现了设备文件的驱动, 就能把这个驱动代到IO层. 在fatfs_diskio_init()里面,libfatfs/diskio.c disk_initialize()实现了其中的联系: DRV_FP(pdrv) = fopen(DRV_DEVSTR(pdrv), \"r+b\"); 编译 export PATH=${PATH}:${BDK_ROOT}/bin:/home/byj/repo/git/thunder/new/sdk-master/tools/bin make 片上ROM thunder片上有rom, 地址是0x87D000000000, 16K, 上电从这里启动rom根据GPIO的配置决定从SPI或eMMC load 192K的代码到L2 cache. non-trusted启动时, code在spi/eMMC的0x20000 trusted启动时, code在spi/eMMC的0x50000. 代码被读到L2的scratch region, 地址是0x1000000, 然后跳转到0x1000100 BDK头 一共256字节 # BDK image heade for Thunder is # Offset Size Description # 0x00 4 Raw instruction for skipping header (above) # 0x04 4 Length of the image, includes header # 0x08 8 Magic string \"THUNDERX\" # 0x10 4 CRC32 of image + header. These bytes are zero when calculating the CRC # 0x14 4 Zero, reserved for future use # 0x18 64 ASCII Image name. Must always end in zero # 0x58 32 ASCII Version. Must always end in zero # 0x78 136 Zero, reserved for future use # 0x100 - Beginning of image. Header is always 256 bytes. debug libbdk-os/bdk-start.S必要时可以打开BDK_GPIO_BOOT_DEBUG, 这个是个GPIO的脚, 每个boot阶段会输出不同频率的波形. libbdk-arch/bdk-warn.hBDK_TRACE_ENABLE_INIT bdk的image构成 stage1就是boot-stub.bin stage2就是diagnostics.bin 启动 libbdk-os/bdk-start.S 拷贝代码到链接地址0, 把栈设在__bdk_initial_stack_end, 代码里面的.align 12意思是4K对齐. 这个时候有栈了, 下面是C代码 __bdk_init 为EL1 2 3设置异常基地址 把内存属性设为cached normal memory, no mmu 设置时钟 初始化两个串口 //为什么这里能直接调write? --在libbdk-os/bdk-fs.c中, 静态链接了_write(), _open()等函数. //这里面有静态变量file_handle, 里面的什么fs-op都填好了, 但1 2 3都是虚的/console //里面的console_write调的是最后一个打开的文件fd, 默认是3, //再往下是dev_write, 这里根据dev来进一步调用dev->ops->write //最后是libbdk-os/bdk-fs-uart.c里面的uart_write, 轮询方式 write(1, banner...) 锁L2 校验CRC bdk_thread_initialize() 用malloc申请16K的空间, 给栈用 --哪里给的malloc的初始空间呢? //libc/newlib-2.1.0/newlib/libc/stdlib/malloc.c 用一个新的thread来跑下面的 __bdk_init_main, 所有core都会跑这里 释放在bdk-start.S设置的栈 每个core都设置必须的sysreg __bdk_config_init, 设置一些config值, 比如LMAC 探测多node的配置, 估计是哪里配的 __bdk_init_node() bdk_clock_setup, 时钟 bdk_l2c_initialize, cache bdk_rng_enable, 随机数 bdk_pcie_global_initialize bdk_qlm_init bdk_twsix_initialize bdk_mdio_initialize __bdk_fs_init_late if (__bdk_fs_mem_init) result |= __bdk_fs_mem_init(); if (__bdk_fs_mmc_init) result |= __bdk_fs_mmc_init(); if (__bdk_fs_mpi_init) result |= __bdk_fs_mpi_init(); if (__bdk_fs_pcie_init) result |= __bdk_fs_pcie_init(); if (__bdk_fs_ram_init) result |= __bdk_fs_ram_init(); if (__bdk_fs_rom_init) result |= __bdk_fs_rom_init(); if (__bdk_fs_sata_init) result |= __bdk_fs_sata_init(); if (__bdk_fs_xmodem_init) result |= __bdk_fs_xmodem_init(); if (__bdk_fs_fatfs_init) result |= __bdk_fs_fatfs_init(); 新起线程main 加链接时依赖 main, 每个板子都有自己的main, 比如normal-boot-crb-2s/boot-stub.c 是能watchdog 把第10个gpio拉高 初始化和BMC连的twsi为slave模式 boot大于3次了, 请求BMC的power cycle update_bmc_status(BMC_STATUS_REQUEST_POWER_CYCLE) 从gpio_strap寄存器读启动模式, spi, emmc, ccpi还是pcie 打印\"BDK version\"等 初始化CCPI 初始化主node的dram libdram_config() 初始化node 1 的dram libdram_config() 设QLM参考时钟 QLM初始化 BGX和PHY初始化? SATA PCIE 为了从spi load代码, 初始化mpi bdk_fs_mpi_init() 按D进BDK, 否则进ATF boot_image() 重构后的main, normal-boot/boot-stub.c main 按了x就用/fatfs/safe-mode.cfg 否则是/fatfs/default.cfg 应该是把这个文件load到内存, 以便后面的api bdk_brd_cfg_get_int()使用 bdk_loadenv(\"/fatfs/xxx.cfg\") 看门狗 boot_read_config() MULTI_NODE = bdk_brd_cfg_get_int(MULTI_NODE, BDK_BRD_CFG_MULTI_NODE); BMC_TWSI = bdk_brd_cfg_get_int(BMC_TWSI, BDK_BRD_CFG_BMC_TWSI); DRAM_VERBOSE = bdk_brd_cfg_get_int(DRAM_VERBOSE, BDK_BRD_CFG_DRAM_VERBOSE); WATCHDOG_TIMEOUT = bdk_brd_cfg_get_int(WATCHDOG_TIMEOUT, BDK_BRD_CFG_WATCHDOG_TIMEOUT); DRAM_NODE0 = bdk_brd_cfg_get_str(DRAM_NODE0, BDK_BRD_CFG_DRAM_NODE, 0); DRAM_NODE1 = bdk_brd_cfg_get_str(DRAM_NODE1, BDK_BRD_CFG_DRAM_NODE, 1); BRD_DISABLE_TWSI = bdk_brd_cfg_get_int(BRD_DISABLE_TWSI, BDK_BRD_CFG_DISABLE_TWSI); BRD_DISABLE_DRAM = bdk_brd_cfg_get_int(BRD_DISABLE_DRAM, BDK_BRD_CFG_DISABLE_DRAM); BRD_DISABLE_CCPI = bdk_brd_cfg_get_int(BRD_DISABLE_CCPI, BDK_BRD_CFG_DISABLE_CCPI); BRD_DISABLE_QLM = bdk_brd_cfg_get_int(BRD_DISABLE_QLM, BDK_BRD_CFG_DISABLE_QLM); BRD_DISABLE_BGX = bdk_brd_cfg_get_int(BRD_DISABLE_BGX, BDK_BRD_CFG_DISABLE_BGX); BRD_DISABLE_USB = bdk_brd_cfg_get_int(BRD_DISABLE_USB, BDK_BRD_CFG_DISABLE_USB); BRD_DISABLE_PCI = bdk_brd_cfg_get_int(BRD_DISABLE_PCI, BDK_BRD_CFG_DISABLE_PCI); board_init_early(), 每个板子都有, 比如2s GPIO控制USB的电源 boot_init_twsi() boot_init_ccpi_link() boot_init_dram(node0) boot_init_ccpi_node() boot_init_dram(node1) boot_init_qlm_clk() boot_init_qlm_mode() boot_init_bgx() boot_init_usb() boot_init_pci() board_init_late() 2s的依赖 void __bdk_require_depends(void) { BDK_REQUIRE(QLM); BDK_REQUIRE(MDIO); BDK_REQUIRE(PCIE); BDK_REQUIRE(GPIO); BDK_REQUIRE(RNG); BDK_REQUIRE(KEY_MEMORY); BDK_REQUIRE(MPI); BDK_REQUIRE(DRAM_CONFIG); BDK_REQUIRE(TWSI); BDK_REQUIRE(USB); } 内存 在normal-boot-crb-2s/boot-stub.c 打开DRAM_VERBOSE DRAM_NODE0是具体的板子, 比如crb_2s_V3, 在libdram/configs/config-crb-2s-V3.c bdk有三种 基本上都是一个main加上libbdk.a chain, normal, diag $(CHAINLOADER): $(BOARD_ROOT)/chainloader.o $(BOARD_ROOT)/boot-common.o $(BOARD_OBJS) $(BDK_ROOT)/libbdk/libbdk.a $(DIAGNOSTICS): $(BOARD_ROOT)/diagnostics.o $(BOARD_OBJS) $(BDK_ROOT)/libbdk/libbdk.a $(BOOT_STUB): $(BOARD_ROOT)/boot-stub.o $(BOARD_ROOT)/boot-common.o $(BOARD_OBJS) $(BDK_ROOT)/libbdk/libbdk.a diagnostic bdk-boot/diagnostics.c main() bdk_lua_start() bdk_lua_init 依赖 void __bdk_require_depends(void) { BDK_REQUIRE(QLM); BDK_REQUIRE(PCIE); BDK_REQUIRE(FS_PCIE); BDK_REQUIRE(GPIO); BDK_REQUIRE(RNG); BDK_REQUIRE(KEY_MEMORY); BDK_REQUIRE(MDIO); BDK_REQUIRE(MPI); BDK_REQUIRE(DRAM_CONFIG); BDK_REQUIRE(DRAM_TEST); BDK_REQUIRE(ENVIRONMENT); BDK_REQUIRE(FS_XMODEM); BDK_REQUIRE(FS_RAM); BDK_REQUIRE(FS_SATA); BDK_REQUIRE(CSR_DB); BDK_REQUIRE(POWER_BURN); BDK_REQUIRE(TRAFFIC_GEN); BDK_REQUIRE(ERROR_DECODE); BDK_REQUIRE(TWSI); BDK_REQUIRE(SATA); BDK_REQUIRE(USB); BDK_REQUIRE(TNS); } csr访问, 在lua环境下 bdk对csr的抽象实在太牛了, mark! libbdk-lua/cavium-csr.c register_cavium_csr() create_csr_table() 这是个对象比如这个访问方式: cavium.csr.MIO_TWSX_SW_TWSI(1).data = 0x80 实际调用的是 cavium_csr_field_newindex() 这是用table的元方法__index和__newindex实现的其他可用的访问方式是: cavium.csr.NAME.read() cavium.csr.NAME.write(value) cavium.csr.NAME.display(optional) cavium.csr.NAME.decode(optional) cavium.csr.NAME.encode(table) cavium.csr.NAME.FIELD 是调用cavium_csr_field_index cavium.csr.NAME.FIELD=是调用cavium_csr_field_newindex cavium.csr.NAME返回一个table, 但直接这么写会返回错误 cavium.csr返回一个迭代变量 终极文件lua-modules/csr_db.lua 这里补充一下这个db怎么生成的. ./csr-tool/csr-tool.py-23-OUTPUT_FILENAME_TYPEDEFS = \"output/bdk-csrs.h\" ./csr-tool/csr-tool.py:24:OUTPUT_FILENAME_DB = \"output/bdk-csrs.c\" ./csr-tool/csr-tool.py-25-OUTPUT_FILENAME_LUA = \"output/bdk-csrs.lua\" ./csr-tool/Makefile-23- mv output/bdk-csrs*.h ../libbdk-arch/ ./csr-tool/Makefile:24: mv output/bdk-csrs.c ../libbdk-arch/ ./csr-tool/Makefile-25- mv output/bdk-csrs.lua ../lua-modules/csr_db.lua lua pprint(cavium) pprint(_G) for name in cavium.csr() do local s=string.match(name, \".*GPIO.*\"); if s then print(s) end end cavium.csr.GPIO_BIT_CFGX(28).display() cavium.csr.GPIO_BIT_CFGX(28).TX_OD=1 cavium.c.bdk_mdio_45_read(0,1,0,0,0) should be 997 : 在 lua 里只是一个语法糖。 函数调用的时候 obj:func(arg) 其实等价于 obj.func(obj,arg) 函数定义时 function obj:func(arg) 等价于 function obj.func(self,arg) "},"notes/CPU_ARM64_thunder_efi_rtc.html":{"url":"notes/CPU_ARM64_thunder_efi_rtc.html","title":"thunder RTC时间和efi","keywords":"","body":" 现象 从rtc-efi.ko说开去 从UEFI看起 kernel调用efi的service机制是什么? 现象 很简单, RTC时间不对, 每次重启就回到1970年. 从rtc-efi.ko说开去 还记得最开始fedora启动的时候, 就是这个ko老打印错误, 当时只是简单的删掉它了, 没有多想.但现在回过头来再看RTC问题的时候, 才发现这个ko就是key. 这个ko的代码很简单, 在drivers/rtc/rtc-efi.c两个重要的函数, efi_read_time()和efi_set_time() 既然时间不对, 我们就先来看efi_read_time() static int efi_read_time(struct device *dev, struct rtc_time *tm) { efi_status_t status; efi_time_t eft; efi_time_cap_t cap; status = efi.get_time(&eft, &cap); if (status != EFI_SUCCESS) { /* should never happen */ dev_err(dev, \"can't read time\\n\"); return -EINVAL; } if (!convert_from_efi_time(&eft, tm)) return -EIO; return rtc_valid_tm(tm); } 这里面有个efi.get_time(&eft, &cap), 应该就是获取时间了, 而正如这个ko的名字rtc-efi所暗示的, 它不是直接调用linux驱动来读时间, 而是调efi的run time service. 这个efi.get_time在哪呢? include/linux/efi.h里面说的很清楚, efi所有的runtime的东东都在这个结构体里面 /* * All runtime access to EFI goes through this structure: */ extern struct efi { efi_system_table_t *systab; /* EFI system table */ unsigned int runtime_version; /* Runtime services version */ unsigned long mps; /* MPS table */ unsigned long acpi; /* ACPI table (IA64 ext 0.71) */ unsigned long acpi20; /* ACPI table (ACPI 2.0) */ unsigned long smbios; /* SMBIOS table (32 bit entry point) */ unsigned long smbios3; /* SMBIOS table (64 bit entry point) */ unsigned long sal_systab; /* SAL system table */ unsigned long boot_info; /* boot info table */ unsigned long hcdp; /* HCDP table */ unsigned long uga; /* UGA table */ unsigned long uv_systab; /* UV system table */ unsigned long fw_vendor; /* fw_vendor */ unsigned long runtime; /* runtime table */ unsigned long config_table; /* config tables */ unsigned long esrt; /* ESRT table */ efi_get_time_t *get_time; efi_set_time_t *set_time; efi_get_wakeup_time_t *get_wakeup_time; efi_set_wakeup_time_t *set_wakeup_time; efi_get_variable_t *get_variable; efi_get_next_variable_t *get_next_variable; efi_set_variable_t *set_variable; efi_set_variable_nonblocking_t *set_variable_nonblocking; efi_query_variable_info_t *query_variable_info; efi_update_capsule_t *update_capsule; efi_query_capsule_caps_t *query_capsule_caps; efi_get_next_high_mono_count_t *get_next_high_mono_count; efi_reset_system_t *reset_system; efi_set_virtual_address_map_t *set_virtual_address_map; struct efi_memory_map *memmap; unsigned long flags; } efi; 从UEFI看起 实际操作硬件的地方在uefi, 在ArmPlatformPkg/ThunderPkg/Drivers/Ds1337RtcDxe/Ds1337RtcDxe.c 在Ds1337RtcDxeInitialize()这个初始化函数中, 就挂了几个函数 EFI_STATUS EFIAPI Ds1337RtcDxeInitialize ( IN EFI_HANDLE ImageHandle, IN EFI_SYSTEM_TABLE *SystemTable ) { ... SystemTable->RuntimeServices->GetTime = GetTime; SystemTable->RuntimeServices->SetTime = SetTime; SystemTable->RuntimeServices->GetWakeupTime = GetWakeupTime; SystemTable->RuntimeServices->SetWakeupTime = SetWakeupTime; } 真正去读RTC的是里面的GetTime函数, 这个函数调用ThunderxTwsiRead8Bytes()去最终读取RTC时间. kernel调用efi的service机制是什么? 我猜这里面的关键就应该是这个SystemTable了, 但还有些需要考虑的, 比如uefi传递这个指针, 是虚拟地址吗? linux应该知道这个映射吗? 或者直接使用物理地址? 还是先从kernel的调用的地方看起, 回到前面的efi_read_time() 这个函数调用了 status = efi.get_time(&eft, &cap) 这里面的efi看起来是一个全局的结构体变量, 而get_time是哪个? 在drivers/firmware/efi/runtime-wrappers.c中, 它被初始化为virt_efi_get_time void efi_native_runtime_setup(void) { efi.get_time = virt_efi_get_time; efi.set_time = virt_efi_set_time; efi.get_wakeup_time = virt_efi_get_wakeup_time; efi.set_wakeup_time = virt_efi_set_wakeup_time; efi.get_variable = virt_efi_get_variable; efi.get_next_variable = virt_efi_get_next_variable; efi.set_variable = virt_efi_set_variable; efi.set_variable_nonblocking = virt_efi_set_variable_nonblocking; efi.get_next_high_mono_count = virt_efi_get_next_high_mono_count; efi.reset_system = virt_efi_reset_system; efi.query_variable_info = virt_efi_query_variable_info; efi.update_capsule = virt_efi_update_capsule; efi.query_capsule_caps = virt_efi_query_capsule_caps; } 最后调用status = efi_call_virt(get_time, tm, tc); 这里面的efi_call_virt是个宏, 在arch/arm64/include/asm/efi.h中定义 #define efi_call_virt(f, ...) \\ ({ \\ efi_##f##_t *__f; \\ efi_status_t __s; \\ \\ kernel_neon_begin(); \\ efi_virtmap_load(); \\ __f = efi.systab->runtime->f; \\ __s = __f(__VA_ARGS__); \\ efi_virtmap_unload(); \\ kernel_neon_end(); \\ __s; \\ }) 我们能够得出一下几点: 要想调用efi的函数, 必须先load efi的内存映射; 此时使用efi_mm, arch/arm64/kernel/efi.c 实际调用的是efi.systab->runtime->f, 很显然, 这个f的地址应该是uefi填的, 是在uefi内存映射下的地址 看起来linux调用efi的service并没有改变CPU的运行级别 neon是适用于ARM Cortex-A系列处理器的一种128位SIMD(Single Instruction, Multiple Data,单指令、多数据)扩展结构 "},"notes/CPU_ARM64_thunder_uefi_fdt.html":{"url":"notes/CPU_ARM64_thunder_uefi_fdt.html","title":"thunder uefi和fdt","keywords":"","body":" 默认的grub 关于MAC地址 内核里面的UEFI 启动顺序 makefile目标 acpi table Thunder_${PLATFORM}.dsc ACPI 关于FDT ArmPlatformPkg/ThunderPkg/Dts/thunder-88xx.dts ArmPlatformPkg/ThunderPkg/Dts/thunder-88xx.dtsi soc 中断 soc 两个串口 pcie MDIO 默认的grub menuentry 'Thunder Boot' { linux /boot/Image root=/dev/sda2 console=ttyAMA0,115200n8 earlycon=pl011,0x87e024000000 debug coherent_pool=16M rootwait rw uefi_debug boot } 关于MAC地址 setvar N1ETH0 -guid A70B59ED-6228-4883-BBF0-5FD91C14EFF6 -bs -rt -nv =0x123456789abc01 这里的guid是 ./ArmPlatformPkg/ThunderPkg/Thunder_cn88xx.dec:34: gThunderPlatformTokenSpaceGuid = { 0xa70b59ed, 0x6228, 0x4883, { 0xbb, 0xf0, 0x5f, 0xd9, 0x1c, 0x14, 0xef, 0xf6 } } 内核里面的UEFI 在内核里面打开了EFI=y内核支持UEFI的runtime service, 并且打开了uefi_stub, 使内核能够作为一个UEFI app来启动. 这里还打开了一个EFI_VARS的功能, 好像是能够通过/sys文件系统修改UEFI的变量 启动顺序 makefile目标 DEBUG ?= 0 PLATFORM=cn88xx 目标 build -a AARCH64 -t ARMGCC -p ArmPlatformPkg/ThunderPkg/Thunder_${PLATFORM}.dsc -b ${TARGET} cp ./Build/Thunder_${PLATFORM}/${TARGET}_ARMGCC/FV/THUNDER_EFI.fd ${WORKSPACE}/THUNDER_EFI.fd shell目标 build -a AARCH64 -t ARMGCC -p ShellPkg/ShellPkg.dsc -b ${TARGET} acpi table 每个板子都有 ArmPlatformPkg/ThunderPkg/AcpiTables/Crb1s.asl ArmPlatformPkg/ThunderPkg/AcpiTables/Crb2s.asl Thunder_${PLATFORM}.dsc platform也有GUID common的模块 [LibraryClasses.common] ArmLib|ArmPkg/Library/ArmLib/AArch64/AArch64Lib.inf ArmCpuLib|ArmPkg/Drivers/ArmCpuLib/ArmCortexAEMv8Lib/ArmCortexAEMv8Lib.inf ArmPlatformLib|ArmPlatformPkg/ThunderPkg/Library/ThunderLib/ThunderLib.inf ThunderxConfigLib|ArmPlatformPkg/ThunderPkg/Library/ThunderxConfigLib/ThunderxConfigLib.inf ThunderxFdtLib|ArmPlatformPkg/ThunderPkg/Library/ThunderxFdtLib/ThunderxFdtLib.inf EfiResetSystemLib|ArmPkg/Library/ArmPsciResetSystemLib/ArmPsciResetSystemLib.inf ThunderxSmcLib|ArmPlatformPkg/ThunderPkg/Library/ThunderxSmcLib/ThunderxSmcLib.inf ThunderxTwsiLib|ArmPlatformPkg/ThunderPkg/Library/ThunderxTwsiLib/ThunderxTwsiLib.inf ThunderxWdogLib|ArmPlatformPkg/ThunderPkg/Library/ThunderxWdogLib/ThunderxWdogLib.inf ArmPlatformSysConfigLib|ArmPlatformPkg/ThunderPkg/Library/ThunderSysConfigLib/ThunderSysConfigLib.inf TimerLib|ArmPkg/Library/ArmArchTimerLib/ArmArchTimerLib.inf PciExpressLib|ArmPlatformPkg/ThunderPkg/Library/BasePciExpressLib/BasePciExpressLib.inf PciLib|MdePkg/Library/BasePciLibPciExpress/BasePciLibPciExpress.inf GIC gArmTokenSpaceGuid.PcdSystemMemoryBase|0x01400000 gArmTokenSpaceGuid.PcdSystemMemoryBase_Node2|0x10000400000 # # ARM General Interrupt Controller # gArmTokenSpaceGuid.PcdGicDistributorBase|0x801000000000 gArmTokenSpaceGuid.PcdGicInterruptInterfaceBase|0x801000002000 PL011就是arm体系下的串口 serial@101f0000 { compatible = \"arm,pl011\"; reg = ; interrupts = ; }; gEfiMdeModulePkgTokenSpaceGuid.PcdSerialRegisterBase|0x87e024000000 --从手册可以查到, 这个地址就是串口寄存器的地址, 看起来, thunder的串口是按照arm标准设计的. 为什么有的寄存器的地址非常小???? ## PL180 MMC/SD card controller gArmPlatformTokenSpaceGuid.PcdPL180SysMciRegAddress|0x1C010048 gArmPlatformTokenSpaceGuid.PcdPL180MciBaseAddress|0x1C050000 默认的grub启动路径, 固定的pci, 固定的uuid分区 # # GRUB Loader from SATA DISK from EFI partition. # gArmPlatformTokenSpaceGuid.PcdDefaultBootDescription|L\"GRUB\" gArmPlatformTokenSpaceGuid.PcdDefaultBootDevicePath|L\"PciRoot(0x1)/Pci(0x8,0x0)/Sata(0x0,0x0,0x0)/HD(1,GPT,E3AE6975-BACE-464E-91E1-BB4FE9954047,0x800,0x31801)/\\EFI\\BOOT\\BOOTAA64.EFI\" gArmPlatformTokenSpaceGuid.PcdDefaultBootType|0 看起来变量是保存在flash上的 #FTW Non-volatile flash storage gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageVariableBase|0xF00000 gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageVariableBase64|0xF00000 gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageVariableSize|0x9000 gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageFtwWorkingBase|0xF09000 gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageFtwWorkingBase64|0xF09000 gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageFtwWorkingSize|0x1000 gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageFtwSpareBase|0xF0A000 gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageFtwSpareBase64|0xF0A000 gEfiMdeModulePkgTokenSpaceGuid.PcdFlashNvStorageFtwSpareSize|0x0000A000 每个inf描述被包含的一个模块??? # # ACPI Support # MdeModulePkg/Universal/Acpi/AcpiTableDxe/AcpiTableDxe.inf ArmPlatformPkg/ThunderPkg/AcpiPlatformDxe/AcpiPlatformDxe.inf ArmPlatformPkg/ThunderPkg/AcpiTables/AcpiTables.inf # # FDT support # ArmPlatformPkg/ThunderPkg/FdtPlatformDxe/FdtPlatformDxe.inf ArmPlatformPkg/ThunderPkg/MdeModulePkg/Universal/Variable/RuntimeDxe/VariableRuntimeDxe.inf ArmPlatformPkg/ThunderPkg/MdeModulePkg/Universal/FaultTolerantWriteDxe/FaultTolerantWriteDxe.inf # # FAT filesystem + GPT/MBR partitioning # MdeModulePkg/Universal/Disk/DiskIoDxe/DiskIoDxe.inf MdeModulePkg/Universal/Disk/PartitionDxe/PartitionDxe.inf MdeModulePkg/Universal/Disk/UnicodeCollation/EnglishDxe/EnglishDxe.inf 最后这三个有意思 #ASIX Electronics Corp. AX88772B ETH to USB doungle supprt OptionRomPkg/Bus/Usb/UsbNetworking/Ax88772b/Ax88772b.inf #SMBIOS MdeModulePkg/Universal/SmbiosDxe/SmbiosDxe.inf ArmPlatformPkg/ThunderPkg/Drivers/PlatformSmbiosDxe/PlatformSmbiosDxe.inf { SmbiosLib|ArmPlatformPkg/ThunderPkg/Library/SmbiosLib/SmbiosLib.inf } #RTC ArmPlatformPkg/ThunderPkg/Drivers/Ds1337RtcDxe/Ds1337RtcDxe.inf 还包含了FDT的blob, 里面貌似也没什么特别的, 似乎是个common的 #To include FDT blob in UEFI image MdeModulePkg/Universal/Fdt/FdtTableDxe/FdtTableDxe.inf ACPI 看了半天FDT, 这个ACPI到底是搞什么飞机?????? PopulateMacAddressCrb2S() 似乎内核配置里面所有的ACPI都是n 注: 因为关闭了CONFIG_EXPERT 关于FDT 前面说了, ArmPlatformPkg/ThunderPkg/FdtPlatformDxe/FdtPlatformDxe.inf是FDT的入口 这是一个DXE_DRIVER, 入口是FdtPlatformEntryPoint 在FdtPlatform.c中 /** Entrypoint of Fdt Platform driver. **/ EFI_STATUS EFIAPI FdtPlatformEntryPoint ( IN EFI_HANDLE ImageHandle, IN EFI_SYSTEM_TABLE *SystemTable ) { BOARD_CFG* HwConfig; HwConfig = ThunderXGetBoardConfig(); //这里貌似主要是搞BGX ThunderxPatchFdt(HwConfig); //这里需要重点关注 主要是搞bgx mac地址是从一个变量读出来的 UnicodeSPrint(mac_variable_name, 9 * sizeof(CHAR16), L\"N%dETH%d\", it->node, it->id) //N0ETH0 err = gRT->GetVariable(mac_variable_name, &gThunderPlatformTokenSpaceGuid, NULL, &mac_size, mac_address); 然后写到FDT里面 err = fdt_appendprop(device_tree_base, fdt_node, \"local-mac-address\", mac_address_char, 6) return EFI_SUCCESS; } 这里的ThunderxBoardType需要适配 ArmPlatformPkg/ThunderPkg/Dts/thunder-88xx.dts /dts-v1/; /include/ \"thunder-88xx.dtsi\" / { model = \"Cavium ThunderX CN88XX board\"; compatible = \"cavium,thunder-88xx\"; aliases { serial0 = &uaa0; serial1 = &uaa1; }; memory@00000000 { device_type = \"memory\"; reg = ; }; memory@10000000000 { device_type = \"memory\"; reg = ; }; }; ArmPlatformPkg/ThunderPkg/Dts/thunder-88xx.dtsi compatible = \"cavium,thunder-88xx\"; interrupt-parent = ; psci { compatible = \"arm,psci-0.2\"; method = \"smc\"; }; CPU0: cpu@000 { device_type = \"cpu\"; compatible = \"cavium,thunder\", \"arm,armv8\"; reg = ; enable-method = \"psci\"; }; timer { compatible = \"arm,armv8-timer\"; interrupts = , , , ; }; pmu { compatible = \"arm,armv8-pmuv3\"; interrupts = ; }; soc 中断 gic0: interrupt-controller@8010,00000000 { compatible = \"arm,gic-v3\"; #interrupt-cells = ; #address-cells = ; #size-cells = ; #redistributor-regions = ; ranges; interrupt-controller; reg = , /* GICD */ , /* GICR Node 0 */ ; /* GICR Node 1 */ interrupts = ; its: gic-its@8010,00020000 { compatible = \"arm,gic-v3-its\"; msi-controller; reg = ; }; its1: gic-its@9010,00020000 { compatible = \"arm,gic-v3-its\"; msi-controller; reg = ; }; }; soc 两个串口 uaa0: serial@87e0,24000000 { compatible = \"arm,pl011\", \"arm,primecell\"; reg = ; interrupts = ; clocks = ; clock-names = \"apb_pclk\"; }; uaa1: serial@87e0,25000000 { compatible = \"arm,pl011\", \"arm,primecell\"; reg = ; interrupts = ; clocks = ; clock-names = \"apb_pclk\"; pcie 下面的不属于soc, 是独立的 一个node有4个pcie(ECAM), 对应的有4个SMMU pcie0: pcie0@0x8480,00000000 { compatible = \"cavium,thunder-pcie\"; device_type = \"pci\"; msi-parent = ; bus-range = ; #size-cells = ; #address-cells = ; #stream-id-cells = ; reg = ; /* Configuration space */ ranges = , /* mem ranges */ ; }; pcie7: pcie7@0x94b0,00000000 { compatible = \"cavium,thunder-pcie\"; device_type = \"pci\"; msi-parent = ; bus-range = ; #size-cells = ; #address-cells = ; #stream-id-cells = ; reg = ; /* Configuration space */ ranges = , /* mem ranges */ ; }; smmu0@0x8300,00000000 { compatible = \"arm,smmu-v2\"; reg = ; #global-interrupts = ; interrupts = , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ; mmu-masters = ; thunderx,smmu-64-bit-writes-only; }; smmu7@0x9330,00000000 { compatible = \"arm,smmu-v2\"; reg = ; #global-interrupts = ; interrupts = , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ; mmu-masters = ; thunderx,smmu-64-bit-writes-only; }; MDIO 首先列出所有支持的qlm模式下的phy, 同样是2个node, 每个node有两个bgx 那么就有00 10 20 30表示4个bgxsgmii00 sgmii01 sgmii02 sgmii03: marvell,88e1240 xfi00 xfi01 xfi02 xfi03: xfi00: xfi00 { qlm-mode = \"0,xfi\",\"0,xfi-10g-kr\"; reg = ; --这里的reg要不要改???? 这个应该就是mdio的地址 compatible = \"cortina,cs4223-slice\"; }; xlaui00: cortina,cs4223-slice xaui00: broadcom,bcm8706 rxaui00 rxaui01: marvell,88x3120 下面就是bgx, 一共4个, 指定phy-handle, 很全, 没什么好改的. FDT就这么多了. "},"notes/CPU_ARM64_thunder_atf.html":{"url":"notes/CPU_ARM64_thunder_atf.html","title":"thunder atf","keywords":"","body":" thunder的main, 不知道被哪里调用 atf能控制kernel看到那些pci设备? 有对板类型的判断 一些默认值 bl1 bl2 bl31 thunder的main, 不知道被哪里调用 atf/plat/thunder/bootstrap/main.c 这里面有些初始化 flash_init() 下面是管SMMU的, Initialize thunder io view of non secure software init_thunder_io() atf能控制kernel看到那些pci设备? struct ecam_device{ int ecam_id; int bus; int dev; int fun; int ns_visible; int nxt_fn; ecam_probe probe_fn; unsigned long probe_arg; }; struct ecam_device devs0[] = { {0, 0, 1, 0, TRUE, -1, NULL, 0}, /* PCCBR_MRML */ {0, 0, 2, 0, FALSE, -1, NULL, 0}, /* SMMU 0 */ {0, 0, 3, 0, FALSE, -1, NULL, 0}, /* GIC */ {0, 0, 4, 0, FALSE, -1, NULL, 0}, /* GTI */ {0, 0, 6, 0, TRUE, -1, NULL, 0}, /* GPIO */ {0, 0, 7, 0, FALSE, -1, NULL, 0}, /* MPI */ {0, 0, 8, 0, FALSE, -1, NULL, 0}, /* MIO_PTP */ {0, 0, 9, 0, FALSE, -1, NULL, 0}, /* RNM */ {0, 0, 16, 0, TRUE, -1, NULL, 0}, /* USB 0 */ {0, 0, 17, 0, TRUE, -1, NULL, 0}, /* USB 1 */ {0, 1, 0, 0, TRUE, 0, ecam_probe_true, 0}, /* MRML */ {0, 1, 0, 1, TRUE, 0, ecam_probe_true, 0}, /* RST */ {0, 1, 0, 5, TRUE, 0, ecam_probe_true, 0}, /* OCX */ {0, 1, 0, 12, TRUE, 0, ecam_probe_true, 0}, /* MIO_EMM */ {0, 1, 0, 64, FALSE, -1, ecam_probe_false, 0}, /* UAA 0 */ {0, 1, 0, 65, FALSE, -1, ecam_probe_false, 0}, /* UAA 1 */ {0, 1 , 0, 72, TRUE, 0, ecam_probe_true, 0}, /* TWSI 0 */ {0, 1 , 0, 73, TRUE, 0, ecam_probe_true, 0}, /* TWSI 1 */ {0, 1 , 0, 74, TRUE, 0, ecam_probe_true, 0}, /* TWSI 2 */ {0, 1 , 0, 75, TRUE, 0, ecam_probe_true, 0}, /* TWSI 3 */ {0, 1 , 0, 76, TRUE, 0, ecam_probe_true, 0}, /* TWSI 4 */ {0, 1 , 0, 77, TRUE, 0, ecam_probe_true, 0}, /* TWSI 5 */ {0, 1 , 0, 80, TRUE, 0, ecam_probe_true, 0}, /* LMC 0 */ {0, 1 , 0, 81, TRUE, 0, ecam_probe_true, 0}, /* LMC 1 */ {0, 1 , 0, 82, TRUE, 0, ecam_probe_true, 0}, /* LMC 2 */ {0, 1 , 0, 83, TRUE, 0, ecam_probe_true, 0}, /* LMC 3 */ {0, 1 , 0, 112, TRUE, 0, ecam_probe_pem, 0}, /* PEM 0 */ {0, 1 , 0, 113, TRUE, 0, ecam_probe_pem, 1}, /* PEM 1 */ {0, 1 , 0, 114, TRUE, 0, ecam_probe_pem, 2}, /* PEM 2 */ {0, 1 , 0, 115, TRUE, 0, ecam_probe_pem, 3}, /* PEM 3 */ {0, 1 , 0, 116, TRUE, 0, ecam_probe_pem, 4}, /* PEM 4 */ {0, 1 , 0, 117, TRUE, 0, ecam_probe_pem, 5}, /* PEM 5 */ {0, 1, 0, 128, TRUE, 0, ecam_probe_bgx, 0}, /* BGX 0 */ {0, 1, 0, 129, TRUE, 0, ecam_probe_bgx, 1}, /* BGX 1 */ {0, 2, 0, 0, TRUE, -1, NULL, 0}, /* RAD */ {0, 3, 0, 0, TRUE, -1, NULL, 0}, /* ZIP */ {0, 4, 0, 0, TRUE, -1, NULL, 0}, /* HFA */ {-1, 0, 0, 0, 0, -1, NULL, 0}, }; struct ecam_device devs1[] = { {1, 0, 1, 0, FALSE, -1, NULL, 0}, /* SMMU 1 */ {1, 0, 4, 0, TRUE, -1, ecam_probe_sata, 2}, /* SATA 0 */ {1, 0, 5, 0, TRUE, -1, ecam_probe_sata, 2}, /* SATA 1 */ {1, 0, 6, 0, TRUE, -1, ecam_probe_sata, 2}, /* SATA 2 */ {1, 0, 7, 0, TRUE, -1, ecam_probe_sata, 2}, /* SATA 3 */ {1, 0, 8, 0, TRUE, -1, ecam_probe_sata, 3}, /* SATA 4 */ {1, 0, 9, 0, TRUE, -1, ecam_probe_sata, 3}, /* SATA 5 */ {1, 0, 10, 0, TRUE, -1, ecam_probe_sata, 3}, /* SATA 6 */ {1, 0, 11, 0, TRUE, -1, ecam_probe_sata, 3}, /* SATA 7 */ {-1, 0, 0, 0, 0, -1, NULL, 0}, }; struct ecam_device devs2[] = { {2, 0, 1, 0, FALSE, -1, NULL, 0}, /* SMMU 2 */ {2, 0, 2, 0, TRUE, -1, NULL, 0}, /* PCCBR_NIC */ {2, 0, 3, 0, TRUE, -1, NULL, 0}, /* TNS */ {2, 1, 0, 0, TRUE, -1, NULL, 0}, /* NIC */ {-1, 0, 0, 0, 0, -1, NULL, 0}, }; struct ecam_device devs3[] = { {3, 0, 1, 0, FALSE, -1, NULL, 0}, /* SMMU 3 */ {3, 0, 4, 0, TRUE, -1, ecam_probe_sata, 6}, /* SATA 8 */ {3, 0, 5, 0, TRUE, -1, ecam_probe_sata, 6}, /* SATA 9 */ {3, 0, 6, 0, TRUE, -1, ecam_probe_sata, 6}, /* SATA 10 */ {3, 0, 7, 0, TRUE, -1, ecam_probe_sata, 6}, /* SATA 11 */ {3, 0, 8, 0, TRUE, -1, ecam_probe_sata, 7}, /* SATA 12 */ {3, 0, 9, 0, TRUE, -1, ecam_probe_sata, 7}, /* SATA 13 */ {3, 0, 10, 0, TRUE, -1, ecam_probe_sata, 7}, /* SATA 14 */ {3, 0, 11, 0, TRUE, -1, ecam_probe_sata, 7}, /* SATA 15 */ {-1, 0, 0, 0, 0, -1, NULL, 0}, }; struct ecam_device devs4[] = { {0, 0, 1, 0, TRUE, -1, NULL, 0}, /* PCCBR_MRML */ {0, 0, 2, 0, FALSE, -1, NULL, 0}, /* SMMU 0 */ {0, 0, 3, 0, FALSE, -1, NULL, 0}, /* GIC */ {0, 0, 4, 0, FALSE, -1, NULL, 0}, /* GTI */ {0, 0, 6, 0, TRUE, -1, NULL, 0}, /* GPIO */ {0, 0, 7, 0, FALSE, -1, NULL, 0}, /* MPI */ {0, 0, 8, 0, FALSE, -1, NULL, 0}, /* MIO_PTP */ {0, 0, 9, 0, FALSE, -1, NULL, 0}, /* RNM */ {0, 0, 16, 0, TRUE, -1, NULL, 0}, /* USB 0 */ {0, 0, 17, 0, TRUE, -1, NULL, 0}, /* USB 1 */ {0, 1, 0, 0, TRUE, 0, ecam_probe_true, 0}, /* MRML */ {0, 1, 0, 1, TRUE, 0, ecam_probe_true, 0}, /* RST */ {0, 1, 0, 5, TRUE, 0, ecam_probe_true, 0}, /* OCX */ {0, 1, 0, 12, TRUE, 0, ecam_probe_true, 0}, /* MIO_EMM */ {0, 1, 0, 64, FALSE, -1, ecam_probe_false, 0}, /* UAA 0 */ {0, 1, 0, 65, FALSE, -1, ecam_probe_false, 0}, /* UAA 1 */ {0, 1 , 0, 72, TRUE, 0, ecam_probe_true, 0}, /* TWSI 0 */ {0, 1 , 0, 73, TRUE, 0, ecam_probe_true, 0}, /* TWSI 1 */ {0, 1 , 0, 74, TRUE, 0, ecam_probe_true, 0}, /* TWSI 2 */ {0, 1 , 0, 75, TRUE, 0, ecam_probe_true, 0}, /* TWSI 3 */ {0, 1 , 0, 76, TRUE, 0, ecam_probe_true, 0}, /* TWSI 4 */ {0, 1 , 0, 77, TRUE, 0, ecam_probe_true, 0}, /* TWSI 5 */ {0, 1 , 0, 80, TRUE, 0, ecam_probe_true, 0}, /* LMC 0 */ {0, 1 , 0, 81, TRUE, 0, ecam_probe_true, 0}, /* LMC 1 */ {0, 1 , 0, 82, TRUE, 0, ecam_probe_true, 0}, /* LMC 2 */ {0, 1 , 0, 83, TRUE, 0, ecam_probe_true, 0}, /* LMC 3 */ {0, 1 , 0, 112, TRUE, 0, ecam_probe_pem, 0}, /* PEM 0 */ {0, 1 , 0, 113, TRUE, 0, ecam_probe_pem, 1}, /* PEM 1 */ {0, 1 , 0, 114, TRUE, 0, ecam_probe_pem, 2}, /* PEM 2 */ {0, 1 , 0, 115, TRUE, 0, ecam_probe_pem, 3}, /* PEM 3 */ {0, 1 , 0, 116, TRUE, 0, ecam_probe_pem, 4}, /* PEM 4 */ {0, 1 , 0, 117, TRUE, 0, ecam_probe_pem, 5}, /* PEM 5 */ {0, 1, 0, 128, TRUE, 0, ecam_probe_bgx, 0}, /* BGX 0 */ {0, 1, 0, 129, TRUE, 0, ecam_probe_bgx, 1}, /* BGX 1 */ {0, 2, 0, 0, TRUE, -1, NULL, 0}, /* RAD */ {0, 3, 0, 0, TRUE, -1, NULL, 0}, /* ZIP */ {0, 4, 0, 0, TRUE, -1, NULL, 0}, /* HFA */ {-1, 0, 0, 0, 0, -1, NULL, 0}, }; struct ecam_device devs5[] = { {1, 0, 1, 0, FALSE, -1, NULL, 0}, /* SMMU 1 */ {1, 0, 4, 0, TRUE, -1, ecam_probe_sata, 2}, /* SATA 0 */ {1, 0, 5, 0, TRUE, -1, ecam_probe_sata, 2}, /* SATA 1 */ {1, 0, 6, 0, TRUE, -1, ecam_probe_sata, 2}, /* SATA 2 */ {1, 0, 7, 0, TRUE, -1, ecam_probe_sata, 2}, /* SATA 3 */ {1, 0, 8, 0, TRUE, -1, ecam_probe_sata, 3}, /* SATA 4 */ {1, 0, 9, 0, TRUE, -1, ecam_probe_sata, 3}, /* SATA 5 */ {1, 0, 10, 0, TRUE, -1, ecam_probe_sata, 3}, /* SATA 6 */ {1, 0, 11, 0, TRUE, -1, ecam_probe_sata, 3}, /* SATA 7 */ {-1, 0, 0, 0, 0, -1, NULL, 0}, }; struct ecam_device devs6[] = { {2, 0, 1, 0, FALSE, -1, NULL, 0}, /* SMMU 2 */ {2, 0, 2, 0, TRUE, -1, NULL, 0}, /* PCCBR_NIC */ {2, 0, 3, 0, TRUE, -1, NULL, 0}, /* TNS */ {2, 1, 0, 0, TRUE, -1, NULL, 0}, /* NIC */ {-1, 0, 0, 0, 0, -1, NULL, 0}, }; struct ecam_device devs7[] = { {3, 0, 1, 0, FALSE, -1, NULL, 0}, {3, 0, 4, 0, TRUE, -1, ecam_probe_sata, 6}, /* SATA 8 */ {3, 0, 5, 0, TRUE, -1, ecam_probe_sata, 6}, /* SATA 9 */ {3, 0, 6, 0, TRUE, -1, ecam_probe_sata, 6}, /* SATA 10 */ {3, 0, 7, 0, TRUE, -1, ecam_probe_sata, 6}, /* SATA 11 */ {3, 0, 8, 0, TRUE, -1, ecam_probe_sata, 7}, /* SATA 12 */ {3, 0, 9, 0, TRUE, -1, ecam_probe_sata, 7}, /* SATA 13 */ {3, 0, 10, 0, TRUE, -1, ecam_probe_sata, 7}, /* SATA 14 */ {3, 0, 11, 0, TRUE, -1, ecam_probe_sata, 7}, /* SATA 15 */ {-1, 0, 0, 0, 0, -1, NULL, 0}, }; 有对板类型的判断 strcmp(env_ptr, \"BOARD=crb_1s\") 这里似乎只有通过twsi向BMC发shutdown, 在PSCI服务中. 这难道和不能整体关机有关? 一些默认值 atf/plat/thunder/include/thunder_def.h #define DRAM_TOTAL_SIZE 0x80000000 /* Location of trusted dram on the base thunder */ #define TZDRAM_BASE 0x00000000 #define TZDRAM_SIZE 0x00200000 //2M #define NTDRAM_BASE (TZDRAM_BASE + TZDRAM_SIZE) #define NTDRAM_SIZE (DRAM_TOTAL_SIZE - TZDRAM_SIZE) #define SHARED_MEM_BASE NTDRAM_BASE #define SHARED_MEM_SIZE 0x00200000 #define BL1_BASE 0x00001000 #define BL1_MAX_SIZE 0x00079000 #define BL31_BASE 0x00080000 #define BL31_MAX_SIZE 0x00100000 #define BL31_LIMIT (BL31_BASE + BL31_MAX_SIZE) #define BL2_BASE 0x00100000 #define BL2_MAX_SIZE 0x00080000 #define BL2_LIMIT (BL2_BASE + BL2_MAX_SIZE) bl1 bl2 大概浏览了一下, 大约就是加载下一个bl然后运行, 期间要设置好各自的内存map. bootloader一直都比较混乱 bl31 bl31_main() bl31_arch_setup() bl31_platform_setup() gic_setup() thunder_pwrc_setup() thunder_setup_topology() bl31_lib_init() runtime_svc_init() //运行时服务, 都是静态编译到下面的标号的, 一个服务是一个描述符 &__RT_SVC_DESCS_START__ //每个服务rt_svc_descs[index].init bl31_prepare_next_image_entry() "},"notes/CPU_ARM64_thunder_kernel_boot.html":{"url":"notes/CPU_ARM64_thunder_kernel_boot.html","title":"thunder kernel启动打印流程","keywords":"","body":"启动流程 EFI stub: Booting Linux Kernel... start_kernel() in init/main.c lockdep_init() set_task_stack_end_magic(&init_task) smp_setup_processor_id() debug_objects_early_init() boot_init_stack_canary() cgroup_init_early() [ 0.000000] Initializing cgroup subsys cpuset [ 0.000000] Initializing cgroup subsys cpu [ 0.000000] Initializing cgroup subsys cpuacct local_irq_disable() early_boot_irqs_disabled = true boot_cpu_init() page_address_init() pr_notice(\"%s\", linux_banner) [ 0.000000] Linux version 3.18.0-gdcccbc1-dirty (root@Tfedora) (gcc version 4.9.2 20150212 (Red Hat 4.9.2-6) (GCC) ) #4 SMP Thu Apr 23 15:05:33 CST 2015 setup_arch(&command_line) in arch/arm64/kernel/setup.c setup_processor() [ 0.000000] CPU: AArch64 Processor [430f0a10] revision 0 cpuinfo_store_boot_cpu() cpuinfo_detect_icache_policy(info) [ 0.000000] Detected VIPT I-cache on CPU0 setup_machine_fdt(__fdt_pointer) //这里的__fdt_pointer是uefi或grub传过来的 early_init_dt_scan() early_init_dt_verify() early_init_dt_scan_nodes() 这里只做三件事: 用/chosen填boot_command_line, root, memory of_scan_flat_dt(early_init_dt_scan_chosen, boot_command_line) of_scan_flat_dt(early_init_dt_scan_root, NULL) of_scan_flat_dt(early_init_dt_scan_memory, NULL); *cmdline_p = boot_command_line early_ioremap_init() disable_acpi() parse_early_param() //此时cmdline已经从fdt里面获取到了 //调用相应的声明early_param(\"earlycon\", setup_of_earlycon) [ 0.000000] Early serial console at I/O port 0x0 (options '') [ 0.000000] bootconsole [uart0] enabled local_async_enable() efi_init() efi_get_fdt_params(&params, uefi_debug) [ 0.000000] efi: Getting EFI parameters from FDT: uefi_init() [ 0.000000] EFI v2.40 by Cavium Thunder cn88xx EFI Mar 20 2015 12:00:56 efi_config_init(NULL) [ 0.000000] efi: ACPI=0xfffff000 ACPI 2.0=0xfffff014 arm64_memblock_init() in arch/arm64/mm/init.c acpi_boot_table_init() paging_init() map_mem() bootmem_init() arm64_numa_init() [ 0.000000] numa: Adding memblock 0 [0x1400000 - 0x800000000] on node 0 //20M ~ 32G(或64G) [ 0.000000] numa: Adding memblock 1 [0x10000400000 - 0x10800000000] on node 1 //1T(+4M ~ +32G(或64G)) ...setup_node_data() [ 0.000000] Initmem setup node 0 [mem 0x40000000-0x7ffffffff] //1G ~ 32G [ 0.000000] NODE_DATA [mem 0x7ffb10000-0x7ffb1ffff] //顶端64K [ 0.000000] Initmem setup node 1 [mem 0x10040000000-0x107ffffffff] //1T(+1G ~ 32G) [ 0.000000] NODE_DATA [mem 0x107ffe60000-0x107ffe6ffff] //顶端64K arm64_memory_present() sparse_init() zone_sizes_init() free_area_init_nodes() [ 0.000000] Zone ranges: [ 0.000000] DMA [mem 0x01400000-0xffffffff] //20M ~ 4G [ 0.000000] Normal [mem 0x100000000-0x107ffffffff] //4G ~ (1T+32G) [ 0.000000] Movable zone start for each node [ 0.000000] Early memory node ranges [ 0.000000] node 0: [mem 0x01400000-0x7ffffffff] [ 0.000000] node 1: [mem 0x10000400000-0x107ffffffff] [ 0.000000] Initmem setup node 0 [mem 0x01400000-0x7ffffffff] [ 0.000000] Initmem setup node 1 [mem 0x10000400000-0x107ffffffff] request_standard_resources() efi_idmap_init() //没用acpi if (acpi_disabled) unflatten_device_tree() in drivers/of/fdt.c //根据firmware传过来的fdt创建设备树 psci_dt_init() in arch/arm64/kernel/psci.c [ 0.000000] psci: probing for conduit method from DT. psci_0_2_init() [ 0.000000] psci: PSCIv0.2 detected in firmware. [ 0.000000] psci: Using standard PSCI v0.2 function IDs cpu_read_bootcpu_ops() of_smp_init_cpus() smp_build_mpidr_hash() //setup_arch()结束 mm_init_cpumask() setup_command_line() setup_nr_cpu_ids() setup_per_cpu_areas() pcpu_embed_first_chunk() [ 0.000000] PERCPU: Embedded 2 pages/cpu @ffff8107fb830000 s82688 r8192 d40192 u131072 smp_prepare_boot_cpu() build_all_zonelists() [ 0.000000] Built 2 zonelists in Node order, mobility grouping on. Total pages: 1047296 [ 0.000000] Policy zone: Normal page_alloc_init() pr_notice(\"Kernel command line: %s\\n\", boot_command_line) [ 0.000000] Kernel command line: BOOT_IMAGE=/boot/vmlinuz root=/dev/sda3 console=ttyAMA0,115200n8 earlycon=pl011,0x87e024000000 coherent_pool=16M rootwait rw transparent_hugepage=never [ 0.000000] log_buf_len individual max cpu contribution: 4096 bytes [ 0.000000] log_buf_len total cpu_extra contributions: 389120 bytes [ 0.000000] log_buf_len min size: 16384 bytes [ 0.000000] log_buf_len: 524288 bytes [ 0.000000] early log buf free: 12420(75%) [ 0.000000] PID hash table entries: 4096 (order: -1, 32768 bytes) [ 0.000000] Memory: 66972608K/67084288K available (7944K kernel code, 1075K rwdata, 5376K rodata, 832K init, 1322K bss, 111680K reserved) [ 0.000000] Virtual kernel memory layout: [ 0.000000] vmalloc : 0xffff000000000000 - 0xffff77ffffff0000 (122879 GB) [ 0.000000] vmemmap : 0xffff780000000000 - 0xffff7c0000000000 ( 4096 GB maximum) [ 0.000000] 0xffff780000004600 - 0xffff780039c00000 ( 923 MB actual) [ 0.000000] PCI I/O : 0xffff7ffffa000000 - 0xffff7ffffb000000 ( 16 MB) [ 0.000000] fixed : 0xffff7ffffbde0000 - 0xffff7ffffbdf0000 ( 64 KB) [ 0.000000] modules : 0xffff7ffffc000000 - 0xffff800000000000 ( 64 MB) [ 0.000000] memory : 0xffff800000000000 - 0xffff8107fec00000 (1081324 MB) [ 0.000000] .init : 0xffff800000d90000 - 0xffff800000e60000 ( 832 KB) [ 0.000000] .text : 0xffff800000080000 - 0xffff800000d82b04 ( 13323 KB) [ 0.000000] .data : 0xffff800000e60000 - 0xffff800000f6cdb0 ( 1076 KB) [ 0.000000] SLUB: HWalign=128, Order=0-3, MinObjects=0, CPUs=96, Nodes=2 [ 0.000000] Hierarchical RCU implementation. [ 0.000000] RCU dyntick-idle grace-period acceleration is enabled. [ 0.000000] NR_IRQS:64 nr_irqs:64 0 [ 0.000000] ITS: /soc/interrupt-controller@8010,00000000/gic-its@8010,00020000 [ 0.000000] ITS: allocated 8192 Devices @7c0800000 (psz 64K, shr 3) [ 0.000000] ITS: using cache flushing for cmd queue [ 0.000000] ITS: /soc/interrupt-controller@8010,00000000/gic-its@9010,00020000 [ 0.000000] ITS: allocated 8192 Devices @7c1000000 (psz 64K, shr 3) [ 0.000000] ITS: using cache flushing for cmd queue [ 0.000000] GIC: using LPI property table @0x00000007c0100000 [ 0.000000] ITS: Allocated 32512 chunks for LPIs [ 0.000000] CPU0: found redistributor 0 region 0:0x0000801080000000 [ 0.000000] CPU0: using LPI pending table @0x00000007c0110000 [ 0.000000] GIC: using cache flushing for LPI property table [ 0.000000] Architected cp15 timer(s) running at 100.00MHz (phys). [ 0.000000] sched_clock: 56 bits at 100MHz, resolution 10ns, wraps every 2748779069440ns [ 0.000000] Console: colour dummy device 80x25 [ 0.000000] allocated 16777216 bytes of page_cgroup [ 0.000000] please try 'cgroup_disable=memory' option if you don't want memory cgroups [ 0.030000] Calibrating delay loop (skipped), value calculated using timer frequency.. 200.00 BogoMIPS (lpj=1000000) [ 0.030006] pid_max: default: 98304 minimum: 768 [ 0.036060] Security Framework initialized [ 0.040028] AppArmor: AppArmor initialized [ 0.044155] Yama: becoming mindful. [ 0.049169] Dentry cache hash table entries: 8388608 (order: 10, 67108864 bytes) [ 0.081894] Inode-cache hash table entries: 4194304 (order: 9, 33554432 bytes) [ 0.107257] Mount-cache hash table entries: 131072 (order: 4, 1048576 bytes) [ 0.110023] Mountpoint-cache hash table entries: 131072 (order: 4, 1048576 bytes) [ 0.119248] Initializing cgroup subsys memory [ 0.120035] Initializing cgroup subsys devices [ 0.124514] Initializing cgroup subsys freezer [ 0.128995] Initializing cgroup subsys net_cls [ 0.130005] Initializing cgroup subsys blkio [ 0.134308] Initializing cgroup subsys perf_event [ 0.139053] Initializing cgroup subsys net_prio [ 0.140005] Initializing cgroup subsys hugetlb [ 0.144672] ftrace: allocating 28292 entries in 7 pages [ 0.283823] hw perfevents: enabled with arm/armv8-pmuv3 PMU driver, 7 counters available [ 0.290049] Remapping and enabling EFI services. [ 0.294996] Freed 0xe40000 bytes of EFI boot services memory [ 0.030000] CPU1: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU1 [ 0.030000] CPU1: found redistributor 1 region 0:0x0000801080020000 [ 0.030000] CPU1: using LPI pending table @0x00000007c4ef0000 [ 0.030000] CPU2: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU2 [ 0.030000] CPU2: found redistributor 2 region 0:0x0000801080040000 [ 0.030000] CPU2: using LPI pending table @0x00000007c4fa0000 [ 0.030000] CPU3: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU3 [ 0.030000] CPU3: found redistributor 3 region 0:0x0000801080060000 [ 0.030000] CPU3: using LPI pending table @0x00000007c5060000 [ 0.030000] CPU4: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU4 [ 0.030000] CPU4: found redistributor 4 region 0:0x0000801080080000 [ 0.030000] CPU4: using LPI pending table @0x00000007c51c0000 [ 0.030000] CPU5: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU5 [ 0.030000] CPU5: found redistributor 5 region 0:0x00008010800a0000 [ 0.030000] CPU5: using LPI pending table @0x00000007c5260000 [ 0.030000] CPU6: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU6 [ 0.030000] CPU6: found redistributor 6 region 0:0x00008010800c0000 [ 0.030000] CPU6: using LPI pending table @0x00000007c5300000 [ 0.030000] CPU7: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU7 [ 0.030000] CPU7: found redistributor 7 region 0:0x00008010800e0000 [ 0.030000] CPU7: using LPI pending table @0x00000007c53a0000 [ 0.030000] CPU8: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU8 [ 0.030000] CPU8: found redistributor 8 region 0:0x0000801080100000 [ 0.030000] CPU8: using LPI pending table @0x00000007c5460000 [ 0.030000] CPU9: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU9 [ 0.030000] CPU9: found redistributor 9 region 0:0x0000801080120000 [ 0.030000] CPU9: using LPI pending table @0x00000007c5500000 [ 0.030000] CPU10: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU10 [ 0.030000] CPU10: found redistributor a region 0:0x0000801080140000 [ 0.030000] CPU10: using LPI pending table @0x00000007c5620000 [ 0.030000] CPU11: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU11 [ 0.030000] CPU11: found redistributor b region 0:0x0000801080160000 [ 0.030000] CPU11: using LPI pending table @0x00000007c56c0000 [ 0.030000] CPU12: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU12 [ 0.030000] CPU12: found redistributor c region 0:0x0000801080180000 [ 0.030000] CPU12: using LPI pending table @0x00000007c57a0000 [ 0.030000] CPU13: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU13 [ 0.030000] CPU13: found redistributor d region 0:0x00008010801a0000 [ 0.030000] CPU13: using LPI pending table @0x00000007c5850000 [ 0.030000] CPU14: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU14 [ 0.030000] CPU14: found redistributor e region 0:0x00008010801c0000 [ 0.030000] CPU14: using LPI pending table @0x00000007c58f0000 [ 0.030000] CPU15: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU15 [ 0.030000] CPU15: found redistributor f region 0:0x00008010801e0000 [ 0.030000] CPU15: using LPI pending table @0x00000007c5a10000 [ 0.030000] CPU16: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU16 [ 0.030000] CPU16: found redistributor 100 region 0:0x0000801080200000 [ 0.030000] CPU16: using LPI pending table @0x00000007c5ad0000 [ 0.030000] CPU17: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU17 [ 0.030000] CPU17: found redistributor 101 region 0:0x0000801080220000 [ 0.030000] CPU17: using LPI pending table @0x00000007c5ba0000 [ 0.030000] CPU18: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU18 [ 0.030000] CPU18: found redistributor 102 region 0:0x0000801080240000 [ 0.030000] CPU18: using LPI pending table @0x00000007c5c40000 [ 0.030000] CPU19: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU19 [ 0.030000] CPU19: found redistributor 103 region 0:0x0000801080260000 [ 0.030000] CPU19: using LPI pending table @0x00000007c5cf0000 [ 0.030000] CPU20: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU20 [ 0.030000] CPU20: found redistributor 104 region 0:0x0000801080280000 [ 0.030000] CPU20: using LPI pending table @0x00000007c5e30000 [ 0.030000] CPU21: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU21 [ 0.030000] CPU21: found redistributor 105 region 0:0x00008010802a0000 [ 0.030000] CPU21: using LPI pending table @0x00000007c5ed0000 [ 0.030000] CPU22: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU22 [ 0.030000] CPU22: found redistributor 106 region 0:0x00008010802c0000 [ 0.030000] CPU22: using LPI pending table @0x00000007c5f70000 [ 0.030000] CPU23: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU23 [ 0.030000] CPU23: found redistributor 107 region 0:0x00008010802e0000 [ 0.030000] CPU23: using LPI pending table @0x0000010003e40000 [ 0.030000] CPU24: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU24 [ 0.030000] CPU24: found redistributor 108 region 0:0x0000801080300000 [ 0.030000] CPU24: using LPI pending table @0x0000010003e50000 [ 0.030000] CPU25: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU25 [ 0.030000] CPU25: found redistributor 109 region 0:0x0000801080320000 [ 0.030000] CPU25: using LPI pending table @0x0000010003e60000 [ 0.030000] CPU26: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU26 [ 0.030000] CPU26: found redistributor 10a region 0:0x0000801080340000 [ 0.030000] CPU26: using LPI pending table @0x0000010003e70000 [ 0.030000] CPU27: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU27 [ 0.030000] CPU27: found redistributor 10b region 0:0x0000801080360000 [ 0.030000] CPU27: using LPI pending table @0x0000010003ea0000 [ 0.030000] CPU28: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU28 [ 0.030000] CPU28: found redistributor 10c region 0:0x0000801080380000 [ 0.030000] CPU28: using LPI pending table @0x0000010003eb0000 [ 0.030000] CPU29: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU29 [ 0.030000] CPU29: found redistributor 10d region 0:0x00008010803a0000 [ 0.030000] CPU29: using LPI pending table @0x0000010003ec0000 [ 0.030000] CPU30: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU30 [ 0.030000] CPU30: found redistributor 10e region 0:0x00008010803c0000 [ 0.030000] CPU30: using LPI pending table @0x0000010003ed0000 [ 0.030000] CPU31: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU31 [ 0.030000] CPU31: found redistributor 10f region 0:0x00008010803e0000 [ 0.030000] CPU31: using LPI pending table @0x0000010003f00000 [ 0.030000] CPU32: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU32 [ 0.030000] CPU32: found redistributor 200 region 0:0x0000801080400000 [ 0.030000] CPU32: using LPI pending table @0x0000010003f10000 [ 0.030000] CPU33: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU33 [ 0.030000] CPU33: found redistributor 201 region 0:0x0000801080420000 [ 0.030000] CPU33: using LPI pending table @0x0000010003f20000 [ 0.030000] CPU34: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU34 [ 0.030000] CPU34: found redistributor 202 region 0:0x0000801080440000 [ 0.030000] CPU34: using LPI pending table @0x0000010003f30000 [ 0.030000] CPU35: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU35 [ 0.030000] CPU35: found redistributor 203 region 0:0x0000801080460000 [ 0.030000] CPU35: using LPI pending table @0x0000010003f60000 [ 0.030000] CPU36: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU36 [ 0.030000] CPU36: found redistributor 204 region 0:0x0000801080480000 [ 0.030000] CPU36: using LPI pending table @0x0000010003f70000 [ 0.030000] CPU37: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU37 [ 0.030000] CPU37: found redistributor 205 region 0:0x00008010804a0000 [ 0.030000] CPU37: using LPI pending table @0x0000010003f80000 [ 0.030000] CPU38: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU38 [ 0.030000] CPU38: found redistributor 206 region 0:0x00008010804c0000 [ 0.030000] CPU38: using LPI pending table @0x0000010003f90000 [ 0.030000] CPU39: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU39 [ 0.030000] CPU39: found redistributor 207 region 0:0x00008010804e0000 [ 0.030000] CPU39: using LPI pending table @0x0000010003fc0000 [ 0.030000] CPU40: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU40 [ 0.030000] CPU40: found redistributor 208 region 0:0x0000801080500000 [ 0.030000] CPU40: using LPI pending table @0x0000010003fd0000 [ 0.030000] CPU41: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU41 [ 0.030000] CPU41: found redistributor 209 region 0:0x0000801080520000 [ 0.030000] CPU41: using LPI pending table @0x0000010003fe0000 [ 0.030000] CPU42: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU42 [ 0.030000] CPU42: found redistributor 20a region 0:0x0000801080540000 [ 0.030000] CPU42: using LPI pending table @0x0000010003ff0000 [ 0.030000] CPU43: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU43 [ 0.030000] CPU43: found redistributor 20b region 0:0x0000801080560000 [ 0.030000] CPU43: using LPI pending table @0x0000010004020000 [ 0.030000] CPU44: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU44 [ 0.030000] CPU44: found redistributor 20c region 0:0x0000801080580000 [ 0.030000] CPU44: using LPI pending table @0x0000010004030000 [ 0.030000] CPU45: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU45 [ 0.030000] CPU45: found redistributor 20d region 0:0x00008010805a0000 [ 0.030000] CPU45: using LPI pending table @0x0000010004040000 [ 0.030000] CPU46: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU46 [ 0.030000] CPU46: found redistributor 20e region 0:0x00008010805c0000 [ 0.030000] CPU46: using LPI pending table @0x0000010004050000 [ 0.030000] CPU47: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU47 [ 0.030000] CPU47: found redistributor 20f region 0:0x00008010805e0000 [ 0.030000] CPU47: using LPI pending table @0x0000010004060000 [ 0.030000] CPU48: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU48 [ 0.030000] CPU48: found redistributor 10000 region 1:0x0000901080000000 [ 0.030000] CPU48: using LPI pending table @0x0000010004070000 [ 0.030000] CPU49: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU49 [ 0.030000] CPU49: found redistributor 10001 region 1:0x0000901080020000 [ 0.030000] CPU49: using LPI pending table @0x00000100040a0000 [ 0.030000] CPU50: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU50 [ 0.030000] CPU50: found redistributor 10002 region 1:0x0000901080040000 [ 0.030000] CPU50: using LPI pending table @0x00000100040b0000 [ 0.030000] CPU51: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU51 [ 0.030000] CPU51: found redistributor 10003 region 1:0x0000901080060000 [ 0.030000] CPU51: using LPI pending table @0x00000100040c0000 [ 0.030000] CPU52: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU52 [ 0.030000] CPU52: found redistributor 10004 region 1:0x0000901080080000 [ 0.030000] CPU52: using LPI pending table @0x00000100040d0000 [ 0.030000] CPU53: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU53 [ 0.030000] CPU53: found redistributor 10005 region 1:0x00009010800a0000 [ 0.030000] CPU53: using LPI pending table @0x0000010004100000 [ 0.030000] CPU54: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU54 [ 0.030000] CPU54: found redistributor 10006 region 1:0x00009010800c0000 [ 0.030000] CPU54: using LPI pending table @0x0000010004110000 [ 0.030000] CPU55: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU55 [ 0.030000] CPU55: found redistributor 10007 region 1:0x00009010800e0000 [ 0.030000] CPU55: using LPI pending table @0x0000010004120000 [ 0.030000] CPU56: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU56 [ 0.030000] CPU56: found redistributor 10008 region 1:0x0000901080100000 [ 0.030000] CPU56: using LPI pending table @0x0000010004130000 [ 0.030000] CPU57: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU57 [ 0.030000] CPU57: found redistributor 10009 region 1:0x0000901080120000 [ 0.030000] CPU57: using LPI pending table @0x0000010004160000 [ 0.030000] CPU58: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU58 [ 0.030000] CPU58: found redistributor 1000a region 1:0x0000901080140000 [ 0.030000] CPU58: using LPI pending table @0x0000010004170000 [ 0.030000] CPU59: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU59 [ 0.030000] CPU59: found redistributor 1000b region 1:0x0000901080160000 [ 0.030000] CPU59: using LPI pending table @0x0000010004180000 [ 0.030000] CPU60: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU60 [ 0.030000] CPU60: found redistributor 1000c region 1:0x0000901080180000 [ 0.030000] CPU60: using LPI pending table @0x0000010004190000 [ 0.030000] CPU61: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU61 [ 0.030000] CPU61: found redistributor 1000d region 1:0x00009010801a0000 [ 0.030000] CPU61: using LPI pending table @0x00000100041c0000 [ 0.030000] CPU62: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU62 [ 0.030000] CPU62: found redistributor 1000e region 1:0x00009010801c0000 [ 0.030000] CPU62: using LPI pending table @0x00000100041d0000 [ 0.030000] CPU63: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU63 [ 0.030000] CPU63: found redistributor 1000f region 1:0x00009010801e0000 [ 0.030000] CPU63: using LPI pending table @0x00000100041e0000 [ 0.030000] CPU64: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU64 [ 0.030000] CPU64: found redistributor 10100 region 1:0x0000901080200000 [ 0.030000] CPU64: using LPI pending table @0x00000100041f0000 [ 0.030000] CPU65: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU65 [ 0.030000] CPU65: found redistributor 10101 region 1:0x0000901080220000 [ 0.030000] CPU65: using LPI pending table @0x0000010004220000 [ 0.030000] CPU66: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU66 [ 0.030000] CPU66: found redistributor 10102 region 1:0x0000901080240000 [ 0.030000] CPU66: using LPI pending table @0x0000010004230000 [ 0.030000] CPU67: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU67 [ 0.030000] CPU67: found redistributor 10103 region 1:0x0000901080260000 [ 0.030000] CPU67: using LPI pending table @0x0000010004240000 [ 0.030000] CPU68: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU68 [ 0.030000] CPU68: found redistributor 10104 region 1:0x0000901080280000 [ 0.030000] CPU68: using LPI pending table @0x0000010004250000 [ 0.030000] CPU69: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU69 [ 0.030000] CPU69: found redistributor 10105 region 1:0x00009010802a0000 [ 0.030000] CPU69: using LPI pending table @0x0000010004280000 [ 0.030000] CPU70: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU70 [ 0.030000] CPU70: found redistributor 10106 region 1:0x00009010802c0000 [ 0.030000] CPU70: using LPI pending table @0x0000010004290000 [ 0.030000] CPU71: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU71 [ 0.030000] CPU71: found redistributor 10107 region 1:0x00009010802e0000 [ 0.030000] CPU71: using LPI pending table @0x00000100042a0000 [ 0.030000] CPU72: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU72 [ 0.030000] CPU72: found redistributor 10108 region 1:0x0000901080300000 [ 0.030000] CPU72: using LPI pending table @0x00000100042b0000 [ 0.030000] CPU73: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU73 [ 0.030000] CPU73: found redistributor 10109 region 1:0x0000901080320000 [ 0.030000] CPU73: using LPI pending table @0x00000100042c0000 [ 0.030000] CPU74: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU74 [ 0.030000] CPU74: found redistributor 1010a region 1:0x0000901080340000 [ 0.030000] CPU74: using LPI pending table @0x00000100042d0000 [ 0.030000] CPU75: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU75 [ 0.030000] CPU75: found redistributor 1010b region 1:0x0000901080360000 [ 0.030000] CPU75: using LPI pending table @0x0000010004300000 [ 0.030000] CPU76: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU76 [ 0.030000] CPU76: found redistributor 1010c region 1:0x0000901080380000 [ 0.030000] CPU76: using LPI pending table @0x0000010004310000 [ 0.030000] CPU77: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU77 [ 0.030000] CPU77: found redistributor 1010d region 1:0x00009010803a0000 [ 0.030000] CPU77: using LPI pending table @0x0000010004320000 [ 0.030000] CPU78: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU78 [ 0.030000] CPU78: found redistributor 1010e region 1:0x00009010803c0000 [ 0.030000] CPU78: using LPI pending table @0x0000010004330000 [ 0.030000] CPU79: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU79 [ 0.030000] CPU79: found redistributor 1010f region 1:0x00009010803e0000 [ 0.030000] CPU79: using LPI pending table @0x0000010004360000 [ 0.030000] CPU80: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU80 [ 0.030000] CPU80: found redistributor 10200 region 1:0x0000901080400000 [ 0.030000] CPU80: using LPI pending table @0x0000010004370000 [ 0.030000] CPU81: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU81 [ 0.030000] CPU81: found redistributor 10201 region 1:0x0000901080420000 [ 0.030000] CPU81: using LPI pending table @0x0000010004380000 [ 0.030000] CPU82: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU82 [ 0.030000] CPU82: found redistributor 10202 region 1:0x0000901080440000 [ 0.030000] CPU82: using LPI pending table @0x0000010004390000 [ 0.030000] CPU83: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU83 [ 0.030000] CPU83: found redistributor 10203 region 1:0x0000901080460000 [ 0.030000] CPU83: using LPI pending table @0x00000100043c0000 [ 0.030000] CPU84: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU84 [ 0.030000] CPU84: found redistributor 10204 region 1:0x0000901080480000 [ 0.030000] CPU84: using LPI pending table @0x00000100043d0000 [ 0.030000] CPU85: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU85 [ 0.030000] CPU85: found redistributor 10205 region 1:0x00009010804a0000 [ 0.030000] CPU85: using LPI pending table @0x00000100043e0000 [ 0.030000] CPU86: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU86 [ 0.030000] CPU86: found redistributor 10206 region 1:0x00009010804c0000 [ 0.030000] CPU86: using LPI pending table @0x00000100043f0000 [ 0.030000] CPU87: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU87 [ 0.030000] CPU87: found redistributor 10207 region 1:0x00009010804e0000 [ 0.030000] CPU87: using LPI pending table @0x0000010004420000 [ 0.030000] CPU88: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU88 [ 0.030000] CPU88: found redistributor 10208 region 1:0x0000901080500000 [ 0.030000] CPU88: using LPI pending table @0x0000010004430000 [ 0.030000] CPU89: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU89 [ 0.030000] CPU89: found redistributor 10209 region 1:0x0000901080520000 [ 0.030000] CPU89: using LPI pending table @0x0000010004440000 [ 0.030000] CPU90: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU90 [ 0.030000] CPU90: found redistributor 1020a region 1:0x0000901080540000 [ 0.030000] CPU90: using LPI pending table @0x0000010004450000 [ 0.030000] CPU91: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU91 [ 0.030000] CPU91: found redistributor 1020b region 1:0x0000901080560000 [ 0.030000] CPU91: using LPI pending table @0x0000010004480000 [ 0.030000] CPU92: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU92 [ 0.030000] CPU92: found redistributor 1020c region 1:0x0000901080580000 [ 0.030000] CPU92: using LPI pending table @0x0000010004490000 [ 0.030000] CPU93: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU93 [ 0.030000] CPU93: found redistributor 1020d region 1:0x00009010805a0000 [ 0.030000] CPU93: using LPI pending table @0x00000100044a0000 [ 0.030000] CPU94: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU94 [ 0.030000] CPU94: found redistributor 1020e region 1:0x00009010805c0000 [ 0.030000] CPU94: using LPI pending table @0x00000100044b0000 [ 0.030000] CPU95: Booted secondary processor [ 0.030000] Detected VIPT I-cache on CPU95 [ 0.030000] CPU95: found redistributor 1020f region 1:0x00009010805e0000 [ 0.030000] CPU95: using LPI pending table @0x00000100044c0000 [ 0.340309] Brought up 96 CPUs [ 2.238950] SMP: Total of 96 processors activated. [ 2.243500] devtmpfs: initialized [ 2.243640] evm: security.selinux [ 2.250013] evm: security.SMACK64 [ 2.253349] evm: security.SMACK64EXEC [ 2.257034] evm: security.SMACK64TRANSMUTE [ 2.260003] evm: security.SMACK64MMAP [ 2.263691] evm: security.ima [ 2.266674] evm: security.capability [ 2.274053] regulator-dummy: no parameters [ 2.283325] NET: Registered protocol family 16 [ 2.310008] cpuidle: using governor ladder [ 2.340007] cpuidle: using governor menu [ 2.344033] vdso: 2 pages (1 code @ ffff800000e80000, 1 data @ ffff800000e70000) [ 2.350090] hw-breakpoint: found 6 breakpoint and 4 watchpoint registers. [ 2.358622] software IO TLB [mem 0x04000000-0x08000000] (64MB) mapped at [ffff800002c00000-ffff800006bfffff] [ 2.360649] DMA: preallocated 16384 KiB pool for atomic allocations [ 2.367062] Serial: AMBA PL011 UART driver [ 2.371019] uart-pl011 87e024000000.serial: ttyAMA0 at MMIO 0x87e024000000 (irq = 37, base_baud = 0) is a PL011 rev3 [ 2.380016] console [ttyAMA0] enabled [ 2.380016] console [ttyAMA0] enabled [ 2.387352] bootconsole [uart0] disabled [ 2.387352] bootconsole [uart0] disabled [ 2.390317] uart-pl011 87e025000000.serial: ttyAMA1 at MMIO 0x87e025000000 (irq = 38, base_baud = 0) is a PL011 rev3 [ 2.471267] vgaarb: loaded [ 2.471267] SCSI subsystem initialized [ 2.480157] usbcore: registered new interface driver usbfs [ 2.480157] usbcore: registered new interface driver hub [ 2.490118] usbcore: registered new device driver usb [ 2.492226] arm-smmu 830000000000.smmu0: probing hardware configuration... [ 2.499090] arm-smmu 830000000000.smmu0: SMMUv2 with: [ 2.510007] arm-smmu 830000000000.smmu0: stage 1 translation [ 2.515740] arm-smmu 830000000000.smmu0: stage 2 translation [ 2.520003] arm-smmu 830000000000.smmu0: nested translation [ 2.525650] arm-smmu 830000000000.smmu0: coherent table walk [ 2.530004] arm-smmu 830000000000.smmu0: stream matching with 128 register groups, mask 0x7fff [ 2.538689] arm-smmu 830000000000.smmu0: 128 context banks (0 stage-2 only) [ 2.550006] arm-smmu 830000000000.smmu0: Stage-1: 48-bit VA -> 48-bit IPA [ 2.556868] arm-smmu 830000000000.smmu0: Stage-2: 48-bit IPA -> 48-bit PA [ 2.560028] arm-smmu 830000000000.smmu0: registered 1 master devices [ 2.568034] arm-smmu 831000000000.smmu1: probing hardware configuration... [ 2.580004] arm-smmu 831000000000.smmu1: SMMUv2 with: [ 2.585044] arm-smmu 831000000000.smmu1: stage 1 translation [ 2.590003] arm-smmu 831000000000.smmu1: stage 2 translation [ 2.595736] arm-smmu 831000000000.smmu1: nested translation [ 2.600003] arm-smmu 831000000000.smmu1: coherent table walk [ 2.605737] arm-smmu 831000000000.smmu1: stream matching with 128 register groups, mask 0x7fff [ 2.610003] arm-smmu 831000000000.smmu1: 128 context banks (0 stage-2 only) [ 2.617039] arm-smmu 831000000000.smmu1: Stage-1: 48-bit VA -> 48-bit IPA [ 2.630003] arm-smmu 831000000000.smmu1: Stage-2: 48-bit IPA -> 48-bit PA [ 2.636888] arm-smmu 831000000000.smmu1: registered 1 master devices [ 2.641663] arm-smmu 832000000000.smmu2: probing hardware configuration... [ 2.648526] arm-smmu 832000000000.smmu2: SMMUv2 with: [ 2.650004] arm-smmu 832000000000.smmu2: stage 1 translation [ 2.655737] arm-smmu 832000000000.smmu2: stage 2 translation [ 2.660003] arm-smmu 832000000000.smmu2: nested translation [ 2.665649] arm-smmu 832000000000.smmu2: coherent table walk [ 2.680004] arm-smmu 832000000000.smmu2: stream matching with 128 register groups, mask 0x7fff [ 2.688688] arm-smmu 832000000000.smmu2: 128 context banks (0 stage-2 only) [ 2.690003] arm-smmu 832000000000.smmu2: Stage-1: 48-bit VA -> 48-bit IPA [ 2.696865] arm-smmu 832000000000.smmu2: Stage-2: 48-bit IPA -> 48-bit PA [ 2.710027] arm-smmu 832000000000.smmu2: registered 1 master devices [ 2.718027] arm-smmu 833000000000.smmu3: probing hardware configuration... [ 2.720004] arm-smmu 833000000000.smmu3: SMMUv2 with: [ 2.725043] arm-smmu 833000000000.smmu3: stage 1 translation [ 2.730003] arm-smmu 833000000000.smmu3: stage 2 translation [ 2.735736] arm-smmu 833000000000.smmu3: nested translation [ 2.740003] arm-smmu 833000000000.smmu3: coherent table walk [ 2.745737] arm-smmu 833000000000.smmu3: stream matching with 128 register groups, mask 0x7fff [ 2.760003] arm-smmu 833000000000.smmu3: 128 context banks (0 stage-2 only) [ 2.767039] arm-smmu 833000000000.smmu3: Stage-1: 48-bit VA -> 48-bit IPA [ 2.770004] arm-smmu 833000000000.smmu3: Stage-2: 48-bit IPA -> 48-bit PA [ 2.776890] arm-smmu 833000000000.smmu3: registered 1 master devices [ 2.781680] arm-smmu 930000000000.smmu4: probing hardware configuration... [ 2.788543] arm-smmu 930000000000.smmu4: SMMUv2 with: [ 2.800005] arm-smmu 930000000000.smmu4: stage 1 translation [ 2.805738] arm-smmu 930000000000.smmu4: stage 2 translation [ 2.810003] arm-smmu 930000000000.smmu4: nested translation [ 2.815649] arm-smmu 930000000000.smmu4: coherent table walk [ 2.820004] arm-smmu 930000000000.smmu4: stream matching with 128 register groups, mask 0x7fff [ 2.828689] arm-smmu 930000000000.smmu4: 128 context banks (0 stage-2 only) [ 2.840004] arm-smmu 930000000000.smmu4: Stage-1: 48-bit VA -> 48-bit IPA [ 2.846866] arm-smmu 930000000000.smmu4: Stage-2: 48-bit IPA -> 48-bit PA [ 2.850028] arm-smmu 930000000000.smmu4: registered 1 master devices [ 2.858036] arm-smmu 931000000000.smmu5: probing hardware configuration... [ 2.860004] arm-smmu 931000000000.smmu5: SMMUv2 with: [ 2.865044] arm-smmu 931000000000.smmu5: stage 1 translation [ 2.880003] arm-smmu 931000000000.smmu5: stage 2 translation [ 2.885736] arm-smmu 931000000000.smmu5: nested translation [ 2.890003] arm-smmu 931000000000.smmu5: coherent table walk [ 2.895737] arm-smmu 931000000000.smmu5: stream matching with 128 register groups, mask 0x7fff [ 2.900004] arm-smmu 931000000000.smmu5: 128 context banks (0 stage-2 only) [ 2.907040] arm-smmu 931000000000.smmu5: Stage-1: 48-bit VA -> 48-bit IPA [ 2.920003] arm-smmu 931000000000.smmu5: Stage-2: 48-bit IPA -> 48-bit PA [ 2.926890] arm-smmu 931000000000.smmu5: registered 1 master devices [ 2.931669] arm-smmu 932000000000.smmu6: probing hardware configuration... [ 2.938531] arm-smmu 932000000000.smmu6: SMMUv2 with: [ 2.940004] arm-smmu 932000000000.smmu6: stage 1 translation [ 2.945738] arm-smmu 932000000000.smmu6: stage 2 translation [ 2.950003] arm-smmu 932000000000.smmu6: nested translation [ 2.955650] arm-smmu 932000000000.smmu6: coherent table walk [ 2.970004] arm-smmu 932000000000.smmu6: stream matching with 128 register groups, mask 0x7fff [ 2.978688] arm-smmu 932000000000.smmu6: 128 context banks (0 stage-2 only) [ 2.980004] arm-smmu 932000000000.smmu6: Stage-1: 48-bit VA -> 48-bit IPA [ 2.986866] arm-smmu 932000000000.smmu6: Stage-2: 48-bit IPA -> 48-bit PA [ 2.990029] arm-smmu 932000000000.smmu6: registered 1 master devices [ 2.998036] arm-smmu 933000000000.smmu7: probing hardware configuration... [ 3.010004] arm-smmu 933000000000.smmu7: SMMUv2 with: [ 3.015043] arm-smmu 933000000000.smmu7: stage 1 translation [ 3.020003] arm-smmu 933000000000.smmu7: stage 2 translation [ 3.025736] arm-smmu 933000000000.smmu7: nested translation [ 3.030003] arm-smmu 933000000000.smmu7: coherent table walk [ 3.035737] arm-smmu 933000000000.smmu7: stream matching with 128 register groups, mask 0x7fff [ 3.050003] arm-smmu 933000000000.smmu7: 128 context banks (0 stage-2 only) [ 3.057039] arm-smmu 933000000000.smmu7: Stage-1: 48-bit VA -> 48-bit IPA [ 3.060003] arm-smmu 933000000000.smmu7: Stage-2: 48-bit IPA -> 48-bit PA [ 3.066891] arm-smmu 933000000000.smmu7: registered 1 master devices [ 3.070340] NetLabel: Initializing [ 3.073730] NetLabel: domain hash size = 128 [ 3.078074] NetLabel: protocols = UNLABELED CIPSOv4 [ 3.090036] NetLabel: unlabeled traffic allowed by default [ 3.095631] Switched to clocksource arch_sys_counter [ 3.126141] AppArmor: AppArmor Filesystem Enabled [ 3.135798] NET: Registered protocol family 2 [ 3.140916] TCP established hash table entries: 524288 (order: 6, 4194304 bytes) [ 3.150310] TCP bind hash table entries: 65536 (order: 4, 1048576 bytes) [ 3.157485] TCP: Hash tables configured (established 524288 bind 65536) [ 3.164139] TCP: reno registered [ 3.167386] UDP hash table entries: 32768 (order: 4, 1048576 bytes) [ 3.174329] UDP-Lite hash table entries: 32768 (order: 4, 1048576 bytes) [ 3.182140] NET: Registered protocol family 1 [ 3.187696] RPC: Registered named UNIX socket transport module. [ 3.193621] RPC: Registered udp transport module. [ 3.198313] RPC: Registered tcp transport module. [ 3.203012] RPC: Registered tcp NFSv4.1 backchannel transport module. [ 3.213858] futex hash table entries: 32768 (order: 6, 4194304 bytes) [ 3.220514] Initialise system trusted keyring [ 3.225077] audit: initializing netlink subsys (disabled) [ 3.230510] audit: type=2000 audit(3.230:1): initialized [ 3.239371] HugeTLB registered 512 MB page size, pre-allocated 0 pages [ 3.250251] zpool: loaded [ 3.252863] zbud: loaded [ 3.256397] VFS: Disk quotas dquot_6.5.2 [ 3.260621] Dquot-cache hash table entries: 8192 (order 0, 65536 bytes) [ 3.272549] NFS: Registering the id_resolver key type [ 3.277612] Key type id_resolver registered [ 3.281799] Key type id_legacy registered [ 3.285810] nfs4filelayout_init: NFSv4 File Layout Driver Registering... [ 3.292608] fuse init (API version 7.23) [ 3.297030] SGI XFS with ACLs, security attributes, realtime, no debug enabled [ 3.309280] msgmni has been set to 32768 [ 3.313515] Key type big_key registered [ 3.318517] Key type asymmetric registered [ 3.322620] Asymmetric key parser 'x509' registered [ 3.327541] bounce: pool size: 64 pages [ 3.331609] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 252) [ 3.339594] io scheduler noop registered (default) [ 3.344392] io scheduler deadline registered [ 3.348680] io scheduler cfq registered [ 3.352884] pci_hotplug: PCI Hot Plug PCI Core version: 0.5 [ 3.358458] pciehp: PCI Express Hot Plug Controller Driver version: 0.4 [ 3.365622] thunder_pcie_probe: ECAM0 CFG BASE 0x848000000000 gser_base0:ffff00001d000000 [ 3.373795] thunder_pcie_probe: ECAM0 CFG BASE 0x848000000000 gser_base1:ffff00002a800000 [ 3.381963] PCI host bridge /pcie0@0x8480,00000000 ranges: [ 3.387455] MEM 0x801000000000..0x807fffffffff -> 0x801000000000 [ 3.393631] MEM 0x830000000000..0x87ffffffffff -> 0x830000000000 [ 3.399890] thunder-pcie 848000000000.pcie0: PCI host bridge to bus 0000:00 [ 3.406854] pci_bus 0000:00: root bus resource [bus 00-ff] [ 3.412334] pci_bus 0000:00: root bus resource [mem 0x801000000000-0x807fffffffff] [ 3.419892] pci_bus 0000:00: root bus resource [mem 0x830000000000-0x87ffffffffff] [ 3.430713] pci 0000:01:00.0: disabling ASPM on pre-1.1 PCIe device. You can enable it with 'pcie_aspm=force' [ 3.440945] pci 0000:02:00.0: disabling ASPM on pre-1.1 PCIe device. You can enable it with 'pcie_aspm=force' [ 3.451159] pci 0000:03:00.0: disabling ASPM on pre-1.1 PCIe device. You can enable it with 'pcie_aspm=force' [ 3.461372] pci 0000:04:00.0: disabling ASPM on pre-1.1 PCIe device. You can enable it with 'pcie_aspm=force' [ 3.472119] thunder_pcie_probe: ECAM1 CFG BASE 0x849000000000 gser_base0:ffff00001d000000 [ 3.480303] thunder_pcie_probe: ECAM1 CFG BASE 0x849000000000 gser_base1:ffff00002a800000 [ 3.488467] PCI host bridge /pcie1@0x8490,00000000 ranges: [ 3.493954] MEM 0x831000000000..0x83100fffffff -> 0x831000000000 [ 3.500126] MEM 0x810000000000..0x817fffffffff -> 0x810000000000 [ 3.506386] thunder-pcie 849000000000.pcie1: PCI host bridge to bus 0001:00 [ 3.513348] pci_bus 0001:00: root bus resource [bus 00-ff] [ 3.518822] pci_bus 0001:00: root bus resource [mem 0x831000000000-0x83100fffffff] [ 3.526386] pci_bus 0001:00: root bus resource [mem 0x810000000000-0x817fffffffff] [ 3.534873] thunder_pcie_probe: ECAM2 CFG BASE 0x84a000000000 gser_base0:ffff00001d000000 [ 3.543050] thunder_pcie_probe: ECAM2 CFG BASE 0x84a000000000 gser_base1:ffff00002a800000 [ 3.551219] PCI host bridge /pcie2@0x84a0,00000000 ranges: [ 3.556696] MEM 0x832000000000..0x83200fffffff -> 0x832000000000 [ 3.562871] MEM 0x843000000000..0x8430ffffffff -> 0x843000000000 [ 3.569115] thunder-pcie 84a000000000.pcie2: PCI host bridge to bus 0002:00 [ 3.576078] pci_bus 0002:00: root bus resource [bus 00-ff] [ 3.581559] pci_bus 0002:00: root bus resource [mem 0x832000000000-0x83200fffffff] [ 3.589116] pci_bus 0002:00: root bus resource [mem 0x843000000000-0x8430ffffffff] [ 4.600163] pci 0002:01:00.0: disabling ASPM on pre-1.1 PCIe device. You can enable it with 'pcie_aspm=force' [ 4.610203] pci 0002:00:03.0: can't claim BAR 0 [mem 0x842000000000-0x84200000ffff 64bit]: no compatible bridge window [ 4.620895] pci 0002:00:03.0: can't claim BAR 2 [mem 0x842040000000-0x84207fffffff 64bit]: no compatible bridge window [ 4.631580] pci 0002:00:03.0: can't claim BAR 4 [mem 0x842000f00000-0x842000ffffff 64bit]: no compatible bridge window [ 4.642431] thunder_pcie_probe: ECAM3 CFG BASE 0x84b000000000 gser_base0:ffff00001d000000 [ 4.650602] thunder_pcie_probe: ECAM3 CFG BASE 0x84b000000000 gser_base1:ffff00002a800000 [ 4.658765] PCI host bridge /pcie3@0x84b0,00000000 ranges: [ 4.664245] MEM 0x833000000000..0x83300fffffff -> 0x833000000000 [ 4.670418] MEM 0x818000000000..0x81ffffffffff -> 0x818000000000 [ 4.676662] thunder-pcie 84b000000000.pcie3: PCI host bridge to bus 0003:00 [ 4.683620] pci_bus 0003:00: root bus resource [bus 00-ff] [ 4.689094] pci_bus 0003:00: root bus resource [mem 0x833000000000-0x83300fffffff] [ 4.696654] pci_bus 0003:00: root bus resource [mem 0x818000000000-0x81ffffffffff] [ 4.704547] thunder_pcie_probe: ECAM4 CFG BASE 0x948000000000 gser_base0:ffff00001d000000 [ 4.712722] thunder_pcie_probe: ECAM4 CFG BASE 0x948000000000 gser_base1:ffff00002a800000 [ 4.720888] PCI host bridge /pcie4@0x9480,00000000 ranges: [ 4.726365] MEM 0x901000000000..0x907fffffffff -> 0x901000000000 [ 4.732537] MEM 0x930000000000..0x97ffffffffff -> 0x930000000000 [ 4.738782] thunder-pcie 948000000000.pcie4: PCI host bridge to bus 0004:00 [ 4.745741] pci_bus 0004:00: root bus resource [bus 00-ff] [ 4.751218] pci_bus 0004:00: root bus resource [mem 0x901000000000-0x907fffffffff] [ 4.758775] pci_bus 0004:00: root bus resource [mem 0x930000000000-0x97ffffffffff] [ 4.769273] pci 0004:01:00.0: disabling ASPM on pre-1.1 PCIe device. You can enable it with 'pcie_aspm=force' [ 4.779800] thunder_pcie_probe: ECAM5 CFG BASE 0x949000000000 gser_base0:ffff00001d000000 [ 4.787976] thunder_pcie_probe: ECAM5 CFG BASE 0x949000000000 gser_base1:ffff00002a800000 [ 4.796143] PCI host bridge /pcie5@0x9490,00000000 ranges: [ 4.801623] MEM 0x931000000000..0x93100fffffff -> 0x931000000000 [ 4.807791] MEM 0x910000000000..0x917fffffffff -> 0x910000000000 [ 4.814039] thunder-pcie 949000000000.pcie5: PCI host bridge to bus 0005:00 [ 4.820997] pci_bus 0005:00: root bus resource [bus 00-ff] [ 4.826471] pci_bus 0005:00: root bus resource [mem 0x931000000000-0x93100fffffff] [ 4.834032] pci_bus 0005:00: root bus resource [mem 0x910000000000-0x917fffffffff] [ 4.842588] thunder_pcie_probe: ECAM6 CFG BASE 0x94a000000000 gser_base0:ffff00001d000000 [ 4.850763] thunder_pcie_probe: ECAM6 CFG BASE 0x94a000000000 gser_base1:ffff00002a800000 [ 4.858925] PCI host bridge /pcie6@0x94a0,00000000 ranges: [ 4.864406] MEM 0x932000000000..0x93200fffffff -> 0x932000000000 [ 4.870578] MEM 0x943000000000..0x9430ffffffff -> 0x943000000000 [ 4.876823] thunder-pcie 94a000000000.pcie6: PCI host bridge to bus 0006:00 [ 4.883782] pci_bus 0006:00: root bus resource [bus 00-ff] [ 4.889256] pci_bus 0006:00: root bus resource [mem 0x932000000000-0x93200fffffff] [ 4.896816] pci_bus 0006:00: root bus resource [mem 0x943000000000-0x9430ffffffff] [ 5.910186] pci 0006:01:00.0: disabling ASPM on pre-1.1 PCIe device. You can enable it with 'pcie_aspm=force' [ 5.920387] thunder_pcie_probe: ECAM7 CFG BASE 0x94b000000000 gser_base0:ffff00001d000000 [ 5.928552] thunder_pcie_probe: ECAM7 CFG BASE 0x94b000000000 gser_base1:ffff00002a800000 [ 5.936725] PCI host bridge /pcie7@0x94b0,00000000 ranges: [ 5.942206] MEM 0x933000000000..0x93300fffffff -> 0x933000000000 [ 5.948374] MEM 0x918000000000..0x91ffffffffff -> 0x918000000000 [ 5.954624] thunder-pcie 94b000000000.pcie7: PCI host bridge to bus 0007:00 [ 5.961582] pci_bus 0007:00: root bus resource [bus 00-ff] [ 5.967056] pci_bus 0007:00: root bus resource [mem 0x933000000000-0x93300fffffff] [ 5.974616] pci_bus 0007:00: root bus resource [mem 0x918000000000-0x91ffffffffff] [ 5.982530] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c0000000 gser_base0:ffff00001d000000 [ 5.990705] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c0000000 gser_base1:ffff00002a800000 [ 5.998868] PCI host bridge /pem0@0x87e0,c0000000 ranges: [ 6.004265] No bus range found for /pem0@0x87e0,c0000000, using [bus 00-ff] [ 6.011395] MEM 0x881008000000..0x88100fffffff -> 0x08000000 [ 6.017217] MEM 0x882010000000..0x882017ffffff -> 0x10000000 [ 6.023042] IO 0x883000000000..0x883007ffffff -> 0x00000000 [ 6.028861] Requested IO range too big, new size set to 64K [ 6.034424] I/O range found for /pem0@0x87e0,c0000000. Please provide an io_base pointer to save CPU base address [ 6.044721] thunder-pcie: probe of 87e0c0000000.pem0 failed with error -22 [ 6.051763] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c1000000 gser_base0:ffff00001d000000 [ 6.059927] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c1000000 gser_base1:ffff00002a800000 [ 6.068100] PCI host bridge /pem1@0x87e0,c1000000 ranges: [ 6.073496] No bus range found for /pem1@0x87e0,c1000000, using [bus 00-ff] [ 6.080629] MEM 0x885020000000..0x885027ffffff -> 0x20000000 [ 6.086450] MEM 0x886028000000..0x88602fffffff -> 0x28000000 [ 6.092279] IO 0x887000000000..0x887007ffffff -> 0x00000000 [ 6.098098] Requested IO range too big, new size set to 64K [ 6.103665] I/O range found for /pem1@0x87e0,c1000000. Please provide an io_base pointer to save CPU base address [ 6.113963] thunder-pcie: probe of 87e0c1000000.pem1 failed with error -22 [ 6.120998] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c2000000 gser_base0:ffff00001d000000 [ 6.129161] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c2000000 gser_base1:ffff00002a800000 [ 6.137334] PCI host bridge /pem2@0x87e0,c2000000 ranges: [ 6.142728] No bus range found for /pem2@0x87e0,c2000000, using [bus 00-ff] [ 6.149853] MEM 0x889030000000..0x889037ffffff -> 0x30000000 [ 6.155683] MEM 0x88a038000000..0x88a03fffffff -> 0x38000000 [ 6.161512] IO 0x88b000000000..0x88b007ffffff -> 0x00000000 [ 6.167332] Requested IO range too big, new size set to 64K [ 6.172898] I/O range found for /pem2@0x87e0,c2000000. Please provide an io_base pointer to save CPU base address [ 6.183194] thunder-pcie: probe of 87e0c2000000.pem2 failed with error -22 [ 6.190228] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c3000000 gser_base0:ffff00001d000000 [ 6.198391] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c3000000 gser_base1:ffff00002a800000 [ 6.206564] PCI host bridge /pem3@0x87e0,c3000000 ranges: [ 6.211959] No bus range found for /pem3@0x87e0,c3000000, using [bus 00-ff] [ 6.219084] MEM 0x891040000000..0x891047ffffff -> 0x40000000 [ 6.224914] MEM 0x892048000000..0x89204fffffff -> 0x48000000 [ 6.230742] IO 0x893000000000..0x893007ffffff -> 0x00000000 [ 6.236561] Requested IO range too big, new size set to 64K [ 6.242128] I/O range found for /pem3@0x87e0,c3000000. Please provide an io_base pointer to save CPU base address [ 6.252423] thunder-pcie: probe of 87e0c3000000.pem3 failed with error -22 [ 6.259442] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c4000000 gser_base0:ffff00001d000000 [ 6.267621] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c4000000 gser_base1:ffff00002a800000 [ 6.275792] PCI host bridge /pem4@0x87e0,c4000000 ranges: [ 6.281185] No bus range found for /pem4@0x87e0,c4000000, using [bus 00-ff] [ 6.288311] MEM 0x895050000000..0x895057ffffff -> 0x50000000 [ 6.294140] MEM 0x896058000000..0x89605fffffff -> 0x58000000 [ 6.299961] IO 0x897000000000..0x897007ffffff -> 0x00000000 [ 6.305788] Requested IO range too big, new size set to 64K [ 6.311355] I/O range found for /pem4@0x87e0,c4000000. Please provide an io_base pointer to save CPU base address [ 6.321651] thunder-pcie: probe of 87e0c4000000.pem4 failed with error -22 [ 6.328671] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c5000000 gser_base0:ffff00001d000000 [ 6.336849] thunder_pcie_probe: ECAM0 CFG BASE 0x87e0c5000000 gser_base1:ffff00002a800000 [ 6.345019] PCI host bridge /pem5@0x87e0,c5000000 ranges: [ 6.350413] No bus range found for /pem5@0x87e0,c5000000, using [bus 00-ff] [ 6.357538] MEM 0x899060000000..0x899067ffffff -> 0x60000000 [ 6.363367] MEM 0x89a068000000..0x89a06fffffff -> 0x68000000 [ 6.369187] IO 0x89b000000000..0x89b007ffffff -> 0x00000000 [ 6.375014] Requested IO range too big, new size set to 64K [ 6.380580] I/O range found for /pem5@0x87e0,c5000000. Please provide an io_base pointer to save CPU base address [ 6.390876] thunder-pcie: probe of 87e0c5000000.pem5 failed with error -22 [ 6.397895] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c0000000 gser_base0:ffff00001d000000 [ 6.406074] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c0000000 gser_base1:ffff00002a800000 [ 6.414245] PCI host bridge /pem6@0x97e0,c0000000 ranges: [ 6.419631] No bus range found for /pem6@0x97e0,c0000000, using [bus 00-ff] [ 6.426764] MEM 0x981008000000..0x98100fffffff -> 0x08000000 [ 6.432593] MEM 0x982010000000..0x982017ffffff -> 0x10000000 [ 6.438414] IO 0x983000000000..0x983007ffffff -> 0x00000000 [ 6.444240] Requested IO range too big, new size set to 64K [ 6.449799] I/O range found for /pem6@0x97e0,c0000000. Please provide an io_base pointer to save CPU base address [ 6.460096] thunder-pcie: probe of 97e0c0000000.pem6 failed with error -22 [ 6.467115] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c1000000 gser_base0:ffff00001d000000 [ 6.475293] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c1000000 gser_base1:ffff00002a800000 [ 6.483462] PCI host bridge /pem7@0x97e0,c1000000 ranges: [ 6.488849] No bus range found for /pem7@0x97e0,c1000000, using [bus 00-ff] [ 6.495982] MEM 0x985020000000..0x985027ffffff -> 0x20000000 [ 6.501811] MEM 0x986028000000..0x98602fffffff -> 0x28000000 [ 6.507631] IO 0x987000000000..0x987007ffffff -> 0x00000000 [ 6.513459] Requested IO range too big, new size set to 64K [ 6.519019] I/O range found for /pem7@0x97e0,c1000000. Please provide an io_base pointer to save CPU base address [ 6.529315] thunder-pcie: probe of 97e0c1000000.pem7 failed with error -22 [ 6.536349] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c2000000 gser_base0:ffff00001d000000 [ 6.544522] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c2000000 gser_base1:ffff00002a800000 [ 6.552692] PCI host bridge /pem8@0x97e0,c2000000 ranges: [ 6.558078] No bus range found for /pem8@0x97e0,c2000000, using [bus 00-ff] [ 6.565212] MEM 0x989030000000..0x989037ffffff -> 0x30000000 [ 6.571041] MEM 0x98a038000000..0x98a03fffffff -> 0x38000000 [ 6.576862] IO 0x98b000000000..0x98b007ffffff -> 0x00000000 [ 6.582688] Requested IO range too big, new size set to 64K [ 6.588247] I/O range found for /pem8@0x97e0,c2000000. Please provide an io_base pointer to save CPU base address [ 6.598543] thunder-pcie: probe of 97e0c2000000.pem8 failed with error -22 [ 6.605577] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c3000000 gser_base0:ffff00001d000000 [ 6.613752] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c3000000 gser_base1:ffff00002a800000 [ 6.621922] PCI host bridge /pem9@0x97e0,c3000000 ranges: [ 6.627308] No bus range found for /pem9@0x97e0,c3000000, using [bus 00-ff] [ 6.634440] MEM 0x991040000000..0x991047ffffff -> 0x40000000 [ 6.640269] MEM 0x992048000000..0x99204fffffff -> 0x48000000 [ 6.646090] IO 0x993000000000..0x993007ffffff -> 0x00000000 [ 6.651917] Requested IO range too big, new size set to 64K [ 6.657476] I/O range found for /pem9@0x97e0,c3000000. Please provide an io_base pointer to save CPU base address [ 6.667773] thunder-pcie: probe of 97e0c3000000.pem9 failed with error -22 [ 6.674806] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c4000000 gser_base0:ffff00001d000000 [ 6.682979] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c4000000 gser_base1:ffff00002a800000 [ 6.691149] PCI host bridge /pem10@0x97e0,c4000000 ranges: [ 6.696622] No bus range found for /pem10@0x97e0,c4000000, using [bus 00-ff] [ 6.703842] MEM 0x995050000000..0x995057ffffff -> 0x50000000 [ 6.709663] MEM 0x996058000000..0x99605fffffff -> 0x58000000 [ 6.715492] IO 0x997000000000..0x997007ffffff -> 0x00000000 [ 6.721319] Requested IO range too big, new size set to 64K [ 6.726878] I/O range found for /pem10@0x97e0,c4000000. Please provide an io_base pointer to save CPU base address [ 6.737261] thunder-pcie: probe of 97e0c4000000.pem10 failed with error -22 [ 6.744382] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c5000000 gser_base0:ffff00001d000000 [ 6.752555] thunder_pcie_probe: ECAM0 CFG BASE 0x97e0c5000000 gser_base1:ffff00002a800000 [ 6.760726] PCI host bridge /pem11@0x97e0,c5000000 ranges: [ 6.766199] No bus range found for /pem11@0x97e0,c5000000, using [bus 00-ff] [ 6.773418] MEM 0x999060000000..0x999067ffffff -> 0x60000000 [ 6.779240] MEM 0x99a068000000..0x99a06fffffff -> 0x68000000 [ 6.785068] IO 0x99b000000000..0x99b007ffffff -> 0x00000000 [ 6.790896] Requested IO range too big, new size set to 64K [ 6.796455] I/O range found for /pem11@0x97e0,c5000000. Please provide an io_base pointer to save CPU base address [ 6.806838] thunder-pcie: probe of 97e0c5000000.pem11 failed with error -22 [ 6.814142] gti: thunderx-gti, ver 1.0 [ 6.818323] Serial: 8250/16550 driver, 32 ports, IRQ sharing enabled [ 6.828229] of_dma_request_slave_channel: dma-names property of node '/soc/serial@87e0,24000000' missing or empty [ 6.838492] uart-pl011 87e024000000.serial: no DMA platform data [ 6.844494] of_dma_request_slave_channel: dma-names property of node '/soc/serial@87e0,25000000' missing or empty [ 6.854746] uart-pl011 87e025000000.serial: no DMA platform data [ 6.865855] brd: module loaded [ 6.871395] loop: module loaded [ 6.875141] ahci 0001:00:08.0: SSS flag set, parallel bus scan disabled [ 6.881780] ahci 0001:00:08.0: AHCI 0001.0300 32 slots 1 ports 6 Gbps 0x1 impl SATA mode [ 6.889860] ahci 0001:00:08.0: flags: 64bit ncq sntf ilck stag pm led clo only pmp fbs pio slum part ccc apst [ 6.899857] ahci 0001:00:08.0: port 0 is not capable of FBS [ 6.905930] scsi host0: ahci [ 6.908992] ata1: SATA max UDMA/133 abar m2097152@0x814000000000 port 0x814000000100 irq 5 [ 6.917394] ahci 0001:00:09.0: SSS flag set, parallel bus scan disabled [ 6.924017] ahci 0001:00:09.0: AHCI 0001.0300 32 slots 1 ports 6 Gbps 0x1 impl SATA mode [ 6.932100] ahci 0001:00:09.0: flags: 64bit ncq sntf ilck stag pm led clo only pmp fbs pio slum part ccc apst [ 6.942093] ahci 0001:00:09.0: port 0 is not capable of FBS [ 6.948105] scsi host1: ahci [ 6.951109] ata2: SATA max UDMA/133 abar m2097152@0x815000000000 port 0x815000000100 irq 32 [ 6.959572] ahci 0001:00:0a.0: SSS flag set, parallel bus scan disabled [ 6.966196] ahci 0001:00:0a.0: AHCI 0001.0300 32 slots 1 ports 6 Gbps 0x1 impl SATA mode [ 6.974284] ahci 0001:00:0a.0: flags: 64bit ncq sntf ilck stag pm led clo only pmp fbs pio slum part ccc apst [ 6.984279] ahci 0001:00:0a.0: port 0 is not capable of FBS [ 6.990290] scsi host2: ahci [ 6.993282] ata3: SATA max UDMA/133 abar m2097152@0x816000000000 port 0x816000000100 irq 6 [ 7.001674] ahci 0001:00:0b.0: SSS flag set, parallel bus scan disabled [ 7.008284] ahci 0001:00:0b.0: AHCI 0001.0300 32 slots 1 ports 6 Gbps 0x1 impl SATA mode [ 7.016377] ahci 0001:00:0b.0: flags: 64bit ncq sntf ilck stag pm led clo only pmp fbs pio slum part ccc apst [ 7.026370] ahci 0001:00:0b.0: port 0 is not capable of FBS [ 7.032373] scsi host3: ahci [ 7.035364] ata4: SATA max UDMA/133 abar m2097152@0x817000000000 port 0x817000000100 irq 33 [ 7.043886] ahci 0005:00:08.0: SSS flag set, parallel bus scan disabled [ 7.050514] ahci 0005:00:08.0: AHCI 0001.0300 32 slots 1 ports 6 Gbps 0x1 impl SATA mode [ 7.058592] ahci 0005:00:08.0: flags: 64bit ncq sntf ilck stag pm led clo only pmp fbs pio slum part ccc apst [ 7.068585] ahci 0005:00:08.0: port 0 is not capable of FBS [ 7.074607] scsi host4: ahci [ 7.077599] ata5: SATA max UDMA/133 abar m2097152@0x914000000000 port 0x914000000100 irq 7 [ 7.086003] ahci 0005:00:09.0: SSS flag set, parallel bus scan disabled [ 7.092631] ahci 0005:00:09.0: AHCI 0001.0300 32 slots 1 ports 6 Gbps 0x1 impl SATA mode [ 7.100714] ahci 0005:00:09.0: flags: 64bit ncq sntf ilck stag pm led clo only pmp fbs pio slum part ccc apst [ 7.110707] ahci 0005:00:09.0: port 0 is not capable of FBS [ 7.116714] scsi host5: ahci [ 7.119705] ata6: SATA max UDMA/133 abar m2097152@0x915000000000 port 0x915000000100 irq 34 [ 7.128198] ahci 0005:00:0a.0: SSS flag set, parallel bus scan disabled [ 7.134826] ahci 0005:00:0a.0: AHCI 0001.0300 32 slots 1 ports 6 Gbps 0x1 impl SATA mode [ 7.142909] ahci 0005:00:0a.0: flags: 64bit ncq sntf ilck stag pm led clo only pmp fbs pio slum part ccc apst [ 7.152902] ahci 0005:00:0a.0: port 0 is not capable of FBS [ 7.158903] scsi host6: ahci [ 7.161905] ata7: SATA max UDMA/133 abar m2097152@0x916000000000 port 0x916000000100 irq 8 [ 7.170307] ahci 0005:00:0b.0: SSS flag set, parallel bus scan disabled [ 7.176922] ahci 0005:00:0b.0: AHCI 0001.0300 32 slots 1 ports 6 Gbps 0x1 impl SATA mode [ 7.185014] ahci 0005:00:0b.0: flags: 64bit ncq sntf ilck stag pm led clo only pmp fbs pio slum part ccc apst [ 7.195006] ahci 0005:00:0b.0: port 0 is not capable of FBS [ 7.201017] scsi host7: ahci [ 7.204009] ata8: SATA max UDMA/133 abar m2097152@0x917000000000 port 0x917000000100 irq 35 [ 7.212903] libphy: Fixed MDIO Bus: probed [ 7.217108] libphy: mdio-octeon: probed [ 7.221529] mdio-octeon 87e005003800.mdio: Version 1.0 [ 7.226716] libphy: mdio-octeon: probed [ 7.230599] mdio-octeon 87e005003880.mdio: Version 1.0 [ 7.235789] libphy: mdio-octeon: probed [ 7.239657] mdio-octeon 97e005003800.mdio: Version 1.0 [ 7.244848] libphy: mdio-octeon: probed [ 7.248674] mdio-octeon 97e005003880.mdio: Version 1.0 [ 7.253824] tun: Universal TUN/TAP device driver, 1.6 [ 7.258864] tun: (C) 1999-2004 Max Krasnyansky [ 7.260020] ata1: SATA link down (SStatus 0 SControl 300) [ 7.270522] thunder-BGX, ver 1.0 [ 7.273812] thunder-BGX 0000:01:10.0: BGX0 QLM mode: XFI [ 7.279329] thunder-BGX 0000:01:10.1: BGX1 QLM mode: XLAUI [ 7.285044] thunder-BGX 0004:01:10.0: BGX2 QLM mode: XFI [ 7.290560] thunder-nic, ver 1.0 [ 7.350035] ata3: SATA link down (SStatus 0 SControl 300) [ 7.390029] ata4: SATA link down (SStatus 0 SControl 300) [ 7.400407] thunder-nic 0002:01:00.0: SRIOV enabled, numer of VF available 2 [ 7.430018] ata5: SATA link down (SStatus 0 SControl 300) [ 7.470017] ata6: SATA link down (SStatus 0 SControl 300) [ 7.500053] ata2: SATA link up 6.0 Gbps (SStatus 133 SControl 300) [ 7.507847] ata2.00: ATA-8: WDC WD4000FYYZ-01UL1B2, 01.01K03, max UDMA/133 [ 7.514728] ata2.00: 7814037168 sectors, multi 0: LBA48 NCQ (depth 31/32), AA [ 7.520016] ata7: SATA link down (SStatus 0 SControl 300) [ 7.527441] thunder-nic 0006:01:00.0: SRIOV enabled, numer of VF available 1 [ 7.534629] ata2.00: configured for UDMA/133 [ 7.538988] thunder-nicvf, ver 1.0 [ 7.539134] scsi 1:0:0:0: Direct-Access ATA WDC WD4000FYYZ-0 1K03 PQ: 0 ANSI: 5 [ 7.539613] sd 1:0:0:0: [sda] 7814037168 512-byte logical blocks: (4.00 TB/3.63 TiB) [ 7.539728] sd 1:0:0:0: Attached scsi generic sg0 type 0 [ 7.539917] sd 1:0:0:0: [sda] Write Protect is off [ 7.540066] sd 1:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 7.560024] ata8: SATA link down (SStatus 0 SControl 300) [ 7.570358] sda: sda1 sda2 sda3 [ 7.571189] sd 1:0:0:0: [sda] Attached SCSI disk [ 7.590644] thunder-nicvf 0002:01:00.1: enabling device (0004 -> 0006) [ 7.620523] thunder-nicvf 0002:01:00.2: enabling device (0004 -> 0006) [ 7.650501] thunder-nicvf 0006:01:00.1: enabling device (0004 -> 0006) [ 7.680539] PPP generic driver version 2.4.2 [ 7.685044] VFIO - User Level meta-driver version: 0.3 [ 7.690834] xhci_hcd 0000:00:10.0: xHCI Host Controller [ 7.696059] xhci_hcd 0000:00:10.0: new USB bus registered, assigned bus number 1 [ 7.703915] usb usb1: New USB device found, idVendor=1d6b, idProduct=0002 [ 7.710704] usb usb1: New USB device strings: Mfr=3, Product=2, SerialNumber=1 [ 7.717914] usb usb1: Product: xHCI Host Controller [ 7.722784] usb usb1: Manufacturer: Linux 3.18.0-gdcccbc1-dirty xhci-hcd [ 7.729472] usb usb1: SerialNumber: 0000:00:10.0 [ 7.734399] hub 1-0:1.0: USB hub found [ 7.738170] hub 1-0:1.0: 1 port detected [ 7.742401] xhci_hcd 0000:00:10.0: xHCI Host Controller [ 7.747622] xhci_hcd 0000:00:10.0: new USB bus registered, assigned bus number 2 [ 7.755393] usb usb2: New USB device found, idVendor=1d6b, idProduct=0003 [ 7.762185] usb usb2: New USB device strings: Mfr=3, Product=2, SerialNumber=1 [ 7.769395] usb usb2: Product: xHCI Host Controller [ 7.774270] usb usb2: Manufacturer: Linux 3.18.0-gdcccbc1-dirty xhci-hcd [ 7.780965] usb usb2: SerialNumber: 0000:00:10.0 [ 7.785938] hub 2-0:1.0: USB hub found [ 7.789717] hub 2-0:1.0: 1 port detected [ 7.794000] xhci_hcd 0000:00:11.0: xHCI Host Controller [ 7.799225] xhci_hcd 0000:00:11.0: new USB bus registered, assigned bus number 3 [ 7.807225] usb usb3: New USB device found, idVendor=1d6b, idProduct=0002 [ 7.814015] usb usb3: New USB device strings: Mfr=3, Product=2, SerialNumber=1 [ 7.821230] usb usb3: Product: xHCI Host Controller [ 7.826095] usb usb3: Manufacturer: Linux 3.18.0-gdcccbc1-dirty xhci-hcd [ 7.832787] usb usb3: SerialNumber: 0000:00:11.0 [ 7.837712] hub 3-0:1.0: USB hub found [ 7.841491] hub 3-0:1.0: 1 port detected [ 7.845693] xhci_hcd 0000:00:11.0: xHCI Host Controller [ 7.850924] xhci_hcd 0000:00:11.0: new USB bus registered, assigned bus number 4 [ 7.858523] usb usb4: New USB device found, idVendor=1d6b, idProduct=0003 [ 7.865308] usb usb4: New USB device strings: Mfr=3, Product=2, SerialNumber=1 [ 7.872521] usb usb4: Product: xHCI Host Controller [ 7.877387] usb usb4: Manufacturer: Linux 3.18.0-gdcccbc1-dirty xhci-hcd [ 7.884078] usb usb4: SerialNumber: 0000:00:11.0 [ 7.888972] hub 4-0:1.0: USB hub found [ 7.892750] hub 4-0:1.0: 1 port detected [ 7.896917] xhci_hcd 0004:00:10.0: xHCI Host Controller [ 7.902150] xhci_hcd 0004:00:10.0: new USB bus registered, assigned bus number 5 [ 7.909980] usb usb5: New USB device found, idVendor=1d6b, idProduct=0002 [ 7.916767] usb usb5: New USB device strings: Mfr=3, Product=2, SerialNumber=1 [ 7.923983] usb usb5: Product: xHCI Host Controller [ 7.928849] usb usb5: Manufacturer: Linux 3.18.0-gdcccbc1-dirty xhci-hcd [ 7.935541] usb usb5: SerialNumber: 0004:00:10.0 [ 7.940445] hub 5-0:1.0: USB hub found [ 7.944214] hub 5-0:1.0: 1 port detected [ 7.948406] xhci_hcd 0004:00:10.0: xHCI Host Controller [ 7.953637] xhci_hcd 0004:00:10.0: new USB bus registered, assigned bus number 6 [ 7.961207] usb usb6: New USB device found, idVendor=1d6b, idProduct=0003 [ 7.967984] usb usb6: New USB device strings: Mfr=3, Product=2, SerialNumber=1 [ 7.975201] usb usb6: Product: xHCI Host Controller [ 7.980072] usb usb6: Manufacturer: Linux 3.18.0-gdcccbc1-dirty xhci-hcd [ 7.986760] usb usb6: SerialNumber: 0004:00:10.0 [ 7.991665] hub 6-0:1.0: USB hub found [ 7.995434] hub 6-0:1.0: 1 port detected [ 8.000361] xhci_hcd 0004:00:11.0: xHCI Host Controller [ 8.005586] xhci_hcd 0004:00:11.0: new USB bus registered, assigned bus number 7 [ 8.013422] usb usb7: New USB device found, idVendor=1d6b, idProduct=0002 [ 8.020211] usb usb7: New USB device strings: Mfr=3, Product=2, SerialNumber=1 [ 8.027421] usb usb7: Product: xHCI Host Controller [ 8.032291] usb usb7: Manufacturer: Linux 3.18.0-gdcccbc1-dirty xhci-hcd [ 8.038979] usb usb7: SerialNumber: 0004:00:11.0 [ 8.043878] hub 7-0:1.0: USB hub found [ 8.047642] hub 7-0:1.0: 1 port detected [ 8.051853] xhci_hcd 0004:00:11.0: xHCI Host Controller [ 8.057073] xhci_hcd 0004:00:11.0: new USB bus registered, assigned bus number 8 [ 8.064654] usb usb8: New USB device found, idVendor=1d6b, idProduct=0003 [ 8.071439] usb usb8: New USB device strings: Mfr=3, Product=2, SerialNumber=1 [ 8.078649] usb usb8: Product: xHCI Host Controller [ 8.083520] usb usb8: Manufacturer: Linux 3.18.0-gdcccbc1-dirty xhci-hcd [ 8.090212] usb usb8: SerialNumber: 0004:00:11.0 [ 8.095124] hub 8-0:1.0: USB hub found [ 8.098886] hub 8-0:1.0: 1 port detected [ 8.103813] ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver [ 8.110375] ehci-pci: EHCI PCI platform driver [ 8.114869] ohci_hcd: USB 1.1 'Open' Host Controller (OHCI) Driver [ 8.121054] ohci-pci: OHCI PCI platform driver [ 8.125544] uhci_hcd: USB Universal Host Controller Interface driver [ 8.132115] usbcore: registered new interface driver usb-storage [ 8.138294] mousedev: PS/2 mouse device common for all mice [ 8.145036] device-mapper: uevent: version 1.0.3 [ 8.149963] device-mapper: ioctl: 4.28.0-ioctl (2014-09-17) initialised: dm-devel@redhat.com [ 8.159038] ledtrig-cpu: registered to indicate activity on CPUs [ 8.160050] usb 3-1: new high-speed USB device number 2 using xhci_hcd [ 8.171561] EFI Variables Facility v0.08 2004-May-17 [ 8.176759] pstore: Registered efi as persistent store backend [ 8.182776] TCP: cubic registered [ 8.186091] NET: Registered protocol family 17 [ 8.190595] Key type dns_resolver registered [ 8.195229] Loading compiled-in X.509 certificates [ 8.201682] Loaded X.509 cert 'Magrathea: Glacier signing key: 8f5d3440f3cec1b6a1a857b85f38455ec45a6b96' [ 8.211176] registered taskstats version 1 [ 8.216213] Key type trusted registered [ 8.220831] Key type encrypted registered [ 8.225602] AppArmor: AppArmor sha1 policy hashing enabled [ 8.231093] ima: No TPM chip found, activating TPM-bypass! [ 8.236612] evm: HMAC attrs: 0x1 [ 8.240828] drivers/rtc/hctosys.c: unable to open rtc device (rtc0) [ 8.247425] md: Waiting for all devices to be available before autodetect [ 8.254212] md: If you don't use raid, use raid=noautodetect [ 8.260309] md: Autodetecting RAID arrays. [ 8.264394] md: Scanned 0 and added 0 devices. [ 8.268824] md: autorun ... [ 8.271617] md: ... autorun DONE. [ 8.293522] EXT3-fs (sda3): error: couldn't mount because of unsupported optional features (240) [ 8.302591] EXT2-fs (sda3): error: couldn't mount because of unsupported optional features (240) [ 8.320943] usb 3-1: New USB device found, idVendor=0b95, idProduct=772b [ 8.327633] usb 3-1: New USB device strings: Mfr=1, Product=2, SerialNumber=3 [ 8.334764] usb 3-1: Product: AX88772C [ 8.338502] usb 3-1: Manufacturer: ASIX [ 8.343373] usb 3-1: SerialNumber: 0000C2 [ 8.620550] EXT4-fs (sda3): mounted filesystem with ordered data mode. Opts: (null) [ 8.628210] VFS: Mounted root (ext4 filesystem) on device 8:3. [ 8.647294] devtmpfs: mounted [ 8.650304] Freeing unused kernel memory: 832K (ffff800000d90000 - ffff800000e60000) [ 9.010755] random: systemd urandom read with 36 bits of entropy available [ 9.021374] systemd[1]: systemd 216 running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ -LZ4 -SECCOMP +BLKID +ELFUTILS +KMOD +IDN) [ 9.039536] systemd[1]: Detected architecture 'arm64'. Welcome to Fedora 21 (Twenty One)! [ 9.256111] NET: Registered protocol family 10 [ 9.262111] systemd[1]: Inserted module 'ipv6' [ 9.266929] systemd[1]: Set hostname to . [ 10.125066] systemd[1]: Configuration file /usr/lib/systemd/system/auditd.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway. [ 10.219054] systemd[1]: Expecting device dev-ttyAMA0.device... Expecting device dev-ttyAMA0.device... [ 10.240108] systemd[1]: Starting Forward Password Requests to Wall Directory Watch. [ 10.247876] systemd[1]: Started Forward Password Requests to Wall Directory Watch. [ 10.255514] systemd[1]: Starting Arbitrary Executable File Formats File System Automount Point. [ OK ] Set up automount Arbitrary Executab...ats File System Automount Point. [ 10.290080] systemd[1]: Set up automount Arbitrary Executable File Formats File System Automount Point. [ 10.299491] systemd[1]: Starting Swap. [ OK ] Reached target Swap. [ 10.320078] systemd[1]: Reached target Swap. [ 10.324358] systemd[1]: Starting Root Slice. [ OK ] Created slice Root Slice. [ 10.550088] systemd[1]: Created slice Root Slice. [ 10.554803] systemd[1]: Starting /dev/initctl Compatibility Named Pipe. [ OK ] Listening on /dev/initctl Compatibility Named Pipe. [ 10.580100] systemd[1]: Listening on /dev/initctl Compatibility Named Pipe. [ 10.587068] systemd[1]: Starting Delayed Shutdown Socket. [ OK ] Listening on Delayed Shutdown Socket. [ 10.610060] systemd[1]: Listening on Delayed Shutdown Socket. [ 10.615814] systemd[1]: Starting Device-mapper event daemon FIFOs. [ OK ] Listening on Device-mapper event daemon FIFOs. [ 10.640070] systemd[1]: Listening on Device-mapper event daemon FIFOs. [ 10.646606] systemd[1]: Starting LVM2 metadata daemon socket. [ OK ] Listening on LVM2 metadata daemon socket. [ 10.670081] systemd[1]: Listening on LVM2 metadata daemon socket. [ 10.676195] systemd[1]: Starting udev Control Socket. [ OK ] Listening on udev Control Socket. [ 10.700075] systemd[1]: Listening on udev Control Socket. [ 10.705495] systemd[1]: Starting udev Kernel Socket. [ OK ] Listening on udev Kernel Socket. [ 10.730061] systemd[1]: Listening on udev Kernel Socket. [ 10.735382] systemd[1]: Starting User and Session Slice. [ OK ] Created slice User and Session Slice. [ 10.760078] systemd[1]: Created slice User and Session Slice. [ 10.765841] systemd[1]: Starting Journal Socket. [ OK ] Listening on Journal Socket. [ 10.790070] systemd[1]: Listening on Journal Socket. [ 10.795075] systemd[1]: Starting System Slice. [ OK ] Created slice System Slice. [ 10.820075] systemd[1]: Created slice System Slice. [ 10.824974] systemd[1]: Starting Journal Socket (/dev/log). [ 10.831793] systemd[1]: Starting Monitoring of LVM2 mirrors, snapshots etc. using dmeventd or progress polling... Starting Monitoring of LVM2 mirrors... dmeventd or progress polling... [ 10.861608] systemd[1]: Started Device-Mapper Multipath Device Controller. [ 10.868663] systemd[1]: Mounting Debug File System... Mounting Debug File System... [ 10.891344] systemd[1]: Starting udev Coldplug all Devices... Starting udev Coldplug all Devices... [ 10.911484] systemd[1]: Mounting POSIX Message Queue File System... Mounting POSIX Message Queue File System... [ 10.931616] systemd[1]: Mounting Huge Pages File System... Mounting Huge Pages File System... [ 10.951613] systemd[1]: Starting Create list of required static device nodes for the current kernel... Starting Create list of required st... nodes for the current kernel... [ 10.981539] systemd[1]: Mounting RPC Pipe File System... Mounting RPC Pipe File System... [ 11.114963] systemd[1]: Starting system-getty.slice. [ OK ] Created slice system-getty.slice. [ 11.140107] systemd[1]: Created slice system-getty.slice. [ 11.145528] systemd[1]: Starting system-serial\\x2dgetty.slice. [ OK ] Created slice system-serial\\x2dgetty.slice. [ 11.170085] systemd[1]: Created slice system-serial\\x2dgetty.slice. [ 11.176414] systemd[1]: Starting Collect Read-Ahead Data... Starting Collect Read-Ahead Data... [ 11.201539] systemd[1]: Starting Replay Read-Ahead Data... Starting Replay Read-Ahead Data... [ 11.221533] systemd[1]: Starting Slices. [ 11.224310] systemd-readahead[660]: Bumped block_nr parameter of 8:0 to 20480. This is a temporary hack and should be removed one day. [ OK ] Reached target Slices. [ 11.250108] systemd[1]: Reached target Slices. [ 11.262437] systemd[1]: Mounting Temporary Directory... Mounting Temporary Directory... [ 11.292712] systemd[1]: tmp.mount: Directory /tmp to mount over is not empty, mounting anyway. [ OK ] Mounted RPC Pipe File System. [ 11.320071] systemd[1]: Mounted RPC Pipe File System. [ OK ] Mounted Huge Pages File System. [ 11.340066] systemd[1]: Mounted Huge Pages File System. [ OK ] Mounted POSIX Message Queue File System. [ 11.360058] systemd[1]: Mounted POSIX Message Queue File System. [ OK ] Mounted Debug File System. [ 11.380067] systemd[1]: Mounted Debug File System. [ OK ] Mounted Temporary Directory. [ 11.400087] systemd[1]: Mounted Temporary Directory. [ OK ] Started Collect Read-Ahead Data. [ 11.420068] systemd[1]: Started Collect Read-Ahead Data. [ OK ] Started Replay Read-Ahead Data. [ 11.440058] systemd[1]: Started Replay Read-Ahead Data. [ OK ] Listening on Journal Socket (/dev/log). [ 11.460070] systemd[1]: Listening on Journal Socket (/dev/log). [ OK ] Started Create list of required sta...ce nodes for the current kernel. [ 11.490068] systemd[1]: Started Create list of required static device nodes for the current kernel. [ OK ] Started udev Coldplug all Devices. [ 11.530092] systemd[1]: Started udev Coldplug all Devices. [ 11.551726] systemd[1]: Starting udev Wait for Complete Device Initialization... Starting udev Wait for Complete Device Initialization... [ 11.581566] systemd[1]: Starting LVM2 metadata daemon... Starting LVM2 metadata daemon... [ OK ] Started LVM2 metadata daemon. [ 11.620072] systemd[1]: Started LVM2 metadata daemon. [ 11.625229] systemd[1]: Starting Journal Service... Starting Journal Service... [ 11.651539] systemd[1]: Starting Remount Root and Kernel File Systems... Starting Remount Root and Kernel File Systems... [ 11.681565] systemd[1]: Starting Setup Virtual Console... Starting Setup Virtual Console... [ 12.709128] systemd[1]: Started Set Up Additional Binary Formats. [ 12.729705] systemd[1]: Started Load legacy module configuration. [ 12.759745] systemd[1]: Started Load Kernel Modules. [ 12.764770] systemd[1]: Starting Apply Kernel Variables... Starting Apply Kernel Variables... [ 12.792215] systemd[1]: Mounted Configuration File System. [ 12.797765] systemd[1]: Mounting FUSE Control File System... Mounting FUSE Control File System... [ OK ] Started Remount Root and Kernel File Systems. [ 12.840090] systemd[1]: Started Remount Root and Kernel File Systems. [ OK ] Started Apply Kernel Variables. [ 12.870079] systemd[1]: Started Apply Kernel Variables. [ OK ] Mounted FUSE Control File System. [ 12.890142] systemd[1]: Mounted FUSE Control File System. [ 12.904405] systemd[1]: Started Import network configuration from initramfs. [ 12.911518] systemd[1]: Started First Boot Wizard. [ 12.916317] systemd[1]: Starting Load/Save Random Seed... Starting Load/Save Random Seed... [ 12.960670] systemd[1]: Started Rebuild Hardware Database. [ 12.966200] systemd[1]: Started Create System Users. [ 12.971299] systemd[1]: Starting Create Static Device Nodes in /dev... Starting Create Static Device Nodes in /dev... [ 12.991497] systemd[1]: Started Rebuild Dynamic Linker Cache. [ 12.997297] systemd[1]: Starting Configure read-only root support... Starting Configure read-only root support... [ OK ] Started Monitoring of LVM2 mirrors,...ng dmeventd or progress polling. [ 13.040112] systemd[1]: Started Monitoring of LVM2 mirrors, snapshots etc. using dmeventd or progress polling. [ OK ] Started Setup Virtual Console. [ 13.070074] systemd[1]: Started Setup Virtual Console. [ OK ] Started Load/Save Random Seed. [ 13.090069] systemd[1]: Started Load/Save Random Seed. [ OK ] Started Create Static Device Nodes in /dev. [ 13.110106] systemd[1]: Started Create Static Device Nodes in /dev. [ 13.127670] systemd[1]: Starting udev Kernel Device Manager... Starting udev Kernel Device Manager... [ 13.151606] systemd[1]: Starting Local File Systems (Pre). [ OK ] Reached target Local File Systems (Pre). [ 13.180071] systemd[1]: Reached target Local File Systems (Pre). [ 13.186152] systemd[1]: Mounting NFSD configuration filesystem... Mounting NFSD configuration filesystem... [ 13.211506] systemd[1]: Starting Show Plymouth Boot Screen... Starting Show Plymouth Boot Screen... [ 13.243976] Installing knfsd (copyright (C) 1996 okir@monad.swb.de). [ 13.252106] systemd[1]: Mounted NFSD configuration filesystem. [ 13.359144] systemd[1]: Started Configure read-only root support. [ 14.063882] systemd[1]: Started udev Kernel Device Manager. [ 14.347716] systemd[1]: Found device /dev/ttyAMA0. [ 14.543916] shpchp: Standard Hot Plug PCI Controller Driver version: 0.4 [ 14.676676] thunder-nicvf 0002:01:00.2 enP2p1s0f2: renamed from eth1 [ 14.870703] thunder-nicvf 0002:01:00.1 enP2p1s0f1: renamed from eth0 [ 14.950492] thunder-nicvf 0006:01:00.1 enP6p1s0f1: renamed from eth2 [ 15.884562] systemd[1]: Started Journal Service. [ 15.898456] systemd-journald[673]: Received request to flush runtime journal from PID 1 [ 16.161623] random: nonblocking pool is initialized [ 17.441694] asix 3-1:1.0 eth0: register 'asix' at usb-0000:00:11.0-1, ASIX AX88772B USB 2.0 Ethernet, f0:1e:34:00:27:3e [ 17.452539] usbcore: registered new interface driver asix [ 17.477546] asix 3-1:1.0 enp0s17u1: renamed from eth0 ÿ[ OK ] Mounted NFSD configuration filesystem. [ OK ] Started Configure read-only root support. [ OK ] Started udev Kernel Device Manager. [ OK ] Found device /dev/ttyAMA0. G[ OK ] Started Journal Service. Starting Flush Journal to Persistent Storage... [ OK ] Started Flush Journal to Persistent Storage. [ OK ] Started udev Wait for Complete Device Initialization. Starting Activation of DM RAID sets... [ OK ] Started Show Plymouth Boot Screen. [ OK ] Reached target Paths. [ OK ] Started Activation of DM RAID sets. [ OK ] Reached target Local File Systems. Starting Create Volatile Files and Directories... Starting Tell Plymouth To Write Out Runtime Data... [ OK ] Reached target Encrypted Volumes. [ OK ] Started Tell Plymouth To Write Out Runtime Data. [ OK ] Started Create Volatile Files and Directories. Starting Security Auditing Service... [ 17.790258] audit: type=1305 audit(17.790:2): audit_pid=957 old=0 auid=4294967295 ses=4294967295 res=1 [ OK ] Started Security Auditing Service. Starting Update UTMP about System Boot/Shutdown... [ OK ] Started Update UTMP about System Boot/Shutdown. [ OK ] Reached target System Initialization. [ OK ] Listening on Cockpit Web Server Socket. [ OK ] Listening on RPCbind Server Activation Socket. [ OK ] Listening on PC/SC Smart Card Daemon Activation Socket. [ OK ] Reached target Timers. [ OK ] Listening on D-Bus System Message Bus Socket. [ OK ] Reached target Sockets. [ OK ] Reached target Basic System. Starting Hardware RNG Entropy Gatherer Daemon... [ OK ] Started Hardware RNG Entropy Gatherer Daemon. Starting Resets System Activity Logs... Starting ABRT Automated Bug Reporting Tool... [ OK ] Started ABRT Automated Bug Reporting Tool. Starting ABRT kernel log watcher... [ OK ] Started ABRT kernel log watcher. Starting Install ABRT coredump hook... Starting Network Manager... Starting Preprocess NFS configuration... Starting GSSAPI Proxy Daemon... Starting Self Monitoring and Reporting Technology (SMART) Daemon... [ OK ] Started Self Monitoring and Reporting Technology (SMART) Daemon. Starting irqbalance daemon... [ OK ] Started irqbalance daemon. Starting System Logging Service... Starting NTP client/server... Starting D-Bus System Message Bus... [ OK ] Started D-Bus System Message Bus. Starting Login Service... [ OK ] Started Resets System Activity Logs. [ OK ] Started Install ABRT coredump hook. [ OK ] Started Preprocess NFS configuration. [ OK ] Started GSSAPI Proxy Daemon. [ OK ] Started System Logging Service. [ OK ] Started Login Service. [ OK ] Started NTP client/server. [ OK ] Started Network Manager. [ OK ] Reached target Network. Starting rolekit - role server... Starting Notify NFS peers of a restart... Starting OpenSSH server daemon... [ OK ] Started OpenSSH server daemon. [ OK ] Started Notify NFS peers of a restart. [ OK ] Reached target NFS client services. [ OK ] Reached target Remote File Systems (Pre). [ OK ] Reached target Remote File Systems. Starting Permit User Sessions... [ OK ] Started Permit User Sessions. Starting Job spooling tools... [ OK ] Started Job spooling tools. Starting Terminate Plymouth Boot Screen... Starting Wait for Plymouth Boot Screen to Quit... Fedora release 21 (Twenty One) Kernel 3.18.0-gdcccbc1-dirty on an aarch64 (ttyAMA0) Tfedora login: Fedora release 21 (Twenty One) Kernel 3.18.0-gdcccbc1-dirty on an aarch64 (ttyAMA0) "},"notes/as_title_cpu2.html":{"url":"notes/as_title_cpu2.html","title":"PPC","keywords":"","body":"如题 "},"notes/CPU_PPC启动多核Linux_流程和内存映射.html":{"url":"notes/CPU_PPC启动多核Linux_流程和内存映射.html","title":"PPC启动多核linux: 流程和内存映射","keywords":"","body":" PPC e500ms框图 PPC Memory Map知识 虚实地址转换与寄存器知识 主核linux启动 uboot阶段 kernel阶段 从核linux启动 uboot阶段 linux阶段 PPC e500ms框图 PPC Memory Map知识 这里指内存及各种外设组件在CPU地址空间的map. LAW寄存器组: 共32个. 负责各组件在36位物理地址空间内的排布 组件ID是预定义的, 有点像mac地址, 代表组件在SOC级的bus中, 传输\"地址寻址\"的事务. 传输事务有源和目的. 有的组件可以是源, 也可以是目的. 有的组件只能是其中一种. 能做源的组件, 都可以主动发起访问, 比如CPU, 和PCIe以及DMA控制器. 主要ID有: Source/Target ID Transaction source Transaction target Notes 0x00 PCI-Express 1 PCI-Express 1 — 0x0F Reserved Local Space 0x10 Reserved Memory Complex 1 DDR controller 1 or CPC1 SRAM 0x18 Buffer Manager (control) Buffer Manager Software Portal 0x1F Reserved eLBC — 0x3C Queue Manager (control) Queue Manager Software Portal — 0x48 Pre-boot loader (PBL) Reserved — 0x70 DMA 1 Reserved 0x80 Core 0 (instruction) Reserved — 0x81 Core 0 (data) Reserved 0xC0 Frame Manager 1 ID 1 Reserved — LAW不负责虚拟地址到物理地址的转换, 它只是负责物理地址的重新排布. 典型的物理地址布局 CCSRBAR负责映射soc寄存器基地址 boot space 转换 每个核在复位的时候, 都有个4K的MMU映射在0x0_FFFF_F000, 并且从物理地址0x0_FFFF_FFFC执行代码. 意思是, 在复位的时候, MMU有个默认的1:1 4K映射. 第一条取指从地址0x0_FFFF_FFE0开始(burst read). boot space 转换的作用是提供映射不同的组件到0x0_FFFF_F000 如果一个地址落在boot window(8 Mbytes at 0x0_FF80_0000 to 0x0_FFFF_FFFF)里面, boot space转换就生效(但需要enable), 这三个寄存器控制其map到哪里: BSTRH, BSTRL, and BSTAR, 转换到4GB-BSTAR[SIZE] to 4GB-1 虚实地址转换与寄存器知识 MSR L1 MMU也有tlb, 但对软件不可见; 硬件把L1的TLB作为L2的TBL的缓存(inclusive cache), 自动管理. 两类TLB: TLB0和TLB0, 是并行工作的; TLB0的page size是固定4K, 比TLB1少了些通用性, 但entry数多. 多4K page的kernel应该比较友好. 多个tlb hit到一个虚拟地址是错误的.这种情况下, tlb会返回无效地址, 并且产生machine check 虚拟地址转换为物理地址过程:tlb匹配过程:tlb的标记位含义: 主核linux启动 uboot阶段 uboot启动到命令行后, 主核tlb如下:其代码运行在0x1ff0944c范围 idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 1 0 0x00000000 -- 0V-- a 0-----UrUwUxSrSwSx -> 0x1:00000000 --------------Iprot 2 0 0x40000000 -- 0V-- a 0-----UrUwUxSrSwSx -> 0x1:40000000 --------------Iprot 5 0 0xff800000 -- 0V-- 6 0-I-G-------SrSwSx -> 0xf:ff800000 --------------Iprot 6 0 0xffc00000 -- 0V-- 6 0-I-G-------SrSwSx -> 0xf:ffc00000 --------------Iprot 8 0 0xff200000 -- 0V-- 5 0-I-G-------SrSw-- -> 0xf:ff200000 --------------Iprot 9 0 0xf4000000 -- 0V-- 5 0-----------SrSw-- -> 0xf:f4000000 --------------Iprot 10 0 0xf4100000 -- 0V-- 5 0-I-G-------SrSw-- -> 0xf:f4100000 --------------Iprot 11 0 0xf4200000 -- 0V-- 5 0-----------SrSw-- -> 0xf:f4200000 --------------Iprot 12 0 0xf4300000 -- 0V-- 5 0-I-G-------SrSw-- -> 0xf:f4300000 --------------Iprot 13 0 0xf0000000 -- 0V-- 6 0-I-G-------SrSw-- -> 0xf:00000000 --------------Iprot 16 0 0x80000000 -- 0V-- 9 0-I-G-------SrSw-- -> 0xf:80000000 --------------Iprot 17 0 0x90000000 -- 0V-- 9 0-I-G-------SrSw-- -> 0xf:90000000 --------------Iprot 18 0 0xff300000 -- 0V-- 4 0-I-G-------SrSw-- -> 0xf:ff300000 --------------Iprot 19 0 0xeffff000 -- 0V-- 1 0-I-G-------SrSwSx -> 0x1:1ffff000 --------------Iprot 37 0 0xfe000000 -- 0V-- 7 0-I-G-------SrSw-- -> 0xf:fe000000 --------------Iprot 38 0 0xff000000 -- 0V-- 2 0-I-G-------SrSw-- -> 0xf:ff000000 --------------Iprot 39 0 0xff004000 -- 0V-- 2 0-I-G-------SrSw-- -> 0xf:ff004000 --------------Iprot 从这里开始, uboot开始load linux.itb并执行.linux.itb包括uImage, fdt, rootfsuImage也是个itb, 是个压缩过的linux二进制. 不用解析elf. Created with Raphaël 2.1.4uboot_wrappers.cuboot_wrappers.cfant/board.cfant/board.ccommon/cmd_bootm.ccommon/cmd_bootm.cpowerpc/lib/bootm.cpowerpc/lib/bootm.cprepare_images()fdt addr 0x... //initrd isam mchosen 0xc10000 0x9d61c8;fdt boa//做dtb的fixup ft_board_setup//要非零物理地址启动, base要是实际LAW配的物理地址 注1: fdt_fixup_memory(base, size)bootm 0x7a0002a4 - 0x11000000bootm_find_os boot_get_kernel//for dts bootm_find_otherbootm_load_osboot_relocate_fdtdo_bootm_linuxboot_prep_linux boot_body_linux boot_jump_linux注2: kernel = 把images->ep强转为函数指针; 注3: (*kernel) ((bd_t *)of_flat_tree, 0, 0, EPAPR_MAGIC, getenv_bootm_mapsize(), 0, 0) 注1:在fant上, ddr被配置在物理地址4G以上. 而uboot默认base为0, 不修改会导致kernel无法启动.对DDR来说, 它的ID是0x10和0x11, 它不可能是一个memory transaction的源, 只可能是目的.LAW寄存器: ddr使用fe00_0c70和fe00_0c80, 被map到36位物理地址0x1:00000000, 两段各2G, 一共4G.具体LAW寄存器要看芯片手册第二章, memory map 注2:在uboot解析uImage的时候, 它知道:Load Address: 00000000Entry Point: 00000000注意这个Entry Point就是uboot要跳转到kernel的地址 注3:对kernel来说, 第一条命令在0xc0000000对uboot来说, kernel被load后, 第一条命令在0x00000000; 如果要用仿真器调试kernel, 第一个断点应打在0x00000000uboot跳转到0地址, 后面的所有都是kernel负责, 包括配kernel自己的tlb kernel阶段 vmlinux的链接地址是0xc000_0000, 被uboot load到物理地址0x1:00000000.vmlinux使用uboot配的tlb来运行kernel的第一条指令, 该指令位于head.S在这条TLB中, 物理地址0x1:00000000被映射到虚拟地址0 idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 1 0 0x00000000 -- 0V-- a 0-----UrUwUxSrSwSx -> 0x1:00000000 --------------Iprot 所以, 可以说kernel是从虚拟地址0开始执行的. kernel在执行过程中, 用tlbsx命令, 能根据虚拟地址, 查到实际运行的物理地址, 并以此来配置TLB.当前的虚拟地址, 可以用跳转指令bl, 然后读lr寄存器得到.如: bl invstr invstr: mflr r6 kernel启动流程如下: Created with Raphaël 2.1.4init/main.cinit/main.chead_fsl_booke.Shead_fsl_booke.Spowerpc/kernel/setup_32.cpowerpc/kernel/setup_32.cpowerpc/kernel/prom.cpowerpc/kernel/prom.cpowerpc/mm/init_32.cpowerpc/mm/init_32.cpowerpc/mm/fsl_booke_mmu.cpowerpc/mm/fsl_booke_mmu.cc0000000 : 此时还在用uboot配的TLB, 虚拟地址从0开始invalidate所有uboot的TLB, 但保留当前运行的kernel代码所用的TLB.注1: //建立第一条kernel的TLB__early_start: include fsl_booke_entry_mapping.S到这里已经是0xC000_0000地址了使用特殊指令mtivor0 .. mtivor15建立中断表: set_ivor如果是从核, 跳转到 __secondary_start建立第一个线程的ptr到init_task,建立栈跳转到第一个C函数 early_init清零bss, 识别CPU类型machine_init到这里kernel已经被relocated好了early_init_devtree根据device treee, 获取initrd等信息建立MEMBLOCKs: 内存区间表扫描并解析dtearly_init_mmu()MMU_init从命令行解析MMU配置 reserve hugetlbadjust_total_lowmem为kernel固定768M \"low memory\"映射 注2: map_mem_in_cams_addr(物理地址,0xc0000000,768,3,0)初始化MMU硬件start_kernel()lockdep_init();cgroup_init_early();local_irq_disable();boot_cpu_init();page_address_init();打印Linux versionsetup_arch(&command_line);初始化mm, percpu, softirq, 页表分配器, 打印并解析命令行建立log buf, pid hash表, vfs的cache trap_init(); mm_init();sched_init();rcu_init();tick_nohz_init();context_tracking_init();radix_tree_init();init_IRQ();tick_init();init_timers();softirq_init();time_init();perf_event_init();call_function_init();开中断local_irq_enable();开始有中断初始化page_cgroup numa_policy sched_clock pidmap anon_vma thread_info_cache signal 等等...初始化cgroup cpuset ftracerest_init(): 开始第一个线程:kernel_init和第二个线程kthreadd到这里所有初始化结束, 进入idlewhile(1) do_idle() 注1:第一个64M TLB, 映射kernel代码段的. 从0xc0000000映射到4G物理地址 idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 注2:用TLB1中的3个TLB, 固定映射kernel的768M空间这里物理地址是从全局变量 memstart_addr得到的, 而这个全局变量是由dtb解析来的.所谓kernel的lowmem就是0xc0000000开始的768M, PPC用3条TLB来做固定映射. kmalloc可以直接从这段区域直接申请内存. 看起来应该是这样: 0xc0000000, 0xd0000000, 0xe0000000各256M idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 1 0 0xd0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:10000000 --------------Iprot 2 0 0xe0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:20000000 --------------Iprot 这要求dts要修改base到4G memory { //从4G开始, 大小4G reg = ; device_type = \"memory\"; } 从核linux启动 从核也是从0xFFFF_FFFC启动. 但会等待主核kick off uboot阶段 uboot启动从核流程如下: Created with Raphaël 2.1.4主流程 @主核主流程 @主核mpc85xx/fdt.c @主核mpc85xx/fdt.c @主核mpc85xx/mp.c @主核mpc85xx/mp.c @主核mpc85xx/release.S @所有从核mpc85xx/release.S @所有从核省略前面过程, 此时uboot已经在ram中运行setup_mp()boot page是最大DDR地址-4K 注0: 找到__second_half_boot_page的物理地址 找到spin table的物理地址注1: 找到虚拟地址CONFIG_BPTR_VIRT_ADDR的tlb index重新配置这个TLB index, 使得CONFIG_BPTR_VIRT_ADDR map到boot page拷贝__secondary_start_page到CONFIG_BPTR_VIRT_ADDR, 即拷贝到boot page的物理地址注2: plat_mp_up(): 把boot page的地址写入bstrh和bstrl寄存器release 从核注3: 0xFFFF_FFFC: __secondary_reset_vector: b __secondary_start_page0xFFFF_F000: __secondary_start_pageCPU core初始化, errata修复, 打开CPU cache, 分支预测等组件.配置TLB index5, 映射CCSR. 注4:解析spin table地址, 建立spin table的TLB映射 注5:TLB更新从__bootpg_addr取__second_half_boot_page的物理地址, rfi(return from interrupt)跳转到这个地址 这个地址在上面刚刚配置的TLB内.注6: __second_half_boot_page: 建立spin table entry置位spin table中本CPU的标记循环等待spin table被填入addrPC地址: 0x1fee4048等待spin table中每个CPU的标记, 1表示对应的从核已经跑到主核继续运行\"boot m\"命令ft_fixup_cpu在dtb中添加spin table信息:获取spin table总地址在每个cpu节点下面:\"enable-method\" = \"spin-table\"\"cpu-release-addr\"=该cpu的spin table物理地址并把相关内存添加到reserve memory 注0:__secondary_start_page的编译地址是: 0x0ff43000__second_half_boot_page的编译地址是:0x0ff44000它们是挨着的两个4K空间.__spin_table_addr在第1个4K: 0x0ff43284, 这是个指针, 编译时为0__spin_table在第2个4K: 0x0ff44204 注1:CONFIG_BPTR_VIRT_ADDR默认是0xfffff000, 但fant改写为0xeffff000 idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 19 0 0xeffff000 -- 0V-- 1 0-I-G-------SrSwSx -> 0x1:1ffff000 --------------Iprot 注2:从核也是从0xFFFF_FFFC启动. 设置bstr寄存器后, 0xFFFF_F000开始的4K会被映射到bstr寄存器指向的地址, 即主核设置的boot page; boot page包含release.S中的__secondary_start_page开始的4K代码, 这4K代码的最后一句正好是0xFFFF_FFFC: b __secondary_start_page, 从核从这句开始执行. 注3:从核在复位状态下的默认TLB为: idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xfffff000 -- 0V-- 1 0-I---------SrSwSx -> 0x0:fffff000 --------------Iprot 这里的paddr只是从核认为的物理地址, 实际上是bstr寄存器指向的地址. 注4:配好CCSR后, TLB: idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xfffff000 -- 0V-- 1 0-I---------SrSwSx -> 0x0:fffff000 --------------Iprot 5 0 0xfe000000 -- 0V-- 1 0-I-G-------SrSw-- -> 0xf:fe000000 --------------Iprot 注5:从__spin_table_addr指向的地址, 找到spin table地址; __spin_table_addr在第1个4K: 0x0ff43284所以在reset地址来看, 是0xfffff284, 此时它的内容是1fee4100, 应该是__spin_table在ram里relocate后的地址.这是个差点到512M的物理地址.此时TLB为: idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xfffff000 -- 0V-- 1 0-I---------SrSwSx -> 0x0:fffff000 --------------Iprot 1 0 0x1fee4000 -- 0VTs 1 0--MG-------SrSwSx -> 0x1:1fee4000 --------------Iprot 5 0 0xfe000000 -- 0V-- 1 0-I-G-------SrSw-- -> 0xf:fe000000 --------------Iprot 注6:这里已经不是boot space范畴了. __second_half_boot_page在普通的ram中. 前面的汇编打开了cache, 所以这段代码是可以被cache的. spin table定义如下: struct { uint64_t entry_addr; uint64_t r3; uint32_t rsvd1; uint32_t pir; ...占位, 共64字节, 一个cacheline } 总的spin table在0x1fee4100(见上), 每个CPU占64字节(0x40).r10寄存器指向本CPU在spin table的entry. 这里我们调试的是CPU2, 此时r10是0x1fee4180 ppcP3041[2,s] % md 0x1fee4180 0x1fee4180: 1fee_4180 00000000 00000001 00000000 00000002 ................ 1fee_4190 00000000 00000002 00000000 00000000 ................ 1fee_41a0 00000000 00000000 00000000 00000000 ................ 1fee_41b0 00000000 00000000 00000000 00000000 ................ linux阶段 linux启动后, 在/sys/firmware/devicetree/base下能看到相关的dtb配置.比如对CPU2来说, 它的spin table物理地址是0000 0001 1fee 4180, 和上面分析是一致的. /sys/firmware/devicetree/base/cpus/PowerPC,e500mc@2 # hexdump -C enable-method 00000000 73 70 69 6e 2d 74 61 62 6c 65 00 |spin-table.| 0000000b /sys/firmware/devicetree/base/cpus/PowerPC,e500mc@2 # hexdump cpu-release-addr 0000000 0000 0001 1fee 4180 0000008 Created with Raphaël 2.1.4第一个进程kernel_init @主核第一个进程kernel_init @主核kernel/smp.c @主核kernel/smp.c @主核kernel/cpu.c @主核kernel/cpu.c @主核powerpc/kernel/smp.c @主核powerpc/kernel/smp.c @主核...初始化多核: do_pre_smp_initcalls();lockup_detector_init();smp_init();sched_init_smp();每个CPU都有idle threads struct task_struct *idle_threads[NR_CPUS];smp_initfor each cpu: cpu_up(cpu)cpu_up获取该CPU的idle = idle thread 为这个CPU新建线程__cpu_up(cpu, idle)全局变量unsigned int cpu_callin_map[NR_CPUS];初始化idle进程的thread_infokick_cpu激活这个CPU的线程 通知这个CPU已经online打印从核已启动 Brought up 4 CPUsdo_basic_setup(): 初始化共享内存 驱动 irq 系统调用 执行initcalls从ramdis加载默认ko至此系统正常running: 依次尝试执行/init /sbin/init /etc/init /bin/init /bin/sh Created with Raphaël 2.1.4powerpc/kernel/smp.c @主核powerpc/kernel/smp.c @主核platforms/85xx/smp.c @主核platforms/85xx/smp.c @主核mpc85xx/release.S @从核mpc85xx/release.S @从核head_fsl_booke.S @从核head_fsl_booke.S @从核powerpc/kernel/smp.c @从核powerpc/kernel/smp.c @从核循环等待spin table被填入addrPC地址: 0x1fee4048此时还在uboot代码里面smp_85xx_kick_cpu从device tree获取cpu-release-addr例如CPU2: *cpu_rel_addr=0x11fee4180将该地址ioremap得到spin table虚拟地址注1: 将__early_start的物理地址写入spin table从核启动物理地址生效:0x10000009c等待从核release注2: mask spin table 状态为released打开本地中断注3: 为该启动地址配置64M的1:1TLBTLB配好后, rfi命令跳转到启动地址rfi命令跳转到linux代码//建立第一条kernel的TLB__early_start: include \"fsl_booke_entry_mapping.S\"这部分和主核一样注4: 虚拟地址从0xc000_0000开始了设置中断向量表从核, 跳转到 __secondary_start注5: 从核直接从TLBCAM[index] load tlb配置使低768M内存常驻TLB配置初始current和current_thread_info到全局变量建立栈跳转到start_secondarystart_secondarycpu_callin_map[cpu]置位等待从核给cpu_callin_map[CPU]置位 timeout说明从核没起来通知主核, 启动成功初始化cputime配置numacpu上线打开本地中断注6: 从核从IDLE开始运行cpu_startup_entry(CPUHP_AP_ONLINE_IDLE) 注1:在linux下面, spin table定义如下: struct epapr_spin_table { u32 addr_h; u32 addr_l; u32 r3_h; u32 r3_l; u32 reserved; u32 pir; }; 这里的kernel代码有个bug, CPU是PPC32时, kernel不写addr_h, 一般系统的DDR配置在物理0地址, addr_h不写也没问题.但fant的物理地址在0x1_0000_0000, 即从4G开始, 那么__pa(__early_start)是大于32位的, 不写addr_h会导致从核得到的启动地址不对, 从核无法启动. 在fant上的相关地址打印如下: 注for注1: cpu_rel_addr=0xc3fdc4ac *cpu_rel_addr=0x11fee4180 high_memory=0xf0000000 virt_to_phys(high_memory)=0x30000000 __early_start=0xc000009c, __pa(__early_start)=0x10000009c 修改方法很简单, 同时写入addr_h和addr_l, 从核就能成功启动. //增加写入addr_h out_be32(&spin_table->addr_h, __pa(__early_start) >> 32); out_be32(&spin_table->addr_l, __pa(__early_start)); 注for注1: 还有一个bugvirt_to_phys(high_memory)=0x30000000这里是有问题的.我们的phy地址是4G以上的, 所以这里的0x30000000显然不对.问题出在arch/powerpc/include/asm/io.hvirt_to_phys返回的是unsigned long, 在PPC32机器上, unsigned long是32位, 不够装下我们的phy地址. static inline unsigned long virt_to_phys(volatile void * address) { //__pa的返回类型是64位的, 但从本函数返回被截断了. return __pa((unsigned long)address); } 历史背景: virt_to_phys被kernel里面很多核心代码和众多driver引用. unsigned long在32位机器上是32位, 在64位机器上是64位, 所以这个api被设计成默认虚拟地址和物理地址等宽. 一般情况下, 这样设计没问题.PPC4080, CPU是32位, 但物理地址有36位, 这种情况下, 如果DDR被配置在物理0地址, 大小不超过4G, 这样做也是没问题的.但fant很特殊, DDR被配置在了物理4G地址开始, 其物理地址\"真的\"超过了32位. 在这种情况下, virt_to_phys返回的物理地址会被截断.修改:也很简单, 把unsigned long换成phys_addr_t, phys_addr_t足够容下物理地址.此时打印如下:virt_to_phys(high_memory)地址变正确了. cpu_rel_addr=0xc3fdc4ac *cpu_rel_addr=0x11fee4180 high_memory=0xf0000000 virt_to_phys(high_memory)=0x130000000 __early_start=0xc000009c, __pa(__early_start)=0x10000009c 注2:从核的spin table被主核更新为, 从核要跳转到物理地址00000001 0000009c执行. ppcP3041[2,s] % md 0x1fee4180 0x1fee4180: 1fee_4180 00000001 0000009c 00000000 00000002 ................ 1fee_4190 00000000 00000002 00000000 00000000 ................ 1fee_41a0 00000000 00000000 00000000 00000000 ................ 1fee_41b0 00000000 00000000 00000000 00000000 ................ 此时TLB为: idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xfffff000 -- 0V-- 1 0-I---------SrSwSx -> 0x0:fffff000 --------------Iprot 1 0 0x1fee4000 -- 0VTs 1 0--MG-------SrSwSx -> 0x1:1fee4000 --------------Iprot 5 0 0xfe000000 -- 0V-- 1 0-I-G-------SrSw-- -> 0xf:fe000000 --------------Iprot 注3:TLB更新为 idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0x00000000 -- 0V-- 8 0-----------SrSwSx -> 0x1:00000000 --------------Iprot 1 0 0x1fee4000 -- 0VTs 1 0--MG-------SrSwSx -> 0x1:1fee4000 --------------Iprot 5 0 0xfe000000 -- 0V-- 1 0-I-G-------SrSw-- -> 0xf:fe000000 --------------Iprot 但注意此时虚拟地址还不是0xc0000000开始的, 而是从0开始的. 注4:此时就只剩linux配的一条有效tlb idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 注5:从核不用计算low_mem(768M), 而是直接读TLBCAM的配置, 从而配置成和主核一样的3个常驻TLB, 给linux 的low_mem使用.与之对应的是high_mem(256M), 在high_mem的内存需要动态TLB才能访问.这中间的操作由bl loadcam_entry和bl restore_to_as0包围, 期间多配了一个带Ts标记的TLB. idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 1 0 0xd0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:10000000 --------------Iprot 2 0 0xe0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:20000000 --------------Iprot 63 0 0xc0000000 -- 0VTs 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 注6:从核开始运行IDLE进程, tlb最终为只有768M固定映射. idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 1 0 0xd0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:10000000 --------------Iprot 2 0 0xe0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:20000000 --------------Iprot "},"notes/CPU_PPC_kernel升级记录.html":{"url":"notes/CPU_PPC_kernel升级记录.html","title":"PPC kernel升级记录","keywords":"","body":" 新kernel启动没打印 现象 老kernel启动 新kernel启动 复现 跳转到kernel后, 新kernel会一直machine check 调查tlb 老kernel正常的tlb 新kernel的tlb不正常 问题, uboot怎么\"跳转\"到linux的? 用仿真器查看uboot的寄存器配置 看代码 仿真器打uboot断点 总结: 这里实际上有两个问题 那么switch_to_as1到底有什么作用 为什么memstart_addr为0 最终修改 驱动问题 内核不再有IRQF_DISABLED 修改 内核不再提供memory_accessor 修改 gpio的gpiochip_remove()函数不再返回int 修改 gpio没有member dev 修改 __noreturn编译标记 真的要noreturn吗? option1: 修改我们自己的driver 或者 option2: 修改kernel, 去掉noreturn要求 kernel代码路径 kernel路径 uboot DTS 参考: dts说明 编译dts FMan: Frame Manager kernel4.9和kernel3.12的部分DTS不同点 新kernel启动没打印 现象 老kernel启动 do_linux_exec:current_time = 56650 ms ## Booting kernel from Legacy Image at 7a0002a4 ... Image Name: Linux-3.12.37-rt51 Image Type: PowerPC Linux Kernel Image (gzip compressed) Data Size: 3620039 Bytes = 3.5 MiB Load Address: 00000000 Entry Point: 00000000 Verifying Checksum ... OK ## Flattened Device Tree blob at 11000000 Booting using the fdt blob at 0x11000000 Uncompressing Kernel Image ... OK reserving fdt memory region: addr=c10000 size=9d61c8 reserving fdt memory region: addr=20000000 size=60000000 Loading Device Tree to 03fdc000, end 03fff6e6 ... OK WARNING: Missing crypto node memory reserve level 1 set memory reserve region 0, base 0x20000000, size 0x60000000 ok WARNING: could not find compatible node: FDT_ERR_NOTFOUND## 2 bytes read in 0 ms load_image_from_cf_card: load image /images/cf_low_2g_reclaim to 1fddcb08 done, size 2 reclaim flag in file:1, in uboot env:31 [ 0.000000] memblock_reserve: [0x00000000000000-0x00000000790000] early_init_devtree+0x104/0x338 [ 0.000000] memblock_reserve: [0x00000003fdc000-0x00000003feb000] early_init_devtree+0x128/0x338 [ 0.000000] memblock_reserve: [0x00000000c10000-0x000000015e7000] early_init_devtree+0x18c/0x338 新kernel启动 没有一行kernel打印, 前面的打印到reclaim flag in file:1, in uboot env:31, 都有 复现 启动kernel, uboot会把CF卡里的linux.itb加载到内存里, 并从中解析出uImage, dtb和rootfs 和下面的命令效果一样: ext2load _cf 0:0 0x7a000000 /images/49/uImage ext2load _cf 0:0 0x11000000 /images/49/fant-3041.dtb ext2load _cf 0:0 0x7a000000 /images/312/uImage ext2load _cf 0:0 0x11000000 /images/312/fant-3041.dtb ext2load _cf 0:0 0xc10000 /images/linuxA/rootfs.cpio.xz bootm 0x7a000000 - 0x11000000 跳转到kernel后, 新kernel会一直machine check 用仿真器查看, 新kernel一直陷在0xc0000540, 不论单步运行, 还是run free, 每次停住CPU都在0xc0000540这是个machine check异常. ppcP3041[0,c] % th Core 0 halted; pc=0xc0000540 此时看vmlinux的链接起始地址代码: 0xc0000000, 和vmlinux的objdump比较, 完全对不上很可能是tlb不对, 导致虚拟地址0xc0000000映射到的物理地址没有代码. 我们要查一下tlb 调查tlb 老kernel正常的tlb 正常的tlb中, 0xc0000000被映射到物理地址0x1:00000000, 这个是ddr的地址, 从4G开始, 原因见下. ppcP3041[0,h] % tlbr * idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 1 0 0xd0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:10000000 --------------Iprot 2 0 0xe0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:20000000 --------------Iprot 新kernel的tlb不正常 这里出现了3个0xc0000000的映射 ppcP3041[0,c] % th Core 0 halted; pc=0xc0000540 ppcP3041[0,h] % tlbr * idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x0:00000000 --------------Iprot 3 0 0xc0000000 -- 0VTs 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 63 0 0xc0000000 -- 0VTs 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 问题, uboot怎么\"跳转\"到linux的? 在uboot解析uImage的时候, 它知道: Load Address: 00000000 Entry Point: 00000000 注意这个Entry Point就是uboot要跳转到kernel的地址 这个地址被uboot转换为函数指针, 跳过去. kernel = (void (*)(bd_t *, ulong, ulong, ulong, ulong, ulong, ulong))images->ep; (*kernel) ((bd_t *)of_flat_tree, 0, 0, EPAPR_MAGIC, getenv_bootm_mapsize(), 0, 0); 整个uboot启动kernel的过程打印, 和bootm的简要调用如下: ## Booting kernel from Legacy Image at 7a0002a4 ... Image Name: Linux-3.12.37-rt51 Image Type: PowerPC Linux Kernel Image (gzip compressed) Data Size: 3620039 Bytes = 3.5 MiB Load Address: 00000000 Entry Point: 00000000 Verifying Checksum ... OK ## Flattened Device Tree blob at 11000000 Booting using the fdt blob at 0x11000000 Uncompressing Kernel Image ... OK reserving fdt memory region: addr=c10000 size=9d61c8 reserving fdt memory region: addr=20000000 size=60000000 Loading Device Tree to 03fdc000, end 03fff6e6 ... OK board/_common/uboot_wrappers.c do_linux_exec //准备linux.itb, 包括uImage, fdt, rootfs prepare_images(addr, &kernel_addr, &kernel_size, &dtb_addr, &dtb_size, &rootfs_addr, &rootfs_size); //执行fdt命令 fdt addr 0x... mchosen 0xc10000 0x9d61c8; fdt boa board/freescale/board.c ft_board_setup() base = getenv_bootm_low(); size = board_get_bootm_size(); fdt_fixup_memory(blob, (u64) base, (u64) size); board_reserve_memory(blob); //执行bootm命令 bootm 0x7a0002a4 - 0x11000000 do_bootm do_bootm_states bootm_start(cmdtp, flag, argc, argv) bootm_find_os(cmdtp, flag, argc, argv) /* 打印## Booting kernel from Legacy Image at */ boot_get_kernel /* 打印## Flattened Device Tree blob at */ bootm_find_other(cmdtp, flag, argc, argv) /* Load the OS */ bootm_disable_interrupts() bootm_load_os(images, &load_end, 0) lmb_reserve(&images->lmb, images->os.load,(load_end - images->os.load)); /* Relocate the ramdisk */ boot_ramdisk_high() /* relocate flat device tree */ boot_fdt_add_mem_rsv_regions() boot_relocate_fdt() /* boot linux */ boot_selected_os() do_bootm_linux boot_prep_linux boot_body_linux boot_jump_linux void (*kernel)(bd_t *, ulong r4, ulong r5, ulong r6, ulong r7, ulong r8, ulong r9); kernel = (void (*)(bd_t *, ulong, ulong, ulong, ulong, ulong, ulong))images->ep; (*kernel) ((bd_t *)of_flat_tree, 0, 0, EPAPR_MAGIC, getenv_bootm_mapsize(), 0, 0); 总结: 对kernel来说, 第一条命令在0xc0000000 对uboot来说, kernel被load后, 第一条命令在0x00000000 uboot跳转到0地址, 后面的所有都是kernel负责, 包括配kernel自己的tlb. 用仿真器查看uboot的寄存器配置 UBOOT tlb:uboot把虚拟0地址映射到物理地址0x1:00000000. 1 0 0x00000000 -- 0V-- a 0-----UrUwUxSrSwSx -> 0x1:00000000 --------------Iprot 2 0 0x40000000 -- 0V-- a 0-----UrUwUxSrSwSx -> 0x1:40000000 --------------Iprot 37 0 0xfe000000 -- 0V-- 7 0-I-G-------SrSw-- -> 0xf:fe000000 --------------Iprot 上面说了, 0x1:00000000是ddr的地址, 是哪里配的?这个要看LAW寄存器, 在PPC中, LAW寄存器组共32个, 管SOC中, 各个模块的物理地址布局的.注意, 我用的是布局, 指的是各个模块, 在36位物理地址中的位置. 和虚拟地址无关.所以LAW寄存器, 是给模块用来灵活的配置物理地址布局的每个SOC模块都有个全局的ID, 用来在SOC内部寻址, 比如一个transaction, 它的源可能是PCIe, 目的是DDR.在MIPS的Octeon系列上, 不用LAW寄存器组, 而是直接固定每个模块的地址, 因为它有64位地址空间可以用.对DDR来说, 它的ID是0x10和0x11, 它不可能是一个memory transaction的源, 只可能是目的.LAW寄存器: ddr使用fe00_0c70和fe00_0c80, 被map到36位物理地址0x1:00000000, 两段各2G, 一共4G.具体LAW寄存器要看芯片手册第二章, memory map这就对上了. ppcP3041[0,h] % md 0xfe000c00 256 0xfe000c00: fe00_0c00 0000000f ff800000 81f00016 00000000 ................ fe00_0c10 0000000f f4000000 81800014 00000000 ................ fe00_0c20 0000000f f4200000 83c00014 00000000 ..... .......... fe00_0c30 0000000f ff000000 81f0000e 00000000 ................ fe00_0c40 0000000f ff200000 81f00013 00000000 ..... .......... fe00_0c50 0000000f 00000000 81d00018 00000000 ................ fe00_0c60 0000000f 80000000 8000001c 00000000 ................ fe00_0c70 00000001 00000000 8100001e 00000000 ................ fe00_0c80 00000001 80000000 8100001e 00000000 ................ fe00_0c90 0000000f ff300000 8000000f 00000000 .....0.......... 看代码 现在比较清楚了, 老kernel的tlb配的是对的. 而新kernel的tlb配的很乱. 0号tlb似乎是默认的, 新kernel认为0xc000000应该对应物理0地址. idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x0:00000000 --------------Iprot 3 0 0xc0000000 -- 0VTs 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 63 0 0xc0000000 -- 0VTs 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 在arch/powerpc/kernel/Makefile中, 最后用的是head_fsl_booke.o 87 extra-y := head_$(BITS).o 88 extra-$(CONFIG_40x) := head_40x.o 89 extra-$(CONFIG_44x) := head_44x.o 90 extra-$(CONFIG_FSL_BOOKE) := head_fsl_booke.o 91 extra-$(CONFIG_8xx) := head_8xx.o 92 extra-y += vmlinux.lds 仿真器打uboot断点 断点就打在0地址, 必须硬件断点才行.0x1:00000000: kernel的物理地址0x0:00000000: uboot下生效的虚拟地址, map到kernel的物理地址0x0:c0000000: kernel的编译虚拟地址. kernel会自己配tlb, 把物理地址map到0xc0000000 # 带x的是硬断点. bs x 0x00000000 # 能停住 ppcP3041[0,R] % tc Core 0 running Core 0 hardware breakpoint hit; pc=0x00000000 ppcP3041[0,B] % md 0x00000000 0x00000000: 0000_0000 60000000 48001b81 7c7e1b78 7c9f2378 `...H...|~.x|.#x 0000_0010 3b200000 3b000000 3ae00000 48000005 ; ..;...:...H... 0000_0020 7cc802a6 7ce000a6 54e4dffe 7cf00aa6 |...|...T...|... 0000_0030 54e7801e 7ce72378 7cf69ba6 7c003724 T...|.#x|...|.7$ #此时的tlb还是uboot配的. OK, 现在开始单步调试kernel. #断点设在uboot的虚拟地址0, 这是kernel的第一条指令 bs x 0x00000000 #让target继续运行 tc #uboot里面load kernel ext2load _cf 0:0 0x7a000000 /images/49/uImage ext2load _cf 0:0 0x11000000 /images/49/fant-3041.dtb bootm 0x7a000000 - 0x11000000 #boom最后会跳转到kenel的第一条指令, 触发断点 #Core 0 hardware breakpoint hit; pc=0x00000000 #首先要get_phys_addr 对应的head_fsl_booke.S c0000000 : _ENTRY(_start); //第一命令是nop, 占位的 c0000000: 60 00 00 00 nop //get_phys_addr是新kernel新增的. /* Translate device tree address to physical, save in r30/r31 */ //就像注释里说的, r3是device tree的地址, 在tlb中搜索r3, 返回物理地址 c0000004: 48 00 1b 81 bl c0001b84 mr r30,r3 mr r31,r4 //默认0地址 li r25,0 /* phys kernel start (low) */ li r24,0 /* CPU number */ li r23,0 /* phys kernel start (high) */ //到这里不管boot怎么使用tlb, kernel都会做invalidate操作. 但有个例外, 我们当前正在运行的tlb是uboot配的, 虽然虚拟地址不一定是0xc0000000, 但这条tlb必须有IPROT=1标记, 这样不会被invalidate掉. //kernel会配置64M的kernel空间到TLB1[0] //实际配tlb是这个文件. #include \"fsl_booke_entry_mapping.S\" //这个文件没问题, 到这里就只有kernel的tlb 0xc0000000生效 0 0 0xc0000000 -- 0V-- 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot //继续往下走是建立中断表, 使用特殊指令mtivor0 .. mtivor15 set_ivor: SET_IVOR(0, CriticalInput); SET_IVOR(1, MachineCheck); SET_IVOR(2, DataStorage); SET_IVOR(3, InstructionStorage); SET_IVOR(4, ExternalInput); SET_IVOR(5, Alignment); SET_IVOR(6, Program); SET_IVOR(7, FloatingPointUnavailable); SET_IVOR(8, SystemCall); SET_IVOR(9, AuxillaryProcessorUnavailable); SET_IVOR(10, Decrementer); SET_IVOR(11, FixedIntervalTimer); SET_IVOR(12, WatchdogTimer); SET_IVOR(13, DataTLBError); SET_IVOR(14, InstructionTLBError); SET_IVOR(15, DebugCrit); //设中断基址寄存器. /* Check to see if we're the second processor, and jump to the secondary_start code if so */ //如果是从核, 跳转到secondary_start //主核往下走 //建立第一个线程的ptr到init_task, 从此current到当前线程的指针可以使用 //建立栈 //跳转到early_init, 第一个C函数 c00003c4: 48 64 58 c9 bl c0645c8c ... c00003d0: 48 64 59 15 bl c0645ce4 ... //这之前都没问题 //这个函数有问题, 在里面异常了. c00003d4: 48 64 76 6d bl c0647a40 head_fsl_booke.S中的查找物理地址的函数: 通过tlbsx命令, 知道虚拟地址, 可以找到物理地址. //Translate the effec addr in r3 to phys addr. The phys addr will be put //into r3(higher 32bit) and r4(lower 32bit) get_phys_addr: //把msr赋值到r8, msr是Machine State Register (MSR) c0001b84: 7d 00 00 a6 mfmsr r8 //mfspr r9,SPRN_PID, spr是一些列的special的寄存器, 很多, 后面用枚举来指定具体spr //pid是processor ID c0001b88: 7d 30 0a a6 mfpid r9 //有时候反汇编和实际的汇编不是完全对应的, 比如下面: //MAS6是spr里面的 MMU assist register 6, MAS从0到8, 有9个. //rlwinm r9,r9,16,0x3fff0000 /* turn PID into MAS6[SPID] */ c0001b8c: 55 29 80 9e rlwinm r9,r9,16,2,15 //rlwimi r9,r8,28,0x00000001 /* turn MSR[DS] into MAS6[SAS] */ c0001b90: 51 09 e7 fe rlwimi r9,r8,28,31,31 //mtspr SPRN_MAS6,r9 c0001b94: 7d 36 9b a6 mtspr 630,r9 //tlbsx 0,r3 /* must succeed */ //在tlb中寻找EA地址r3对应的物理地址 c0001b98: 7c 00 1f 24 tlbsx 0,r3 mfspr r8,SPRN_MAS1 mfspr r12,SPRN_MAS3 rlwinm r9,r8,25,0x1f /* r9 = log2(page size) */ li r10,1024 slw r10,r10,r9 /* r10 = page size */ addi r10,r10,-1 and r11,r3,r10 /* r11 = page offset */ andc r4,r12,r10 /* r4 = page base */ or r4,r4,r11 /* r4 = devtree phys addr */ //无条件返回 blr fsl_booke_entry_mapping.S /* 1. Find the index of the entry we're executing in */ //LR寄存器实际上是SPR寄存器组中的一个, 保存了return addr(bl命令的下一条命令的地址) bl invstr /* Find our address */ invstr: mflr r6 /* Make it accessible */ //上面一句执行完毕后: gpr6=0x00000020, r6是LR, LR是现在这个指令的虚拟地址 /* 2. Invalidate all entries except the entry we're executing in */ ... /* 3. Setup a temp mapping and jump to it */ //到这里, tlbr * 命令显示, 只有一个有效的tlb了. 就是uboot配的那条 1 0 0x00000000 -- 0V-- a 0-----UrUwUxSrSwSx -> 0x1:00000000 --------------Iprot /* 4. Clear out PIDs & Search info */ ... /* 5. Invalidate mapping we started in */ //到这里都是OK的.此时tlb为: ppcP3041[0,s] % tlbr * idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0x00000000 -- 0--- 0 0----------------- -> 0x0:00000000 ------------------- 1 0 0x00000000 -- 0V-- a 0-----UrUwUxSrSwSx -> 0x1:00000000 ------------------- 2 0 0x00000000 -- 0VTs 1 0-----------SrSwSx -> 0x1:00000000 --------------Iprot /* 6. Setup KERNELBASE mapping in TLB1[0] */ c0000230: 7c 00 07 a4 tlbwe //到这里还是对的. kernel新增加了0xc0000000的映射. 但原来的uboot的tlb还在 Core 0 single-stepped; pc=0x00000234 ppcP3041[0,s] % tlbr * idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 1 0 0x00000000 -- 0--- a 0-----UrUwUxSrSwSx -> 0x1:00000000 ------------------- 2 0 0x00000000 -- 0VTs 1 0-----------SrSwSx -> 0x1:00000000 --------------Iprot /* 7. Jump to KERNELBASE mapping */ ... /* 8. Clear out the temp mapping */ c0000284: 7c 00 07 a4 tlbwe c0000288: 39 20 00 0c li r9,12 c000028c: 7c 00 4e 24 tlbivax 0,r9 c0000290: 7c 00 04 6c tlbsync c0000294: 7c 00 04 ac msync //到这里, uboot配的tlb被清理了. 只有kernel的tlb. Core 0 single-stepped; pc=0xc0000288 ppcP3041[0,s] % tlbr * idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot //这个文件结束, 没毛病. MMU_init有问题! c00003d4: 48 64 76 6d bl c0647a40 //这里有问题 c0647af8: 48 00 03 e5 bl c0647edc //readelf vmlinux -a | grep -E \"total_lowmem|__max_low_memory\" //__max_low_memory = c06a_8d20 30000000 00000000 //total_lowmem=c06b_8110 00000001 00000000 //所以这里ram=768M ram = min((phys_addr_t)__max_low_memory, (phys_addr_t)total_lowmem); //后记: 这里有问题, 本意是从as0空间切换到as1空间. //是git show 78a235efdc42ff363de81fdbc171385e8b86b69b 修改的: 大意是用tlb1来map第一个tlb的时候, 默认是map 64M 给kernel使用. 但有时候64M不够, 所以他要切换到AS1. //注释掉就好了. i = switch_to_as1(); unsigned long virt = PAGE_OFFSET; //readelf vmlinux -a | grep memstart_addr //这里的memstart_addr似乎应该是实际物理地址才好, 但这里全是0 //memstart_addr = c06a_8d28 00000000 00000000 phys_addr_t phys = memstart_addr; /* * Create a tlb entry with the same effective and physical address as * the tlb entry used by the current running code. But set the TS to 1. * Then switch to the address space 1. It will return with the r3 set to * the ESEL of the new created tlb. */ c0647f1c: 4b 9b 9e 01 bl c0001d1c //过了, 但多了一个tlb entry(序号63), 和entry0一样, 但多了Ts标记; ... //这里有问题 c0647f30: 4b 9c bc 19 bl c0013b48 ... struct tlbcamrange { unsigned long start; unsigned long limit; phys_addr_t phys; } tlbcam_addrs[NUM_TLBCAMS]; 强制phys = 0x100000000; //在for以后 //readelf vmlinux -a | grep tlbcam_addrs ppcP3041[0,s] % md 0xc06bacc8 0xc06bacc8: c06b_acc8 c0000000 cfffffff 00000001 00000000 ................ c06b_acd8 d0000000 dfffffff 00000001 10000000 ................ c06b_ace8 e0000000 efffffff 00000001 20000000 ............ ... c06b_acf8 00000000 00000000 00000000 00000000 ................ // arch/powerpc/mm/tlb_nohash_low.S loadcam_multi(0, i, max_cam_idx); c0013938: 7d 08 02 a6 mflr r8 //返回as0空间 restore_to_as0(i, 0, 0, 1); _GLOBAL(loadcam_multi) c0013938: 7d 08 02 a6 mflr r8 //用bl和mfbl获取当前虚拟地址, 增加一条tlb entry c0013968: 7c 00 07 a4 tlbwe //再增加一条tlb, 在另一个地址空间? ppcP3041[0,s] % tlbr * idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 1 0 0x00000000 -- 0--- a 0-----UrUwUxSrSwSx -> 0x1:00000000 ------------------- 2 0 0x00000000 -- 0-Ts 1 0-----------SrSwSx -> 0x1:00000000 ------------------- 3 0 0xc0000000 -- 0VTs 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot /* Switch to AS=1 */ --本来就是AS1空间呀??? c0013970: 7c c0 00 a6 mfmsr r6 //ori r6,r6,MSR_IS|MSR_DS c0013974: 60 c6 00 30 ori r6,r6,48 c0013978: 7c c0 01 24 mtmsr r6 ... c0013988: 4b ff ff 59 bl c00138e0 ... _GLOBAL(loadcam_entry) c00138e0: 7c a8 02 a6 mflr r5 ... c0013928: 4c 00 01 2c isync c001392c: 7c 00 07 a4 tlbwe // 总结: 这里实际上有两个问题 map_mem_in_cams()函数负责map kernel的lowmem, 所谓kernel的lowmem就是0xc0000000开始的768M, PPC用3条TLB来做固定映射. kmalloc可以直接从这段区域直接申请内存. 看起来应该是这样: 0xc0000000, 0xd0000000, 0xe0000000各256MppcP3041[0,h] % tlbr * idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 1 0 0xd0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:10000000 --------------Iprot 2 0 0xe0000000 -- 0V-- 9 0--M--------SrSwSx -> 0x1:20000000 --------------Iprot map_mem_in_cams()函数, 从memsart_addr获取物理地址, 作为0xc0000000对应的物理地址.phys_addr_t phys = memstart_addr;而程序运行到这里的时候, memstart_addr为0. 在default分支上的也修复了同样的问题. 该问题的根本原因是我们配置了ddr在4G开始的物理地址. upstream的一个修改也会造成异常: git show 78a235efdc42ff363de81fdbc171385e8b86b69b commit 78a235efdc42ff363de81fdbc171385e8b86b69b Author: Kevin Hao Date: Tue Dec 24 15:12:07 2013 +0800 powerpc/fsl_booke: set the tlb entry for the kernel address in AS1 We use the tlb1 entries to map low mem to the kernel space. In the current code, it assumes that the first tlb entry would cover the kernel image. But this is not true for some special cases, such as when we run a relocatable kernel above the 64M or set CONFIG_KERNEL_START above 64M. So we choose to switch to address space 1 before setting these tlb entries. Signed-off-by: Kevin Hao Signed-off-by: Scott Wood 暂时修改: $ hg diff arch/powerpc/mm/fsl_booke_mmu.c diff --git a/arch/powerpc/mm/fsl_booke_mmu.c b/arch/powerpc/mm/fsl_booke_mmu.c --- a/arch/powerpc/mm/fsl_booke_mmu.c +++ b/arch/powerpc/mm/fsl_booke_mmu.c @@ -212,6 +212,10 @@ unsigned long map_mem_in_cams(unsigned l unsigned long virt = PAGE_OFFSET; phys_addr_t phys = memstart_addr; + phys = 0x100000000; + + pr_info(\"low mem: virt = 0x%lx, phys = %pa \\n\", virt, &phys); + return map_mem_in_cams_addr(phys, virt, ram, max_cam_idx, dryrun); } @@ -242,9 +246,9 @@ void __init adjust_total_lowmem(void) /* adjust lowmem size to __max_low_memory */ ram = min((phys_addr_t)__max_low_memory, (phys_addr_t)total_lowmem); - i = switch_to_as1(); + //i = switch_to_as1(); __max_low_memory = map_mem_in_cams(ram, CONFIG_LOWMEM_CAM_NUM, false); - restore_to_as0(i, 0, 0, 1); + //restore_to_as0(i, 0, 0, 1); pr_info(\"Memory CAM mapping: \"); for (i = 0; i = (dt_root_addr_cells + dt_root_size_cells)) { u64 base, size; @@ -1054,7 +1055,7 @@ int __init early_init_dt_scan_memory(uns if (size == 0) continue; - pr_debug(\" - %llx , %llx\\n\", (unsigned long long)base, + pr_info(\" - %llx , %llx\\n\", (unsigned long long)base, (unsigned long long)size); early_init_dt_add_memory_arch(base, size); 那么switch_to_as1到底有什么作用 我们以从核启动为例, head.s配置初始的64M tlb for 0xc000_000后, 调用了switch_to_as1.在调用switch_to_as1之前, tlb是: idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 调用后, tlb多了一条一样的映射, 但有Ts标记. Ts的解释是\"compared with AS of the current access\", 即当前是指令access还是数据access, 为什么要这么分? idx w vaddr GsLpidVTsSizeTidWIMGEUrUwUxSrSwSx paddr VfX0X1U0U1U2U3Iprot 0 0 0xc0000000 -- 0V-- 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 63 0 0xc0000000 -- 0VTs 8 0--M--------SrSwSx -> 0x1:00000000 --------------Iprot 为什么memstart_addr为0 memstart_addr是个全局变量, 在这个地址打硬件断点, 写时触发: readelf -a vmlinux | grep memstart_addr 得到c06a8d28 再根据断点的pc地址, 得到memstart_addr实际是在下面的函数里修改的.memstart_addr初始值是全F early_init_dt_scan_memory() early_init_dt_scan_memory() if (base base地址是从dts解析的. 即使 memory { reg = ; device_type = \"memory\"; } 但uboot还是会改成: Freescale # fdt print /memory memory { reg = ; device_type = \"memory\"; }; reg里面的base和size都是2个cell. 所以uboot配成了base=0, size=4G 最终修改 修改dts: $ hg diff board/fant/fant-3041.dts diff --git a/board/fant/fant-3041.dts b/board/fant/fant-3041.dts --- a/board/fant/fant-3041.dts +++ b/board/fant/fant-3041.dts @@ -35,7 +35,18 @@ }; memory { - reg = ; + /* fant configs DDR to start from 4G in bootprom/uboot + * We pass physical ddr start address:0x100000000 to let + * kernel know which physical address should map to 0xc0000000 + * Previously this was done by adding PHYSICAL_START_ADDR = 0x100000000 + * to kernel config in kernel 3.12 + * Check uboot ft_board_setup(), ensure it does not change + * the base address here. + * The misconfigured base address(eg. 0x0) causes kernel not boot. + * As a down-side, we pass a fixed ddr size, because we don't + * want uboot to do fdt_fixup_memory(), which forces the base to be 0x0. + */ + reg = ; device_type = \"memory\"; }; 修改uboot: $ hg diff diff --git a/board/freescale/board.c b/board/freescale/board.c --- a/board/freescale/board.c +++ b/board/freescale/board.c @@ -750,15 +750,18 @@ phys_size_t board_get_bootm_size(void) int ft_board_setup(void *blob, bd_t * bd) { - int ret = 0; + ft_cpu_setup(blob, bd); +#if 0 +/* don't overwrite base to 0, base must be set in .dts file phys_addr_t base; phys_size_t size; - ft_cpu_setup(blob, bd); base = getenv_bootm_low(); size = board_get_bootm_size(); fdt_fixup_memory(blob, (u64) base, (u64) size); +*/ +#endif board_reserve_memory(blob); 驱动问题 内核不再有IRQF_DISABLED 从4.1开始, 内核不再有IRQF_DISABLED commit d8bf368d0631d4bc2612d8bf2e4e8e74e620d0cc Author: Valentin Rothberg Date: Thu Mar 5 15:23:08 2015 +0100 genirq: Remove the deprecated 'IRQF_DISABLED' request_irq() flag entirely The IRQF_DISABLED flag is a NOOP and has been scheduled for removal since Linux v2.6.36 by commit 6932bf37bed4 (\"genirq: Remove IRQF_DISABLED from core code\"). According to commit e58aa3d2d0cc (\"genirq: Run irq handlers with interrupts disabled\"), running IRQ handlers with interrupts enabled can cause stack overflows when the interrupt line of the issuing device is still active. This patch ends the grace period for IRQF_DISABLED (i.e., SA_INTERRUPT in older versions of Linux) and removes the definition and all remaining usages of this flag. 修改 删掉 内核不再提供memory_accessor /* * 'struct memory_accessor' is a generic interface to provide * in-kernel access to persistent memory such as i2c or SPI EEPROMs */ struct memory_accessor { ssize_t (*read)(struct memory_accessor *, char *buf, off_t offset, size_t count); ssize_t (*write)(struct memory_accessor *, const char *buf, off_t offset, size_t count); }; 从kernel4.6开始, 内核不再提供memory_accessor commit bec3c11bad0e7ac05fb90f204d0ab6f79945822b Author: Andrew Lunn Date: Fri Feb 26 20:59:24 2016 +0100 misc: at24: replace memory_accessor with nvmem_device_read Now that the AT24 uses the NVMEM framework, replace the memory_accessor in the setup() callback with nvmem API calls. commit 3ccea0e1fdf896645f8cccddcfcf60cb289fdf76 Author: Andrew Lunn Date: Fri Feb 26 20:59:21 2016 +0100 eeprom: at25: Remove in kernel API for accessing the EEPROM The setup() callback is not used by any in kernel code. Remove it. Any new code which requires access to the eeprom can use the NVMEM API. 修改 去掉该引用 gpio的gpiochip_remove()函数不再返回int 从3.18开始, gpiochip_remove()返回void 修改 +#if LINUX_VERSION_CODE > KERNEL_VERSION(3,18,0) + gpiochip_remove(chip); + return 0; +#else return gpiochip_remove(chip); +#endif gpio没有member dev 从kernel4.5开始, include/linux/gpio/driver.h里 struct gpio_chip把dev成员改为了parent commit 58383c78425e4ee1c077253cf297b641c861c02e Author: Linus Walleij Date: Wed Nov 4 09:56:26 2015 +0100 gpio: change member .dev to .parent The name .dev in a struct is normally reserved for a struct device that is let us say a superclass to the thing described by the struct. struct gpio_chip stands out by confusingly using a struct device *dev to point to the parent device (such as a platform_device) that represents the hardware. As we want to give gpio_chip:s real devices, this is not working. We need to rename this member to parent. 修改 chip->label = \"gpio-cpld\"; + +#if LINUX_VERSION_CODE > KERNEL_VERSION(4,5,0) + chip->parent = &pdev->dev; +#else chip->dev = &pdev->dev; +#endif __noreturn编译标记 从4.8开始, PPC的kernel要求ppc_md.{halt, restart}有__noreturn编译标记 Linux Mint 19.1 Tessa $ git log -S __noreturn arch/powerpc/include/asm/machdep.h commit 95ec77c06e8e63fff50c497eca0668bf6da39813 Author: Daniel Axtens Date: Tue Jul 12 10:54:52 2016 +1000 powerpc: Make ppc_md.{halt, restart} __noreturn powernv marks it's halt and restart calls as __noreturn. However, ppc_md does not have this annotation. Add the annotation to ppc_md, and then to every halt/restart function that is missing it. Additionally, I have verified that all of these functions do not return. Occasionally I have added a spin loop to be sure. Signed-off-by: Daniel Axtens Signed-off-by: Michael Ellerman # 查版本 git describe --contains 95ec77c06e8e63fff50c497eca0668bf6da39813 v4.8-rc1~85^2~97 而reboot_helper/reboot_helper.c没有这个标记, 编译不通过. 真的要noreturn吗? 在/home/coder/project/linux-fsl/arch/powerpc/kernel/setup-common.c中, 如果ppc_md.restart(cmd)不返回, 后面不就没法执行了吗? void kernel_restart(char *cmd) { kernel_restart_prepare(cmd); migrate_to_reboot_cpu(); syscore_shutdown(); if (!cmd) pr_emerg(\"Restarting system\\n\"); else pr_emerg(\"Restarting system with command '%s'\\n\", cmd); kmsg_dump(KMSG_DUMP_RESTART); machine_restart(cmd); } void machine_restart(char *cmd) { machine_shutdown(); if (ppc_md.restart) ppc_md.restart(cmd); smp_send_stop(); do_kernel_restart(cmd); mdelay(1000); machine_hang(); } option1: 修改我们自己的driver #include diff --git a/reboot_helper/reboot_helper.c b/reboot_helper/reboot_helper.c --- a/reboot_helper/reboot_helper.c +++ b/reboot_helper/reboot_helper.c @@ -166,6 +166,9 @@ static void machine_restart_warm(char *c { preserve_ram_contents(); } +#if LINUX_VERSION_CODE > KERNEL_VERSION(4,8,0) +static __noreturn void machine_restart_warm(char *cmd); +#endif 或者 使用 register_reboot_notifier, 会在kernel_restart_prepare调用 register_restart_handler, 会在最后do_kernel_restart调用 option2: 修改kernel, 去掉noreturn要求 kernel代码路径 https://source.codeaurora.org/external/qoriq/qoriq-components https://source.codeaurora.org/external/qoriq/qoriq-components/yocto-sdk/log/?h=thud http://git.yoctoproject.org kernel路径 http://git.yoctoproject.org/cgit/cgit.cgi/linux-yocto-4.9/log/?h=standard/base git clone https://git.yoctoproject.org/git/linux-yocto-4.9 -b standard/base --single-branch --depth=100 uboot https://source.codeaurora.org/external/qoriq/qoriq-components/u-boot/ git clone https://source.codeaurora.org/external/qoriq/qoriq-components/u-boot -b git.denx.de/master --single-branch --depth=100 DTS 参考: dts说明 树莓派dts说明 编译dts cd /repo/yingjieb/ms/buildrootppc/output/build/linux-custom cp ../../../board/fant/fant-4040.dts arch/powerpc/boot/dts/ scripts/dtc/dtc -O dts arch/powerpc/boot/dts/fant-4040.dts -o full-ref.dts FMan: Frame Manager P4080有两个FMan kernel4.9 Documentation/devicetree/bindings/powerpc/fsl/fman.txt kernel3.12 Documentation/devicetree/bindings/powerpc/fsl/fman_device_tree.txt kernel4.9和kernel3.12的部分DTS不同点 FMan是个组合, 包括:ports, MACs等. kernel4.9简化了配置 去掉了Parser KeyGen Coarse-Classification Policer 增加了IEEE1588(管以太网精密时钟同步的) compatible去掉了\"simple-bus\" FMan内部ram, 所有FMan共享 muram@0 { compatible = \"fsl,fman-muram\"; ranges = ; } FMan port三种类型:Ethernet receiver (RX)Ethernet transmitter (TX)Offline/Host command (O/H)compatible全变了 4.9\"fsl,fman-v2-port-rx\" for FManV2 RX ports\"fsl,fman-v2-port-tx\" for FManV2 TX ports\"fsl,fman-v2-port-oh\" for FManV2 OH ports还有v3的, 比如\"fsl,fman-v3-port-oh\" for FManV3 OH ports 3.12 去掉了\"fsl,fman-parser\" : 对incoming的fame的报文头解析, 生成结果\"fsl,fman-keygen\" : 生成key用来分发到多Q\"fsl,fman-cc\" : FMan内部的树结构, 再分流表中搜索, 用来exact matching\"fsl,fman-policer\" : 配置流的优先级\"fsl,p4080-fman-bmi\", \"fsl,fman-bmi\" : 和BMan对接, 用来alloc/free frame buffer\"fsl,p4080-fman-qmi\", \"fsl,fman-qmi\" : 和QMan的接口, 用来把frame描述符(FD)入Q和出Q mEMAC/dTSEC/XGEC 4.9\"fsl,fman-dtsec\" for dTSEC MAC\"fsl,fman-xgec\" for XGEC MAC\"fsl,fman-memac for mEMAC MAC 3.12必须有\"fsl,fman-mac\"\"fsl,fman-mac-dtsec\"\"fsl,fman-mac-xgec\" MDIO内部phy叫tbi phy 4.9 3.12 "},"notes/as_title_cpu3.html":{"url":"notes/as_title_cpu3.html","title":"MIPS","keywords":"","body":"如题 "},"notes/CPU_MIPS_octeon地址空间和寄存器访问.html":{"url":"notes/CPU_MIPS_octeon地址空间和寄存器访问.html","title":"octeon 地址空间和寄存器访问","keywords":"","body":"Address Map and Accessing CSRs on the OCTEON CPU Cn71xx这款处理器和我们之前用的powerpc系列处理器有不小的区别，刚开始可能不太适应，但熟悉了之后你会发现mips的处理器更简单。今天先来说一下地址空间和寄存器访问这个最基本的问题。 寄存器如何访问 地址空间 物理地址空间 虚拟地址空间 Linux下的内存视图 内核态 用户态 xkphys空间在用户态也可以访问 寄存器如何访问 先来看一个例子，u-boot启动代码，start.s里面如何访问寄存器：比如OCTEON_GPIO_TX_SET，很大，这是个64bit的寄存器地址：直接赋值给a4，就可以直接访问了。这个地址在芯片手册里是这样定义的 大家可能会注意到，芯片手册GPIO_TX_SET的地址是0x0001070000000888，而代码里用的地址却是0x8001070000000888，最高位相差1.这个涉及到一个mips规定的“xkphys”地址空间，见下图：“xkphys”是物理地址的一个“窗口”，通过这个窗口，就可以访问任何的物理地址。 结论：从芯片手册查到的物理地址，与上0x8000000000000000，就能得到虚拟地址，cpu就可以访问这个虚拟地址。 地址空间 这款CPU是个64bit的cpu，拥有巨大的64bit地址空间。 物理地址空间 还是以GPIO_TX_SET为例，它的物理地址是0x0001070000000888，共49bit的有效地址。与芯片手册说明一致： Cavium把地址空间分为两类，IO空间和MEM空间。物理地址最高bit是1的都属于IO空间，为0则属于MEM空间。其他的像DID等位段，看看就行了，作用不大。 MEM空间特指：DDR内存 剩下的都是IO空间，包括：所有寄存器，PCI的MEM/IO/CONFIG空间，bootbus空间。 IO空间和MEM空间最大的区别是：对MEM空间（即DDR）的访问，都经过cache；而对IO空间的访问都不经过cache。 这里，有个和powerpc的重大区别：powerpc的寄存器空间基址可配，但这款cpu的所有寄存器地址都是固定地址。见下图：特别的，PCI的MEM空间也是固定的 虚拟地址空间 08-Ch_3_Software_Overview_r3_June_2009第10小节 Linux下的内存视图 内核态 用户态 xkphys空间在用户态也可以访问 在内核menuconfig里面可配，默认是打开的.也就是说，用户态可以访问全部的物理内存和全部的SOC寄存器。这是个很强大的功能，用户态网口收包，不经过kernel就靠的是它。 cd Cavium_Networks/OCTEON-SDK/linux/kernel_2.6/linux make menuconfig "},"notes/CPU_MIPS_octeon操作记录.html":{"url":"notes/CPU_MIPS_octeon操作记录.html","title":"octeon 操作记录","keywords":"","body":" uboot从usb load到spi flash 编译kernel+debian并运行 安装必须的包 编译前配置好环境变量 make hello 升级uboot rootfs over nfs 78xx uboot nfs 关闭nmi wdt 使用SD卡编译debian 新版uboot起kernel evb7000-sff sd 编译SE uboot运行SE uboot静态ip uboot自动起linux strip vmlinux oct-sim tftp vmlinux tftpboot 运行iperf测试 debian 打开dhcp client动态获取ip 查kernel版本（编译时） linux编译 只编译debian目录 debain启动runlevel 0 使用分离的cpio做initramfs uboot从usb load到spi flash usb start usb info usb storage usb tree fatls usb 0:1 fatls usb 0:1 /78 sf probe fatload usb 0:1 0 /78/spi-boot.bin sf update $(fileaddr) 0 $(filesize) fatload usb 0:1 0 /78/u-boot-octeon_generic_spi_stage2.bin sf update $(fileaddr) 0x10000 $(filesize) fatload usb 0:1 0 /78/u-boot-octeon_ebb7800.bin sf update $(fileaddr) 0x100000 $(filesize) 编译kernel+debian并运行 #编译内核 #到linux下面 make kernel-deb #编译debian并烧到U盘中，注意这里sdc是新插入的U盘 #到dabian目录下 $sudo make DISK=/dev/sdc compact-flash OCTEON_ROOT=/home/byj/repo/hg/OCTEON-SDK-3.1-pristine Octeon ebb6100# fatload ide0 0 0 vmlinux.64 Octeon ebb6100# printenv loadaddr loadaddr=0x20000000 Octeon ebb6100# bootoctlinux 0 coremask=0xf root=/dev/sda2 loglevel=8 #注意:默认有个run level的问题, 不指定会到init 0 Octeon ebb6100# fatload mmc 0 0 vmlinux.64 Octeon ebb6100# printenv loadaddr loadaddr=0x20000000 Octeon ebb6100# bootoctlinux 0 coremask=0xf root=/dev/sda2 rw 3 loglevel=8 #保存到uboot命令, 注意分号前面需要转义 setenv sda2boot fatload mmc 0 0 vmlinux.64\\;bootoctlinux 0 coremask=0xf root=/dev/sda2 rw 3 mem=4G run sda2boot 安装必须的包 apt install build-essential apt install libncurses5-dev apt install flex bison apt install automake 编译前配置好环境变量 cd OCTEON-SDK-3.1 source env-setup OCTEON_CN71XX make hello cd examples/hello make run 升级uboot 先tftp 再bootloaderupdate rootfs over nfs #安装nfs, server端和client端都要装 yum install nfs-utils nfs-utils-lib apt-get install nfs-utils nfs-utils-lib #配置nfs $vi /etc/exports $cat /etc/exports /home/yingjie/gentoo4oct/hg-nfsroot/nfsroot *(async,rw,no_root_squash,no_subtree_check) $sudo exportfs -a #开始nfs服务 chkconfig nfs on service nfs start /etc/init.d/nfs start #客户端 #在mint上测试, 需要按照 $ apt install nfs-client $ sudo mount -t nfs -o nolock 192.168.1.99:/home/yingjie/gentoo4oct/hg-nfsroot/nfsroot nfsroot/ #但是提示 mount.nfs: access denied by server while mounting 192.168.1.99:/home/yingjie/gentoo4oct/hg-nfsroot/nfsroot #网上提示需要在server端运行 sudo exportfs -a #再次在client端mount就好了 #可以在client上查看server export哪些目录 $ showmount -e 192.168.1.99 Export list for 192.168.1.99: /home/yingjie/gentoo4oct/hg-nfsroot/nfsroot * 78xx uboot nfs #需要内核支持NFS和DNS Octeon ebb7800# setenv ipaddr 192.168.1.113 Octeon ebb7800# setenv serverip 192.168.1.99 Octeon ebb7800# setenv gatewayip 192.168.1.1 Octeon ebb7800# setenv netmask 255.255.255.0 Octeon ebb7800# setenv nfsroot /home/yingjie/gentoo4oct/hg-nfsroot/nfsroot Octeon ebb7800# tftp $(loadaddr) $(serverip):vmlinux.78nfs.strip Octeon ebb7800# bootoctlinux $(loadaddr) numcores=$(numcores) root=/dev/nfs rw nfsroot=$(serverip):$(nfsroot),nolock,tcp ip=$(ipaddr):$(serverip):$(gatewayip):$(netmask)::eth0:off mem=128G #保存为命令, 注意$和;前面的\\ Octeon ebb7800# setenv nfsboot tftp \\$(loadaddr) \\$(serverip):vmlinux.78nfs.strip\\;bootoctlinux \\$(loadaddr) numcores=\\$(numcores) root=/dev/nfs rw nfsroot=\\$(serverip):\\$(nfsroot),nolock,tcp ip=\\$(ipaddr):\\$(serverip):\\$(gatewayip):\\$(netmask)::eth0:off mem=128G Octeon ebb7800# run nfsboot 关闭nmi wdt #78 devmem 0x1010000020000 64 0 for i in `seq 0 47`;do busybox devmem $((i*8+0x1010000020000)) 64;done for i in `seq 0 47`;do busybox devmem $((i*8+0x1010000020000)) 64 0;done #68 0x0001070100100000 for i in `seq 2 27`;do busybox devmem $((i*8+0x0001070100100000)) 64;done for i in `seq 2 31`;do busybox devmem $((i*8+0x0001070100100000)) 64 0;done 使用SD卡编译debian . env-setup OCTEON_CN70XX cd linux/ make kernel-deb cd linux/debian/ make DISK=/dev/mmcblk0 P1=p1 P2=p2 P1_SIZE=256M compact-flash 新版uboot起kernel mem=2G evb7000-sff sd Octeon evb7000_sff# fatls mmc 1 Octeon evb7000_sff# fatload mmc 1 0 octboot2.bin [root@dev-lnx1 OCTEON-SDK]# . env-setup OCTEON_CN71XX [root@dev-lnx1 OCTEON-SDK]# cd linux/ [root@dev-lnx1 linux]# make kernel Octeon evb7000_sff# tftp 0 vmlinux.64 tftp: Octeon evb7000_sff# tftp 0 passthrough Octeon evb7000_sff# bootoct 0 numcores=4 sd: Octeon evb7000_sff# fatload mmc 1 0 passthrough Octeon evb7000_sff# bootoct 0 numcores=4 example: #为啥压缩了就不行呢？ Octeon evb7000_sff# fatls mmc 1 8271440 vmlinux.64 21950338 vmlinux-br-testbench-2014-10-08.strip.lzma Octeon evb7000_sff# fatload mmc 1 0x30000000 vmlinux-br-testbench-2014-10-08.strip.lzma reading vmlinux-br-testbench-2014-10-08.strip.lzma 21950338 bytes read in 2617 ms (8 MiB/s) Octeon evb7000_sff# unlzma 0x30000000 $(loadaddr) Uncompressed size: 29469392 = 0x1C1AAD0 Octeon evb7000_sff# bootoctlinux $(loadaddr) numcores=$(numcores) mem=2G #这个可以 # mount /dev/mmcblk0p2 rootfs/ debian fatload mmc 1 0 vmlinux.64 bootoctlinux $(loadaddr) numcores=$(numcores) mem=2G root=/dev/mmcblk0p2 embedded Octeon evb7000_sff# fatls mmc 1 8271440 vmlinux.64 21950338 vmlinux-br-testbench-2014-10-08.strip.lzma 29469392 vmlinux-br-tb.strip 3 file(s), 0 dir(s) Octeon evb7000_sff# fatload mmc 1 0 vmlinux-br-tb.strip reading vmlinux-br-tb.strip 29469392 bytes read in 3437 ms (8.2 MiB/s) Octeon evb7000_sff# bootoctlinux $(loadaddr) numcores=$(numcores) mem=2G argv[2]: numcores=4 argv[3]: mem=2G Allocating memory for ELF segment: addr: 0xffffffff80100000 (adjusted to: 0x100000), size 0x1d89ce0 ## Loading big-endian Linux kernel with entry point: 0xffffffff806cf260 ... Bootloader: Done loading app on coremask: 0xf Starting cores: 0xf 编译SE cd examples/hello make run uboot运行SE #运行SE, tftp方式下载: Octeon evb7000_sff# tftp 0 passthrough Octeon evb7000_sff# bootoct 0 numcores=4 #运行SE, 从sd加载: Octeon evb7000_sff# fatload mmc 1 0 passthrough Octeon evb7000_sff# bootoct 0 numcores=4 uboot静态ip setenv ethact octeth0 setenv ethprime octeth0 setenv serverip 192.168.1.100 setenv ipaddr 192.168.1.33 saveenv uboot自动起linux Octeon evb7000_sff# setenv bootcmd run linux_mmc Octeon evb7000_sff# printenv autoload=n baudrate=115200 bf=bootoct $(flash_unused_addr) forceboot numcores=$(numcores) boardname=evb7000_sff bootcmd=linux_mmc bootdelay=0 bootloader_flash_update=bootloaderupdate burn_app=erase $(flash_unused_addr) +$(filesize);cp.b $(fileaddr) $(flash_unused_addr) $(filesize) dram_size_mbytes=1024 env_addr=1fbf0000 env_size=10000 eth1addr=02:54:71:69:f3:01 eth2addr=02:54:71:69:f3:02 eth3addr=02:54:71:69:f3:03 eth4addr=02:54:71:69:f3:04 eth5addr=02:54:71:69:f3:05 eth6addr=02:54:71:69:f3:06 eth7addr=02:54:71:69:f3:07 eth8addr=02:54:71:69:f3:08 ethact=octeth0 ethaddr=02:54:71:69:f3:00 flash_base_addr=1f400000 flash_size=800000 flash_unused_addr=1f680000 flash_unused_size=580000 ipaddr=192.168.1.112 linux_mmc=fatload mmc 0 $(loadaddr) vmlinux.64;bootoctlinux $(loadaddr) loadaddr=0x20000000 ls=fatls mmc 0 numcores=4 octeon_failsafe_mode=0 octeon_ram_mode=0 serial#=2.0G1409-000206 serverip=192.168.1.99 stderr=serial stdin=serial,pci,bootcmd stdout=serial uboot_flash_addr=bf520000 uboot_flash_size=60160000 ver=U-Boot 2013.07 ( (U-BOOT build: 93, SDK version: 3.1.1-525), svnversion: u-boot:99862M, exec:)-svn99835 (Build time: Jun 07 2014 - 07:13:20) Environment size: 1246/65532 bytes strip vmlinux /home/byj/repo/hg/xrepo/buildroot/output/host/usr/bin/mips64-octeon-linux-gnu-strip -o vmlinux.strip vmlinux yingjie@aircraft ~/xrepo/buildroot/output/images ../host/usr/bin/mips64-octeon-linux-gnu-strip -o vmlinux.strip vmlinux oct-sim #terminal1: byj@byj-mint ~/repo/hg/OCTEON-SDK-3.1-pristine/linux/kernel $oct-sim /home/byj/repo/hg/xrepo/buildroot/output/images/vmlinux.strip -envfile=u-boot-env -memsize=2048 -uart0=2020 -noperf -numcores=1 -quiet #terminal2: $telnet localhost 2020 tftp vmlinux Octeon evb7000_sff# dhcp Octeon evb7000_sff# tftp 192.168.1.99:vmlinux.strip Octeon evb7000_sff# bootoctlinux $(loadaddr) numcores=$(numcores) mem=2G tftpboot #安装和配置atftpd $apt install atftpd $mkdir -p /home/byj/tmp/tftpboot $sudo chmod -R 777 /home/byj/tmp/tftpboot $sudo chown -R nobody /home/byj/tmp/tftpboot $sudo vi /etc/default/atftpd #填下面的 USE_INETD=false OPTIONS=\"--daemon --tftpd-timeout 300 --retry-timeout 5 --mcast-port 1758 --mcast-addr 239.239.239.0-255 --mcast-ttl 1 --maxthread 100 --verbose=5 /home/byj/tmp/tftpboot\" $sudo invoke-rc.d atftpd start #或者 $sudo /etc/init.d/atftpd start #重新配置的话需要 $sudo /etc/init.d/atftpd restart #停止 $sudo /etc/init.d/atftpd stop #板子上 #配网口 Octeon evb7000_sff# dhcp Octeon evb7000_sff# tftp 0 192.168.1.16:filename 运行iperf测试 #服务器上 [yingjie@aircraft temp]$ iperf3 -s -p 12345 -i 1 #板子上 root@octeon:~# iperf3 -c 192.168.1.99 -p 12345 -i 1 -t 10 -f M debian 打开dhcp client动态获取ip https://wiki.debian.org/NetworkConfiguration root@octeon:~# dhclient #查看路由是否正确 root@octeon:~# route root@octeon:~# ip route 查kernel版本（编译时） #内核源码树下 make kernelrelease ARCH=mips CROSS_COMPILE=mips64-octeon-linux-gnu- linux编译 cd linux byj@byj-mint ~/repo/hg/OCTEON-SDK-3.1/linux $make help grep: kernel/linux/.config: No such file or directory Supply the build target: kernel - Build the Linux kernel supporting all Cavium Octeon reference boards kernel-deb - Linux kernel without the rootfs sim - Octeon simulation environment setup-octeon2 - Enable config options for running on OcteonII hardware setup-octeon2-sim - Enable config options for running on OcteonII simulation flash - Copy kernel onto compact flash at mount /mnt/cf1 strip - Strip symbols out of the kernel image tftp - Copy a stripped kernel to /tftpboot test - Test an existing simulator build clean - Remove all generated files and the KERNEL CONFIG sudo make kernel-deb make menuconfig ARCH=mips 只编译debian目录 #需要进入root模式 #之前编译过kernel-deb（或者在内核源码树里面make vmlinux.64 modules ARCH=mips CROSS_COMPILE=mips64-octeon-linux-gnu-） cd linux/debian/ make target-dir #现在在当前目录下会有target-dir目录，这个就是debian的rootfs #然后的问题是，如何把debian这个rootfs集成到kernel里？ #参考gentoo的这篇文档 http://wiki.gentoo.org/wiki/Custom_Initramfs #进入linux/kernel/linux make menuconfig ARCH=mips 注意要加ARCH！！！！！ -->General setup -->[*]Initial RAM filesystem and RAM disk (initramfs/initrd) support -->填好rootfs的目录（../../debian/target-dir） #最后结果是： #太大了编不下！debian目录target-dir有1.7个G，压缩后的cpio还有500M root@byj-mint /home/byj/repo/hg/OCTEON-SDK-3.1-pristine/linux/kernel/linux #llh usr/ total 1.5G -rw-r--r-- 1 root root 511M 9月 21 21:50 built-in.o -rwxr-xr-x 1 root root 19K 9月 21 21:13 gen_init_cpio -rw-r--r-- 1 byj byj 13K 9月 13 11:32 gen_init_cpio.c -rw-r--r-- 1 root root 511M 9月 21 21:49 initramfs_data.cpio.gz -rw-r--r-- 1 root root 511M 9月 21 21:49 initramfs_data.o -rw-r--r-- 1 byj byj 1.3K 9月 13 11:32 initramfs_data.S -rw-r--r-- 1 byj byj 5.4K 9月 13 11:32 Kconfig -rw-r--r-- 1 byj byj 2.4K 9月 13 11:32 Makefile -rw-r--r-- 1 root root 0 9月 21 21:22 modules.builtin -rw-r--r-- 1 root root 0 9月 21 21:50 modules.order debain启动runlevel 0 在bootoctlinux 0 root=/dev/sda2 之后紧接着传一个runlevel参数就可以了， 就是那个3 cat /proc/cmdline bootoctlinux 0 root=/dev/sda2 3 console=ttyS0,115200 使用分离的cpio做initramfs $ cd linux $ make kernel-deb $ cp kernel/linux/vmlinux.64 /var/lib/tftpboot/ $ cd linux/embedded_rootfs $ make initramfs $ ls -l rootfs.cpio.gz | gawk -- '{printf \"%x\\n\", ($5 + 0x10000) - ($5 % 0x10000)}' 2e0000 $ cp rootfs.cpio.gz /var/lib/tftpboot/ Octeon ebh5600# freeprint Printing bootmem block list, descriptor: 0x0000000000024108, head is 0x00000000081c0940 Descriptor version: 3.0 Block address: 0x00000000081c0940, size: 0x0000000007e2d2c0, next: 0x0000000026000000 Block address: 0x0000000026000000, size: 0x00000000da000000, next: 0x0000000410000000 Block address: 0x0000000410000000, size: 0x000000000ff00000, next: 0x0000000000000000 Octeon ebh5600# namedalloc my_initrd 0x2e0000 0x81d0000 Allocated 0x00000000002e0000 bytes at address: 0x00000000081d0000, name: my_initrd Octeon ebh5600# tftpboot 0x81d0000 rootfs.cpio.gz Using octmgmt0 device TFTP from server 10.1.1.1; our IP address is 10.2.0.33 Filename 'rootfs.cpio.gz'. Load address: 0x81d0000 Loading: ##################### done Bytes transferred = 3000755 (2dc9b3 hex), 6398 Kbytes/sec WARNING: Data loaded outside of the reserved load area, memory corruption may occur. WARNING: Please refer to the bootloader memory map documentation for more information. Octeon ebh5600# tftpboot $(loadaddr) vmlinux.64 Using octmgmt0 device TFTP from server 10.1.1.1; our IP address is 10.2.0.33 Filename 'vmlinux.64'. Load address: 0x20000000 Loading: ################################################################# ################################################################# ################################################################# ################################################################# ################################################################# ################################################################# ############################### done Bytes transferred = 60285919 (397e3df hex), 6930 Kbytes/sec Octeon ebh5600# bootoctlinux $(loadaddr) numcores=$(numcores) endbootargs rd_name=my_initrd mem=1024M argv[2]: numcores=12 argv[3]: endbootargs ELF file is 64 bit Attempting to allocate memory for ELF segment: addr: 0xffffffff81100000 (adjusted to: 0x0000000001100000), size 0x9fc080 Allocated memory for ELF segment: addr: 0xffffffff81100000, size 0x9fc080 Processing PHDR 0 Loading 984c80 bytes at ffffffff81100000 Clearing 77400 bytes at ffffffff81a84c80 ## Loading Linux kernel with entry point: 0xffffffff81105f90 ... Bootloader: Done loading app on coremask: 0xfff Linux version 2.6.32.13-Cavium-Octeon (hello@xyz.zzz) (gcc version 4.3.3 (Cavium Inc. Development Version) ) #251 SMP Tue Jun 15 17:27:41 PDT 2010 CVMSEG size: 2 cache lines (256 bytes) bootconsole [early0] enabled CPU revision is: 000d0409 (Cavium Octeon+) Checking for the multiply/shift bug... no. Checking for the daddiu bug... no. Determined physical RAM map: memory: 00000000002e0000 @ 00000000081d0000 (usable after init) memory: 000000000004a000 @ 0000000001a46000 (usable) memory: 0000000006400000 @ 0000000001b00000 (usable) memory: 0000000007800000 @ 0000000008500000 (usable) memory: 0000000032400000 @ 0000000020000000 (usable) Wasting 376656 bytes for tracking 6726 unused pages Initial ramdisk at: 0xa8000000081d0000 (3014656 bytes) Zone PFN ranges: Normal 0x00001a46 -> 0x00052400 Movable zone start PFN for each node early_node_map[5] active PFN ranges "},"notes/CPU_MIPS_octeon_包处理性能.html":{"url":"notes/CPU_MIPS_octeon_包处理性能.html","title":"octeon 系列处理器包处理性能","keywords":"","body":" 时间估算 IP forwarding(IP Toolkit) CN68 vs CN78 linux vs SE CN78性能 Ipsec Ipsec加密会导致frame增大 CN68 vs CN78 L2 passthrough CN78 时间估算 比如IP forwarding 单核处理速度是3.3M pps报文这个东西的处理流程可以想象成汽车组装流水线, 3.3M pps说的是这个流水线0.3us就能出一辆整车.0.3us说的是在上百个流水线节点最慢的节点需要的时间, 他是整个系统的瓶颈, 决定了出整车的速度.但从0开始算到第一个整车出来, 整个过程需要的时间是latency.这个latency应该远远大于0.3us(流水线深度越深, 这个差距应该越大)据反馈一个报文的latency在100us左右.下面是实际的ping测试, 两台thunder服务器之间有个光口交换机. $ ping 192.168.1.21 PING 192.168.1.21 (192.168.1.21) 56(84) bytes of data. 64 bytes from 192.168.1.21: icmp_seq=1 ttl=64 time=0.348 ms 64 bytes from 192.168.1.21: icmp_seq=2 ttl=64 time=0.140 ms 64 bytes from 192.168.1.21: icmp_seq=3 ttl=64 time=0.131 ms 64 bytes from 192.168.1.21: icmp_seq=4 ttl=64 time=0.106 ms 64 bytes from 192.168.1.21: icmp_seq=5 ttl=64 time=0.110 ms 64 bytes from 192.168.1.21: icmp_seq=6 ttl=64 time=0.103 ms 64 bytes from 192.168.1.21: icmp_seq=7 ttl=64 time=0.126 ms 64 bytes from 192.168.1.21: icmp_seq=8 ttl=64 time=0.116 ms IP forwarding(IP Toolkit) CN68 vs CN78 CPU-core pps bps CN68-1.5G-800SCLK-1066DDR-40G line card-4flow-simple executive 64byte-1core 3.3M 1.7G 64byte-16core 41M 21.2G 64byte-32core 43M 22.2G 128byte-1core 3.3M 3.4G 128byte-12core 33M 33.7G 256byte-1core 3.3M 6.7G 256byte-8core 18.1M 37G 1518byte-1core 3.2M 39.4G ---- ---- ---- CN78-1.8G-1200SCLK-1866DDR-100G line card-4flow-simple executive 64byte-1core 3.6M 1.8G 64byte-16core 55.4M 28.3G 64byte-32core 99.8M 51G 64byte-48core 104.9M 54G 128byte-1core 3.6M 512byte-1core 3.6M 512byte-8core 22.6M 93G 1518byte-1core 3.6M 1518byte-8core 8.0M 1518byte-4core 8.0M 97G 结论: CN78对比CN68有整体性的提升, 主要解决了40G到100G的扩展性问题, 可以看到, 78基本上能够达到100G, 而68由于设计规格和IO规格只有40G 单看单核性能78有一定程度提升, 主要来自于Core/IO/DDR的频率提升 在线性度上78也有提升 linux vs SE CPU-core pps bps CN68-1.5G-800SCLK-1066DDR-40G line card-4flow-simple executive 64byte-1core 3.3M 1.7G 64byte-16core 41M 21.2G 64byte-32core 43M 22.2G(?) 128byte-1core 3.3M 3.4G 128byte-12core 33M 33.7G 256byte-1core 3.3M 6.7G 1518byte-1core 3.2M 39.4G ---- ---- ---- CN68-1.3G-800SCLK-1333DDR-40G line card-4flow-Linux Forwarding-native 64byte-1core 0.68M 0.35G 64byte-8core 5.3M 2.7G 64byte-16core 10.8M 5.5G 64byte-32core 15.2M 7.7G 512byte-1core 0.68M 2.7G 512byte-8core 5.4M 22.1G 512byte-12core 8.9M 36.5G 1518byte-1core 0.68M 8.2G ---- ---- ---- CN68-1.3G-800SCLK-1333DDR-40G line card-4flow-Linux Forwarding-ip offload module 64byte-1core 1.6M 64byte-8core 12.6M 64byte-16core 24.7M 64byte-32core 33.6M 512byte-1core 1.6M 1518byte-1core 1.6M CN78性能 Ipsec Ipsec加密会导致frame增大 CN68 vs CN78 CPU-core pps bps CN68-1.3G-800SCLK-1066DDR-40G line card-70 flow-10 sec burst-AES128-SHA1 64byte-1core 0.57M 64byte-8core 4.56M 64byte-16core 9.06M 64byte-32core 17.87M 128byte-1core 0.52M 128byte-32core 16.35M 512byte-1core 0.35M 512byte-24core 8.40M 1024byte-1core 0.25M 1024byte-16core 3.96M 1450byte-1core 0.19M 1450byte-16core 3.15M 36G ---- ---- ---- CN78-1.8G-1200SCLK-2100DDR-100G line card-60 flow-AES128-SHA1 64byte-1core 0.75M 64byte-8core 5.76M 64byte-16core 11.4M 64byte-32core 20.3M 128byte-1core 0.69M 512byte-1core 0.47M 1024byte-1core 0.33M 1450byte-1core 0.27M 1450byte-8core 2.1M 1450byte-16core 4.3M 1450byte-32core 8.1M 93G 结论: 基本同Ip forwarding L2 passthrough CN78 CPU-core pps bps CN78-1.8G-1200SCLK-1866DDR-100G line card-4flow-simple executive 64byte-1core 5.9M 3G 64byte-16core 90.7M 64byte-32core 104.1M 53G 64byte-48core 104.1M 128byte-1core 5.9M 512byte-1core 5.9M 512byte-4core 22.9M 93G 1518byte-1core 5.9M 1518byte-2core 8.0M 97G "},"notes/CPU_MIPS_octeon_ddr调试记录.html":{"url":"notes/CPU_MIPS_octeon_ddr调试记录.html","title":"octeon DDR调试","keywords":"","body":" CN70XX ddr ecc 配置文件里的odt配置 如何打印所有寄存器? ddr测试 CN70XX ddr ecc in uboot setenv ddr_maximum_adjacent_rlevel_delay_increment 1 in lib_octeon_shared.c, do the following changes: #define RLEVEL_BITMASK_BUBBLE_BITS_ERROR 4 Replace with: #define RLEVEL_BITMASK_BUBBLE_BITS_ERROR 12 #define RLEVEL_ADJACENT_DELAY_ERROR 2 Replace with: #define RLEVEL_ADJACENT_DELAY_ERROR 30 #define RLEVEL_NONSEQUENTIAL_DELAY_ERROR 5 Replace with: #define RLEVEL_NONSEQUENTIAL_DELAY_ERROR 50 配置文件里的odt配置 注意这里的配置和那个结构体对不上 #define OCTEON_EVB7100_CN61XX_DRAM_ODT_1RANK_CONFIGURATION \\ /* DIMMS reserved WODT_MASK LMCX_MODEREG_PARAMS1 RODT_CTL RODT_MASK reserved */ \\ /* ===== ======== ============== ========================================== ========= ============== ======== */ \\ /* 1 */ { 0, 0x00000001ULL, OCTEON_EVB7100_MODEREG_PARAMS1_1RANK_1SLOT, 3, 0x00000000ULL, 0 }, \\ /* 2 */ { 0, 0x00050005ULL, OCTEON_EVB7100_MODEREG_PARAMS1_1RANK_2SLOT, 3, 0x00010004ULL, 0 } #define OCTEON_EVB7100_CN61XX_DRAM_ODT_2RANK_CONFIGURATION \\ /* DIMMS reserved WODT_MASK LMCX_MODEREG_PARAMS1 RODT_CTL RODT_MASK reserved */ \\ /* ===== ======== ============== ========================================== ========= ============== ======== */ \\ /* 1 */ { 0, 0x00000101ULL, OCTEON_EVB7100_MODEREG_PARAMS1_2RANK_1SLOT, 3, 0x00000000ULL, 0 }, \\ /* 2 */ { 0, 0x09090606ULL, OCTEON_EVB7100_MODEREG_PARAMS1_2RANK_2SLOT, 3, 0x01010404ULL, 0 } #define OCTEON_EVB7100_CN61XX_DRAM_ODT_4RANK_CONFIGURATION \\ /* DIMMS reserved WODT_MASK LMCX_MODEREG_PARAMS1 RODT_CTL RODT_MASK reserved */ \\ /* ===== ======== ============== ========================================== ========= ============== ======== */ \\ /* 1 */ { 0, 0x01030203ULL, OCTEON_EVB7100_MODEREG_PARAMS1_4RANK_1SLOT, 3, 0x01010202ULL, 0 } static const unsigned char rodt_ohms[] = { 0, 20, 30, 40, 60, 120 }; static const unsigned char rtt_nom_ohms[] = { 0, 60, 120, 40, 20, 30 }; static const unsigned char rtt_nom_table[] = { 0, 2, 1, 3, 5, 4 }; static const unsigned char rtt_wr_ohms[] = { 0, 60, 120 }; static const unsigned char dic_ohms[] = { 40, 34 }; 结构体c typedef struct { uint8_t odt_ena; uint64_t odt_mask; #if defined(SWIG) || defined(SWIGLUA) uint64_t odt_mask1; #else cvmx_lmcx_modereg_params1_t odt_mask1; #endif uint8_t qs_dic; uint64_t rodt_ctl; uint8_t dic; } dimm_odt_config_t; 如何打印所有寄存器? 在代码里面加 uint32_t model = cvmx_get_proc_id(); if (cvmx_get_core_num() == 0) cvmx_csr_db_print_decode_by_prefix(model, \"lmc\", 1); ddr测试 //内存测试se, 在uboot下面运行 tftp hw-ddr2 多核带ecc bootoct 0 coremask=0x0f endbootargs -ecc 多核不带ecc bootoct 0 coremask=0x0f endbootargs 单核带ecc bootoct 0 -ecc 单核不带ecc bootoct 0 tftp hw-write-ddr2 bootoct 0 coremask=0x0f //打开ddr打印 Octeon cust_fpxtb(PKGA)# # setenv ddr_verbose 1 Octeon cust_fpxtb(PKGA)# # setenv ddr_debug 1 setenv ddr_trace_init 1 Octeon cust_fpxtb(PKGA)# # saveenv //查看是否打开ecc OCTEON_REMOTE_DEBUG=1 OCTEON_REMOTE_PROTOCOL=GDB:135.251.9.60,30000 oct-remote-csr cvmx_lmc0_config //查单双bit Ecc统计 OCTEON_REMOTE_DEBUG=1 OCTEON_REMOTE_PROTOCOL=GDB:135.251.9.60,30000 oct-remote-csr cvmx_lmc0_int //单双ecc Octeon cust_fpxtb(PKGA)# # read64 0x00011800880001F0 attempting to read from addr: 0x80011800880001f0 0x80011800880001f0: 0x0 //强制read leveling Octeon cust_fpxtb(PKGA)# # setenv ddr_rlevel_rank0_byte0 3 Octeon cust_fpxtb(PKGA)# # saveenv //设auto ODT setenv ddr_rodt_ctl_auto 1 //loop数值, 设为10好点 setenv ddr_rlevel_average //recompute --更差 ddr_rlevel_compute //强制一组经验好参数 if ((s = getenv(\"ddr_good\")) != NULL) { lmc_rlevel_rank.cn70xx.byte4 = 6; lmc_rlevel_rank.cn70xx.byte3 = 6; lmc_rlevel_rank.cn70xx.byte2 = 6; lmc_rlevel_rank.cn70xx.byte1 = 4; lmc_rlevel_rank.cn70xx.byte0 = 4; ddr_print(\"******Force DDR good********\\n\"); } //强制rleveling Octeon cust_fpxtb(PKGA)# # setenv ddr_rlevel_rank0_byte4 6 Octeon cust_fpxtb(PKGA)# # setenv ddr_rlevel_rank0_byte3 6 Octeon cust_fpxtb(PKGA)# # setenv ddr_rlevel_rank0_byte2 6 Octeon cust_fpxtb(PKGA)# # setenv ddr_rlevel_rank0_byte1 4 Octeon cust_fpxtb(PKGA)# # setenv ddr_rlevel_rank0_byte0 4 Octeon cust_fpxtb(PKGA)# # saveenv "},"notes/CPU_MIPS_octeon_BDK.html":{"url":"notes/CPU_MIPS_octeon_BDK.html","title":"octeon CN78xx BDK","keywords":"","body":"BDK是boot阶段执行的程序, 类似boot, 主要负责初始化和检测硬件. 编译BDK bdk from uboot octeon main bdk 流程 78的配置 编译BDK apt install gcc-multilib export BDK_ROOT=/home/byj/repo/git/octeon/bdk-trunk export OCTEON_ROOT=/home/byj/repo/hg/octeon-sdk/OCTEON-SDK-3.1.1 export PATH=${PATH}:${BDK_ROOT}/bin:${OCTEON_ROOT}/tools/bin make distclean make bdk from uboot usb start fatls usb 0:1 /target/ fatload usb 0:1 0x400000 /target/bdk-boot-cn78xx.bin go 0x400000 octeon main 在bdk-boot/bdk-full-no-romfs.c bdk 流程 __start(bdk-start.S) __init(bdk-start.S) 拷贝code(header + data)到0x80002000 --为什么不是cache? 还是说在没有flush之前, cache里面的东西不动? 用COP0_CVMMEMCTL设置栈在cache中, 大小是54*128, 详细见HRM小节2.5 Virtual Addresses and CVMSEG __bdk_init(bdk-init.c) __bdk_fs_init() __bdk_init_exception() 如果没有初始化内存, 则锁cache ... __bdk_init_main(bdk-init-main.c) cp0, config, bootbus, mdio初始化 main(bdk-full-no-romfs.c) 新的进程 bdk_lua_start() bdk_fs_remote_init() bdk_fs_rom_init() bdk_fs_mem_init() bdk_fs_nor_init() bdk_fs_mmc_init() bdk_fs_mpi_init() bdk_fs_pcie_init() bdk_fs_ram_init() bdk_fs_xmodem_init() bdk_fs_sata_init() __bdk_lua_main() lua.c lua_State *L = luaL_newstate() bdk_lua_init() luaL_getsubtable(L, LUA_REGISTRYINDEX, \"_PRELOAD\"); PRELOAD(\"bit64\", luaopen_bit64); PRELOAD(\"readline\", luaopen_readline); PRELOAD(\"rpc-support\", luaopen_rpc_support); //host模式下 PRELOAD(\"socket\", luaopen_socket_core) PRELOAD(\"oremote-internal\", luaopen_oremote) //target模式下 PRELOAD(\"cavium-internal\", luaopen_cavium); //注册子项, cavium_model, cavium_c, cavium_config, cavium_constants, cavium_perf, cavium_mmc, dram, csr //比如cavium_c lua_newtable(L) //把每个function都注册到lua里面, 这些function都通过一个cavium_c_call来调用 //同时单独注册了bdk_csr_read, bdk_csr_write, 因为这两个都是宏 lua_setfield(L, -2, \"c\"); //把这个newtable填到L里面, key就是c lua_pushcfunction(L, &pmain); lua_pushinteger(L, argc); /* 1st argument */ lua_pushlightuserdata(L, argv); /* 2nd argument */ status = lua_pcall(L, 2, 1, 0); result = lua_toboolean(L, -1); /* get result */ finalreport(L, status); lua_close(L); pmain() luaL_openlibs() handle_luainit() handle_script() dotty() /rom/init.lua while Ture: /rom/main.lua board-setup board-ebb7800 //其他的调用方法有octeon.c.bdk_csr_read, octeon.c.bdk_csr_write, 还有全在libbdk/bdk-functions.c里面 local set_config = octeon.c.bdk_config_set set_config(octeon.CONFIG_PHY_IF0_PORT1, 0xff000001) ... 78的配置 void __bdk_require_depends(void) { BDK_REQUIRE(QLM); BDK_REQUIRE(PCIE); BDK_REQUIRE(PCIE_EEPROM); BDK_REQUIRE(FS_PCIE); BDK_REQUIRE(GPIO); BDK_REQUIRE(RNG); BDK_REQUIRE(KEY_MEMORY); BDK_REQUIRE(MPI); BDK_REQUIRE(DRAM_CONFIG); BDK_REQUIRE(DRAM_TEST); BDK_REQUIRE(ENVIRONMENT); BDK_REQUIRE(FS_XMODEM); BDK_REQUIRE(FS_RAM); BDK_REQUIRE(FS_SATA); BDK_REQUIRE(CSR_DB); BDK_REQUIRE(TRAFFIC_GEN); BDK_REQUIRE(ERROR_DECODE); BDK_REQUIRE(SATA); BDK_REQUIRE(TWSI); BDK_REQUIRE(USB); } "},"notes/kernel_增加ECC中断.html":{"url":"notes/kernel_增加ECC中断.html","title":"octeon 增加ECC中断","keywords":"","body":"背景 有的板子kernel启动的时候有ecc错, kernel里面有个polling任务会打印 EDAC MC0: 1 CE DIMM 1 rank 1 bank 4 row 65056 col 1020 on any memory ( page:0x0 offset:0x0 grain:0 syndrome:0x0) 一个workaround思路是, 检测到ecc错的时候重启板子. 因为这个ecc错和重启相关. 尝试1 在init脚本里面检测 while true; do key=\"CE DIMM\" if `dmesg | grep \"$key\" >/dev/null` ; then echo \"i find: $key\" do_reboot \"CE reset\" fi key=\"UE DIMM\" if `dmesg | grep \"$key\" >/dev/null` ; then echo \"i find: $key\" do_reboot \"UE reset\" fi sleep 1 done 这样做的问题是太晚了, 有的时候这个脚本根本没跑到kernel就打calltrace挂住了. 尝试2 在那个polling任务里监测, 还是太晚了. 这个任务可能都还没来得及运行, kernel就挂住了. 终极方案 挂中断这里先把代码放上来 在~/repo/hg/xrepo/OCTEON-SDK-3.1/linux/kernel/linux/drivers/edac/octeon_edac-lmc.c 最后加 #if 1 #include static irqreturn_t interrupt_octeon_lmc(int irq, void *dev_id) { cvmx_ciu_cib_lmcx_rawx_t cib_lmc_raw; union cvmx_lmcx_int int_reg; char msg[64]; printk(\"%s:%d irq=%d\", __FUNCTION__, __LINE__, irq); cib_lmc_raw.u64 = cvmx_read_csr(CVMX_CIU_CIB_LMCX_RAWX(0,0)); //clear the source cvmx_write_csr(CVMX_CIU_CIB_LMCX_RAWX(0,0), cib_lmc_raw.u64); int_reg.u64 = cvmx_read_csr(CVMX_LMCX_INT(0)); printk(\"DDR ecc error! LMC0_INT=0x%llx\", int_reg.u64); if (int_reg.s.sec_err || int_reg.s.ded_err) { union cvmx_lmcx_fadr fadr; fadr.u64 = cvmx_read_csr(CVMX_LMCX_FADR(0)); snprintf(msg, sizeof(msg), \"DIMM %d rank %d bank %d row %d col %d\", fadr.cn61xx.fdimm, fadr.cn61xx.fbunk, fadr.cn61xx.fbank, fadr.cn61xx.frow, fadr.cn61xx.fcol); } if (int_reg.s.sec_err) { printk(\"single error detected: %s\", msg); int_reg.s.sec_err = -1; /* Done, re-arm */ } if (int_reg.s.ded_err) { printk(\"double error detected: %s\", msg); int_reg.s.ded_err = -1; /* Done, re-arm */ } if (int_reg.s.nxm_wr_err) { snprintf(msg, sizeof(msg), \"NXM_WR_ERR: Write to non-existent memory\"); printk(\"nxm error detected: %s\", msg); int_reg.s.nxm_wr_err = -1; /* Done, re-arm */ } //clear INT cvmx_write_csr(CVMX_LMCX_INT(0), int_reg.u64); return IRQ_HANDLED; } static int octeon_setup_lmc_interrupt(void) { unsigned int hwirq; int irq; cvmx_ciu_cib_lmcx_enx_t cib_lmc_en; if (!OCTEON_IS_MODEL(OCTEON_CN70XX)) return 0; printk(\"ECC workaround for cn70xx!\"); hwirq = (1 这里面有好几个点 hwirq是硬件中断号, 和具体的中断控制器(可以认为一个中断控制器是一个domain)有关, 这个domain里面的map函数控制hwirq号到irq号的映射 irq号是linux的一个概念, 对应这个irq的软件控制实例(也就是结构体) 要挂中断, 先要把hwirq map到irq, 这通过irq_create_mapping()实现, 这个函数会申请irq相关的结构体并返回irq号然后用request_irq()来挂中断处理函数, 最后一个参数会被传到handler里面 handler里面一般要先清中断, 可以调printk打印(为什么?) core_initcall()是个系列函数中的一个, 可以在kernel启动的时候挂初始化函数, 位置比较靠前. 启动打印 03-19 06:56:05 [ 0.000000] NR_IRQS:598 03-19 06:56:05 [ 34.196761] Calibrating delay loop (skipped) preset value.. 2400.00 BogoMIPS (lpj=12000000) 03-19 06:56:05 [ 34.204941] pid_max: default: 32768 minimum: 501 03-19 06:56:05 [ 34.209775] Mount-cache hash table entries: 256 03-19 06:56:05 [ 34.217025] Checking for the daddi bug... no. 03-19 06:56:05 [ 34.221223] ftrace: allocating 14160 entries in 56 pages 03-19 06:56:05 [ 34.248215] Performance counters: octeon PMU enabled, 4 64-bit counters available to each CPU, irq 7 03-19 06:56:05 [ 34.308272] SMP: Booting CPU01 (CoreId 1)... 03-19 06:56:05 [ 34.312471] CPU revision is: 000d9602 (Cavium Octeon III) 03-19 06:56:05 [ 34.312475] FPU revision is: 00739600 03-19 06:56:05 [ 34.312743] Brought up 2 CPUs 03-19 06:56:05 [ 34.325172] devtmpfs: initialized 03-19 06:56:05 [ 34.330228] ECC workaround for cn70xx! 03-19 06:56:05 [ 34.333820] map irq=122 for ecc error 03-19 06:56:05 [ 34.337512] attach ecc handler done! 03-19 06:56:05 [ 34.341020] interrupt_octeon_lmc:229 irq=122 03-19 06:56:05 [ 34.345272] DDR ecc error! LMC0_INT=0x0 03-19 06:56:05 [ 34.349093] LCM0_INT_EN=0x0 LCM0_INT=0x0 03-19 06:56:05 [ 34.353002] LCM0_FADR=0x2000000 LCM0_ECC_SYND=0xc500 03-19 06:56:05 [ 34.358113] NET: Registered protocol family 16 03-19 06:56:05 [ 34.363522] Installing handlers for error tree at: ffffffff80727130 后记 这个挂中断的方案后来也被证明还是太晚了, 最后还是因为ddr的参数不对导致的.但是在linux下面挂一个中断处理函数, 大体如此. "},"notes/CPU_MIPS_octeon_hw-ddr2代码走读.html":{"url":"notes/CPU_MIPS_octeon_hw-ddr2代码走读.html","title":"octeon hw-ddr2代码走读","keywords":"","body":" bootoct 0x20000000 coremask=0x0f endbootargs -ecc hw-ddr2 用CVMX_SHARED声明的变量是共享的, 其他核直接用 但怎么调到这个函数来的? 但肯定在main之前 A真正的中断处理函数 然后是main bootoct 0x20000000 coremask=0x0f endbootargs -ecc do_bootocteon() setup_se_app_boot_info() load_elf(addr, argc, argv, stack_size, heap_size, &core_mask, -1); read_elf_info(ei, elf_addr); 为每个core建立tlb, 建立堆和栈 拷贝只读的segment, 一份 拷贝共享的segment, 一份 拷贝可写的segment, 每个core一份 octeon_setup_boot_desc_block() octeon_setup_boot_vector(coremask) //把start_app函数地址写入boot vector octeon_bootloader_shutdown() //这里是关闭u-boot start_cores(coremask) //把不在coremask里面的core的boot vector写为idle_core_start_app(), 这最后是个while(1) octeon_setup_boot_vector(0, &invert_coremask); //给其他核发NMI, 其他核要么执行start_app(), 要么执行idle_core_start_app() cvmx_write_csr(CVMX_CIU_NMI, -2ull & cvmx_coremask_get64(&avail_coremask)); //如果core 0不是第一个core, 也就是说mask从其他core开始, 那么core 0就在这里等 while (!start_core0) ((void (*)(void)) (((boot_init_vector_t *) (BOOT_VECTOR_BASE))[0].app_start_func_addr)) (); 其实就是上面的start_app(), 注意:这个函数每个在mask里的核都会跑 set_except_base_addr(boot_info_ptr->exception_base); /*这个函数有个性, 所有核都跑这里, 都判断同一个变量, **因为所有SE的内存布局几乎都是1:1的, **所以这个变量地址落在同一个物理地址上, 只有core0会改这个地址 */ octeon_sync_cores() //最后在汇编里跳到entry_point octeon_setup_crt0_tlb((uint32_t) & stack_top, (uint32_t) & entry_point, uint32_t) & desc_vaddr); hw-ddr2 mipsisa64-octeon-elf-readelf -e hw-ddr2 找到入口地址Entry point address: 0x10000d68这个地址是__start, 是个是编译器里面的东西, 编译器里面还有libc的库 $ mipsisa64-octeon-elf-nm -Al hw-ddr2 | grep 10000d68 hw-ddr2:10000d68 T __start /usr/local/Cavium_Networks/octsw/toolchain/scripts/../src/newlib/libc/sys/octeon/crt0.S:31 反汇编hw-ddr2 mipsisa64-octeon-elf-objdump -d hw-ddr2 > objdump.log 10000d68 : 10000e88: 0c045bbc jal 10116ef0 //这是个库函数 __octeon_app_init octeon_os_set_console octeon_os_set_uart_num __octeon_clock_get_rate __octeon_bootmem_init 用CVMX_SHARED声明的变量是共享的, 其他核直接用 static CVMX_SHARED struct cvmx_interrupt_cpu cpu_ip2 = { .ip = 2 }; static CVMX_SHARED struct cvmx_interrupt_cpu cpu_ip3 = { .ip = 3 }; static CVMX_SHARED struct cvmx_interrupt *cvmx_ipx_irq[8]; static CVMX_SHARED union cvmx_ciux_to_irq cvmx_ciux_to_irq; 但怎么调到这个函数来的? 但肯定在main之前 __cvmx_app_init(uint64_t app_desc_addr) core = cvmx_get_core_num(); if ( sys_info_ptr->init_core == core ) cvmx_bootmem_init(bootinfo->phy_mem_desc_addr); first_core_initialized = 1; //所有的核都会等这个变量 while (!first_core_initialized); cvmx_interrupt_initialize(); //78是ciu3, 68是ciu2 cvmx_interrupt_ciu_initialize(sys_info_ptr); //写0到CVMX_CIU_INTX_EN0/1,先把中断全部禁止 //下面是注册两个钩子函数 if (cvmx_is_init_core()) cvmx_interrupt_register = cvmx_interrupt_register_ciu; cvmx_interrupt_map = cvmx_interrupt_map_ciu; if (cvmx_is_init_core()) //__cvmx_interrupt_ciu见A cpu_ip2.irq.handler = __cvmx_interrupt_ciu; cpu_ip3.irq.handler = __cvmx_interrupt_ciu; //这里面的cpu_ip2/3都是全局的share变量, 那个map函数会转换irq号 cvmx_interrupt_register(cvmx_interrupt_map(CVMX_IRQ_MIPS2),&cpu_ip2.irq); //这里cvmx_ipx_irq是个全局变量数组, 也是share的 int bit = hw_irq_number & 0x3f; info->data = bit; info->mask = __cvmx_interrupt_cpu_mask_irq; info->unmask = __cvmx_interrupt_cpu_unmask_irq; cvmx_ipx_irq[bit] = info; cvmx_interrupt_register(cvmx_interrupt_map(CVMX_IRQ_MIPS3),&cpu_ip3.irq); cvmx_interrupt_register(cvmx_interrupt_map(CVMX_IRQ_MIPS6), &__cvmx_interrupt_perf); //所有的异常处理函数(非中断) cvmx_interrupt_state.exception_handler = __cvmx_interrupt_default_exception_handler; low_level_loc = CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, sys_info_ptr->exception_base_addr)); memcpy(low_level_loc + 0x80, (void *)cvmx_interrupt_stage1, 0x80); memcpy(low_level_loc + 0x100, (void *)cvmx_interrupt_cache_error_v3, 0x80); memcpy(low_level_loc + 0x180, (void *)cvmx_interrupt_stage1, 0x80); memcpy(low_level_loc + 0x200, (void *)cvmx_interrupt_stage1, 0x80); //这里面传了个flag为0, 但据我分析应该是加上CVMX_ERROR_FLAGS_ECC_SINGLE_BIT cvmx_error_initialize(0); cvmx_error_initialize_cn70xx() //用cvmx_error_add()把各种error加进去 __cvmx_error_custom_initialize() //这里面把CVMX_CIU_CIB_LMCX_RAWX的sec_err ded_err 替换成了__cvmx_cn6xxx_lmc_ecc_error_display //但前提是1.CIU_CIB_L2C_EN(0)的相应位要为1; 2. LMC(0)_INT_EN也要为1(目前为0), 这条不需要 //__cvmx_cn6xxx_lmc_ecc_error_display()里面会调cvmx_safe_printf()来打印 cvmx_error_change_handler(...__cvmx_cn6xxx_lmc_ecc_error_display) cvmx_error_enable_group(CVMX_ERROR_GROUP_INTERNAL, 0); //这里应该是把LMC相应的东东使能了, 但我觉得L2C也应该加上 cvmx_error_enable_group(CVMX_ERROR_GROUP_LMC, 0); cvmx_error_poll() //等等 REGISTER_AND_UNMASK_ERR_IRQ(CVMX_IRQ_L2C); //简写, 声明一个_irq的静态共享变量_irq, 并把handler弄成__cvmx_interrupt_ecc static CVMX_SHARED struct cvmx_interrupt _irq.handler = __cvmx_interrupt_ecc //这个宏也会调cvmx_interrupt_register_ciu(), 但应该是走的另外分支 info->data = hw_irq_number; info->mask = __cvmx_interrupt_ciu_mask_irq; info->unmask =__cvmx_interrupt_ciu_unmask_irq; //这个东西是个全局的share变量, 估计在中断里用 cvmx_ciu_to_irq[reg][bit] = info; REGISTER_AND_UNMASK_ERR_IRQ(CVMX_IRQ_RST); REGISTER_AND_UNMASK_ERR_IRQ(CVMX_IRQ_MIO); cpu_ip2.irq.unmask(&cpu_ip2.irq); cpu_ip3.irq.unmask(&cpu_ip3.irq); 好了,现在开始让SR.BEV为1, 可以用在ram里面的中断向量了, 但地址在哪里? 应该是ebase地址吧? cvmx_debug_init(); cvmx_coredump_init(); A真正的中断处理函数 __cvmx_interrupt_ciu //这里面大约是调每个irq的handler //而这个irq号就是cvmx_ciux_to_irq.en0/1_to_irq[]表里面的号 //也就是用REGISTER_AND_UNMASK_ERR_IRQ注册的东西 irq->handler(irq, registers); //大多数都是下面这个, 这个是前面注册的 __cvmx_interrupt_ecc() //下面这个函数应该是遍历在cvmx_error_initialize_cn70xx()add进去的东西, 并调相应的handler cvmx_error_poll() 然后是main main() cvmx_user_app_init(); //先检查一些BIST //如果是CVMX_USE_1_TO_1_TLB_MAPPINGS = 1, 这是在makefile里面写的, 这里是1 //设1:1TLB, 应该是所有内存, 注意这里是每个核都跑的 cvmx_bootmem_init(sys_info_ptr->phy_mem_desc_addr); if (cvmx_is_init_core()) cvmx_qlm_init(); hw_cons_open(); //这里面注册了串口的中断函数 if (cvmx_get_core_num() == initialCore) //随机数, patten会用 cvmx_rng_enable (); //把cache弄小到一个line hw_ddr_useOneCacheWay (); //两个性能计数 cvmx_l2c_config_perf (0, CVMX_L2C_EVENT_WRITE_REQUEST, 0); cvmx_l2c_config_perf (1, CVMX_L2C_EVENT_READ_REQUEST, 0); //默认禁止ECC //然后就是开始跑了 run_tests (&ppMemoryMap[cvmx_get_core_num()].range[0], 3, -1); "},"notes/CPU_MIPS_octeon_reboot调试和ddr中断.html":{"url":"notes/CPU_MIPS_octeon_reboot调试和ddr中断.html","title":"octeon reboot调试和DDR中断","keywords":"","body":"reboot会概率卡死, 怀疑和cpld拉住复位有关. 和pcie有关 CIU_CIB_RST_RAW(0):0x107000000E400 CIU_CIB_RST_EN(0):0x107000000E500 /user # devmem 0x107000000E400 64 0x0000000000000000 /user # devmem 0x107000000E500 64 0x0000000000000009 0x0001180006001628: 0x0001180006001640: /user # devmem 0x0001180006001640 64 0x0000000000000148 HOST_MODE=1 RST_LINK=0 0x0001180006001680: 0x00011800060016C0: 流程 kernel/reboot.c sys_reboot: kernel_restart() kernel_restart_prepare() blocking_notifier_call_chain(&reboot_notifier_list, SYS_RESTART, cmd); usermodehelper_disable(); /*对每个device调用shutdown*/ device_shutdown(); 对每个device: if (dev->bus && dev->bus->shutdown) { if (initcall_debug) dev_info(dev, \"shutdown\\n\"); dev->bus->shutdown(dev); } else if (dev->driver && dev->driver->shutdown) { if (initcall_debug) dev_info(dev, \"shutdown\\n\"); dev->driver->shutdown(dev); } migrate_to_reboot_cpu() /*调用注册过的所有sys core回调函数*/ syscore_shutdown() pr_emerg(\"Restarting system\\n\") kmsg_dump() machine_restart() octeon_restart() for_each_online_cpu(cpu) cvmx_write_csr(CVMX_CIU_WDOGX(cpu_logical_map(cpu)), 0); cvmx_write_csr(CVMX_RST_SOFT_RST, 1) reboot会调用device_shutdown() -> static void cvm_oct_shutdown(struct platform_device *pdev) 在这个函数里面:会关闭所有interface, 关闭GMX, rx_disable, 解注册并free掉net device, 清理hw pools, 清理pko queue. 调试 /user # reboot /user # WDT=a application_watchdog: /scripts/application_watchdog: collecting system information... /scripts/collect_system_information: line 26: can't create /logs/info_02_uptime: Read-only file system /scripts/collect_system_information: line 29: can't create /logs/info_ps: Read-only file system /scripts/collect_system_information: line 34: can't create /logs/info_sp: Read-only file system /scripts/collect_system_information: line 47: can't create /logs/info_interrupts: Read-only file system /scripts/collect_system_information: line 50: can't create /logs/info_memory: Read-only file system /scripts/collect_system_information: line 51: can't create /logs/info_memory: Read-only file system /scripts/collect_system_information: line 54: can't create /logs/info_stat: Read-only file system The system is going down NOW! Sent SIGTERM to all processes Sent SIGKILL to all processes [ 48.179078] reboot_helper: stored panic_counter = 0 [ 48.197076] reboot_helper: isam_reboot_type='warm' [ 48.214954] reboot-helper: Enabling preserved ram [ 48.232745] flush l2 cache. [ 48.248725] reboot_helper: continuing standard linux reboot [ 48.267860] device eth0 left promiscuous mode [ 48.306015] bonding: bond0: releasing active interface eth-nta [ 48.324985] bonding: bond0: Warning: clearing HW address of bond0 while it still has VLANs. [ 48.346430] bonding: bond0: When re-adding slaves, make sure the bond's HW address matches its VLANs'. [ 48.368930] device bond0 entered promiscuous mode [ 48.456398] reboot: Restarting system [ 48.473173] reboot: Restarting system2 [ 48.490019] cvmx_write_csr CVMX_RST_SOFT_RST [ 48.507409] Error: CIU_CIB_RST_RAWX(0)[INT_LINKX] =====================BOOT======================= 现象1: /proc # cat interrupts CPU0 CPU1 CPU2 CPU3 8: 134755 47296 14313 8189 Core timer 9: 0 0 0 0 CIU cpld0_nmi 10: 0 0 0 0 CIU cpld0 11: 0 0 0 0 CIU pioneer0 24: 8567 0 0 0 CIU eth0 35: 1007 0 0 0 CIU serial 45: 0 0 0 0 CIU i2c-octeon 57: 0 0 0 0 CIU octeon-hw-status 59: 0 0 0 0 CIU i2c-octeon 60: 0 0 0 0 CIU octeon-hw-status 73: 973 0 0 0 CIU-W octeon_wdt 74: 0 973 0 0 CIU-W octeon_wdt 75: 0 0 973 0 CIU-W octeon_wdt 76: 0 0 0 973 CIU-W octeon_wdt 89: 0 0 0 0 CIU octeon-hw-status 90: 0 0 0 0 CIU octeon-hw-status 91: 0 0 0 0 CIU octeon-hw-status 92: 0 0 0 0 CIU octeon-hw-status 93: 0 0 0 0 CIU octeon-hw-status 94: 0 0 0 0 CIU octeon-hw-status 95: 0 0 0 0 CIU octeon-hw-status 96: 0 0 0 0 CIU octeon-hw-status 97: 0 0 0 0 CIU octeon-hw-status 98: 0 0 0 0 CIU octeon-hw-status 99: 0 0 0 0 CIU octeon-hw-status 105: 23036 28147 20942 19105 CIU-M SMP-IPI 109: 1 0 0 0 CIU linux-kernel-bde 113: 0 0 0 0 CIU MSI[0:63] 114: 0 0 0 0 CIU MSI[64:127] 115: 0 0 0 0 CIU MSI[127:191] 116: 0 0 0 0 CIU MSI[192:255] 127: 0 0 0 0 CIU octeon-hw-status ERR: 0 /proc # devmem 0x1a000010 8 0x8b /proc # [ 5203.567484] Data bus error, epc == 0000000010cc490c, ra == 0000000010cc4874 [ 5203.588339] Error: CIU_CIB_RST_RAWX(0)[INT_LINKX] [ 5203.590919] [sched_delayed] sched: RT throttling activated /proc # cat interrupts CPU0 CPU1 CPU2 CPU3 8: 143908 50283 15649 9957 Core timer 9: 0 0 0 0 CIU cpld0_nmi 10: 0 0 0 0 CIU cpld0 11: 0 0 0 0 CIU pioneer0 24: 10078 0 0 0 CIU eth0 35: 1065 0 0 0 CIU serial 45: 0 0 0 0 CIU i2c-octeon 57: 0 0 0 0 CIU octeon-hw-status 59: 0 0 0 0 CIU i2c-octeon 60: 0 0 0 0 CIU octeon-hw-status 73: 1045 0 0 0 CIU-W octeon_wdt 74: 0 1045 0 0 CIU-W octeon_wdt 75: 0 0 1045 0 CIU-W octeon_wdt 76: 0 0 0 1045 CIU-W octeon_wdt 89: 0 0 0 0 CIU octeon-hw-status 90: 0 0 0 0 CIU octeon-hw-status 91: 0 0 0 0 CIU octeon-hw-status 92: 0 0 0 0 CIU octeon-hw-status 93: 0 0 0 0 CIU octeon-hw-status 94: 0 0 0 0 CIU octeon-hw-status 95: 0 0 0 0 CIU octeon-hw-status 96: 0 0 0 0 CIU octeon-hw-status 97: 0 0 0 0 CIU octeon-hw-status 98: 0 0 0 0 CIU octeon-hw-status 99: 0 0 0 0 CIU octeon-hw-status 105: 24342 29600 22330 20376 CIU-M SMP-IPI 109: 1 0 0 0 CIU linux-kernel-bde 113: 0 0 0 0 CIU MSI[0:63] 114: 0 0 0 0 CIU MSI[64:127] 115: 0 0 0 0 CIU MSI[127:191] 116: 0 0 0 0 CIU MSI[192:255] 127: 1 0 0 0 CIU octeon-hw-status ERR: 0 Error: CIU_CIB_RST_RAWX(0)[INT_LINKX] arch/mips/cavium-octeon/executive/cvmx-error-trees.cerror_tree_cn70xx:在这里引用 struct cvmx_error_childbit { u8 valid; u8 bit; struct cvmx_error_muxchild *children; }; struct cvmx_error_muxchild { u64 reg; u64 mask_reg; struct cvmx_error_regbit *bits; struct cvmx_error_childbit *children; }; lmc的error项: {1, 52 /* lmc0 */, (struct cvmx_error_muxchild[]){ {CVMX_ADD_IO_SEG(0x00011800880001F0ull) + ((0) & 0) * 0x1000000ull /* CVMX_LMCX_INT(0) */, CVMX_ADD_IO_SEG(0x00011800880001E8ull) + ((0) & 0) * 0x1000000ull /* CVMX_LMCX_INT_EN(0) */, (struct cvmx_error_regbit[]){ {1, 1, CVMX_ERROR_GROUP_LMC, 1, 0, \"LMCX_INT(0)[SEC_ERR]\"}, {1, 1, CVMX_ERROR_GROUP_LMC, 0, 0, \"LMCX_INT(0)[NXM_WR_ERR]\"}, {1, 1, CVMX_ERROR_GROUP_LMC, 5, 0, \"LMCX_INT(0)[DED_ERR]\"}, {0}}, NULL /*cvmx_error_childbit*/ }, {0}}}, {1, 52 /* lmc0 */, (struct cvmx_error_muxchild[]){ {CVMX_ADD_IO_SEG(0x000107000000E200ull) /* CVMX_CIU_CIB_LMCX_RAWX(0,0) */, CVMX_ADD_IO_SEG(0x000107000000E300ull) /* CVMX_CIU_CIB_LMCX_ENX(0,0) */, (struct cvmx_error_regbit[]){ {1, 1, CVMX_ERROR_GROUP_LMC, 1, 0, \"CIU_CIB_LMCX_RAWX(0,0)[INT_SEC_ERRX]\"}, {1, 1, CVMX_ERROR_GROUP_LMC, 5, 0, \"CIU_CIB_LMCX_RAWX(0,0)[INT_DED_ERRX]\"}, {1, 1, CVMX_ERROR_GROUP_LMC, 0, 0, \"CIU_CIB_LMCX_RAWX(0,0)[INT_NXM_WR_ERR]\"}, {0}}, NULL /*cvmx_error_childbit*/ }, {0}}}, 注册 struct octeon_hw_status_data { u64 reg; u32 bit; u8 reg_is_hwint:1; }; arch/mips/cavium-octeon/octeon-error-tree.c //这个文件负责解析错误, 表示层 arch_initcall(octeon_error_tree_init) static int __init octeon_error_tree_init(void) struct cvmx_error_tree *tree = octeon_error_trees //根据芯片id, 找到cn70xx的tree octeon_error_tree_current = tree->tree //注册notifier_call, 但什么时候调用呢?见下一条 octeon_error_tree_notifier.notifier_call = octeon_error_tree_hw_status //这个函数会遍历那个error tree的字节点, 并打印描述 octeon_hw_status_notifier_register(&octeon_error_tree_notifier) raw_notifier_chain_register(&octeon_hw_status_notifiers, nb) octeon_error_tree_enable(CVMX_ERROR_GROUP_INTERNAL, -1) //?? octeon_error_tree_enable(CVMX_ERROR_GROUP_LMC, -1) octeon_error_tree_enable(CVMX_ERROR_GROUP_L2C, -1) //是不是还要加上CVMX_ERROR_GROUP_LMC,CVMX_ERROR_GROUP_L2C ? 上面注册的notifier_call什么时候调用? octeon-hw-status.c //这个文件提供一个硬件状态寄存器的中断处理和发生中断时的回调机制的抽象层 //RGMII SGMII SPI和octeon-error-tree.c都会调用这个函数 sr[idx].bit = bit->bit; sr[idx].ack_w1c = bit->w1c; sr[idx].has_child = 0; octeon_hw_status_add_source(sr) root->hwint == chain->reg//遍历root, 寻找和入参一样的chain->reg root->irq = irq_create_mapping(NULL, root->hwint) rv = request_threaded_irq(root->irq, NULL,octeon_hw_status_irq, IRQF_ONESHOT,\"octeon-hw-status\", root) //这个应该就是中断处理函数了 octeon_hw_status_irq() visit_leaves(root, false, irq_cb, &d); irq_cb() raw_notifier_call_chain(&octeon_hw_status_notifiers, OCTEON_HW_STATUS_SOURCE_ASSERTED, &ohsd) 猜想, 调用int octeon_hw_status_disable(u64 reg, u64 bit_mask)关闭中断位就可以了. octeon_error_tree_enable()函数 while(对每个项和子项) irq_reg = octeon_error_tree_map_irq_reg(base->reg); sr[0].reg = irq_reg bit; sr[0].reg_is_hwint = 1; sr[0].has_child = 1; octeon_error_tree_add(sr, 1, ARRAY_SIZE(sr) - 1, child, group, unit); octeon_hw_status_add_source() root->irq = irq_create_mapping(NULL, root->hwint); //注册中断, octeon_hw_status_irq()是在任务上下文调用的, rv = request_threaded_irq(root->irq, NULL,octeon_hw_status_irq, IRQF_ONESHOT,\"octeon-hw-status\", root); visit_leaves(root, false, irq_cb, &d); raw_notifier_call_chain(&octeon_hw_status_notifiers,OCTEON_HW_STATUS_SOURCE_ASSERTED, &ohsd); //这就是前面注册的 octeon_hw_status_enable() //递归调字节点的 注:request_threaded_irq()第二个入参是中断handler, 在中断上下文调用. 为NULL的话使用下面默认的irq_default_primary_handler /* * Default primary interrupt handler for threaded interrupts. Is * assigned as primary handler when request_threaded_irq is called * with handler == NULL. Useful for oneshot interrupts. */ static irqreturn_t irq_default_primary_handler(int irq, void *dev_id) { return IRQ_WAKE_THREAD; } ddr ecc中断寄存器 # devmem 0x00011800880001E8 64 0x0000000000000000 # devmem 0x00011800880001F0 64 0x0000000000000000 # devmem 0x00011800880001D8 64 0x000000000AFFB398 # devmem 0x107000000E000 64 0x0000000000000000 # devmem 0x107000000E100 64 0x00000000007FFFFF CIU_CIB_LMC(0)_RAW(0) # devmem 0x107000000E200 64 0x0000000000000008 CIU_CIB_LMC(0)_EN(0) # devmem 0x107000000E300 64 0x0000000000000023 # devmem 0x107000000E200 64 0x0000000000000008 # devmem 0x107000000E300 64 0xffff [ 1026.792931] ERROR: CIB bit 3@800107000000e200 IRQ unhandled, disabling # cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 8: 111448 111454 111460 111423 Core timer 25: 146 0 0 0 CIB ahci 26: 0 0 0 0 CIB xhci-hcd:usb1 27: 0 0 0 0 CIB xhci-hcd:usb3 34: 696 0 0 0 CIU serial 45: 8 0 0 0 CIU i2c-octeon 57: 0 0 0 0 CIU octeon-hw-status 59: 0 0 0 0 CIU i2c-octeon 60: 0 0 0 0 CIU octeon-hw-status 73: 221 0 0 0 CIU-W octeon_wdt 74: 0 221 0 0 CIU-W octeon_wdt 75: 0 0 221 0 CIU-W octeon_wdt 76: 0 0 0 221 CIU-W octeon_wdt 89: 0 0 0 0 CIU cib 90: 0 0 0 0 CIU cib 91: 0 0 0 0 CIU octeon-hw-status 92: 0 0 0 0 CIU octeon-hw-status 93: 0 0 0 0 CIU octeon-hw-status 94: 0 0 0 0 CIU octeon-hw-status 95: 0 0 0 0 CIU octeon-hw-status 96: 0 0 0 0 CIU octeon-hw-status 97: 0 0 0 0 CIU cib 98: 0 0 0 0 CIU octeon-hw-status 99: 0 0 0 0 CIU octeon-hw-status 100: 13 0 0 0 CIU octeon_mmc 105: 2087 2308 739 1575 CIU-M SMP-IPI 113: 0 0 0 0 CIU MSI[0:63] 114: 0 0 0 0 CIU MSI[64:127] 115: 0 0 0 0 CIU MSI[127:191] 116: 0 0 0 0 CIU MSI[192:255] 121: 1108312 0 0 0 CIU oct_ilm 122: 1 0 0 0 CIU cib 127: 0 0 0 0 CIU cib 144: 146 0 0 0 CIU cib 145: 0 0 0 0 CIU cib ERR: 0 static irqreturn_t octeon_irq_cib_handler(int my_irq, void *data) # [ 366.867126] INFO: rcu_sched self-detected stall on CPU { 0} (t=6000 jiffies g=18446744073709551430 c=18446744073709551429 q=15) [5/3476] [ 366.937276] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 3.10.20-rt14-Cavium-Octeon #6 [ 366.956650] Stack : ffffffff80980000 ffffffff812a0000 0000000000000057 ffffffff80980000 000000000000000f ffffffff81290000 ffffffff80980000 0000000000000001 000000000000000f ffffffff81290000 ffffffff80980000 0000000000000001 0000000000000000 ffffffff8016e9e8 0000000000000000 0000000000000000 ffffffff812a0000 ffffffff81290000 ffffffff80862c80 ffffffff809403f7 ffffffff81289d00 ffffffff80940900 0000000000000000 0000000000000000 8000000002270c58 ffffffff80940000 ffffffff80940000 ffffffff806dfbc8 ffffffff8090afa8 ffffffff8090aea0 ffffffff8093a4e8 ffffffff801e3ec8 ffffffff809404f0 ffffffff80862c80 0000000000000000 0000000000000000 0000000000000000 ffffffff80149390 0000000000000000 0000000000000000 ... [ 367.642775] Call Trace: [ 367.656940] [] show_stack+0xc0/0xe0 [ 367.673711] [] rcu_check_callbacks+0x3a8/0x838 [ 367.691434] [] update_process_times+0x54/0x88 [ 367.709070] [] tick_sched_timer+0x70/0x180 [ 367.726447] [] __run_hrtimer+0xa0/0x228 [ 367.743561] [] hrtimer_interrupt+0x2c8/0x3c0 [ 367.761110] [] c0_compare_interrupt+0x68/0x98 [ 367.778749] [] handle_irq_event_percpu+0x80/0x2c0 [ 367.796732] [] handle_percpu_irq+0x98/0xc8 [ 367.814106] [] generic_handle_irq+0x44/0x58 [ 367.831570] [] do_IRQ+0x24/0x30 [ 367.847990] [] plat_irq_dispatch+0xa0/0xc0 [ 367.865364] [] ret_from_irq+0x0/0x4 [ 367.882130] [] notifier_call_chain+0x8c/0xc0 [ 367.899678] [] __atomic_notifier_call_chain+0x1c/0x28 [ 367.918008] [] notify_die+0x34/0x40 [ 367.934774] [] do_ri+0x60/0x280 [ 367.951192] [] ret_from_exception+0x0/0xc [ 367.968479] [] do_ade+0x40/0x798 [ 367.984983] [] ret_from_exception+0x0/0xc [ 368.002269] [] handle_irq_event_percpu+0x70/0x2c0 [ 368.020252] [] handle_percpu_irq+0x98/0xc8 [ 368.037626] [] octeon_irq_cib_handler+0xf0/0x1c8 [ 368.055522] [] handle_irq_event_percpu+0x80/0x2c0 [ 368.073505] [] handle_irq_event+0x60/0x90 [ 368.090791] [] handle_level_irq+0xd0/0x150 [ 368.108165] [] generic_handle_irq+0x44/0x58 很多打印 [ 368.125626] [] do_IRQ+0x24/0x30 [ 368.142044] [] plat_irq_dispatch+0x6c/0xc0 [ 368.159417] [] ret_from_irq+0x0/0x4 [ 368.176181] [] __r4k_wait+0x20/0x40 [ 368.192950] [] cpu_startup_entry+0xe4/0x2b0 [ 368.210415] [] start_kernel+0x4b0/0x4d0 [ 368.227527] "},"notes/CPU_MIPS_octeon中断.html":{"url":"notes/CPU_MIPS_octeon中断.html","title":"octeon 中断","keywords":"","body":" MIPS中断 中断ebase寄存器 octeon CIU kernel代码 generic_handle_irq_desc(irq, desc) 中断初始化 irq号 中断注册方法 从hw irq号得到 从platform_device得到 从of_node得到 中断触发方式的判断 中断处理过程 哪里调用了handle_int()? --直接写进中断向量表0号位置 gpio中断 问题现象 参考代码 MIPS中断 中断ebase寄存器 octeon CIU 中央中断控制器下图中, A是SUM0, B是SUM1;SUM0共有9个地址, 每个core2个. 4(core)*2+1(pcie RC); 也有9个相应的en地址SUM1只有1个地址, 所有core公用. 但有9个对应的en地址, 目的是每个core都能单独使能中断.还有个SUM4(0..3), 每个core一个, 对应IP4. kernel代码 octeon-irq.c static __read_mostly u8 octeon_irq_ciu_to_irq[8][64]; kernel/irq.c CIU: 8个mips中断octeon_irq_init_core(); octeon_irq_ip2 = octeon_irq_ip2_ciu octeon_irq_ip3 = octeon_irq_ip3_ciu; octeon_irq_ip4 = octeon_irq_ip4_ciu ciu_domain = irq_domain_add_tree(ciu_node, &octeon_irq_domain_ciu_ops, dd); irq_set_default_host(ciu_domain); GPIO:gpiod->base_hwirq = base_hwirq irq_domain_add_linear(gpio_node, 16, &octeon_irq_domain_gpio_ops, gpiod) CIB:cib_domain = irq_domain_add_linear(of_node, host_data->max_bits,&octeon_irq_domain_cib_ops,host_data); r = request_irq(parent_irq, octeon_irq_cib_handler, 0,\"cib\", cib_domain); request_threaded_irq(irq, handler, NULL, flags, name, dev) cib中断上下文的处理函数 octeon_irq_cib_handler() 找出中断bit //应答相应位 cvmx_write_csr(host_data->raw_reg, 1ull generic_handle_irq_desc(irq, desc) 基本上, 每个中断线都有一个handle, 在初始化时确定. 一般handle有以下几种: handle_level_irq(unsigned int irq, struct irq_desc *desc) handle_edge_irq(unsigned int irq, struct irq_desc *desc) handle_nested_irq(unsigned int irq, struct irq_desc *desc) handle_simple_irq(unsigned int irq, struct irq_desc *desc) 比如 handle_simple_irq(unsigned int irq, struct irq_desc *desc) handle_irq_event(desc); handle_irq_event_percpu(desc, action) //先调用request_irq()注册的handler, 注意action是个链表 //对每个链表action res = action->handler(irq, action->dev_id); 根据返回值, IRQ_WAKE_THREAD:则调用action->thread_fn() irq_wake_thread(desc, action); IRQ_HANDLED:不调用thread_fn() 中断初始化 init_IRQ() for (i = 0; i data ret = irq_init_cb(desc->dev, desc->interrupt_parent) octeon_irq_init_ciu(struct device_node *ciu_node, struct device_node *parent) irq号 有硬件irq号和linux irq号之分: 硬件irq号 hw: irq_hw_number_t hw 根据硬件irq号算出line和bitunsigned int line = hw >> 6; unsigned int bit = hw & 63; line和bit用于算出linux irq号linux_irq = octeon_irq_ciu_to_irq[line][bit] 例如对lmc中断来说, line是1, bit是52, 就能查表得出linux irq号 每个domain都有一个map函数, 用于hw irq和虚拟irq之间转换 中断注册方法 知道linux irq号, 用request_irq()或request_threaded_irq()就能注册中断了.参考octeon_setup_debug_uart() 但linux irq号怎么得到? 从hw irq号得到 用下面这个函数来map一个hw irq, 返回一个linux irq. 必须先有一个虚拟irq才能开始用中断 unsigned int irq_create_mapping(struct irq_domain *domain, irq_hw_number_t hwirq) 这里面的关键是理解到底那个hwirq是你要注册的中断, 这要看这个domain的map函数 得到 从platform_device得到 irq = platform_get_irq(pdev, 0); result = devm_request_irq(&pdev->dev, i2c->irq, octeon_i2c_isr, 0, DRV_NAME, i2c); 从of_node得到 parent_irq = irq_of_parse_and_map(ciu_node, 0); of_irq_map_one(dev, index, &oirq) irq_create_of_mapping(oirq.controller, oirq.specifier,oirq.size); r = request_irq(parent_irq, octeon_irq_cib_handler,IRQF_NO_THREAD, \"cib\", cib_domain); 中断触发方式的判断 if (irqd_get_trigger_type(irq_data) & IRQ_TYPE_EDGE_BOTH) //设置中断类型 irq_set_irq_type(virq, type); 中断处理过程 asmlinkage void plat_irq_dispatch(void) //重点是这个循环 while (1): 读出cop0_cause和cop0_status //处理ip2 octeon_irq_ip2 //处理ip3 octeon_irq_ip3 读SUM1寄存器 //找到最后一位1 int bit = fls64(ciu_sum) - 1; //转换为linux irq int irq = octeon_irq_ciu_to_irq[1][bit] do_IRQ(irq) generic_handle_irq(irq) generic_handle_irq_desc(irq, desc) desc->handle_irq(irq, desc) //处理ip4 octeon_irq_ip4 do_IRQ(fls(cop0_cause) - 9 + MIPS_CPU_IRQ_BASE) 上面这个函数会被锁到cache里, 在cavium-octeon/setup.c里面 cvmx_l2c_lock_mem_region(__pa_symbol(handle_int), len); cvmx_l2c_lock_mem_region(__pa_symbol(plat_irq_dispatch), len2); 在arch/mips/kernel/genex.S中会调用这个函数更详细代码见arch/mips/kernel/entry.S和arch/mips/kernel/process.c汇编里面的handle_int()函数: //这步很牛, 直接锁定返回地址是ret_from_irq PTR_LA ra, ret_from_irq //这是个c函数, 所以最后返回指令肯定是 jr ra. 所以就跳到ret_from_irq()里面去了 PTR_LA v0, plat_irq_dispatch //直接跳过去. 这是个任意地址跳转指令 jr v0 哪里调用了handle_int()? --直接写进中断向量表0号位置 extern asmlinkage void handle_int(void) @init/main.c asmlinkage void __init start_kernel(void) @arch/mips/kernel/traps.c trap_init() ebase=0xffffffff80000000 if (cpu_has_mips_r2) ebase += (read_c0_ebase() & 0x3ffff000) set_handler(0x180, &except_vec3_generic, 0x80) for (i = 0; i 以上这个表和mips异常表能对起来上述的handle_int, handle_sys, handle_adel, handle_ades, handle_ibe, handle_dbe ... 由下面的宏生成： [arch/mips/kernel/genex.S] BUILD_HANDLER adel ade ade silent /* #4 */ BUILD_HANDLER ades ade ade silent /* #5 */ BUILD_HANDLER ibe be cli silent /* #6 */ BUILD_HANDLER dbe be cli silent /* #7 */ BUILD_HANDLER bp bp sti silent /* #9 */ BUILD_HANDLER ri ri sti silent /* #10 */ BUILD_HANDLER cpu cpu sti silent /* #11 */ BUILD_HANDLER ov ov sti silent /* #12 */ BUILD_HANDLER tr tr sti silent /* #13 */ BUILD_HANDLER fpe fpe fpe silent /* #15 */ BUILD_HANDLER mdmx mdmx sti silent /* #22 */ #ifdef CONFIG_HARDWARE_WATCHPOINTS /* * For watch, interrupts will be enabled after the watch * registers are read. */ BUILD_HANDLER watch watch cli silent /* #23 */ #else BUILD_HANDLER watch watch sti verbose /* #23 */ #endif BUILD_HANDLER mcheck mcheck cli verbose /* #24 */ BUILD_HANDLER mt mt sti silent /* #25 */ BUILD_HANDLER dsp dsp sti silent /* #26 */ BUILD_HANDLER reserved reserved sti verbose /* others */ BUILD_HANDLER 定义为： .macro BUILD_HANDLER exception handler clear verbose __BUILD_HANDLER \\exception \\handler \\clear \\verbose _int .endm .macro __BUILD_HANDLER exception handler clear verbose ext .align 5 NESTED(handle_\\exception, PT_SIZE, sp) .set noat SAVE_ALL FEXPORT(handle_\\exception\\ext) __BUILD_clear_\\clear .set at __BUILD_\\verbose \\exception move a0, sp PTR_LA ra, ret_from_exception j do_\\handler //这里就是调用traps.c里面的do_*函数, 如do_bp, do_tr, do_ri END(handle_\\exception) .endm gpio中断 问题现象 两块板子 发包到cpu 半小时内 GPIO2 不停中断(低电平)CN71xx core0SE-S from ubootPHY(拔出,插入) --(直连, 外上拉电阻)--> GPIO2中断(低电平触发) --> WQE 参考代码 GPIO中断配置~/repo/hg/OCTEON-SDK-3.1.0/linux/kernel/linux/arch/mips/cavium-octeon/octeon-irq.c static void octeon_irq_gpio_setup(struct irq_data *data) { union cvmx_gpio_bit_cfgx cfg; struct octeon_ciu_chip_data *cd; u32 t = irqd_get_trigger_type(data); cd = irq_data_get_irq_chip_data(data); cfg.u64 = 0; cfg.s.int_en = 1; cfg.s.int_type = (t & IRQ_TYPE_EDGE_BOTH) != 0; cfg.s.rx_xor = (t & (IRQ_TYPE_LEVEL_LOW | IRQ_TYPE_EDGE_FALLING)) != 0; /* 140 nS glitch filter*/ cfg.s.fil_cnt = 7; cfg.s.fil_sel = 3; cvmx_write_csr(CVMX_GPIO_BIT_CFGX(cd->gpio_line), cfg.u64); } linux/kernel/linux/Documentation/devicetree/bindings/gpio/cavium-octeon-gpio.txt Example: gpio-controller@1070000000800 { #gpio-cells = ; compatible = \"cavium,octeon-3860-gpio\"; reg = ; gpio-controller; /* Interrupts are specified by two parts: * 1) GPIO pin number (0..15) * 2) Triggering (1 - edge rising * 2 - edge falling * 4 - level active high * 8 - level active low) */ interrupt-controller; #interrupt-cells = ; /* The GPIO pin connect to 16 consecutive CUI bits */ interrupts = , , , , , , , , , , , , , , , ; }; GPIO中断应答 static void octeon_irq_ciu_gpio_ack(struct irq_data *data) { struct octeon_ciu_chip_data *cd; u64 mask; cd = irq_data_get_irq_chip_data(data); mask = 1ull gpio_line); cvmx_write_csr(CVMX_GPIO_INT_CLR, mask); } "},"notes/CPU_MIPS_octeon原子操作.html":{"url":"notes/CPU_MIPS_octeon原子操作.html","title":"octeon 原子操作","keywords":"","body":" gcc __sync_fetch_and_add反汇编 gcc在命令行include一个头文件 基础类型sizeof gcc __builtin高级用法 gcc __sync_fetch_and_add反汇编 int global_int = 0; int main() { __sync_fetch_and_add( &global_int, 1 ); } gcc -O2 -march=mips64 -mabi=64 atomic.c && objdump -d a.out 00000001200008b0 : 1200008b0: 3c030002 lui v1,0x2 1200008b4: 0079182d daddu v1,v1,t9 1200008b8: 64638760 daddiu v1,v1,-30880 1200008bc: dc628050 ld v0,-32688(v1) 1200008c0: 0000000f sync 1200008c4: c0410000 ll at,0(v0) 1200008c8: 24210001 addiu at,at,1 1200008cc: e0410000 sc at,0(v0) 1200008d0: 1020fffc beqz at,1200008c4 1200008d4: 00000000 nop 1200008d8: 0000000f sync 1200008dc: 03e00008 jr ra 1200008e0: 00000000 nop gcc -O2 -march=octeon2 -mabi=64 atomic.c && objdump -d a.out 00000001200008b0 : 1200008b0: 3c030002 lui v1,0x2 1200008b4: 0079182d daddu v1,v1,t9 1200008b8: 64638760 daddiu v1,v1,-30880 1200008bc: dc628050 ld v0,-32688(v1) 1200008c0: 0000010f syncw 1200008c4: 0000010f syncw 1200008c8: c0410000 ll at,0(v0) 1200008cc: 24210001 addiu at,at,1 1200008d0: e0410000 sc at,0(v0) 1200008d4: 1020fffc beqz at,1200008c8 1200008d8: 00000000 nop 1200008dc: 03e00008 jr ra 1200008e0: 00000000 nop gcc在命令行include一个头文件 gcc -include octeon-atomic.h simple-test.c 基础类型sizeof sizeof char is 1 sizeof short is 2 sizeof int is 4 sizeof long is 8 sizeof long long is 8 sizeof void is 1 sizeof void * is 8 sizeof short int is 2 sizeof long int is 8 sizeof int long is 8 gcc __builtin高级用法 https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html#Other-Builtins #define foo(x) \\ ({ \\ typeof (x) tmp = (x); \\ if (__builtin_types_compatible_p (typeof (x), long double)) \\ tmp = foo_long_double (tmp); \\ else if (__builtin_types_compatible_p (typeof (x), double)) \\ tmp = foo_double (tmp); \\ else if (__builtin_types_compatible_p (typeof (x), float)) \\ tmp = foo_float (tmp); \\ else \\ abort (); \\ tmp; \\ }) 有了__builtin_types_compatible_p, 就可以根据数据宽度不同来做不同的动作, 比如 #define __sync_fetch_and_add(ptr, value) \\ ({ \\ typeof(*(ptr)) octeon_ret; \\ if (__builtin_types_compatible_p(typeof(*(ptr)), long)) \\ octeon_ret = cvmx_atomic_fetch_and_add64((long *)(ptr), (value)); \\ else if (__builtin_types_compatible_p(typeof(*(ptr)), int)) \\ octeon_ret = cvmx_atomic_fetch_and_add32((int *)(ptr), (value)); \\ else \\ assert(sizeof(*(ptr)) == 8 || sizeof(*(ptr)) == 4); \\ octeon_ret; \\ }) 注意typeof的用法, typeof(*ptr)可以得到ptr指向数据的数据类型. "},"notes/CPU_MIPS_octeon网口代码分析.html":{"url":"notes/CPU_MIPS_octeon网口代码分析.html","title":"octeon 网口代码分析","keywords":"","body":" link状态分两端 关于phy 代码中检测link状态 相关命令 设备注册及初始化 周期worker sgmii初始化 当执行ifconfig eth0 up时 在加载模块时, 会进行硬件初始化 link状态分两端 +-------------+ | +--------+ | CPU | link1 +-------+ link2 | other | | SGMII|| PHY || system | | (MAC)| +-------+ | +--------+ +-------------+ | 当配置为SGMII模式时，它就是一个MAC；对应的，配置为1000base-x时，它是PHY；对外面连的真正的PHY来说，有两个link状态： link1是指和MAC（PCS）层的link； link2是指和外部系统的link；好像这两个link都可以自协商 关于phy 在open的时候connect phy， 在stop的时候disconnect phy 代码中检测link状态 cvmx-helper.c： cvmx_helper_link_info_t cvmx_helper_link_get(int ipd_port) 这个函数调用具体interface的link函数.【注】这个函数用于没有phy的情况下 在ethernet-sgmii.c里面 cvm_oct_sgmii_open时 if (priv->phydev) 有phy时 int r = phy_read_status(priv->phydev); cvm_oct_adjust_link(dev); 根据priv->phydev->link来改变link状态 else 没有phy时 link_info = cvmx_helper_link_get(priv->ipd_port); priv->poll = cvm_oct_sgmii_poll; 起个pull来轮询 相关命令 modprobe octeon-ethernet.ko ifconfig eth0 192.168.0.8 ifconfig eth1 192.168.1.9 ifconfig eth0 up ifconfig eth1 up 设备注册及初始化 modprobe octeon-ethernet.ko module_platform_driver(cvm_oct_driver); //相当于传统的module_init()和 module_exit(hello_exit) module_driver(cvm_oct_driver, platform_driver_register, platform_driver_unregister) platform_driver_register(struct platform_driver *drv=&cvm_oct_driver) drv->driver.bus = &platform_bus_type; //匹配设备时调用 drv->driver.probe = platform_drv_probe; //设备移除时调用 drv->driver.remove = platform_drv_remove; //关机时调用 drv->driver.shutdown = platform_drv_shutdown; driver_register(&drv->driver); //在fdt中找到匹配的设备后, 调用驱动提供的probe函数 cvm_oct_probe(struct platform_device *pdev) //强制MDIO依赖 octeon_mdiobus_force_mod_depencency(); //fdt中的pip节 pip = pdev->dev.of_node; cvm_oct_get_port_status(pip); //70xx有5个interface, 每个interface又有n个port //全局变量iface_ops iface_ops[0] = &iface_ops_sgmii; iface_ops[1] = &iface_ops_sgmii; iface_ops[2] = &iface_ops_npi; iface_ops[3] = &iface_ops_loop; iface_ops[4] = &iface_ops_agl; //对每个interface和port遍历 case CVMX_HELPER_INTERFACE_MODE_SGMII: case CVMX_HELPER_INTERFACE_MODE_QSGMII: //根据dts里面的关键字设置mac_phy_mode和1000x_mode //只有SGMII和QSGMII有这两个选项 \"cavium,sgmii-mac-phy-mode\" \"cavium,sgmii-mac-1000x-mode\" //内核工作队列, 用于POLL cvm_oct_poll_queue = create_singlethread_workqueue(\"octeon-ethernet\"); cvm_oct_configure_common_hw(); //使能FPA cvmx_fpa_enable(); //用FPA分配packet pool, 每个元素16个cache line; 一共8个pool packet_pool = cvm_oct_alloc_fpa_pool(packet_pool, 16个cache line); //1024个packet buffer, 默认值, 加载模块时可改num_packet_buffers cvm_oct_mem_fill_fpa(packet_pool, num_packet_buffers); //用FPA分配wqe pool wqe_pool = cvm_oct_alloc_fpa_pool(-1, 一个cache line); //用FPA分配output buffers pool output_pool = cvm_oct_alloc_fpa_pool(-1, 8个cache line); //初始化PIP, IPD, PKO. 硬件层初始化 cvmx_helper_initialize_packet_io_global(); cvmx_helper_interface_probe(int interface) __cvmx_helper_init_interface(...) cvmx_helper_ipd_and_packet_input_enable(); //对每个端口 __cvmx_helper_packet_hardware_enable(interface); //详见下面第6条 iface_ops[interface]->enable(interface) //对每个interface和每个port //转换为IPD port, 将所有口加入到pow_receive_group, 默认15. 可选0-15 port = cvmx_helper_get_ipd_port(interface, index); //使能IPD/PIP cvmx_helper_ipd_and_packet_input_enable(); //对每个可用的口, 填写dev的各种信息, 注册dev, sgmii注册的是\"eth0 eth1\" struct octeon_ethernet *priv; struct net_device *dev; dev = alloc_etherdev(sizeof(struct octeon_ethernet)); priv = netdev_priv(dev); //填充priv结构 AGL口也使用SGMII的op register_netdev(dev) //将priv加入到全局链表cvm_oct_list list_add_tail(&priv->list, &cvm_oct_list); cvm_oct_add_ipd_port(priv); queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, 5*HZ); cvm_oct_rx_initialize() queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ); //也是个周期work, 1秒一次 void cvm_oct_rx_refill_worker(struct work_struct *work) cvm_oct_rx_refill_pool(num_packet_buffers / 2); queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ); ... ... ... ... 周期worker 在probe函数里, 挂了一个周期worker, INIT_DELAYED_WORK(&priv->port_periodic_work, cvm_oct_periodic_worker);, 5秒一次 void cvm_oct_periodic_worker(struct work_struct *work) priv = container_of(); //这里的 priv->poll就是在接口open时注册的poll函数, 对SGMII来说, 就是cvm_oct_sgmii_poll poll_fn = priv->poll; poll_fn(priv->netdev); //这里调用的是 struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev) priv->netdev->netdev_ops->ndo_get_stats(priv->netdev); queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, 5*HZ); sgmii初始化 cvm_oct_sgmii_init(struct net_device *dev) struct octeon_ethernet *priv = netdev_priv(dev); cvm_oct_common_init(dev); 从dts获取mac地址 如果没有mac地址, 则用random地址 SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops); dev->netdev_ops->ndo_stop(dev); //phy层检测到link change, 就调用这个函数 priv->link_change = cvm_oct_sgmii_link_change; 当执行ifconfig eth0 up时 int cvm_oct_sgmii_open(struct net_device *dev) cvm_oct_phy_setup_device(dev); //从dts里面找phy-handle phy_node = of_parse_phandle(priv->of_node, \"phy-handle\", 0) //SGMII不走下面, rgmii走下面, cvm_oct_adjust_link是个回调函数 priv->phydev = of_phy_connect(dev, phy_node, cvm_oct_adjust_link, 0, iface) phy_start_aneg(priv->phydev); //有phy走if分支 if (priv->phydev) r = phy_read_status(priv->phydev) cvm_oct_adjust_link(dev); //没有phy走else分支 else link_info = cvmx_helper_link_get(priv->ipd_port) //这个函数获取速率时有点问题, 需要定位 __cvmx_helper_sgmii_link_get() 根据板类型直接返回up(FPXTB) priv->poll = cvm_oct_sgmii_poll; //其实这个函数是周期性调的. cvm_oct_sgmii_poll(dev); link_info = cvmx_helper_link_get(priv->ipd_port) //和以前比, 一样就直接返回了. 第一次不一样, 往下走 cvmx_helper_link_autoconf(priv->ipd_port) link_info = cvmx_helper_link_get(ipd_port); //更内部的link状态记录在cvmx_interfaces[interface].cvif_ipd_port_link_info //如果上面获取的link_info和内部的cvif_ipd_port_link_info一样, 就直接返回. //第一次肯定不一样, 往下走 cvmx_helper_link_set(ipd_port, link_info); __cvmx_helper_sgmii_link_set() //要禁止自协商, 在这里改. PCS层reset+配置 __cvmx_helper_sgmii_hardware_init_link(interface, index); //主要是根据link_info配置GMX层寄存器, 速度, 全双工. __cvmx_helper_sgmii_hardware_init_link_speed(interface, index, link_info) 在加载模块时, 会进行硬件初始化 __cvmx_helper_sgmii_enable(int interface) __cvmx_helper_sgmii_hardware_init(interface, num_ports); __cvmx_helper_setup_gmx(interface, num_ports); __cvmx_helper_sgmii_hardware_init_one_time(interface, index); //初始化link状态 __cvmx_helper_sgmii_link_set(ipd_port, __cvmx_helper_sgmii_link_get(ipd_port)) "}}